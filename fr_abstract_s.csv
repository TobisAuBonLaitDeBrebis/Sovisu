fr_abstract_s
"Le chapitre s’interroge sur la sous-estimation délétère du versant purement managérial de la logistique, dont les processus décisionnels n’échappent pas, dans les faits, à la problématique de la rationalité limitée de Simon (1997), mais aussi aux logiques béhavioristes que l’on retrouve dans la théorie des canaux de distribution, ou encore aux modèles de prédation et de régimes de pouvoir développés notamment par Cox et al. (2001). En bref, comprendre la dynamique des chaînes logistiques, c’est mobiliser un appareillage conceptuel qui n’ignore pas les jeux d’acteurs tels qu’ils structurent, déstructurent et restructurent les organisations et les relations inter-organisationnelles. De ce point de vue, une vision décloisonnée qui associe la logistique à d’autres champs disciplinaires des sciences de gestion et du management apparaît indispensable pour sortir de l’impasse dans laquelle une vision purement technicienne pourrait la conduire, en privilégiant maladroitement l’agent plutôt que l’acteur."
"Cet article vise à apporter des éléments de réponse à la question de recherche suivante : comment se construisent et s'articulent dans le temps les quatre niveaux de compétences (individuel, collectif, organisationnel et environnemental) lors d'un processus d'innovation ? Pour ce faire, notre recherche, de type exploratoire, repose sur une démarche qualitative, centrée sur l'étude de cas longitudinale d'une TPE touristique, l'entreprise H2H (Hotel2Hotel). Notre étude empirique offre une identification des compétences mobilisées lors du processus d'innovation H2H, à partir des quatre niveaux d'analyse que sont l'individuel, le collectif, l'organisationnel et l'environnemental. Elle révèle également l'existence d'un quatrième modèle de management stratégique des compétences : le modèle de "" combinaison temporelle "". Ce modèle serait une quatrième voie de relations (entre stratégie et compétences) aux trois autres modèles déjà existants dans la littérature (déduit, émergent et mixte). Nos résultats permettent ainsi d'aller au-delà de la simple identification des compétences requises pour développer une innovation, en analysant également les différentes stratégies d'émergence, de développement et d'articulation des compétences dans un processus d'innovation."
"Sur la question de l’internationalisation des multinationales des pays émergents, autrement qualifiées de « multinationales émergentes », la recherche en management international a réussi à apporter des réponses et produire de nouvelles approches théoriques. Cette recherche est d’autant plus réussie que les réponses et les approches théoriques proposées offrent des éléments de compréhension et d’explication couvrant les différentes dimensions du modèle d’internationalisation des multinationales émergentes : pourquoi s’internationaliser (motivations) ? Où s’internationaliser (sélection des marchés géographiques) ? Comment s’internationaliser (choix des modes d’entrée) ? Et quand s’internationaliser (temporalité et rythmique des entrées) ? Ce sont les principales interrogations appréhendées dans cet article."
"Cet article propose une analyse des décisions d’arbitrage budgétaire, dans les communes françaises, entre les recettes fiscales et tarifaires, et de leur évolution en période d’austérité. Cette notion d’arbitrage est précisée juridiquement, et analysée sous l’angle de la décision politique et des outils au service du manager public. Une étude qualitative portant sur quatre communes françaises faisant face à la baisse des dotations de l’Etat apporte un éclairage terrain à cette recherche. L’article met en avant les facteurs influençant le processus d’arbitrage : potentiel du levier, pression fiscale ou tarifaire existante, éléments historiques et géographiques, perception des souhaits de la population. Il montre la vision ambiguë qu’ont les managers du levier tarifaire et souligne une hétérogénéité des pratiques et des perceptions. Il montre la forte inertie dont souffre la détermination du niveau des ressources et comment la crise est venue bousculer cette situation et a permis d’affiner les méthodes d’arbitrage budgétaire."
"Notre travail de recherche porte sur la création de valeurs au sein des start-up innovantes. La littérature en Sciences de Gestion propose deux grandes catégories de valeurs : la valeur actionnariale, qui revient aux actionnaires, économique, et la valeur dite partenariale, qui revient aux parties prenantes, intangible donc (Charreaux, 1998). Si la majorité des travaux de recherche sur le sujet concerne depuis des années la valeur actionnariale, nous nous sommes focalisés sur la deuxième catégorie qui constitue pour les parties prenantes en quelque sorte une rémunération d’échange selon Charreaux (1998). Cette thèse est née du constat d’un manque dans la littérature concernant la qualification précise des valeurs créées par l’innovation, avec différents enjeux. L’enjeu sociétal de création de valeurs d’éthique des affaires (Igalens et Joras, 2002) étant de donner un éclairage sur les comportements humains formant un ensemble de sentiments de valeurs afin d’enrichir les principes RSE énoncés en 2000 par l’OCDE. L’enjeu managérial étant la définition d’un nouveau modèle de management par les valeurs visant à accroître la performance globale (Germain et Trébucq, 2004). Enfin, l’enjeu purement lié à la recherche en Sciences de Gestion est de formaliser enfin une grille de système de valeurs propre au management, puisqu’à ce jour aucun système de valeurs n’a encore fait consensus auprès des chercheurs (Bréchon, 2003). Sous un positionnement épistémologique interprétativiste, notre thèse répondra donc à la problématique générale : par quel modèle de management stratégique une Start-Up innovante peut-elle induire une valorisation de ses parties prenantes ? Notre première partie développe une revue de littérature qui se compose de trois chapitres : caractérisation d’une Start-Up, management stratégique et innovation, quelle création de valeurs ? Cette première partie se concluant par une proposition de soixante valeurs vulgarisées et trois questions de recherche : quelles sont les valeurs potentiellement créées au sein des parties prenantes d’une Start-Up ? pour quelle typologie de parties prenantes une Start-Up innovante peut-elle créer des valeurs ? quel modèle de management stratégique peut permettre à une Start-Up innovante de créer des valeurs auprès de ses partenaires ? Notre deuxième partie est une étude de cas terrain qui se confronte à notre revue de littérature en trois chapitres de nouveau : démarche de recherche, étude de cas, et analyse des résultats. Nous répondrons ainsi à nos trois questions de recherche et à notre problématique, et ferons quatre propositions. Nous concluons en effet notre travail en proposant un système universel des valeurs du management, deux nouveaux concepts démontrés (« Ip Financeur » et « Valorizing »), et un modèle de management par les valeurs pour les Start-Up innovantes. Nous proposons enfin nos apports théoriques, managériaux, et méthodologiques tout en exposant conjointement les limites de notre travail."
"Ce travail doctoral s’interroge sur les déterminants des comportements d’épargne salariale. Le chapitre 1 présente un état des lieux de l’épargne salariale. Nous détaillons ensuite les principales règles de fonctionnement de l’épargne salariale. Nous présentons enfin les principaux déterminants des comportements d’épargne salariale identifiés par la littérature. Le chapitre 2 examine les déterminants socio-économiques de l’efficience des portefeuilles d’épargne salariale. Il propose également une analyse des déterminants des erreurs d’investissement des salariés mesurées par un indice. Nous dissocions les investissements faits dans le cadre des augmentations de capital réservées aux salariés et ceux faits au titre des autres plans d’épargne entreprise. Nous nous intéressons d’une part à la décision binaire d’investir ou pas, et à l’efficience du portefeuille d’autre part. Notre échantillon porte sur 30 000 salariés d’un groupe bancaire français et contient des informations sur les caractéristiques individuelles des salariés et le détail des montants investis dans les plans et leurs caractéristiques de rentabilité et de risque. Les caractéristiques des salariés affectent significativement l’efficience des portefeuilles. Nous mettons en évidence une forte concentration en actions de l’entreprise. Nous montrons également que l’investissement des salariés est sous optimal compte tenu de l’écart existant entre le ratio de Sharpe optimal que nous avons calculés et le ratio de Sharpe des salariés. Nous régressons enfin cette différence sur les caractéristiques des salariés. Le chapitre 3 analyse empiriquement les déterminants de l’investissement en actions de l’entreprise. L’actionnariat salarié diminuerait les comportements de retrait des salariés tels que l’absentéisme et la rotation du personnel. Or, la causalité inverse que nous postulerons n’avait jamais été analysée : l’effet de l’absentéisme et de la rotation du personnel sur l’investissement en actions de l’entreprise par ses salariés. Nous montrons que l’absentéisme et la rotation du personnel affectent significativement la participation. Nous validons nos hypothèses de recherche à l’aide de diverses méthodes de régression. Nous analysons un panel d’environ 15 000 salariés de près de 900 filiales d’un groupe français coté appartenant au secteur de la construction, des travaux publics et des concessions sur une période de 5 ans. La participation au plan d’actionnariat salarié dépend du niveau d’absentéisme et de rotation du personnel au sein de l’entreprise. L’absentéisme et la rotation du personnel influencent l’investissement en actions de l’entreprise différemment selon la catégorie socioprofessionnelle du salarié et les motifs d’absences et de sorties Le chapitre 4 administre un questionnaire à des épargnants individuels afin d’évaluer leur connaissance financière. Nous nous sommes inspirés des enquêtes de Lusardi et de la Banque Centrale Européenne afin de réaliser un questionnaire adapté au cas français. Nous constatons l’influence de l’éducation financière sur les décisions d’investir. Le questionnaire a été diffusé auprès des salariés d’un établissement bancaire d’une part et de leurs clients d’autre part. Bien que nos résultats confirment dans l’ensemble la littérature sur la connaissance financière, nous identifions plusieurs nouveaux résultats."
"Cet article s'attache avant tout à bousculer l'approche purement quantitative des politiques de dividende ou de rachat d'actions en présentant une description qualitative de ces politiques. Notre étude terrain auprès de 12 décideurs français apporte certes un éclairage hexagonal mais il permet surtout une meilleure compréhension des éléments explicatifs ou d'influence des politiques de distribution. Notre étude présente la distribution de liquidités aux actionnaires comme le résultat d'un consensus entre plusieurs variables : capacité financière de l'entreprise, influences externes et pression de l'actionnariat. Nos résultats confortent la fonction de signal de la distribution et la politique de lissage des dividendes. De nouveaux éléments venant renforcer la réflexion apparaissent : existence d'une culture de la distribution, influence des caractéristiques des managers et des concurrents. Enfin cet article remet en question le rôle de distribution des rachats d'actions et souligne la forte influence des actionnaires majoritaires."
"Cet article analyse le montant et la forme des politiques de distribution des entreprises familiales cotées. Les évolutions récentes de la notion d'entreprise familiale amènent à s'interroger sur les effets de deux types de conflits d'agence, entre actionnaires et dirigeant (type I), et entre actionnaires majoritaires et minoritaires (type II). Nos résultats montrent que les montants distribués sont liés à ces deux composantes du conflit d'agence, qui est globalement moins fort dans les entreprises familiales. Il apparait ainsi une répartition différente des revenus dans les structures familiales, les distributions aux actionnaires y étant plus faibles. Cet article suggère également que les politiques de distribution sont utilisées pour modifier la répartition des pouvoirs : les entreprises où la famille dirigeante est minoritaire privilégient l'utilisation du rachat, outil de concentration du contrôle."
"Les services d’incendie et de secours français ont été départementalisés depuis la loi du 3 mai 1996. Collectivités territoriales touchées par capillarité par les logiques de résultats instituées par la Loi Organique relative aux Lois de Finances en 2001, ils sont aujourd’hui au coeur de multiples injonctions en termes d’efficience et d’efficacité. Leur fiabilité organisationnelle devient un enjeu central que nous pensons être en interrelation avec les logiques d’utilisation des outils de technologie de l’information, tels que ceux déployés dans le domaine de l’aide à la décision. Nous explorons dans ce travail la fiabilité organisationnelle sous le prisme de la théorie des High Reliability Organizations (Roberts, 1990 ; Weick & Sutcliffe, 2007 ; Vidal, 2011 ; Morel, 2012…) en la rapprochant de la littérature sur les systèmes d’information (Le Moigne & Bartoli (1996), Le Moigne (2006), Laudon & Laudon (2014), etc.). Encapsulée dans un constructivisme téléologique et projectif, cette thèse présente précisément une recherche-intervention visant à déployer de tels outils informationnels au SDIS des Bouches du Rhône dans une optique de fiabilisation de leurs activités. Accompagnée d’une observation participante et de la tenue d’un journal de bord, cette recherche-intervention a été conclue par une série d’entretiens semi-directifs avec les acteurs de l’organisation. Les résultats issus de données primaires et secondaires font l’objet d’interprétations mettant en évidence les impacts des outils informationnels sur la fiabilité du SDIS des Bouches du Rhône. Une discussion a permis de faire émerger des préconisations à destination des praticiens sous la forme de recommandations et de représentations liées à l’implémentation d’un management par les processus. Parmi ces propositions, nous présentons un modèle de maturité de l’organisation à devoir de fiabilité (Vidal, 2011) afin de guider les organisations publiques dans l’évaluation de leurs capacités en termes de fiabilisation de leurs activités."
"Notre recherche étudie les modifications dans la structure de l’entreprise occasionnées par la mise en oeuvre de l’innovation ouverte. Plus précisément, nous nous intéressons aux modifications qui ont lieu dans la structure organisationnelle profonde et formelle de l’entreprise. Nous étudions ces modifications en mobilisant des données primaires (entretiens semi-directifs) et secondaires (rapports annuels et articles de presse) issues de neuf entreprises. Les résultats consistent en une nouvelle définition de l’innovation ouverte entrante, en un modèle contingent de l’innovation ouverte au niveau de l’entreprise et en une compréhension approfondie des modifications observées dans la structure organisationnelle profonde et formelle. La définition de l’innovation ouverte que nous proposons insiste sur trois aspects importants : l’intégration de l’ouverture dans la stratégie de l’entreprise en matière d’innovation, le caractère fréquent des collaborations et le caractère systématique de celles-ci."
"Les théories actuelles sur la consommation insistent sur une rupture de la frontière entre production et consommation que caractérise le phénomène de la prosumption. Cette recherche interroge des pratiques originales et inexplorées de prosumption que nous avons nommées « récup'création ». Le récup'créateur fabrique et produit des objets en transformant des rebuts sans valeur en objets utilitaires et artistiques. En redonnant vie à des déchets, il transforme des riens en biens. Il produit et consomme là où le marché s'est arrêté. L'objectif de cette recherche est de décrire les pratiques de récup'création, de les analyser pour en proposer une catégorisation selon trois critères basés sur des oppositions : logique d'ingénieur vs de bricoleur, démarche artistique vs artisanale et industrialisation en amateur vs en professionnel."
"Lors des prestations de service, les entreprises proposent aux clients des processus au cours desquels les tâches qu'ils doivent réaliser sont plus nombreuses et plus diversifiées. Ce « travail » du client constitue le concept de participation. Dès lors les prestataires s'interrogent sur comment améliorer la participation du client. Pour que le client participe, il faut que deux éléments soient réunis: il doit être motivé à participer et il doit savoir comment participer. Pour ce faire, l'entreprise doit aider le client. A travers l'étude du lien entre ces trois concepts : aide au client, motivation à participer (rationnelle et émotionnelle) et participation, cette recherche répond à la question suivante: comment l'aide au client, grâce à la motivation rationnelle et émotionnelle qu'elle engendre, permet-elle d'améliorer la participation du client ? Pour répondre à cette problématique, deux études qualitatives, l'une étudiant les remarques des clients laissées en sortie de magasin, l'autre portant sur des observations non participantes de parcours clients, sont d'abord réalisées pour stabiliser le modèle avant de procéder à une phase quantitative. Puis, deux études quantitatives sont menées pour mesurer les relations entre aide, motivation et participation. La première étude quantitative, par observations non participantes sur le parcours de 557 clients chez IKEA, montre l'influence positive de la présence d'une aide du prestataire sur l'efficacité de la participation. Cette quasi-expérience propose aussi une nouvelle méthodologie pour mesurer l'efficacité réelle de la participation client en alternant phases déclaratives par questionnaires et phases d'observations. Une deuxième étude quantitative par scénario sur 437 répondants démontre que les émotions constituent une route motivationnelle pour l'efficacité de la participation aussi importante que la motivation rationnelle."
"Instituée depuis les années 2000 par les pouvoirs publics de santé, la politique alimentaire a été définie afin d’optimiser la situation nutritionnelle de la population française. Ce mode d’intervention publique renvoie au schéma de la construction d’une action collective avec un effacement partiel de la figure étatique. Pour comprendre cette évolution, nous mobilisons la logique instrumentale de l’action publique liée à l’alimentation française. Elle révèle une action étatique collaborative qui intervient de concert avec une multitude de parties prenantes. La logique instrumentale, qui combine approches substantive et procédurale, offre de plus, un socle d’actions commun aux acteurs publics et privés de l’industrie alimentaire : cela témoigne à la fois du degré de contrainte et du niveau d’engagement de l’Etat. Elle cristallise enfin la tension permanente entre contrainte et incitation (Simard et al. 2011) qui émane de cette action publique. Dans ce cadre où l’action collective règne, la position de l’Etat n’est pas sans enjeu ni conséquence. Des interrogations liées à l’impact de ce mix instrumental apparaissent (Bergeron, 2010 p. 92). Notre problématique consiste ainsi à préciser dans quelle mesure existe-t-il une cohérence, voire une coordination entre ces nombreux instruments hétérogènes et in fine entre l’ensemble de ces parties prenantes, notamment entre les trois ministères français principalement concernés (ministères de la Santé, de l’Agriculture et de l’Economie) ? Lors de cette étude, nous présentons l’évolution de la politique alimentaire vers l’action publique. Il en résulte l’intrication d’une multitude de parties prenantes qui complexifie la conduite de l’action. Nous dressons la liste des parties prenantes impliquées, leur statut et leurs relations, afin d’appréhender comment les rôles et les responsabilités sont répartis entre elles. Puis nous observons comment leur coopération et leur concurrence sont gérées et avec quels résultats (Simard et al. 2011, Messaoudène, 2013). Pour répondre à ces questionnements, nous avons mené une étude de terrain d’un an (de septembre 2011 à septembre 2012). Durant cette période, nous avons rencontré 26 parties prenantes de cette politique publique (ministères, organisations déconcentrées, entreprises de l’agro-alimentaire, syndicats d’entreprises de l’agro-alimentaire, entreprises de la restauration collective, associations de consommateurs, experts en nutrition). L’ensemble des données collectées a fait l’objet d’une analyse exploratoire et qualitative (codage à l’aide du logiciel NVIVO 8.0). Nos résultats révèlent la complexité de la politique alimentaire française. Celle-ci porte en elle, du fait de la combinaison des instruments de l’action publique qui l’anime, des effets de concurrence. Cela en limite la cohérence et donc potentiellement les impacts sur la prévalence de l’obésité (son objet principal)."
"Notre recherche questionne la diffusion de pratiques innovantes dans un champ de la santé pluri-institutionnalisé et pluraliste. Notre cadre théorique aborde la diffusion de pratiques innovantes entre différents lieux d’activités soutenue par une innovation organisationnelle : le Réseau Créatif de Pratiques (RCP). Le RCP repose sur deux concepts : le « réseau de pratiques » potentiellement favorable à la diffusion de pratiques et les « collectifs créatifs » pour assurer son management face à deux tensions (exploitation versus exploration et généralisation versus contextualisation). Notre démarche méthodologique s’appuie sur deux études de cas longitudinales : une Agence Régionale de Santé (ARS) et un Collectif National des pilotes MAIA (Collectif). La méthodologie qualitative repose sur des entretiens semi-directifs, observations et des données secondaires. Nos deux études de cas montrent comment émerge et s’auto-organise un RCP au regard de deux caractéristiques : sa structuration et son animation. Ces deux cas montrent une situation instable des collectifs créatifs orientés par la politique d’un Haut (ARS) ou de l’expertise opérationnelle d’un Bas (Collectif). Pour autant, ce déséquilibre tend à se résorber à la faveur des efforts récents de mise en visibilité et de justification. Notre modèle d’analyse nous conduit à réaliser une lecture critique des mix organisationnels et managériaux des RCP étudiés et discuter des mécanismes correctifs (un encastrement complémentaire de justification, une structure duale, une double capacité à agir) qui leur sont nécessaires pour parvenir à compenser leur fort enracinement dans l’une des deux autres strates (Haut ou Bas)."
Cette contribution propose d'analyser quatre problèmes fréquemment rencontrés par les praticiens du marketing dans l'utilisation des techniques d'analyse typologique qui peuvent être à la base des pratiques de segmentation. Comment mesurer la « classifiabilité » des données? Quel indice de distance ou de similarité sélectionner? Combien y a-t-il de groupes? Comment comparer les classifications produites par des méthodes différentes et utilisant des distances différentes?
"L’économie belge a progressé à un rythme particulièrement soutenu au 1er trimestre. En dépit d’une incertitude élevée, suscitée notamment par la remontée des prix pétroliers et la crise de la dette souveraine dans la zone euro, les perspectives économiques pour le reste de l’année en cours et pour 2012 demeurent positives. La croissance du volume du PIB sur l’ensemble de l’année 2011 devrait atteindre 2,6 %. En 2012, la croissance du PIB resterait supérieure au rythme potentiel, en affichant 2,4 %."
"La performance s'impose aujourd'hui dans le secteur public. Néanmoins le contenu que recouvre la notion reste sujet à débat. L'objectif de cette recherche est de comprendre comment les responsables publics prennent en charge concrètement le devoir de performance. Il s'agit de décrire à la fois les conceptions qu'ils s'en font et d'identifier les outils de gestion censés y conduire. Pour atteindre cet objectif, la recherche se concentre sur les établissements publics de coopération intercommunale (EPCI) à fiscalité propre institués depuis 1999. Ces organisations ont récupéré les principales compétences traditionnellement dévolues aux communes, dans le but d'en optimiser la gestion et de réaliser des économies d'échelle. Elles constituent de ce fait le lieu privilégié pour l'éwtude du management de la performance dans le secteur public local. Deux études de cas ont été menées, s'appuyant sur 47 entretiens semi-directifs. Elles décrivent de manière transversale les outils de gestion utilisés dans les services opérationnels et fonctionnels. Ce faisant, l'architecture globale du système de management de la performance de l'organisation est reconstituée et permet d'identifier les valeurs centrales. Les résultats révèlent que, contrairement à une idée répandue, les responsables intercommunaux ont de longue date été confrontés à l'exigence de performance. Ils mobilisent cette expérience en développant des stratégies de bricolage et d'appropriation des outils de gestion. Elles visent tant à ancrer les outils dans les pratiques qu'à soumettre au débat la normativité qu'ils véhiculent. En résultent des systèmes de management de la performance fragmentaires, dont la principale mission est d'exercer les responsables à l'art du compromis entre des exigences contradictoires."
"Cette recherche explore l’innovation sociale dans les pôles de compétitivité. Notre analyse propose sept dimensions de l’innovation sociale, mises en évidence dans le fonctionnement de deux pôles de compétitivité de la Région Sud. Pour ce faire, nous mobilisons les outils de gestion inhérents à l’activité des pôles de compétitivité en termes de coordination, de régulation et de management des connaissances. Il en ressort que les pôles de compétitivité constituent des structures d’innovation sociale dont le fonctionnement est propice à la réalisation de l’innovation sociale. De plus, nous mettons en évidence que ce sont les projets portés par les membres des pôles de compétitivité qui répondent aux besoins sociaux territorialisés."
"Certaines smart cities sont aujourd’hui critiquées pour leur vision très techno-centrée qui favorise les géants du numérique face aux citoyens locaux. À Barcelone, une tout autre stratégie a été déployée, faisant basculer la ville dans un modèle de ville expérimentale. Prenant ancrage dans le socle théorique des communs (Ostrom, 1990), la municipalité a en effet mis en place une stratégie digitale qui implique les citoyens pour répondre aux enjeux de souveraineté numérique et faire valoir leurs droits digitaux. Ainsi, cet article scientifique, qui vient en soutien d’une étude de cas du projet DECODE, interroge comment des outils numériques décentralisés, qui reposent sur la technologie blockchain, peuvent favoriser le partage des données tout en respectant la vie privée et les droits des citoyens. Il mobilise la théorie des communs pour comprendre la gestion des communs de la donnée à l’échelle d’une ville"
"En 2004, la France lance une nouvelle politique industrielle pour faire face à une économie de plus en plus concurrentielle. Elle se positionne pour mettre en relation les territoires, les innovations et l’industrie via le rapprochement d’acteurs hétérogènes locaux. Cette politique publique souhaite mobiliser l’innovation via la proximité des acteurs qui facilite la naissance de projets collaboratifs. De plus, la concentration d’acteurs sur un même territoire permet une visibilité à l’échelle internationale, qui peut être source d’attractivité. Enfin, l’ancrage territorial des membres des pôles peut être considéré comme un frein aux délocalisations. À travers cette définition, nous nous apercevons que ces réseaux ont été construits en s’appuyant sur le territoire. En effet, ils représentent un instrument de l’action publique en matière de développement économique local. C’est en prenant en considération le territoire d’un point de vue diachronique et en mobilisant la gouvernance territoriale que les pôles de compétitivité peuvent être considérés comme des systèmes régionaux d’innovation. Grâce à une méthodologie qualitative et des entretiens semi-directifs menés dans deux pôles de la Région Sud, nous proposons de mettre en évidence la prise en compte du territoire et la perspective durable des pôles de compétitivité à travers l’innovation sociale et l’attractivité territoriale durable"
"Cet article introduit la notion de « chaîne de secours » dans l’analyse du désert médical. Il s’inscrit dès lors dans un double mouvement exploratoire. D’un côté, il montre en quoi les acteurs de la « chaîne de secours » se voient contraints d’assumer, à l’échelle territoriale, un rôle tampon lorsque l’accès au soin se fait difficile. De l’autre, il met en évidence les transformations internes que les organismes de secours mettent en œuvre lorsqu’ils sont eux-mêmes confrontés à une carence en matière de professionnels de santé."
"Compte tenu du poids des concours financiers de l’État et de l’effet des transferts de fiscalité sur l’équilibre bud-gétaire de l’État, les finances locales forment une partie importante de l’ensemble interdépendant des finances publiques. Dès lors, la maîtrise des dépenses s’impose comme une nécessaire contribution des collectivités territoriales au redressement des comptes publics et au respect des engagements européens de la France. Dans ce contexte contraint, deux voies principales semblent exister, dans la littérature dédiée, pour maîtriser les dépenses locales. Tout d’abord, il apparaît possible de renforcer le contrôle des dé penses, par exemple en diminuant de manière significative les dépenses ac-tuelles de fonc tion nement, en re por tant ou annulant des investissements, en diminuant les subventions et aides publiques, en augmentant les impôts locaux, en mettant plus de pression sur les fournisseurs, ou en-core en augmentant les tarifs des prestations délivrées. Ensuite, il est aussi possible d’accroître les capacités locales de pilotage, en agissant sé lec ti vement sur l’ac-tion publique et sur les dé penses de façon différenciée en analysant la valeur sociale créée pour l’usager au coût le plus faible, en réalisant des choix assumés, en produisant des arbitrages dans la mise en œuvre des processus budgétaires, ou en restructurant l’offre de service locale en connaissant mieux les satisfactions et besoins des usagers.Les résultats de notre recherche démontrent que les collectivités territoriales françaises, dans leur grande majorité, sont sensibilisées à la recherche de solutions d’économies, depuis une dizaine d’années, en raison de la dégradation de leurs situations financières. Il s’avère également que les outils et dispositifs mo bi­li sés sont majoritairement guidés par une logique de contrôle des ressources. Aussi, les résultats mettent en évidence la faiblesse de la logique de pilotage, ca­rac té ri sée, elle, par des outils et des actions ayant une dimension stratégique et politique. Focalisées sur une logique court­termiste, les solutions actuellement en-gagées, plus orientées contrôle, pourraient ainsi, à terme, impacter négativement l’offre de services lo-caux sans réelle maîtrise préalable."
"Cet article questionne la capacité des musées français à faire preuve de résilience organisationnelle face à la crise sanitaire. La nature complexe et contingente de ces établissements nous a conduits à employer une méthodologie exploratoire holistique à même de répondre à leurs caractéristiques. Nous avons construit, sur la base de données secondaires (rapports officiels, guides et webinaires), une taxonomie pragmatique selon la méthode de Grémy et Le Moan (1977), permettant ainsi de répondre à une confusion généralisée dans les sciences humaines et sociales sur les procédures de classification. Cinq dimensions ressortent dans cette taxonomie, révélant la capacité de résilience muséale."
"Un courant de recherche récent s’intéresse au rôle des Organismes de Gestion de Destination (OGD) dans le succès stratégique des destinations touristiques. Initialement centrés sur la promotion et le marketing, les OGD ont vu leur mission s’enrichir au fil du temps pour devenir de véritables leaders stratégiques. En France, le législateur a voulu que chaque destination en soit dotée avec pour conséquence une forme d’illisibilité. Dans ce contexte, notre recherche s’efforce de répondre à la question suivante : un OGD peut-il constituer un déterminant de succès d’une destination touristique durable, et si oui, selon quels mécanismes ? Après avoir rappelé le cadre réglementaire, l’article mobilise le management stratégique des destinations touristiques. Puis, il expose l’étude qualitative réalisée de façon longitudinale en Dracénie-Provence-Verdon. Cette communauté d’agglomération a réussi à déployer une stratégie efficace centrée sur l’œnotourisme grâce à la création d’un OGD intercommunal original. Les résultats soulignent le rôle d’acteur stratégique de l’OGD et l’intérêt du label « Vignobles & Découvertes » dans la stratégie déployée. Ce faisant cette recherche enrichit la littérature en management stratégique des destinations et sur les OGD. Mots clés : gouvernance, intercommunalité, œnotourisme, OGD."
"Cet article étudie la façon dont la France, modèle de centralisme, a réagi lors de la première phase de la crise de la Covid-19 entre janvier et octobre 2020. Le cadre théorique des relations intergouvernementales est mobilisé pour réfléchir les réussites et les échecs de notre système administrativo-politique face à la pandémie. L’article s’appuie sur une analyse qualitative et quantitative des rapports et discours publiés par les acteurs institutionnels pendant la crise et montre des relations en tension permanente entre une hyper-centralisation et le développement d’une gouvernance horizontale plus souple et réactive."
"L’objectif de cet article est de contraster le message implicite de confiance et de qualité véhiculé par un label avec les pratiques réelles des marchés. La crédibilité d’un label est basée sur le fait que le produit ou service proposé à l’achat est évalué par un tiers de confiance selon un cahier des charges strict et accessible à tous. La contribution de ce travail est à la fois conceptuelle, fondée sur une analyse de la littérature, et exploratoire au moyen d’exemples concrets mettant en lumière les risques encourus par une marque faisant appel à certains labels ou pseudo-labels peu ou moins respectueux des règles éthiques. En utilisant des méthodes d’analyse de contenu lexicale, nous explorons les perceptions de consommateurs et d’experts du marketing, collectés sous forme d’entretiens qualitatifs. Ces analyses montrent que la logique de labellisation est par nature plus exigeante en termes d’éthique que la logique de communication d’une marque. La frontière entre ces deux logiques doit être maintenue dans l’intérêt des consommateurs et de la marque. Finalement, les limites de cette recherche et ses implications managériales sont détaillées."
"Ce travail doctoral est une contribution à la compréhension des effets des labels lorsqu’ils sont associés aux marques. Il s’intéresse plus particulièrement aux effets des labels sur la qualité perçue par le consommateur, les relations à la marque et le consentement à payer. La définition de la qualité et la désignation des instances en charge de sa normalisation sont des problématiques historiques dont on retrouve les traces tant dans les principes du droit romain que chez les théologiens médiévistes et, plus récemment, au cœur de la distinction théorique entre les biens ou les attributs de recherche, d’expérience et de croyance. La thèse aspire à dépasser la dyade marque-acheteur. Elle considère que la qualité ne peut être garantie par le seul offreur dans certaines circonstances. Nous inscrivons notre modélisation dans le cadre des enseignements des recherches sur le capital-marque et nous appuyons sur la théorie économique du signal. Les labels, en tant qu’émanation de l’action d’un tiers de confiance permettent de réduire l’asymétrie informationnelle facteur essentiel de la difficulté à estimer la qualité. Le design de notre expérience repose sur un plan factoriel complet de type 4 × 4 (quatre marques, sans label et trois types de label) en inter-sujets. Au total, 1005 acheteurs de marques d’un produit agroalimentaire de grande consommation ont été interrogés sur internet et en face-à-face. Les qualités psychométriques des instruments de mesure sont vérifiées, les interactions entre marque et label sont étudiées par les chemins de causalité et la mesure du consentement à payer est formalisée par une zone d’acceptabilité du prix. Un effet direct du label sur les variables dépendantes est constaté mais lorsque la marque est prise en compte cet effet disparait, mettant ainsi en évidence une médiation totale par la marque. Des effets indirects de médiation en chaîne sont également mesurés, différents d’un label à l’autre et selon la marque à laquelle ils sont associés. Nos résultats conduisent à donner avec prudence de l’importance aux stratégies d’association d’une marque et d’un label."
"Les réticences que les puissances occidentales ont eu à intervenir en Syrie illustrent le fait que la Méditerranée n'est plus le centre du monde qu'elle a été des siècles durant. Epicentre culturel, commercial et politique, elle apparaît aujourd'hui comme une région éclatée et sans avenir unifié dans une « mondialisation » qui dicte la nouvelle marche du monde. Et, c'est désormais une autre mer-celle de Chine-qui semble impulser la dynamique mondiale. Pourtant, les « révoltes » populaires sans précédent au sud de la Méditerranée ont mis fin à des régimes autoritaires ouvrant une période nouvelle à une jeunesse souhaitant se réinventer un avenir empreint de normes, de valeurs et d'institutions démocratiques. Ces mutations ont été particulièrement spectaculaires du point de vue du régime politique puisque la plupart des pays du Maghreb ainsi que l'Egypte et la Syrie sont de fait engagés dans des transformations plus ou moins radicales, plus ou moins décisives, plus ou moins violentes. Ces transformations politiques indiquent des dynamiques dont il est difficile aujourd'hui,de mesurer la profondeur, la trajectoire et la destinée au delà des changements politiques qui, bien qu'historiques, pourraient se révéler superficiels ou cantonnés à une sphère limitée. Dans ce contexte, il apparaît essentiel de questionner la notion de démocratie et les conditions d'émergence d'une société démocratique. Etant considéré ici que la démocratie est un processus culturel qu'il convient de réinterroger en permanence, il apparaît que les sociétés du Sud de la Méditerranée sont sur des trajectoires dont semblent émerger quelques marqueurs forts d'une transition démocratique (Partie 1). Dès lors les activités artistiques ou culturelles, au coeur des processus de structuration sociétale apparaissent comme des vecteurs essentiels qu'il convient de soutenir (Partie 2)."
"Le développement des formes d'emploi atypiques constitue l'un des traits marquants de l'évolution récente du marché du travail en France. Ce développement suscite des interrogations dans le champ de la gestion, portant notamment sur les impacts de ces nouvelles relations d'emploi sur les attitudes des salariés concernés. Notre travail développe plus particulièrement la question des impacts des contrats temporaires sur l'implication au travail des salariés concernés. Une revue des travaux théoriques consacrées à l'implication des salariés intérimaires nous a conduits à poser l'hypothèse que ceux-ci se trouvent dans une situation d'insécurité néfaste à leur implication. Les recherches empiriques menées sur ces salariés, donnent toutefois à des résultats très contrastés, ne permettant pas de valider sans réserve l'hypothèse de départ. Cette absence de consensus nous a amenés à effectuer une étude qualitative visant à mieux appréhender la nature de l'implicaiton dans le cas des salariés intérimaires. Nous avons ensuite pu proposer un modèle du lien entre emploi temporaire et implication au travail. Ce modèle cherche à montrer que le statut d'emploi n'est pas en soi un obstacle à l'implication : tout dépend de la manière dont les salariés intérimaires interprètent leur relation d'emploi, à travers la notion de précarité subjective. Les résultats obtenus à l'issue d'une étude quantitative menée sur un échantillon de 208 salariés intérimaires nous ont permis de mettre en lumière le rôle médiateur de la précarité perçue sur la relation entre précarité objective des contrats temporaires et implication au travail, et de mettre en évidence l'influence de l'employabilité perçue et du degré de préférence pour l'intérim sur la précarité perçue. Ces résultats nous ont ensuite amené à proposer une typologie des salariés intérimaires, apte à rendre compte de l'hétérogénéité de leurs attitudes."
"Through case studies we explore the dynamics of two environmental conflicts about industrial projects that affect significantly the natural milieus, and we consider the interactions between public and private decisions. The first case stages an alumina plant run by a French company ALTEO in Gardanne (France), which has dumped wastes called “red muds” in the Mediterranean sea as of 1967 until 2015 and which has be authorized to continue rejecting water, without solid particles, but in derogation to legal norms. The second case bears on the project of “biomass” unit by the German group E.ON to provide energy for power plant, in the same city. This very big unit is a challenge because the raw material will be collected for a part in a nearby environment of forests affected by anthropogenic pressures. One describes first the dynamics of the two projects by identifying all the stakeholders and stressing the chain of their decisions, especially focusing on the public ones. One infers the intricate imbrication of several levels of public decision reflecting the discordance between administrative divisions and real natural milieus. One also infers that beyond vested interests the attitudes of the stakeholders reflect different “values” assigned to Nature and divergent because of the fragmentation of knowledge relative to natural milieus. The collection of 1184 public positions allows matching the attitudes with the stakeholder categories. The attitudes toward the projects are also considered as a function of the distance to the plant location, and it comes out that are not necessarily linked to the direct impact for the stakeholder. The conclusion stresses the need for arenas of debate to improve the circulation of information and the convergence of knowledge for an attenuation of the conflicts."
"Notre recherche vise à identifier pour notre partenaire socio-économique, le Pôle Services à la Personne PACA (PSP PACA), les manières possibles dont il peut soutenir l’activité d’innovation de ses adhérents et des acteurs du champ des services à domicile. Nous répondons pour cela à la problématique suivante : « Dans un tel champ d’activité institutionnalisé et cloisonné, comment soutenir à l’échelle collective l’engagement et la capacité d’opérateurs de services à domicile à mener à bien des démarches (entrepreneuriales ou intrapreneuriales) d’innovation ? » Nous avons dans un premier temps, à l’aide de revues de littérature, bâti un modèle théorique d’analyse nous permettant de mettre en évidence l’ensemble des activités cognitives, politiques, constructives et productives, fondamentales dans la conduite de projet entrepreneuriaux visant à transformer ou à renforcer un champ organisationnel. Nous nous sommes pour cela appuyés sur une littérature portant sur la perspective fondée sur le projet, et sur trois ensembles de littérature portant sur l’appropriation d’objets de conception. Nous avons ensuite, dans un deuxième temps mobilisé ce modèle d’analyse dans la lecture approfondie de deux cas de projets conduits par le PSP sous forme d’actions collectives. Deux projets que nous pouvons opposer l’un à l’autre par leur finalité (renforcer un positionnement vs transformer ce positionnement), et par leur orientation (prescriptive ou émancipatoire). La mobilisation de ce modèle d’analyse nous a permis d’examiner les dispositifs (organisationnels, d’animation, d’accompagnement, …) mis en place par l’équipe du pôle au niveau des actions collectives, et leur incidence sur les activités qui s’y sont déroulées. Celle-ci nous a également permis d’identifier, en contexte, les éléments qui ont été par la suite appropriés ou adoptés par les opérateurs de services, durant la conduite de leurs propres projets. Nous avons alors mis en évidence un certain nombre de leviers d’ordre méthodologique, sur lesquels peut s’appuyer l’équipe du pôle pour optimiser – selon les finalités et les orientations qu’elle se donne – l’organisation et l’animation de telles actions collectives. Ces leviers s’articulent selon quatre modalités différentes : A) les modalités d’organisation d’une action collective, B) ses modalités d’animation, C) ses modalités de capitalisation, D) la forme des résultats visés, en vue de faciliter leur mobilisation par la suite par les participants ou des porteurs de projets. Ces résultats nous ont alors conduits à émettre trois principaux axes de discussion théorique : le premier traite de l’intérêt d’intégrer une perspective rationnelle « renouvelée » au sein de la littérature sur l’appropriation des outils de gestion, pour mieux appréhender les implications de leur conception sur leurs usages futurs ; le second traite de l’activité d’élaboration de sens observée durant les actions collectives et des différents rôles que celle-ci a pu jouer ; le troisième porte sur la manière dont l’expérimentation matérielle et l’élaboration de sens permettent de nourrir les capacités d’agir entrepreneuriales de dirigeants d’entreprises. Nous terminons enfin notre recherche par une ouverture sur trois objets de réflexion : la première porte sur le rôle à jouer des actions collectives organisées par le PSP dans le cadre du développement par les dirigeants d’entreprise d’une ambidextrie de réseau ; la seconde porte sur le potentiel représenté par la formation de communautés dans la pérennisation des dynamiques impulsées par le pôle ; enfin, la troisième porte sur le rôle à jouer du PSP PACA en tant qu’acteur d’un « middleground » pour le développement du pouvoir d’agir de ses adhérents."
"Émergence de compétences collectives et élaboration stratégique lors d'une activité créative Rym IBRAHIM , CERGAM Céline DESMOULINS, CERGAM"
"Le manuscrit d’HDR soutenu par Edina Soldo le 13 octobre 2018 est le fruit de l’analyse rétrospective et thématique de ses travaux de recherche en management public menés ces quinze dernières années. Structuré en 2 chapitres, il en propose une lecture dans une perspective nouvelle, en dégageant l’originalité et les principaux apports de ces recherches. Le premier chapitre est l’occasion de développer sa conception du management public ancrée en sciences de gestion mais également instruite par le recours à des disciplines connexes (comme le droit, l’économie ou les sciences politiques notamment). Ces recherches s’inscrivent dans une critique du paradigme dominant du New Public Management (NPM). Le caractère universaliste des solutions qu’il promeut le rend en effet inopérant dans un contexte de contingence historique, institutionnelle et culturelle propre à la diversité des sociétés contemporaines. En outre, sa neutralité normative supposée et son apparente intention de déconnecter les cadres interprétatifs gestionnaires de la problématique des valeurs politiques au fondement du management public, constitue sans doute sa principale faiblesse au regard de la crise démocratique actuelle. Ainsi, du fait d’une approche contingente et territorialisée des problématiques relatives à l’élaboration et la mise en œuvre des politiques et des actions publiques, les recherches présentées mobilisent principalement le courant théorique du management territorial stratégique (MTS), dont les apports conceptuels représentent aujourd’hui un réel renouvellement du management public. Ce faisant, elles se positionnent dans le paradigme émergent de la gestion de la valeur publique et offrent une réflexion intéressante sur la notion de management public démocratique. Le second chapitre revient sur les particularités du projet culturel de territoire, objet central de ces recherches, qui se révèle particulièrement riche et générateur de pratiques managériales innovantes. Il souligne notamment en quoi les impacts d’un projet culturel de territoire sont multidimensionnels, contribuant à renforcer l’attractivité des territoires, au-delà des simples dimensions économique et marketing traditionnellement analysées. Intégrant des critères relatifs aux impacts politiques, sociaux et citoyens de plus long-terme, les recherches menées ont ainsi conduit à formaliser un référentiel d’évaluation de ces impacts en termes d’Attractivité Territoriale Durable (ATD). Pour autant, si les projets culturels se voient aujourd’hui régulièrement mobilisés comme éléments clés du déploiement de stratégies territoriales, deux concepts relatifs à leur mise en œuvre, en conditionnent très largement la réussite : leur ancrage territorial et leur gestion démocratique. Au-delà de ces apports conceptuels, les recherches présentées ont également conduit à la structuration progressive d’un protocole méthodologique original, régulièrement appliqué dans le cadre de démarches évaluatives d’actions publiques territorialisées. Afin d’adapter la pratique évaluative aux contingences des stratégies territoriales, il a été nécessaire de remanier les techniques et outils d’évaluation traditionnellement mobilisés dans le champ du management public. La formalisation d’un protocole inédit d’évaluation stratégique, permettant d’accompagner la gestion démocratique de projets de territoire, constitue ainsi l’un des apports les plus originaux de ces recherches. Adoptant une posture épistémologique pragmatique, ce protocole s’ancre dans une démarche transversale de recherche-action mobilisant un design de recherche en méthodes mixtes."
"La Gendarmerie Nationale est une institution militaire française qui œuvre dans le domaine de la sécurité. De par son activité de police dans les espaces ruraux et suburbains, elle a historiquement adopté une configuration particulière centrée sur la disponibilité des forces et un déploiement élargi sur le territoire. Poussée par des logiques de rationalisation issues de la nouvelle gestion publique et par des mouvements internes de contestation sociale, l’institution s’est engagée depuis la fin du XXème siècle dans un processus de transformation important qui influence en profondeur la gestion de ses personnels. On observe alors une modification des règles du jeu sur lesquelles se base l’exercice du commandement. La condition du gendarme intègre de nouveaux droits jusqu’alors refusés ou fortement restreints par le statut militaire, et certaines marges de manœuvre traditionnellement admises, basées sur la soumission sans réserve aux modes de la discipline et de la disponibilité, sont relativisées. Dans ce contexte, la présente étude s’est particulièrement intéressée à la pratique quotidienne des officiers en position de commandement, ainsi qu’à la manière dont ceux-ci abordent les transformations contemporaines de leur institution. Construite à la manière d’une enquête de terrain ethnographique, la démarche de recherche a permis une analyse en détail du travail de ces « chefs » au travers d’un répertoire de rôles. Ces rôles ont été envisagés comme des moyens d’agir sur le collectif, et décryptés au sein d’une grille de lecture stratégique. L’étude a ainsi pu mettre en évidence le choix qui est fait par certains officiers d’adopter une posture de soutien en lieu et place de la posture d’autorité traditionnelle dans le monde militaire."
"Cette recherche étudie les influences, sur la relation à la marque, d'une publicité transgressive pour une catégorie de produit taboue (les produits d'hygiène féminine), à l'heure des réseaux sociaux. En plus de ce caractère digital, cette recherche étudie également une forme de transgression qui vise à générer de l'adhésion plutôt qu'à choquer. Une netnographie sur la récente campagne de la marque Nana « Viva la Vulva » est étudiée. Les résultats soulignent que les réseaux sociaux donnent un cadre de résonance puissant à la transgression. Elle peut être un moyen de favoriser du BAO digital positif et l'adhésion à la marque. Si la transgression peut être un levier marketing intéressant, elle doit toutefois être minutieusement pensée. En effet, il existe une altération des effets bénéfiques de la transgression lorsque le consommateur perçoit une manipulation. Abstract : This research examines the influences of a transgressive advertising for a taboo product category (feminine hygiene products) on brand relationships in the age of social networking. With a digital point of view, this research studies a form of transgression that aims to generate adhesion rather than shock. A netnography on the recent Nana brand campaign ""Viva la Vulva"" is carried out. The results underline that social networks provide a powerful resonance framework for transgression. It can be a means of promoting positive digital WOM and adhesion. While transgression can be an interesting marketing lever, it needs to be carefully thought out. The beneficial effects of transgression are altered when the consumer perceives a manipulation."
"Alors que la pratique du Faire dans la consommation est en fort développement, on parle de plus en plus d’une forme nouvelle de Faire : le « Faire Ensemble ». En nous appuyant sur les concepts de tribus (Maffesoli, 1988) et sur la Théorie de la Conservation des Ressources (Hobfoll, 2002, 2011 ; Hobfoll et al., 1990), nous proposons d’explorer cette nouvelle forme au travers d’une étude qualitative combinant observation, netnographie et entretiens semi-directifs. Nos résultats montrent que la dimension collective du Faire n’est pas du « Faire Ensemble » mais du Faire soi-même ET de l’être ensemble. En d’autres termes, le « Faire ensemble » correspond à une manière d’être seul mais ensemble (Turkle, 2017), ce qui n’est pas sans rappeler le « grégaire anonyme » de Freitas (1996). Les dimensions collectives et individuelles se combinent dans un partage de ressources (telles que du matériel, de la connaissance, ou de la réassurance), au sein de tribus variées, pour l’acte individuel du consommateur-maker. Notre recherche aboutit à une typologie des formes de collectif en jeu dans le Faire Ensemble, articulant les notions de tribus, de support social et de ressources."
"L'objectif de cette recherche est d'analyser l'implication des parties prenantes dans la conception des plans communaux de développement au Maroc. L'originalité de ce projet réside dans la combinaison des modèles traditionnels d'analyse du positionnement des parties prenantes avec une analyse du processus de mobilisation à l'aide de la théorie de l'acteur-réseau. Notre analyse, à destination des pouvoirs publics, a pour objet d'aider à l'amélioration de la gestion des processus de développement territorial. Au delà d'une simple analyse de l'implication des acteurs, ce travail met également en évidence d'autres éléments ayant émergé du terrain tels que les besoins de formation des élus locaux aux logiques de développement local."
"Cet article a pour objectif d’appréhender la performance sociétale d’un échantillon d’entreprises nord-américaines en cherchant à isoler des profils-types caractéristiques. Il présente les résultats d’une étude exploratoire menée auprès d’un échantillon extrait de la base MSCI ESG STATS ® 2011 et composé de 2848 sociétés cotées. Après construction d’une série d’indicateurs composites, nous avons mené une classification hiérarchique ascendante sur ces indicateurs, qui a permis d’identifier quatre catégories d’entreprises d’inégale importance : les entreprises non engagées (N), les engagées (E), les controversées (C) et les perfectibles (P). Afin mieux cerner les différences intergroupes au sein de la population étudiée, nous avons procédé à une analyse comparative en nous appuyant sur des critères économiques, démographiques et financiers. Si les liens entre les variables financières et les performances ESG demeurent le plus souvent ténus et flous, les variables économiques et démographiques (taille, appartenance sectorielle, dépenses de recherche et développement) semblent au contraire exercer une incidence significative sur la performance sociétale."
"Nos territoires sont soumis à des phénomènes naturels dont les dégâts sont incontestables et parfois dramatiques. Alors que la responsabilité des acteurs publics est engagée à chaque événement, les catastrophes se succèdent et les mêmes problématiques semblent réapparaître. Néanmoins, des actions sont menées. Ainsi, le législateur crée des outils au titre de la prévention et de la gestion des risques dans le but de réduire la vulnérabilité de ces territoires. Les PPRN sont des filtres concernant l’objet risque, le traitant spécifiquement sur une échelle spatio-temporelle. Or, ces documents de réglementation urbanistique, par essence, soulèvent parfois au niveau local des difficultés d’acceptation rendant compliqué leur mise en application. Cette recherche-action, renforcée d’une approche par les parties prenantes, est réalisée dans un contexte d’élaboration d’un PPRI par Submersion marine et apporte une vision transversale et systémique des limites liées à la gestion des connaissances dans son élaboration. Les résultats révèlent une structuration de la connaissance défaillante en raison de sa divergence, sa dispersion, ainsi que par son manque d’intégration des acteurs locaux dans le processus. Afin de concevoir des PPRI/Sm acceptables et adaptés, nous élaborons des stratégies dynamiques dans un continuum temporel. Nos propositions vont de l’intégration d’une concertation des parties prenantes dès le début de la procédure, permettant la prise en compte des savoirs vernaculaires, à la mise en place d’une cellule nationale de gestion des risques, déclinée au niveau du bassin versant, pour une analyse technique et indépendante des problématiques liées aux risques"
"Cet article est consacré à un mode de développement et de transmission spécifique aux PME : l'Owner Buy Out (OBO). Il propose une grille de lecture théorique permettant d'expliciter les leviers de création de valeur inhérents à un OBO stratégique, les options cachées associées à un tel montage ainsi que le processus de valorisation de ces mêmes options. En s'appuyant sur les développements récents que nous suggèrent les théories contractuelles et cognitives, cet article montre que l'OBO peut être défini selon deux cas polaires : l'OBO purement patrimonial qui ne crée aucune valeur économique (pur cash-out) et l'OBO stratégique avec un apport économique sous forme de transmission de capital organisationnel, de connaissances ou de mise en oeuvre d'actions stratégiques. Après avoir explicité la flexibilité managériale et le caractère multidimensionnel que procure un tel montage aux dirigeants-propriétaires de PME, nous proposons un cadre méthodologique fondé sur la théorie des options réelles en vue de mieux circonscrire la nature et les caractéristiques des options cachées inhérentes à une opération OBO et ce dans une optique d'évaluation. Afin d'apprécier la portée pratique de la méthodologie adoptée, des simulations permettent de valoriser les options de développement et de transmission associées à un OBO."
"Note de synthèse La compréhension des idéologies territoriales et des mécanismes de leurs influences sur la gestion publique locale est essentielle. En effet, la gestion territoriale est aujourd'hui un élément majeur et croissant de l'intervention publique. Elle donne lieu à des échanges marchands et non marchands colossaux (commandes, marchés, emplois publics, etc.). Et l'avenir des citoyens se décide à cette échelle, notamment selon la mise en œuvre de stratégies de développement local. Ces dernières donnent lieu à des projets de grande envergure (aménagement, urbanisme, tourisme, événementiel, transports, politique de la ville, etc.). Les consultants et chercheurs sont régulièrement associés à leur conception. Mais sommes-nous sous influence quand nous influençons l'intervention territoriale ? Comprendre les idéologies territoriales est essentiel pour appréhender dans quelle mesure elles agissent sur les capacités de prise de décisions et d'action des décideurs et managers territoriaux. Nous analysons plus spécifiquement la constitution et les impacts de celles liées au développement durable, à la compétitivité territoriale et à la métropolisation."
"L’objectif de cette thèse est de mieux appréhender les décisions de distribution de liquidités aux actionnaires dans le cadre de la théorie de l’agence, ainsi que le choix de l’instrument de distribution : dividende ou rachat d’actions. La structure de l’actionnariat des entreprises européennes pousse à élargir le cadre classique et à étudier à la fois l’influence des conflits entre actionnaires et dirigeants, et ceux entre actionnaires majoritaires et minoritaires. Cette analyse nécessite la prise en compte des déterminants de l’intensité de ces conflits, mais aussi du système de gouvernance auquel est soumis l’entreprise. A travers une étude empirique sur les sociétés françaises du SBF250, nous montrons la pertinence de ce cadre théorique, mais aussi les limites, en France, de l’utilisation des politiques de distribution pour résoudre les conflits d’agence. Si les distributions sont utilisées pour diminuer le risque de Free Cash Flow, elles ont un effet limité face au conflit entre actionnaires majoritaires et minoritaires. Faute de mécanismes de gouvernance alternatifs et efficaces, ces derniers ne parviennent pas à provoquer des distributions, laissant ainsi l’actionnaire dominant libre de diminuer les distributions pour faciliter l’extraction de bénéfices privés. Par ailleurs, cette thèse s’intéresse au choix de l’instrument de distribution. Ainsi, conformément aux hypothèses issues de la littérature, nous montrons que le rachat est préféré lorsque les dirigeants détiennent des stock-options ou lorsque les revenus de la firme sont temporaires. En revanche, la fiscalité n’influence pas significativement le choix de l’instrument. Enfin, notre recherche souligne que le rachat est un outil de distribution peu utilisé en France et que le dividende en reste le moyen privilégié."
"Cette communication propose une réflexion autour de l’analyse financière des collectivités locales françaises, en s’intéressant à la réalité des risques financiers encourus, aux objectifs qui doivent être fixés, à l’information disponible et aux méthodes utilisées. L’originalité de l’approche est de confronter la littérature théorique à la vision et aux méthodes des praticiens grâce à une série d’entretiens qualitatifs. Ce papier éclaire ainsi la faiblesse des risques de solvabilité, et identifie le citoyen comme porteur final, si ce n’est unique, des risques. Il pointe l’importance de l’étude de l’autofinancement et des marges de manœuvre des collectivités. Enfin, il examine les limites de l’information comptable et ses conséquences sur les méthodes utilisées : peu d’analyse du bilan et de la situation patrimoniale malgré les évolutions comptables, difficultés à appréhender la situation financière sur l’ensemble du périmètre d’une collectivité. L’utilisation de ratios semble être la méthode la plus pertinente pour répondre aux objectifs fixés. Le papier propose une discussion sur les ratios les plus pertinents pour éclairer la situation financière des collectivités."
"Les politiques publiques environnementales cherchent à impacter des comportements de consommation. Néanmoins, la relation causale entre l'action publique mise en œuvre et le changement de comportement se caractérise par des discontinuités. Elle doit donc être approfondie en combinant l'angle d'analyse des politiques publiques et du processus de décision du consommateur. En effet, ce dernier dépend également d'autres déterminants psychosociaux et d'autres facteurs contextuels. L'impact spécifique des instruments des politiques publiques doit cependant pouvoir y être distingué. Notre étude sur la politique publique environnementale française visant à l'acquisition de voitures à faibles émissions de carbone permet de comprendre l'impact des instruments des politiques publiques sur le processus de décision d'achat du consommateur. En effet, l'attitude envers les instruments de l'action publique produit des effets sur le processus de décision du consommateur. Cet impact n'est pas direct, mais il modère les relations causales entre les principaux déterminants du comportement. Ces effets modérateurs dépendent de la nature psychologique ou structurelle des instruments des politiques publiques qui impactent des relations spécifiques du processus de décision du consommateur."
"Ce rapport vise à caractériser les publics de musiques actuelles en Pays d’Aix. Il s’agit également d’analyser la satisfaction et les attentes du public par rapport à l’offre de musiques actuelles sur le territoire. Pour ce faire, la méthode retenue a été de nature quantitative par le recueil de données sur le terrain à travers l’administration d’un questionnaire auprès de 251 répondants, comportant 94 items."
"Notre étude concerne les modes d'articulation entre l'intention stratégique des organisations métropolitaines et leur mise en oeuvre. Nous avons souhaité mettre en lumière les processus, les pratiques et les outils de management territorial mobilisés par les organisations métropolitaines pour concrétiser leur intention stratégique. Nous avons mené une étude de cas unique et longitudinale, portant sur la stratégie urbaine de Barcelone, de 1976 à nos jours. Cette métropole constitue en effet un cas exceptionnel de stratégie territoriale, pouvant être étudiée de façon globale. Grâce au cas barcelonais, nous analyserons des pratiques de management territorial, étudiées sous l'angle de l'intention stratégique et dans leurs interactions avec le contexte métropolitain. Mots clés : Intention stratégique, management territorial, gestion paradoxale, organisation publique métropolitaine, méthode de cas unique, Barcelone."
"La théorie positive de l'agence et à sa suite l'approche de l'architecture organisationnelle analysent les décisions d'allocations de ressources déléguées. Supposant un coût élevé de transmission des connaissances nécessaires à ces décisions elles préconisent une large décentralisation des droits à décider. Nous confrontons ces hypothèses à des pratiques bancaires de délégation sur un panel de soixante agents au sein de quatre banques françaises. Sont successivement abordées après analyse des résultats principaux de l'enquête, la question du rôle de l'information spécifique non codifiée dans les décisions des agents et celle des rapports entre la préparation de la décision, le choix des décideurs et la performance de l'établissement en matière de pilotage des risques. Nous concluons à titre transitoire et en préalable à une étude quantitative lourde qu'il existe une probable corrélation entre la fréquence des échanges d'information entre le siège et les agences et le coût du risque. Nous concluons également que les agents du panel accordent une importance forte à la connaissance spécifique non codifiée et que l'ancienneté de la relation client/agent est le meilleur moyen d'acquérir cette connaissance. Enfin, les banques les plus efficaces en matière de sélection du risque semblent être celles qui attribuent des délégations encadrées et différenciées selon les aptitudes de chaque agent à sélectionner les risques de qualité."
no abstract
"Le sujet du dialogue social met traditionnellement aux prises les partenaires sociaux dans le cadre d’un processus et de dispositifs bien identifiés. Le législateur fixe le cadre de ces négociations devant permettre d’instaurer des échanges pertinents et efficaces sur les sujets relevant de ce cadre : il s’agit de la politique économique et sociale au sens large dont les sujets d’emploi et de conditions de travail notamment. Cependant, il est bien évident que d’autres sujets corollaires, pas nécessairement abordés dans le cadre du dialogue social, sont de nature à avoir une influence sur les thèmes abordés entre les partenaires sociaux. Ainsi, les questions stratégiques et financières ont un impact direct sur le contexte et incidemment, le cadre de ces échanges et des négociations. Or, dans le droit français, comme dans la pratique des affaires, ces sujets sont plutôt débattus au sein de la structure de gouvernance de chaque société (conseil d’administration ou structure duale - directoire et conseil de surveillance - selon les cas). Depuis plusieurs décennies, des représentants des salariés (dénommés administrateurs salariés) sont susceptibles de participer aux travaux et aux décisions du conseil . Leur rôle est donc important car ils peuvent représenter de façon indirecte un levier supplémentaire du dialogue entre la direction de l’entreprise et les salariés . Leur positionnement est sujet à débat car certains commentaires ont évoqué (depuis la fusion des instances dans le CSE et la loi PACTE) un risque de substitution du dialogue social, qui serait porté par une généralisation de ces mêmes administrateurs salariés . Dans le cadre de ce chapitre, nous nous focaliserons donc sur cette fonction d’administrateur salarié en adoptant un double prisme. Nous resituons dans un premier temps, la question de leur présence et de leur rôle dans une cadre historique pour souligner la dynamique conduisant à une présence croissante et à leur potentielle généralisation . Nous montrons notamment que la France se distingue par une vision totale de la question de la participation des salariés qui implique notamment leur présence potentielle au sein de la gouvernance. Ce sujet a fait l’objet de contributions originales, notamment depuis 1945, portées par une multiplicité d’acteurs. Ces propositions n’ont pas pour autant été complètement éloignées de la progression du dialogue social en France. Dans un second temps, nous proposons une grille de lecture de la fonction actuelle d’administrateurs salariés pour expliciter leur rôle et leur légitimé en tant qu’acteurs des instances de gouvernance des sociétés contemporaines. Enfin, nous concluons ce chapitre en nous interrogeant sur une question essentielle : les administrateurs salariés représentent-ils un levier agissant en complément ou en substitution du dialogue social ?"
"À mesure que la communauté de recherche française en systèmes d'information s'agrandit, le besoin d'en comprendre les spécificités se fait de plus en plus pressant. Cet article s'attache à mettre en évidence la structure du réseau social qui sous-tend la communauté de l'Association Information et Management (AIM). Elle s'appuie sur l'analyse des réseaux de co-écritures dans la revue Systèmes d'Information et Management (SIM) et les communications au colloque de l'AIM. Cette étude du réseau social, qui manquait à la compréhension des particularismes de la communauté francophone, répond à trois questions : quelle est la structure du réseau social des co-écritures ? Qui sont les acteurs centraux ? Comment le parcours professionnel des chercheurs impacte-t-il leur niveau de centralité ? L'objet de cette étude est de contribuer à la discussion sur les spécificités de la communauté française des SI en vue de renforcer son identité collective. Elle est aussi un moyen pour chacun de ses acteurs de s'interroger sur ses pratiques de co-écritures et sur son rôle dans le réseau social de recherche."
"Initié en 2003 par le Ministère de la Transition écologique français dans le cadre du dispositif d’investissement locatif « Robien », le zonage A/B/C réalisé à l’échelle nationale permet de catégoriser les communes en fonction de la tension du marché de l’immobilier. Si cet indicateur unique met en lumière les zones les plus tendues du territoire, il reste lacunaire en ce qui concerne les zones C dites « détendues ». En effet, en les assimilant à un « reste du territoire » uniforme qui semble vidé de toutes activités, cette cartographie ne rend pas compte de la diversité des formes et des fonctions de ces territoires situés en dehors des grands centres urbains et qui restent souvent dépréciés. L’objectif de cet article est de mettre en lumière les dynamiques territoriales des zones détendues en élaborant une typologie de ces territoires à travers le prisme de l’attractivité territoriale à l’aide d’une classification ascendante hiérarchique."
"Dans cet article, nous souhaitons comprendre quelles sont les sources éventuelles de création de valeur engendrée par des compétences éthiques. Dans une première partie, nous élaborons une grille théorique qui emprunte à la GRH, au management stratégique et à la finance afin de préciser le concept de compétence éthique et d'identifier les principaux mécanismes de la création de valeur qui lui sont applicables. Dans une seconde partie, nous confrontons la grille élaborée à une étude de cas portant sur la démarche EVEIL-Tourisme qui est une démarche collective de progrès visant à diffuser des pratiques touristiques durables en Provence. L'expérience étudiée conduit à une première "" mise à l'épreuve des faits "" de la grille théorique conduisant à deux ensembles de résultats. Premièrement, l'approche inductive suivie apporte des éclaircissements sur le concept de compétence éthique, dont le contenu est précisé, et elle donne lieu à l'élaboration d'un modèle des compétences éthiques. Deuxièmement, les interactions entre de telles compétences et la création de valeur sont examinées au travers de trois mécanismes : les gains qu'elles engendrent, les coûts qu'elles suscitent et les incidences d'un management des parties prenantes en termes de création et de répartition de la valeur créée."
"Le concept de proximité connaît un regain d’intérêt en marketing, parallèlement au développement de la consommation de produits locaux. Cette recherche analyse les fondements du concept, et examine l’impact réel de la dimension spatiale de la proximité sur les attentes, les attitudes, et les comportements de consommation d’un produit de terroir. Les résultats d’une étude empirique confirment les effets de la distance entre le produit et le consommateur, et précisent leur nature et leurs limites ou conditions d’émergence."
"Ce chapitre présente tout d’abord un tour d’horizon des recherches portant sur le management des compétences dans les PME innovantes. Il propose ensuite l’étude de cas d’une PME innovante, l’entreprise H2H (Hotel2Hotel), dans le secteur touristique azuréen. L’analyse de ce cas nous permet d’identifier, d’une part, les compétences mobilisées à la réalisation de l’innovation et, d’autre part, de mettre en évidence le modèle de développement stratégique des compétences lors d’un processus d’innovation dans une PME."
"La gamme croissante de risques et de crises de ce 21ème siècle constitue un véritable défi pour les territoires. L’accent est mis aujourd’hui sur les capacités de résilience à développer. Ces dernières oscillent dans la littérature entre dimensions individuelle, organisationnelle ou territoriale, sans véritable approche intégrée les liant. Cette recherche étudie l’articulation tant conceptuelle que pratique entre résilience territoriale et finances publiques. Elle explore les stratégies de résilience de 8 villes européennes et la manière dont elles articulent leur réflexion stratégique en matière de résilience territoriale et financière. Malgré des pratiques non homogènes, nos résultats révèlent une articulation relativement embryonnaire."
"Souvent considéré comme exemplaire de par son comportement à l'égard de l'environnement, le consommateur responsable est, lui aussi, amené à agir consciemment en désaccord avec ses normes personnelles. Cette présente recherche apporte une compréhension des raisons et circonstances qui incitent les consommateurs responsables à transgresser leurs normes personnelles et propose un éclairage sur la façon dont ces consommateurs gèrent leurs comportements déviants de leurs convictions environnementales grâce aux stratégies de coping. Au terme d'une exploration empirique s'appuyant sur la technique des incidents critiques, cette recherche met à jour l'influence de facteurs émotionnels, sociaux et situationnels dans l'apparition de comportements allant à l'encontre de leurs normes personnelles. Puis sont abordées les différentes stratégies de coping mises en place par le consommateur responsable. Celles-ci sont en ligne avec les stratégies identifiées par la recherche en management et en marketing à la différence que la culpabilité demeure peu évoquée par les répondants."
"Notre recherche propose d’étudier en quoi les pratiques transgressives permettent aux professionnels en Centre d’Accueil de Demandeurs d’Asile (CADA) de créer et maintenir du sens dans leur travail. En effet, ces acteurs sont de plus en plus considérés comme des agents des politiques migratoires (Daadouch, 2017), ce qui remet en cause le sens qu’ils accordent à leur travail et qui ne semble pouvoir être retrouvé qu'au prix d'une certaine désobéissance. C'est précisément cette relation que nous étudions dans cette thèse. Le sens dans le travail est une construction sociale qui s’élabore dans l’interaction entre l’individu avec son environnement de travail (Isaksen, 2000). Cependant, il est possible pour les managers de créer des conditions favorables à l’expérience d’un travail significatif (Frémeaux et Michelson, 2017). L’objectif de cette construction de sens, à travers le sensemaking (Weick, 1995), étant la résilience organisationnelle, nous pensons que la transgression, définie comme un écart à la norme reposant sur l’interaction entre une personne et des situations (Pesqueux, 2010), peut en être une nouvelle source. Pour mener à bien ce travail doctoral, nous avons décidé de réaliser une recherche-action (Pasmore et al., 2008). Ce travail nous permet d’observer que les professionnels de l’asile en proie à la perte de sens usent de pratiques transgressives, majoritairement collectives, pour y remédier. La transgression est ainsi appréhendée comme un moyen d’aménager la règle afin de faire correspondre davantage le travail réel aux valeurs des individus et du collectif. Notre thèse permet ainsi d’enrichir la littérature relative à la transgression en mettant en avant sa dimension collective et sa contribution comme croyances et actions lors du processus de sensemaking. Au-delà, notre travail doctoral contribue à mettre en lumière un nouveau terrain d’étude pour les Sciences de Gestion et propose une nouvelle catégorie d’observation directe participante."
"Cet article analyse le rôle du concept de marque (luxe vs. non-luxe) dans l'impact de l'extension verticale de gamme vers le bas sur la relation consommateur-marque. Une étude expérimentale avant-après menée auprès de clients des marques BMW et Peugeot, interrogés via des clubs de possesseurs sur Internet et des forums automobiles, montre que l'extension vers le bas détériore les principales variables de la relation consommateur-marque (connexions à la marque, attachement à la marque, confiance et engagement dans la marque) pour la marque luxueuse BMW alors qu'aucun effet de dilution n'est observé pour la marque non-luxueuse Peugeot."
"L'objectif de cet article est de focaliser sur la valeur informationnelle délivrée par les communautés virtuelles sur Internet. Cette recherche à partir des motivations pour le recours à Internet par le consommateur souligne l'influence de la méta-information comme un antécédent de la perception de la valeur informationnelle délivrée par les communautés virtuelles. Finalement, les communautés virtuelles sont considérées comme une ressource pertinente par les consommateurs."
"Cet article a pour objectif d’identifier les articles, auteurs et thématiques ayant eu le plus d’impact dans le champ de recherche sur les pays émergents au sein de la littérature en management international. La méthodologie utilisée dans cet article repose sur les techniques bibliométriques. Notre étude bibliométrique porte sur les articles publiés dans les principales revues académiques en management international au cours des quarante dernières années. Nous analysons 1 142 articles publiés par 1 809 auteurs et citant 40 295 références. Les résultats permettent une meilleure compréhension du contenu et de l’évolution quantitative de la recherche sur les pays émergents."
"Cet article étudie la relation entre les composantes variables de la rémunération en actions de l’entreprise des dirigeants et des actionnaires salariés. En utilisant un échantillon de données issu du SBF 120 sur la période 2004-2009, nous étudions en particulier : (i) la relation entre le montant des stock-options attribuées aux dirigeants et l’actionnariat salarié ; (ii) la relation entre le rapport du montant des stock-options sur la rémunération totale des dirigeants et l’actionnariat salarié. La relation est significativement négative dans les deux cas montrant que l’actionnariat salarié tend à limiter la composante stok-options de la rémunération des dirigeants. Ces résultats sont nuancés en fonction de l’année étudiée. Nous montrons en particulier que la loi du 3 décembre 2008 a annulé cet effet."
Nous mobilisons la littérature sur l’Organisation et les Pratiques pour analyser les mécanismes permettant à un Collectif social d’introduire une innovation en santé. Le cas montre comment ce Collectif acquiert des caractéristiques d’une Organisation Partielle par : 1) la résolution d’incidents critique; 2) un programme d’action qui transcende la participation des acteurs; 3) des pratiques complémentaires d’inclusion et d’exclusion de membres pour développer l’identité collective et la masse critique; 4) des pratiques de réification pour être reconnu Acteur; 5) un secrétariat qui théorise l’innovation et développe une relation étroite avec un acteur extérieur en raison de son expertise et de ses ressources.
"La participation des salariés à la gouvernance de l'entreprise fait l'objet d'un débat récurrent. Pour ses détracteurs, elle affaiblit la fonction de contrôle du conseil d'administration alors qu'elle diminue le niveau global d'asymétrie informationnelle pour ses défenseurs. Cet article propose une revue de littérature critique sur le rôle des salariés dans la gouvernance. Il discute également les conditions du déploiement d'une gouvernance salariale contribuant à améliorer la gouvernance des entreprises. Nous suggérons que l'exercice des mandats d'administrateurs salariés dépend de trois séries de facteurs: des facteurs individuels, des facteurs « micro » propres à la gouvernance de l'entreprise, et des facteurs « macro », relatifs au système national de gouvernance. Nous formulons des propositions théoriques définissant les contributions de la représentation des salariés au sein des conseils. Mots clés : gouvernance d'entreprise, représentation des salariés aux conseils. Abstract Employees' participation in corporate governance is the subject of a recurring debate. The cons stress the potential weakening of the board's supervisory function while the pros argue that board level employee representation reduces information asymmetry. This paper presents a comprehensive and critical literature review. It discusses the conditions of the implementation of board employee representation improving corporate governance. We suggest that their mandates are determined by three sets of factors: individual factors, micro level corporate governance factors, and macro level factors related to the national governance system. We state theoretical propositions defining board employee representation's contributions."
"Le système économique actuel s’appuie sur une exploitation, voire une surexploitation, des ressources naturelles pourtant limitées, sans toujours penser au recyclage des produits en fin de vie. Avec un tel système, qualifié de « linéaire », comment envisager toujours plus de croissance économique sans menacer la survie de l’Humanité ? Une approche davantage « circulaire » de l’économie semble à privilégier. L’article met en évidence en quoi les recherches en logistique et supply chain management (SCM) participent aux réflexions académiques et aux décisions managériales visant à promouvoir une nouvelle forme de croissance économique intégrant davantage de critères écologiques, sociaux et sociétaux."
"La croissance de l'entreprise n'emprunte pas qu'une seule voie. Différentes options de croissance s'offrent aux managers et aux dirigeants pour développer leur entreprise. Cet ouvrage pratique, riche en exemples, inventorie six pistes différentes (plus une septième en ligne), en insistant sur la nécessité de veiller à l'adéquation entre l'environnement de l'entreprise et ses choix de développement, ainsi qu'à la recherche d'une cohérence entre les axes stratégiques et les modalités pratiques de croissance. Conciliant théorie et pratique, ce livre passe au crible chaque piste de croissance à partir de la méthode des cas, enrichie de conseils, d'illustrations, d'outils et d'exercices pratiques."
"L’activité de soins dans les hôpitaux génère d’importantes quantités de déchets, en général dangereux pour les personnels qui les manipulent, pour le public et pour l’environnement. L’article met en évidence deux leviers de la maîtrise des accidents liés à la rétro-logistique lors de la manipulation des déchets médicaux hospitaliers. Une étude a été conduite dans le contexte africain afin d’en apprécier l’impact. Les données ont été collectées auprès des principaux acteurs de la chaîne logistique d’évacuation des déchets (agents de surface assurant le tri et la collecte). Les résultats indiquent que le climat de travail a un effet significatif sur la maîtrise des accidents, mais aussi sur la relation entre l’engagement de la direction générale et la maîtrise des accidents."
"Le design, à travers ses attributs sensoriels, joue un rôle fondamental dans la perception du produit et sa compréhension par le consommateur. Par les associations symboliques qu’il génère, il contribue à façonner l’image de marque et ses traits de personnalité. La fleur est un produit singulier, au design particulièrement expressif et évocateur. Elle peut être définie par des traits de caractères humains. Une expérimentation sur deux variétés, les tulipes et les roses, auprès de 509 personnes, a permis de démontrer que la forme (pétales pointu ou arrondis) et la couleur (rose clair ou foncé) impactent sur la personnalité perçue de la fleur. La sincérité, l'excitation et la rudesse sont caractéristiques de la tulipe ; la sophistication est associée aux roses. Les couleurs foncées et les formes pointues sont liées à la compétence et à l’excitation. Le profil de personnalité humaine accentue alors les traits dominants identifiés, l’amabilité de l’individu notamment. Les résultats confirment la puissance du design à modeler les perceptions et le lien étroit entre la personnalité du consommateur et celle de la marque"
"L’objectif de cette recherche est d’analyser, sous l’angle de la théorie de l’agence, le rôle de l’actionnariat salarié sur le niveau de rémunération des dirigeants de sociétés cotées françaises. Sur un échantillon de 120 sociétés cotées appartenant à l’indice boursier CAC All-Tradable de 2009 à 2015, les résultats de l’étude montrent que l’actionnariat salarié joue un rôle direct et indirect sur les déterminants de la rémunération des dirigeants. De plus, la présence d'administrateurs salariés élus siégeant au conseil d’administration tend à limiter la rémunération des dirigeants avec des variantes selon les composantes de la rémunération étudiées."
"Notre étude cherche à identifier, dans un cadre théorique articulant les approches juridico-financières et cognitives de la gouvernance, le profil-type des investisseurs participant au financement d’un rachat d’entreprise par son propriétaire (owner buy out, OBO). En nous appuyant sur une grille d’analyse regroupant les facteurs explicatifs d’une prise de participation dans le cadre d’un OBO, nous avons procédé à une analyse typologique fondée sur une classification hiérarchique ascendante en deux étapes (two-step cluster analysis). Nos résultats, obtenus grâce à une enquête administrée auprès d’un échantillon de 42 investisseurs financiers spécialisés opérant en France, ont fait émerger deux classes d’investisseurs présentant des caractéristiques clairement différenciées. La différence entre les deux classes est d’ordre disciplinaire et cognitif et concerne essentiellement la perception qu’ont les investisseurs des enjeux stratégiques d’une opération OBO et la nature des indicateurs conduisant à la maîtrise du couple risque/rentabilité caractérisant un tel montage."
"De nombreux courants du marketing soulignent le rôle de plus en plus actif joué par les clients dans les organisations. Ce rôle d’acteur du client soulève cependant plusieurs questions qui concernent les sciences de gestion au sens large, que cet article se propose de discuter. Cinq aspects sont notamment abordés : 1) les fonctions que les clients peuvent jouer dans les organisations ; 2) les configurations que les organisations peuvent adopter pour faire avec leurs clients ; 3) la performance des actions organisées qui sont menées avec les clients ; 4) les techniques pouvant être utilisées pour manager les clients ; 5) les enjeux stratégiques soulevés par le rôle actif du client dans les organisations."
"Le système économique actuel s’appuie sur une exploitation, voire une surexploitation, des ressources naturelles qui se raréfient de jour en jour. Avec un tel système, qualifié de « linéaire », comment envisager toujours plus de croissance sans menacer la survie de l’Humanité ? Une approche davantage « circulaire » de l’économie n’est-elle pas à privilégier ? En s’interrogeant sur les fondements scientifiques de la décroissance en référence aux travaux de Georgescu-Roegen (1971, 2006), la présente contribution éclaire en quoi les recherches en logistique et supply chain management (SCM) participent aux réflexions visant à promouvoir un développement économique intégrant davantage de critères écologiques, sociaux et sociétaux. L’accent est tout particulièrement mis sur les enjeux de la logistique durable et sur les implications managériales d’une telle évolution qui concerne les fournisseurs, les industriels, les distributeurs et les prestataires de services logistiques (PSL), mais aussi les consommateurs et les Pouvoirs publics."
"Comment certaines pratiques illégales peuvent-elles se pérenniser ? La littérature en comportement du consommateur s’est jusqu’ici principalement intéressée aux processus d’intégration des pratiques de consommation marginales, illégitimes ou illicites au marché. La question de la pérennisation de ces pratiques, de leur maintien dans le temps voire de leur permanence reste en revanche peu étudiée. Ce travail de recherche mobilise une approche néo-institutionnelle en vue de comprendre comment une pratique illégale de consommation peut se pérenniser via un travail institutionnel mené par les consommateurs. Au terme d’une analyse compréhensive de la pratique du téléchargement illégal en France de 2008 à 2013 mobilisant des entretiens semi-directifs et des données secondaires, nous montrons que le processus de maintien de la pratique repose sur deux dynamiques : l’institutionnalisation de la pratique illégale et l’érosion de l’ordre institutionnel."
"La participation financière des salariés permet d'associer étroitement les salariés à la performance et au développement de l'entreprise. Les mécanismes actuellement en vigueur résultent d'une longue évolution historique et politique. Après avoir rappelé les différents courants fondateurs ayant contribué à l'émergence puis à la diffusion des mécanismes d'épargne salariale, nous proposons une analyse lexicaledes textes parlementaires récents à l'aide du logiciel Iramuteq. Nous mettons en évidence que le projet de transformation sociale initial qu'impliquait la participation financière a évolué vers des questions de patrimoine et de rémunération et que la participation des salariés à la gouvernance a disparu des débats. Mots clés : Participation des salariés, épargne salariale, analyse lexicale. 1 Les noms des auteurs sont classés par ordre alphabétique."
Cet article étudie la relation entre le contexte de travail mesuré par la rotation du personnel et l’absentéisme et la décision des salariés d’investir en actions de leur entreprise. Nous avons mené une étude de cas économétrique sur un panel d’environ 15 000 salariés sur une période de 5 ans collecté auprès d’une entreprise française de construction et de travaux publics. Nos résultats montrent que l’absentéisme et la rotation du personnel influencent de manière positive et significative l’investissement en actions de l’entreprise. Ils soulignent ainsi que les entreprises sont incitées à développer des mécanismes d’actionnariat salarié afin de contrer les comportements de retrait préexistants des salariés.
"Contexte de la Recherche : • Concurrence accru des territoires. • Mondialisation / baisse des coûts des transports. • Accessibilité de l'information. • L'événementialisation des territoires • L'impact des événements mal mesuré. • La réputation une définition non consolidée. • Inexistence de mesure de la réputation (académiques). Méthodologie « Il faut 20 ans pour construire une réputation et cinq minutes pour la détruire. » Warren Buffet Apports attendus Managériaux • Comprendre les mécanismes de la réputation • Établir des « Best practice » Méthodologique : • Le design de la recherche : Qualitatif > Quantitatif > Mixte • La recherche d'efficience et de comparaison des territoires par la méthode DEA Scientifique : • Définition et mesure de la réputation • Analyse de l'organisation d'événement par l'approche par les ressources La réputation du territoire et son mode d'activation par les événements festifs Cadre Théorique : Les théories de la réputation en stratégie L'événement ou la nécessaire hybridation du sport et de la culture Dans la littérature, les événements sont différenciés en trois grandes catégories : la culture, le sport et les affaires (Garat 2009). Culture et sport ont chacun des particularités dans leur organisation et parfois leurs objectifs. Nous ne souhaitons pas les séparer, en particulier parce que les managers publics concernés par cette recherche appartiennent au service culture et sport de leur municipalité ou région dans une majorité des cas (sept régions sur treize ont déjà regroupé leurs services sport et culture)."
"Pour l'économiste François Perroux, la contrainte, l "" échange et le don sont les trois motifs des actes économiques.Son originalité est d "" analyser la participationà la lumière du dernier motif. Il conçoit la participation comme un don à l "" oeuvre commune et adopte une vision de la participation ne se limitant pas au seul partage des bénéfices. Sa critique de l "" échange libéral et de la contrainte marxiste traduit les angoisses de son temps mais les formes contemporaines de l "" échange et de la contrainte rendent actuelles ses analyses. Rapprocher don et participation trouve sa cohérence dans des travaux qui se focalisent sur la participation financière des salariés. Les travaux empiriques les plus récents montrent en effet la pertinence des théories du don et de la réciprocité pour comprendre la participation des salariés."
"Dans notre recherche, 30% des touristes venus pour la première fois affirment l’avoir fait suite à la recommandation d’un de leurs proches. Dans un environnement très contraint en termes de concurrence, mais aussi en termes d’enjeu environnemental, comprendre les mécanismes de recommandation semble une piste de développement que les responsables de stations doivent prendre en compte. Notre recherche, issue d’un travail de terrain sur cinq stations de l’Isère (Alpe d’huez, Deux Alpes, Chamrousse, les 7 Laux, Villard-de-Lans Corrençon) pendant les vacances d’hiver 2018, nous apprend que l’évaluation de la part du touriste, de la capacité d’une station à être un territoire intelligent au sens sociétal et environnemental du terme, est une des clés de voûte de la recommandation pour les touristes."
"Internet a permis l’émergence d’un nouveau type d’entreprises qui ne fournissent plus elles-mêmes les services qui leur sont demandés, mais connectent les consommateurs à des particuliers-fournisseurs afin qu’ils répondent à leurs besoins. Cette étude menée sur 47 plateformes d’hébergement collaboratif actives en France, porte sur les services réellement fournis par ces plateformes et la manière dont elles parviennent à maîtriser la qualité de service fournie par des particuliers."
"Cet article cherche à comprendre comment les organisations publiques territoriales répondent aux injonctions de développement durable. Pour répondre, nous retenons de la littérature sur le développement durable les travaux insistant sur son faible potentiel opérationnel et sa labilité, permettant de formuler des propositions de recherche. Celles-ci sont ensuite confrontées aux résultats d’une analyse lexicale multidimensionnelle assistée par ordinateur, fondée sur des données secondaires de onze métropoles françaises. Les résultats renforcent les propositions issues de la littérature et les complètent."
"Dans un monde complexe, turbulent, imprévisible, radicalement incertain, et dans des organisations complexes et en mutation permanente, la stratégie est une pratique organisationnelle animée, qui se discute, qui s’incarne et qui se vit. Cet ouvrage propose une approche de la stratégie où penser et agir sont étroitement imbriqués. Reconnaître la complexité des organisations et du monde avec lequel elles interagissent, l’accepter et faire avec, c’est considérer les mises en tension qui appellent à explorer la puissance dialogique. Il est alors nécessaire d’expérimenter l’altérité, d’entendre la polyphonie des voix qui s’élèvent dans les organisations et d’inviter au dialogue considéré comme un élément central dans la dynamique stratégique. Cet ouvrage qui est le fruit de réflexions d’auteurs d’origines variées, allie des notions théoriques d’académiques aux retours d’expérience plus pragmatiques de managers et dirigeants d’entreprises. Il articule ainsi divers niveaux de discours, d’expériences, de conceptualisation, qui s’autoalimentent et sont autant d’occasions de réflexion pour aborder la stratégie organisationnelle par le dialogue. Il ouvre une discussion sur les ressorts profonds, individuels et collectifs, de la construction d’une stratégie organisationnelle. Destiné à un lectorat varié, il vise plus particulièrement les chercheurs et praticiens réflexifs de tous secteurs d’activité, engagés dans l’organisation d’actions collectives. Il s’adresse aussi à tous les étudiants en master ou doctorat qui s’intéressent aux questions de management stratégique."
"La littérature managériale fait généralement de l'allocation des ressources et de son contrôle les éléments explicatifs majeurs du niveau de performance atteint par une organisation. Cela suppose une fonction de production bien spécifiée, l'absence de tout problème de coordination des hommes et l'existence d'un contrôle aussi complet que nécessaire des actions humaines. En s'inspirant des travaux d'Harvey Leibenstein et en introduisant le concept d'X-inefficiency, il est possible de compléter cette vision. Mais cela impose d'essayer de comprendre comment peut être contourné ou contrôlé le dilemme inhérent à toute coopération. La performance résulte dans cette vision de jeux qui peuvent porter sur le cadre des interactions et sur les règles. Comprendre ces jeux revient à repenser le contrôle et à donner de l'importance à l'apprentissage et aux expérimentations des acteurs."
"Les hésitations de la reprise économique mondiale, l’aggravation de la crise de la dette souveraine en Europe, les risques d’une nouvelle crise financière et les difficultés budgétaires de nombreux pays avancés ont fortement entamé la confiance des agents économiques de par le monde durant la seconde moitié de 2011. Du fait de ce climat d’incertitudes et d’inquiétudes, les perspectives économiques pour 2012 se sont fortement assombries. De surcroît, la croissance économique en 2012 sera affaiblie par les programmes d’austérité budgétaire mis en place simultanément dans de nombreux pays avancés, notamment en Europe. L’économie belge n’échappe pas à ces différents problèmes et faiblesses et elle devrait être en légère récession en 2012. Face à l’étendue de la crise de confiance et l’ampleur de l’ajustement budgétaire prévu dans l’accord de gouvernement, nous avons en effet fortement revu à la baisse notre prévision de croissance du PIB belge pour 2012, qui s’établit à présent à - 0,3 % contre 1,5 % dans notre étude publiée en octobre."
"Il y a trois mois, en présentant nos prévisions économiques trimestrielles pour 2012, nous soulignions avec un certain optimiste l’amélioration des conditions conjoncturelles constatée au 1er trimestre tant en Belgique qu’au niveau international. Aujourd’hui, ces améliorations semblent sérieusement remises en question et le processus de reprise économique au niveau mondial semble à nouveau s’enrayer. La croissance américaine apparaît à nouveau fragile, l’activité ralentit dans les pays émergents tandis que les problèmes de la dette souveraine et du secteur bancaire dans la zone euro suscitent à nouveau de vives inquiétudes qui font craindre que l’économie mondiale soit confrontée prochainement à une nouvelle crise de confiance comme celle qui avait éclaté l’an passé plus ou moins à la même époque. Dans ce contexte, même en supposant que les pouvoirs publics parviendront à endiguer la crise financière qui secoue la zone euro, les évolutions économiques attendues sur la période de projection restent modestes. Ainsi, en Belgique, la croissance du PIB serait limitée à 0,4 % en 2012 et 1,2 % en 2013."
"Quelles que soient la pertinence et la qualité de l’intention stratégique, ses résultats dépendent en grande partie de sa mise en œuvre par les manageurs territoriaux. Cette phase est difficile et semée d’embûches dans tous les types d’organisation, mais elle semble particulièrement complexe dans les organisations publiques territoriales. Dans cette étude, nous proposons d’analyser dans quelle mesure la notion d’intention stratégique s’applique aux organisations publiques métropolitaines. Nous nous efforçons aussi de comprendre les modalités de la mise en œuvre de la stratégie territoriale dans ces organisations. Pour y parvenir, nous nous appuyons en particulier sur l’approche théorique de la gestion paradoxale. Nous analysons, grâce à la méthode des cas, cinq métropoles européennes : Barcelone, Lyon, Nantes, Marseille et Nottingham. Cette étude contribue à construire le concept de management territorial, à élargir la notion d’intention stratégique en l’appliquant à un champ d’étude novateur, à définir les caractéristiques organisationnelles des organisations publiques métropolitaines et à développer, d’un point de vue théorique, la gestion paradoxale. Cette recherche a aussi permis de présenter les caractéristiques des organisations publiques métropolitaines et d’identifier les outils et les pratiques de management privilégiés dans le cadre de la gestion paradoxale métropolitaine. Mots-clés Intention stratégique, management territorial, gestion paradoxale, organisation publique métropolitaine, méthode des cas, étude internationale"
"Le Lexique d'économie est conçu pour être un outil de formation, d'apprentissage et de révision des notions fondamentales en sciences économiques et de gestion, tout en s'ouvrant aux autres sciences sociales connexes, conformément aux programmes des classes préparatoires et des premiers cycles universitaires. Il se caractérise par : - la simplicité et la rigueur scientifique des définitions avec une identification des champs lexicaux ; - la présentation détaillée des différentes théories, écoles, doctrines et courants économiques ; - la prise en compte des modifications institutionnelles, aussi bien dans le cadre national et européen qu'international ; - la présentation de graphiques et tableaux pour faciliter la compréhension des notions exposées."
"L’article s’intéresse au mariage a priori délicat de l’hyper-connexion et du vintage . Plus précisément, il vise à mieux appréhender l’usage des points de contact physiques et virtuels dans le parcours d’achat des consommateurs vintage . Une recherche netnographique sur le Marché de la mode vintage de Lyon souligne que, sur le plan théorique, l’emploi des points de contacts virtuels des consommateurs de produits vintage contribue au processus de construction identitaire. Sur le plan managérial, elle indique que les pratiques diffèrent d’un réseau social à l’autre et se complètent plutôt qu’elle ne s’opposent. La discussion des résultats aboutit à une lecture originale permettant de mieux anticiper et répondre sur la place qu’occupent de nouveaux types de points de contact dans le contexte étudié."
"La mise en place de la taxe à 75 % sur la part des salaires supérieure à un million d’euros par an a suscité de nombreuses réactions dans le secteur du football professionnel. Si son impact est clair sur la rentabilité des clubs, elle peut également agir sur la présence (ou non) de joueurs de talents ainsi que sur l’incertitude du résultat. L'essentiel : - La fiscalité est un élément contingent de la compétitivité d’une ligue de sport professionnel et un déterminant majeur dans le choix de localisation des joueurs. - La taxe à 75 % votée en France impacte l’allocation des talents intra et inter ligues."
"L’essor d’internet et des réseaux pair-à-pair ont favorisé la constitution de communautés autour de nouveaux projets collectifs qui remettent au premier plan les travaux sur les communs d’Ostrom (1990). Dans cette perspective, un nouveau champ de recherches s’intéresse au rôle de la technologie blockchain comme support de la gouvernance des communs. Ces recherches, encore peu nombreuses, sont essentiellement théoriques. Cet article se fixe ainsi comme objectif d’étudier l’utilisation concrète de cette technologie dans le processus de faire commun. Pour ce faire, cet article s’appuie sur l’étude du cas de la monnaie libre Ğ1. Nos résultats présentent en détail les modalités de gouvernance d’un nouveau type de commun développé par les membres de la monnaie libre Ğ1 : le dispositif socio-technique de création monétaire. Ce cas est intéressant car il permet de mettre en exergue les attributs de la blockchain Duniter spécifiquement développée pour les besoins de l’écosystème Ğ1. Il souligne également le rôle de cette blockchain pour soutenir l’auto-organisation du projet et, notamment, les faisceaux de droits que les membres ont mis en place afin d’allouer le dividende universel et le processus de faire commun."
"Cette étude examine la relation entre la diversité de genre dans les conseils d'administration et la trésorerie (c'est-à-dire le risque de liquidité) et la trésorerie excédentaire des entreprises (c'est-à-dire le risque d'agence). En utilisant un échantillon de sociétés françaises cotées à l'indice SBF 120 sur la période 1998-2015, nous constatons une relation positive significative entre la diversité de genre au sein des conseils d’administration et de surveillance pour deux mesures de la trésorerie des entreprises (c'est-à-dire le ratio de trésorerie et le ratio de trésorerie ajusté par secteur) et une relation négative avec les excédents de trésorerie. Ces résultats confirment que les conseils plus féminisés sont associés à de plus faibles risques d'agence, ce qui implique que les femmes administrateurs s'engagent dans un contrôle plus strict et empêchent l’accumulation de trésorerie au-delà d’un niveau optimal. L'étude fournit des résultats intermédiaires significatifs par rapport à la loi sur les quotas de genre Copé-Zimmermann qui sont robustes à des spécifications et des tests alternatifs."
"Cette recherche interroge le concept d'hospitalité afin d'en spécifier la place et le rôle dans la consommation touristique. Elle se situe en amont de l'approche scientifique au sens où ses conclusions constituent davantage un répertoire de problématiques qu'un panier de recommandations. Deux champs de littérature ont été étudiés, deux terrains qualitatifs ont été réalisés. Ceux-ci ont amené tour à tour à relier hospitalité et représentation sociale puis à s'interroger sur la richesse de l'hospitalité en tant que représentation sociale. Quatre questionnements émergent de ce travail. Ils appellent discussion et évaluation."
"Les modèles économiques classiques sont remis en cause, c’est pourquoi il nous est apparu pertinent d’interroger l’innovation sociale comme levier de changement. Les pôles se retrouvent dans un contexte incertain, du fait d’un désengagement financier de l’Etat, d’une évolution de leurs missions et d’une perspective de réforme de cette politique publique. Nous posons la problématique suivante : « En quoi la mise en oeuvre de l’innovation sociale participe-t-elle au fonctionnement et à l’évolution des pôles de compétitivité ? » Dans une première partie, nous sollicitons les deux conceptions de l’innovation sociale (chapitre 1). En tant que résultat, elle correspond à une réponse aux besoins sociaux et aux effets positifs générés. Quant à son processus, il sollicite des acteurs hétérogènes locaux qui se mettent en réseaux et partagent des ressources pour créer un projet commun. A partir de ces conceptions, nous élaborons une grille d’analyse de sept critères. Puis, afin de proposer un nouveau modèle des pôles (chapitre 2), nous nous appuyons sur le business model de l’ESS et ses trois dimensions, car ce champ est considéré comme propice au développement des innovation sociales. Pour terminer cette première partie, nous élaborons un cadre épistémologique et méthodologique de la recherche (chapitre 3). Notre positionnement épistémologique s’inscrit dans le paradigme du constructivisme pragmatique. D’un point de vue méthodologique, notre recherche qualitative exploratoire suit un raisonnement abductif. Trente-et-un entretiens semi-directifs ont été réalisés dans deux pôles de compétitivité de la Région PACA, ce qui constitue une étude de cas multiples. La seconde partie de notre recherche représente nos résultats (chapitres 4 et 5). Nous mettons en avant les sept dimensions de l’innovation sociale dans le fonctionnement des pôles. Ainsi, les pôles représentent des structures propices à son développement en rassemblant les éléments processuels de celle-ci. Quant au résultat de l’innovation sociale, ce sont les projets des pôles qui répondent à des besoins d’ordre social et génèrent des effets positifs et les rendent accessibles par leur commercialisation. Le business model de l’ESS dans les pôles, révèle des problématiques d’implication et de solidarité. Nous préconisons une installation de communautés de pratiques et le développement des pôles vers une communauté de destin. Par ailleurs, une gestion démocratique pourrait favoriser l’implication des membres. Enfin, nous proposons d’impliquer les pôles dans une attractivité territoriale durable pour les inscrire dans une chaîne de valeur locale en considérant le territoire comme bien commun."
"L a participation des salariés au capital des entreprises dans lesquelles ils travaillent – l'actionnariat salarié – est un phénomène mondial qui connaît aujourd'hui un développement important. Aux Etats-Unis, près d'un sala-rié sur quatre est un actionnaire salarié (National Center of Employee Ownership (NCEO), 2006). En France, cette proportion s'établit à environ un salarié sur dix (Fédération française des associations Actionnaires salariés et anciens Salariés (FAS), 2006). La diffusion des mécanismes d'ac-tionnariat salarié s'est accrue au cours des deux dernières décennies en raison notamment de réformes législatives qui ont accélérées son développement (ERISA de 1974, Delaware Act de 1988 aux Etats-Unis; en France : lois de privatisa-tion de 1986, loi Fabius de 2001, loi Fillon de 2003, loi sur la participation et l'actionnariat salarié du 30 décembre 2006). L'actionnariat salarié a comme particularité d'être aussi bien soutenu par les gouvernements successifs que par les chefs d'entreprises. Pour les premiers, l'actionnariat salarié permet de soulager significativement les systèmes publics de financement des retraites en transférant une partie de la charge sur l'épargne individuelle des salariés. Pour les entreprises, l'actionnariat salarié offre une batterie d'avantages permettant de fidéliser et motiver les salariés tout en stabili-sant le capital et en limitant l'impact des variations boursiè-res sur la prise de décision stratégique (Desbrières, 2002). Les salariés ont progressivement détenu une part de plus en plus significative du capital de leur entreprise au point de souvent devenir des actionnaires de référence. La loi du 30 décembre 2006 est venue consacrer la place gran-dissante de l'actionnariat salarié en garantissant désormais au moins un poste d'administrateur (ou de membre du conseil de surveillance) aux actionnaires salariés lorsqu'ils détiennent collectivement au moins 3 % du capital de l'en-treprise"
"Paru dans « Les administrateurs salariés et la gouvernance d'entreprise », eds A. Conchon et M.-N. Auberger, La documentation française, 2009. Le modèle français de gouvernement d'entreprise se caractérise par un système hybride empruntant certaines caractéristiques au modèle anglo-saxon (notamment la suprématie des actionnaires externes) mais conservant des caractéristiques propres liées à son évolution historique 1 , parmi lesquelles la présence de représentants syndicaux au conseil d'administration des grandes entreprises cotées (précédemment contrôlées par l'Etat) et une représentation croissante des salariés actionnaires 2. Or, l'existence de ces mandats d'administrateurs salariés fait l'objet de débats récurrents. Certains estiment que le conseil d'administration ne constitue pas une instance adaptée à la représentation du personnel, du fait de l'existence du comité d'entreprise, chargé spécifiquement de cette fonction. De plus, les administrateurs salariés seraient amenés à occuper une position quasi-schizophrénique en prenant part à des décisions pouvant avoir des répercussions potentielles sur l'emploi et les conditions de travail. Enfin, ils n'auraient, à l'inverse des dirigeants et des représentants des actionnaires, ni les compétences ni l'expérience pour faire bénéficier le conseil d'administration d'une expertise pointue. En cela, leur légitimité à être présent au conseil d'administration serait remise en question. D'autres soulignent au contraire, que, dans une économie actuelle fondée de plus en plus sur les connaissances et l'immatériel, la reconnaissance de la place prépondérante prise par le capital humain dans le processus de création de valeur justifie par essence la représentation de ce même « capital humain » au conseil d'administration 3. Au regard de ce débat, nous avons voulu adopter une position objectivée en nous interrogeant sur la problématique suivante : les administrateurs salariés ont-ils une influence sur la performance de l'entreprise ? Pour y répondre, nous analysons dans une première partie les différents arguments théoriques permettant d'envisager l'effet de la représentation des salariés au conseil d'administration sur la performance de l'entreprise. Dans une seconde partie, nous présentons les résultats d'une étude longitudinale d'envergure portant sur les sociétés du SBF"
"Cet article étudie le pourboire en tant que pratique interactive de régulation de la relation de service. Une étude qualitative auprès de clients et de personnels en contact permet de mettre en évidence trois pratiques différentes du pourboire. L’étude permet également de comprendre comment le client et le personnel en contact interprètent ces trois pratiques. Enfin, elle permet de comprendre quelles pratiques relèvent de la régulation de la relation de service, et lesquelles n’en relèvent pas."
"Cet article cherche à évaluer l’impact du mode de raisonnement des décideurs (causal ou effectual) sur leur comportement en contexte d’internationalisation. Nos résultats, bases sur un échantillon de 148 décideurs de PME françaises, révèlent que les décideurs effectuaux ont tendance a saisir les opportunités d’internationalisation plus vite que les causaux (1); que les décideurs causaux sont plus enclins a internationaliser leur firme suivant un processus séquentiel et progressif (2); qu’en situation d’internationalisation de leur firme, les décideurs causaux sont davantage freines par des barrières internes que les décideurs effectuaux (3) et que le mode de raisonnement (effectual ou causal) n’a pas d’incidence sur les comportements en présence de barrières externes (4)."
"Les services de gros sont désormais l’objet de pratiques logistiques vertueuses, tout particulièrement en termes de gestion des tournées de livraison dans l’espace urbain, de création d’emplois qualifiés et de recours à des techniques de transport éco-efficientes. Le pilotage des flux physiques par les flux d’information étant au cœur des pratiques logistiques vertueuses, l’attention est portée dans le point de vue à l’adoption d’outils numériques par les grossistes afin de les améliorer."
"Depuis les années 70, la France connaît un changement profond des tendances de localisation des activités économiques dans lequel le secteur des services aux entreprises joue un rôle moteur. Les villes moyennes paraissent souffrir d'un handicap par rapport aux métropoles urbaines. Moins dynamiques, elles s'en différencient principalement par des faiblesses dans leur tissu productif, en ce qui concerne les services aux entreprises, la recherche et les emplois les plus qualifiés. L'analyse statistique des données des recensements de 1982 à 1999 montre cependant que la taille n'intervient pas en tant que telle pour expliquer les écarts de croissance observés : les sources du dynamisme se situeraient plutôt dans le haut niveau de savoir faire des services aux entreprises ; une spécialisation excessive peut nuire à la croissance, de même qu'un nombre trop élevé de retraités. Ces résultats suggèrent quelques pistes aux politiques de développement local des villes moyennes"
"Les festivals, organisations à la frontière entre la filière d’activités évènementielles et la filière des arts, de la culture et de la création, sont dans un environnement instable, turbulent et en constante métamorphose. En effet, les managers se doivent de jongler entre de multiples contingences économiques, technologiques, politiques ou encore sociétales (Soldo, 2018). Si la France compte plus de 3000 festivals, il n’en reste pas moins vrai qu’un nombre important d’organisations culturelles et en particulier de structures évènementielles, ferment temporairement ou définitivement comme l’illustre la « cartocrise - Culture française tu te meurs ». Ainsi, se pose la question de la pérennité de ces structures (Salaun, 2016) ; cette dernière représentant le principal défi pour les managers festivaliers. Face à ce contexte, ce travail doctoral a pour objectif de comprendre comment la mobilisation collective des ressources humaines peut être un levier en faveur de la pérennité des festivals. Plus particulièrement, nous proposons d’apporter des pistes de réponse à la problématique suivante : afin d’assurer leur pérennité, dans quelle mesure les organisations festivalières peuvent-elles favoriser la mobilisation collective de leurs ressources humaines ? En s’inscrivant dans le courant philosophique pragmatiste à la Dewey, ce travail mobilise la méthodologie de l’enquête. Plus particulièrement, le cadre théorique et conceptuel construit sur la base d’une étude pré-exploratoire, permet de formuler deux propositions de recherche. La première résulte d’une analyse pluridisciplinaire de la littérature où nous avons effectué un travail de recensement des spécificités des festivals, que nous avons ensuite synthétisées au sein de facteurs de contingence propres à ces organisations. De ce travail, résulte une première proposition de recherche : P1. Les festivals catalysent des spécificités qui peuvent être regroupées au sein de sept facteurs de contingence : identitaire, économique, territorial, temporel, organisationnel, téléologique et ressources humaines. La seconde proposition se concentre sur le concept de mobilisation collective. Si la mobilisation collective, grâce à ces impacts multidimensionnels importants, est une réponse pertinente face à l’enjeu de pérennité, il n’en reste pas moins vrai que la plupart des variables du concept apparaissent difficilement applicables au regard des facteurs de contingence des festivals. En ce sens, nous formulons une seconde proposition de recherche : P2. Les facteurs de contingence des festivals appellent à une adaptation du concept de mobilisation collective des ressources humaines. Ainsi, et sur la base de la construction d’un modèle tridimensionnel du concept de mobilisation collective des ressources humaines, nous proposons de questionner chacune des dimensions afin d’identifier : (1) les formes d’expression, (2), les sources et (3) les impacts de la mobilisation collective des ressources humaines festivalières. Notre objectif est d’identifier par le biais de l’étude empirique des variables de la mobilisation collective contingentes aux festivals. Pour ce faire, nous avons mobilisé l’étude de cas comme stratégie de recherche car elle offre une compréhension fine et holistique des phénomènes étudiés. Plus particulièrement, quatre festivals furent l’objet de l’étude. Sur la base d’entretiens semi-directifs (n=77), d’observations directes non participantes (n=22) et de données secondaires internes et externes (n=37), nous avons réalisé une analyse thématique afin d’explorer les propositions de recherche et ainsi répondre aux différentes questions. De cette analyse découlent différents résultats venant enrichir le cadre théorique et conceptuel relatif aussi bien aux contingences festivalières qu’à la mobilisation collective. Enfin, sur la base de l’analyse des résultats, différents apports, limites et voies de recherche sont formulés."
"Cette thèse a pour objectif de traiter la thématique de la gestion des émotions dans un contexte de changement technologique. Les organisations négligent encore cet aspect pourtant très important. Notre problématique concerne plus précisément l'intelligence émotionnelle. De nombreux chercheurs mettent en avant les solutions apportées par l’intelligence émotionnelle dans la gestion « humaine » des ressources humaines. Cependant, cette dernière, apporte-t-elle un véritable atout pour les managers dans le cadre des projets de changements technologiques et organisationnels, responsables de l’intégration et l’acceptation de ces transformations par les employés ? Cette thèse autorise de nombreux apports académiques et managériaux : une énumération des freins et surtout des leviers de la co-construction du changement, une modélisation de la courbe du changement assimilée à la courbe de deuil de Kübler-Ross (1969), une précision des aptitudes liées à l’aspect émotionnel des managers de premier rang pour mieux « driver » le changement technologique, à l’introduction de la notion du manager driver doué d’intelligence émotionnelle associant les aspects émotionnel et cognitif dans ses démarches de gestion des ressources humaines, en utilisant une méthodologie de recherche peu commune « l’ethnométhodologie ou l’observation participante clandestine » répandue en anthropologie et en sociologie. Cette méthode est très intéressante pour les sciences de gestion dans la mesure où elle présente une mine d’informations à essayer de comprendre et analyser. Elle permet, également, au chercheur de comprendre des mécanismes difficilement compréhensibles si l’on n’expérimente pas – au même titre que les sujets observés – les phénomènes étudiés"
"Les fonctions régaliennes de l’Etat sont teintées d’une aura d’éminence particulièrement prégnantes dans les organisations qui sont chargées d’en développer les politiques publiques et leur application sur les territoires français. Il en résulte des cultures organisationnelles fortes, dont certaines s’incarnent dans un esprit de corps qui paraît constituer une partie de la cohésion professionnelle de ces métiers : défense, justice, sécurité… Nous souhaitons questionner la difficulté que les organisations publiques peuvent avoir à interpréter et mettre en œuvre des politiques édictées au niveau national, puisque d’une part celles-ci peuvent impliquer de véritables mutations de culture organisationnelle (voir par exemple l’étude de Calciolari et al., 2017) difficiles à mettre en œuvre pour les responsables de ces organisations, et d’autre part la marge de manœuvre qui leur est laissée pourrait impliquer une dilution de l’intention initiale de l’impulsion nationale. Pour cela, nous avons choisi d’étudier la sécurité civile dans le cadre de son évolution actuelle portant sur les risques professionnels encourus par les sapeurs-pompiers. En effet, dans l’optique gestionnaire qui est la nôtre, l’intérêt du sujet se trouve dans l’applicabilité et l’application effective de changements impulsés et de leurs impacts directs dans l’organisation."
"Après avoir présenté de quelle manière la pensée de Jean-Louis Le Moigne s’est développée, ce chapitre évoque ses contributions majeures pour la science puis pour l’ingénierie des systèmes d’information (SI) sans aucune prétention sinon celle, dans un souci didactique, d’être inévitablement réducteur concernant une œuvre qui est le fruit d’une vie entière et d’environ 140 ouvrages et articles publiés."
"Initialement centré sur ses domaines traditionnels (design industriel, de produits, design graphique), le design a, avec le temps, élargi sa base d’intervention à d’autres activités. D’abord aux services, puis à l’organisation elle-même. Ainsi, depuis déjà plusieurs années, le concept de design management fait partie des pratiques observées sur le terrain et constitue un champ de recherche. Parler de design management pose plusieurs questions. D’abord des questions de définition et frontières. Parle-t-on de gestion du design ? De gestion par le design ? De design de la gestion ? Où commence et où s’arrête le design management ? Des thématiques telles que le design de service, le design thinking lui sont-elles indépendantes ou imbriquées ? Comment porter des thématiques fondamentalement interdisciplinaires touchant à la fois aux sciences du design et aux sciences de la gestion dans des univers encore très silotés tels que ceux de la recherche et de la pratique ? Ce dossier thématique, vise à faire le point sur les enjeux du design management. Tout d’abord, en revenant sur son historique, sur les concepts clés développés à travers le temps du design management, sur l’évolution des pratiques et des discours. En dressant un portrait de la situation actuelle, de son champ d’activité, de son usage réel et des avancées théoriques et pratiques réalisées. Finalement, en traitant du design management dans une optique prospective, afin de dégager les pistes futures pour les praticiens et les chercheurs. Il s’agit donc, de réaliser un portrait réaliste du design management en 2018, année de parution de ce numéro 07."
"L’article a pour objectif d’étudier l’émergence d’un Réseau Territorialisé d’Organisations (RTO) appartenant à l’économie sociale et solidaire (ESS). A travers une approche néo-institutionnelle, cette recherche étudie les différentes formes de travail institutionnel des acteurs de ce réseau (Lawrence et Suddaby, 2006 ; Cloutier et al., 2016) tout en questionnant les rôles que joue le territoire dans ce processus (Lawrence et Dover, 2015). L’étude menée permet d’identifier le travail territorial comme une forme particulière de travail institutionnel. La recherche permet aussi de relever les éléments fondateurs et structurants de la phase d’émergence d’un RTO spécifique, composé d’organisations sociales et solidaires."
"The road freight transport sector is undergoing profound changes. The profession of delivery truck driver has enriched and requires new skills in the customer relational field, especially when delivering retailers. An exploratory research, by questionnaires, from 203 drivers, examined the fundamental role of this relational dimension, which contributes to a stronger affective and normative work engagement. The research contributes to the analysis of the influence of job perceptions on employee behavior in terms of work commitment. It highlights the specific characteristics of this short-staffed profession to improve its attractiveness."
"La construction d'une échelle de mesure du sentiment de "" chez-soi "" est entreprise. La revue de littérature et les premiers résultats empiriques sont présentés ici. Trois terrains qualitatifs, l'intervention d'experts académiques et professionnels puis deux terrains quantitatifs permettent de proposer une échelle à huit dimensions. Les critères de variance moyenne extraite et de validité discriminante intra-échelle sont satisfaits. Les dimensions extraites représentent la relation personnelle au "" chez-soi "". La dimension de liberté, présente dans la littérature mais absente de cette structure, devra faire l'objet de tests complémentaires."
"Les conditions conjoncturelles se sont nettement améliorées en Belgique durant la seconde partie de 2010. Dans l’industrie et les services, l’activité s’est raffermie et les entrepreneurs estiment à l’heure actuelle que le climat des affaires est redevenu très favorable. De leur côté, grâce à une forte augmentation de l’emploi, les ménages ont retrouvé un niveau de confiance égal à celui qui prévalait avant le déclenchement de la crise économique et financière. Dans ces conditions, le ralentissement de la croissance économique constaté depuis la mi-2010 devrait être passager et nous estimons que la Belgique retrouvera en 2011 des rythmes de croissance économique plus soutenus. Nous avons ainsi revu à la hausse notre prévision de croissance du PIB belge pour 2011. Elle est à présent fixée à 2,4 %, contre 1,9 % dans notre édition d’octobre dernier"
"Depuis près d’un an maintenant, l’économie belge est secouée par des vents contraires. Malgré cela, elle a fait preuve jusqu’à présent d’une assez bonne résilience eu égard aux chocs encaissés. Ceci fut encore confirmé au premier trimestre de 2008. Gare cependant aux excès d’optimisme ! Les évolutions récentes de l’environnement économique mondial et des indicateurs prospectifs de l’économie belge attestent davantage qu’il y a trois mois la perspective d’un ralentissement de l’économie en 2008 et en première partie de 2009."
"Les développements économiques récents confirment la reprise de l'économie mondiale et le redressement de l'économie belge. Malgré les turbulences financières des derniers mois, l'évolution attendue de la situation économique sur la période de projection reste favorable. Les risques sont néanmoins élevés. En 2010, le PIB belge devrait croître de 1,3 %. Sa croissance devrait se relever à nouveau en 2011, pour atteindre 2,0 %."
"La récession s’est aggravée en début d’année. Le climat économique étant toujours fortement déprimé, les perspectives économiques 2009-2010 sont pessimistes. Certains signes d’amélioration apparus récemment laissent néanmoins penser que le creux de la récession a été atteint au 1er trimestre 2009. La récession devrait perdre en intensité au second semestre 2009 et l’activité économique devrait recommencer à progresser début 2010. L’amélioration attendue n’empêchera pas une forte dégradation du marché du travail sur la période de projection."
"En raison de l’aggravation de la crise financière, du recul de l’activité économique dans de nombreux pays avancés, et d’une dégradation particulièrement sévère des anticipations des ménages et des entreprises, les perspectives macroéconomiques pour la Belgique se sont fortement détériorées au cours des derniers mois. Par rapport à notre projection d’octobre 2008, le taux de croissance annuel moyen du PIB belge prévu pour 2009 a été très fortement révisé en baisse. Il s’établit à - 0,9 %, par rapport à la prévision d’une croissance positive de 0,8 % en octobre dernier."
"L'objectif de cet article est de mieux appréhender les décisions de distribution de liquidités aux actionnaires, et en particulier, les critères du choix entre dividende et rachat d'actions. A travers une étude théorique, puis empirique sur les sociétés françaises du SBF250, nous montrons que le rachat est préféré lorsque les dirigeants détiennent des stock-options ou lorsque les revenus de la firme ont un aspect temporaire. En revanche, la fiscalité des dirigeants et des principaux actionnaires n'influence pas le choix de l'instrument. Ces variables semblent moins explicatives des décisions des entreprises françaises que des entreprises anglo-saxonnes. L'existence d'effets individuels montre que les entreprises françaises ont encore des critères spécifiques et peu financiers pour choisir l'instrument de distribution. Notre recherche souligne également que le rachat est un outil de distribution peu utilisé en France et que le dividende en reste le moyen privilégié."
"Les indicateurs économiques indiquant une reprise économique plus vive que prévu il y a quelques mois, notre prévision de croissance de l’économie belge en 2010 a été révisée en hausse, à 1,5 %, contre 0,7 % dans notre projection d’octobre. Le raffermissement attendu de l’activité économique ne sera cependant pas suffisamment fort à court terme pour empêcher que la situation du marché du travail continue à se détériorer en 2010."
"Si le problème de la sélection des variables mérite l'attention des chercheurs qui s'y penchent, les termes du débat à propos de TYPER sont quelque peu biaises, et un article de fond reste à faire sur «corrélations et distances, contributions et communautés»."
"Intervention lors de la table ronde n°3 : le développement durable par la preuve : argumentaire pour convaincre sur la valeur ajoutée du développement durable, tiré des leçons d'expériences 7 e Assises Nationales du Développement Durable (ANDD), « Le développement durable, levier de sortie de crise ? », Marseille, 25-26 novembre 2013"
"L'objectif de cette communication est double : il s'agit d'une part de mettre en évidence les liens existant entre certaines formes de gouvernance et les politiques de gestion de main d'œuvre, notamment l'externalisation des ressources humaines et le recours à des emplois flexibles, puis de montrer que les conséquences du recours aux emplois flexibles sur les performances individuelles et organisationnelles sont complexes, et que certains coûts cachés doivent être pris en compte"
"L'intérim, par sa nature même, soulève des questions de management complexes. Ce statut d'emploi très particulier engendre des situations d'incertitude, qui constituent l'une des caractéristiques fondamentales de la relation d'emploi intérimaire : incertitude du salarié intérimaire exposé à la précarité de sa relation d'emploi ; incertitude des entreprises de travail temporaire dont l'activité est dépendante des fluctuations économiques et du comportement des salariés en mission ; incertitude enfin des entreprises utilisatrices sur les performances de salariés sur lesquels elles ont peu de prise. L'objectif de cette communication est de montrer que cette incertitude peut être subie, et parfois limitée....mais qu'elle peut aussi être instrumentalisée au profit (ou au détriment) de l'un des participants à la relation. Nous nous appuierons sur l'exemple des salariés intérimaires pour montrer que, dans certains cas, l'instrumentalisation de l'incertitude par les employeurs (ETT ou entreprise utilisatrice) est considérée comme un véritable levier de mobilisation. Nous verrons toutefois que les impacts de ces politiques est délicat à apprécier, en raison des effets pervers induits par une "" gestion stratégique "" de l'incertitude."
"Cet article présente, au travers d’un modèle de pouvoir, les relations entre les organisations sportives. Ce modèle s’appuiera sur les écrits de Machiavel, Galbraith et Baechler. Il a été testé sur le cas particulier de la coupe du monde de rugby 2007 en France. Une analyse sémantique des données mettra en avant un pouvoir particulier présent dans les relations. La mise en lumière du pouvoir dans un cas précis du sport permettra à toutes les organisations sportives ou non de mieux appréhender ses relations de pouvoir."
"L'objectif de cette étude est d'analyser la relation entre les politiques de distribution des entreprises familiales cotées et deux grands types de conflits d'agence dans les entreprises familiales, entre actionnaires et dirigeant (type I), et entre actionnaires majoritaires et minoritaires (type II). Les résultats sur les entreprises du SBF 250 montrent que les montants distribués sont liés à ces deux composantes du conflit d'agence, qui est globalement moins fort dans les entreprises familiales."
"La prise en compte des effets de l'activité industrielle sur l'environnement demeure un phé-nomène récent à l'échelle du développement des sociétés industrielles. Ce n'est qu'à partir des années 1970, suite à la médiatisation des premières grandes pollutions, que la conscience collective des enjeux environnementaux se développe largement, que ce soit dans les milieux po-litiques ou au niveau de la société civile. Pour répondre à ces exigences, les industries doivent s'interroger de façon permanente sur leurs impacts locaux, directs et indirects, sous peine de subir des conséquences néfastes et irréversibles pour la pérennité de leurs activités (boycott des produits, impacts financiers, déficit d'image, perte de marchés…). Les questions environ-nementales représentent désormais un enjeu stratégique pour les entreprises. En mobilisant la théorie des parties prenantes et l'approche sociologique de la théorie néo-institutionnelle, ce papier présente les résultats d'une enquête réalisée à l'échelle nationale auprès de 196 établissements à risques situés sur cinq bassins littoraux (Seine Maritime, Gironde, Loire Atlantique, Bouches-du-Rhône et Nord). Ces bassins ont été identifiés en fonction de trois critères justifiant l'existence d'une vulnérabilité écologique et/ou humaine: forte densité de population, concentration d'établissements à risques, proximité avec les populations et le littoral. Cette étude vise à comprendre la gestion des risques environnementaux au sein d'établissements à risques situés dans des agglomérations littorales françaises densément peuplées. Elle analyse les dispositifs d'évaluation et de gestion des risques environnementaux mis en œuvre, appelés « risques industrialo-environnementaux » (RIE par commodité), et identifie les motivations et les freins de ces actions tels qu'ils sont décrits par les managers responsables de la gestion des RIE. Les résultats montrent que, malgré une prise de conscience collective des enjeux environne-mentaux, les RIE sont gérés de manière significative depuis seulement une dizaine d'années. Si la réglementation et les valeurs des dirigeants guident principalement les établissements vers une gestion plus « responsable », la complexité des textes réglementaires et le manque d'informations semblent expliquer la perte de vitesse observée des démarches environnementales volontaires. En effet, seulement la moitié des managers déclarent avoir adopté un système de management environnemental de type ISO 14001. Le faible nombre d'établissements s'inscrivant dans des partenariats ou dans un système d'éco audit EMAS confirme par ailleurs que les engagements environnementaux peinent à se développer. D'une manière générale, les établissements sondés adoptent des démarches réactives de conformité réglementaire. Les coûts, le manque de moyens humains et financiers, et la faible perception des avantages immédiats semblent, en effet, constituer des freins à la poursuite d'actions environnementales plus ambitieuses. Ainsi, la gestion des RIE apparait davantage comme un moyen onéreux de légitimer et de pérenniser les activités que comme une opportunité économique ou concurrentielle."
"La représentation obligatoire des actionnaires salariés au conseil d' administration : un état des lieux L 'actionnariat salarié est un phénomène mondial qui connait désormais un important développement au point d'apparaître comme un élément majeur du capitalisme actionnarial [Aglietta & Rébérioux, 2004]. Aux Etats-Unis, près d'un salarié sur quatre est aussi actionnaire salarié [NCEO 2006]. En France, cette proportion s'établit à environ un salarié sur dix [source : calculs des auteurs d'après les données FAS]. L'actionnariat salarié est en progression constante en France puisqu'on dénombrait 700.000 action-naires salariés en 1998 contre près de 3 millions en 2006 [FAS 1998-2006]. 15 L'actionnariat salarié a comme particu-larité d'être aussi bien apprécié par les gouvernements successifs que les directions des entreprises privées. Pour les premiers, l'actionnariat salarié permet de soulager les systèmes publics de fi-nancement des retraites en transférant une partie de cette charge sur la capi-talisation individuelle des salariés. Pour les entreprises privées, l'actionnariat salarié offre des avantages indéniables en fidélisant et motivant les salariés tout en offrant l'avantage de mieux sta-biliser le capital et de limiter l'impact des variations boursières sur la prise de décision stratégique [Desbrières, 2002]. Dans ce contexte, les salariés constituent désormais des actionnaires de référence dans le capital des entre-prises. Depuis 2002, une série de lois est venue consacrer la montée en puis-sance des actionnaires salariés en leur garantissant désormais au moins un poste d'administrateur (ou de membre de conseil de surveillance). En vertu de l'article 32 de la loi n° 2006-1770 du 30 décembre 2006, les sociétés cotées ayant un actionnariat salarié su-périeur à 3% du capital doivent obliga-toirement faire élire un administrateur représentant les actionnaires salariés. L'application de cette loi porte à 41 le"
"La recherche vise la compréhension des relations de pouvoir entre les institutions sportives internationales et les organisations sportives. L’étude qualitative des relations de pouvoir sur les événementiels sportifs internationaux permet de confronter l’exercice du pouvoir par l’entité dominante aux théories triangulaires (Machiavel, 1513 ; Baechler, 1978 ; Galbraith, 2007). Trois événementiels permettront de tester le modèle de pouvoir créé à partir des écrits de la science politique et de la sociologie : La Coupe du Monde de Rugby 2007, les America’s Cup 2007 et 2009 et les Jeux Olympiques d’hiver 2018. Le terrain du sport a été privilégié car les positions de chaque entité sont clairement définies par la structure de ce secteur, où l’institution internationale détentrice de l’événement domine la relation avec les organisations candidates et organisatrices. L’objectif d’un tel modèle de pouvoir est de fournir aux organisations présentes sur les événementiels sportifs internationaux des outils de compréhension du type de pouvoir exercé par l’entité dominante. Cette compréhension permettra aux organisations de devenir une force de proposition dans les relations, voire une force de contestation amenant à un renversement du pouvoir sur les événementiels. La volonté d’appréhender le pouvoir est nécessaire pour ces organisations compte tenu des nouveaux enjeux et du développement croissant de ce secteur d’activité. ENG The research’s aim is the understanding of the relations of power between the international sports institutions and sports organizations. A qualitative study of the relations of power on the international sports events allows to confront the exercise of power by the dominant entity with triangular theories (Machiavelli, 1513 ; Baechler, 1978; Galbraith, 2007). Three special events managements enable to test the model of power created from the papers of the political science and the sociology: 2007 Rugby World Cup, America Cup in 2007 and 2009 and 2018 Winter Olympics Games. The theme of sport was privileged because the positions of each entity are clearly defined by the structure of this sector, where the international institution, holder of the event, dominates the relation with the candidate and organizing organizations. The objective of such a model of power is to supply to the present organizations on the international sports events the understanding tools the type of power exercised by the dominant entity. This understanding will allow the organizations to become a strength of proposition in the relations, even a strength of contesting bringing to a reversal of the power on events. The will to dread the power is necessary for these organizations considering the new stakes and the increasing development of this sector."
"Cet article étudie les conséquences de l’actionnariat de l’Etat sur la performance, la structure de gouvernance et les objectifs des entreprises cotées. A travers une étude empirique, cette recherche compare les entreprises détenues partiellement par l’Etat français à des entreprises privées de même taille et secteur. L’article se concentre sur quatre dimensions : la performance financière, les politiques financières, la structure de gouvernance et la répartition du pouvoir entre trois des principales parties prenantes de l’entreprise : les actionnaires, les dirigeants et les salariés. Les résultats ne montrent aucune différence entre entreprises publiques et privées dans la performance et les politiques financières menées. En revanche, les structures de gouvernance et la répartition du pouvoir entre parties prenantes diffèrent. Dans les entreprises cotées publiques, les employées détiennent plus de pouvoir et récupèrent une plus large part du profit. Pour autant, cette répartition spécifique ne vient pas au détriment de la valeur actionnariale puisque les politiques de dividende et la performance de l’action ne différent pas de celles des entreprises privées."
"Le développement durable conduit à de nouveaux enjeux pour les territoires et les politiques publiques, en matière de consommation, de déplacement, d’organisation urbaine, etc. et renouvelle les modes de faire, avec des approches plus intégrées, plus transversales et plus participatives. Parler de développement durable impose de faire le point sur les menaces qui pèsent sur les territoires, mais plus encore cela impose de trouver des solutions innovantes, qui en appellent à l’imaginaire et à l’imagination. C’est l’objet de cet ouvrage que de proposer des formes originales de recherche sur le développement durable, autour des notions de vulnérabilité, d’équité et de créativité territoriale. Ainsi, les auteurs sont-ils amener à dépasser les lectures traditionnelles pour proposer de nouveaux concepts et méthodes d’analyse. La question de la vulnérabilité des territoires débouche sur la notion de vulnérabilité résiliençaire. L’équité territoriale ne se résout pas en une réduction pure et simple des inégalités entre territoires et pose la question des transferts de durabilité entre les pays du Nord et du Sud de la Méditerranée. La capacité créative du consommateur est interpellée. Si les notions ne vont pas de soi, aborder le développement durable dans un continuum vulnérabilité, équité et créativité l’est encore moins. Quelques auteurs tentent une lecture croisée d’expériences conduites dans des pays méditerranéens."
"Les États-Unis ont certainement été les pionniers en termes de participation des salariés, qu'elle soit financière ou à la décision. Kruse, Freeman et Blasi (2010) 1 parlent de shared capitalism ou capitalisme de partage pour désigner toutes les formes de partage des profits proposées aux Etats-Unis : actionnariat salarié, participation aux bénéfices, intéressement, stock options. Kruse, Blasi et Freeman (2013) montrent également que le shared capitalism est le fruit d'une longue histoire. Il remonterait en effet au partage des terres conquises entre les citoyens américains instauré par les pères fondateurs des Etats-Unis 2 lors de l'expansion du pays vers l'ouest au 19 ème siècle. Le système actuel de participation financière américain a été instauré il y a quarante ans et a influencé les évolutions d'autres pays Anglo-Saxons. Le banquier Louis Kelso est le précurseur de la participation financière aux Etats-Unis. Dans les années cinquante, il est l'auteur de la « théorie binaire » qui présente une troisième voie entre capitalisme et socialisme et qui avait comme objectif d'éliminer la pauvreté dans le monde. Selon la théorie binaire, « lorsque la production devient de plus en plus capitalistique, la prospérité individuelle et une croissance soutenue nécessitent une participation large des individus à la production, non seulement en tant que travailleurs, mais également en tant que propriétaires de capital productif. » Les idées de Louis Kelso commencent à être mises en pratique dans les années 1970, lorsque celui-ci convainc le Sénateur Russell Long des bienfaits potentiels de l'actionnariat salarié. Cette prise de conscience politique se traduisit concrètement dans l'Employee Retirement Income Security Act en 1974, qui donne un premier cadre juridique aux Employee Stock Ownership Plans (ESOP), les plans d'actionnariat salarié les plus diffusés aux Etats-Unis. Par rapport à la France où l'actionnariat salarié est plus répandu dans les grandes entreprises, les ESOPs sont essentiellement plébiscités par les petites et moyennes entreprises. En développant les leveraged ESOPs ou plans d'actionnariat financés par endettement, Louis Kelso a fait de l'actionnariat salarié un moyen particulièrement efficace de transmission des PME aux salariés assorti d'avantages fiscaux importants. Mais ce n'est qu'à partir des Tax Reform Acts votés en 1984 et en 1986, et qui vont offrir d'importants avantages fiscaux, que le développement des ESOPs s'accélère. A la même époque, Martin Weitzman, professeur à Harvard, propose une théorie selon laquelle la généralisation des systèmes de partage des profits permettrait de juguler le chômage."
"On constate qu'il est très important d'avoir des méthodes d'évaluation des partitions afin d'évaluer les résultats des analyses typologiques. Cet article développe une méthode fondée sur des critères internes et externes permettant d'estimer l'adéquation entre les partitions devant être évaluées et les données à partir desquelles elles ont été générées. La première partie présente plusieurs critères internes et externes de validation en typologie, et les solutions retenues par le logiciel Evalu-P, conçu par le premier auteur. La seconde partie illustre les étapes de validation d'une partition hypothétique en cinq classes de consommateurs, issue des résultats d'une analyse sémiotique du discours des consommateurs sur le packaging. 43 partitions pour le riz et 54 partitions pour le shampoing ont été générées en utilisant des méthodes de classification hiérarchique et de réallocation. Ces partitions empiriques ont été évaluées par rapport à la partition hypothétique en utilisant cinq critères internes et cinq critères externes de validation."
"Cet article a pour objet d'étudier le territoire-produit de la marque et ses implications pour la stratégie d'extension de marque. Il se fonde sur l'étude des processus cognitifs d'organisation des connaissances et s'appuie sur la théorie de la catégorisation. Une étude empirique sur des produits cosmétiques permet de mettre en évidence la différence entre gamme réelle et gamme perçue qui forme la structure cognitive de la marque. Celle-ci est organisée en fonction de la typicalité perçue. De plus, cette structure a des implications pour le potentiel d'extension de la marque et la considération des extensions."
"Aujourd’hui nombreuses sont les organisations qui prennent en considération les effets engendrés par le stress professionnel et tentent de réduire le coût financier et social provenant de situations de travail vécues comme de plus en plus difficiles. Le phénomène du stress au travail devrait donc tout particulièrement intéresser les responsables RH en raison de ses conséquences sur le bien-être des salariés mais également sur le fonctionnement des organisations (baisse des conflits, meilleure entente, charge de travail mieux répartie, etc…). Plusieurs chercheurs pensent que l’intensité du stress a une influence sur le rendement : l’absence ou l’excès de stress jouant négativement sur la performance de l’individu. Sur le plan organisationnel, les conséquences négatives peuvent se traduire par l’absentéisme et la maladie, le roulement du personnel, les retards, des conflits avec les collègues."
"Le rôle du soutien social sur le bien-être au travail fait l'objet de recherches depuis de nombreuses années. Cette aide est souvent assimilée à un capital social que l'individu saurait plus ou moins bien manager. Si les modèles classiques prenaient généralement en compte les personnes physiquement proches du salarié étudié et les qualités de sociabilité de l'individu, leurs résultats sont désormais bien connus. Il s'agit pour nous d'aller plus loin en montrant que ce sont les individus les moins bien dotés en capital social et en qualités de sociabilité qui bénéficient le plus de leur comportement dans les réseaux virtuels. A travers une méthode quantitative (121 questionnaires collectés auprès d'utilisateurs des réseaux sociaux d'aide professionnelle), nous mettons en évidence le rôle des nouvelles formes de soutien. Nos résultats rendent justice à un nouveau modèle plus comportementaliste adapté aux salariés disposant de la sociabilité et du capital social les plus faibles."
"Un environnement turbulent ou instable sollicite les capacites d.innovation des organisations en meme temps qu.il rend plus difficile la construction d.un accord sur la forme et la portee des innovations a mettre en oeuvre. Dans le cas etudie, l.acceleration de l.action et la mise en tension de l.organisation se font dans un environnement qui rend visible l.incompletude initiale de l.innovation et qui permet aux acteurs de mener des experimentations imprevues. En interne, l.innovation de gestion se construit et se deploie en situation d.asymetrie d.information. En consequence un risque de crise du controle lui est inherent et des jeux comportementaux peuvent surgir et lui donner une forme finale difficile a anticiper. Ces experimentations se construisent en jouant sur la fragmentation de l.innovation, et sur la conservation d.une partie des pratiques habituelles. L.adoption partielle et l.entretien d.une option de retour, apparaissent comme des figures generiques du jeu face aux risques associes a l.innovation par les acteurs en presence."
"Une pensée spécifiquement méditerranéenne faite de sens de la mesure et notamment de sens de la lenteur peut engendrer des approches originales de la consommation et donc du marketing comme l'a prouvé le mouvement Slow Food dans l'alimentation. Afin de générer le même type d'approches dans la locomotion, une analyse ethnologique du phénomène rituel de la passeggiata est ici proposée et développée."
"Dans la lignée des travaux de François Perroux, nous discutons des conditions pour fonder une théorie de la participation des salariés sur le don. Nous constatons d'abord l'échec de l'articulation théorique des deux formes de participation des salariés dans l'entreprise : participation aux décisions et à la gestion et participation aux bénéfices et au capital. Les travaux empiriques constatent la complémentarité des deux formes de participation qui ne trouve pas de justification pour les théories contractuelles de l'entreprise. Afin d'expliquer cette complémentarité, nous proposons de nous référer à Perroux qui lie don et participation. For a gift based theory of employees' participation Abstract: Following the works of the French economist François Perroux, we propose to found a gift based theory of employees' participation. We first notice the failure of the theoretical articulation of the two forms of participation: participation in decision-making and management and financial participation. The empirical works show the complementarity between these two forms of participation which the theories of contracts fail to explain. In order to explain this complementarity, we suggest to refer to Perroux who links gift to participation. _________________________________ 1 Ce travail doit beaucoup à Pierre-Yves Gomez qui l'a inspiré et l'a fait évoluer par ses relectures et ses commentaires. Nous remercions également Xavier Hollandts, Olivier Masclef et les participants du GRACE pour leurs relectures et suggestions."
"L’article traite des rôles que les firmes peuvent faire jouer à leurs clients dans la logistique de dis- tribution de leurs produits. Il montre tout d’abord que quatre stratégies d’utilisation du client sont possibles, selon que la firme implique les clients dans la sortie des produits des magasins et/ou leur transport à domicile. L’article souligne ensuite que deux stratégies coexistent, avec d’un côté des firmes qui contraignent les clients à tenir un seul rôle et de l’autre celles qui les laissent libre sur ce plan. Enfin, cinq facteurs permettant de faire un succès des stratégies axées sur le rôle logistique des clients sont mis en exergue."
"Cet article a pour objectif de proposer un outil d’analyse, à partir des écrits de Machiavel (1513), des différents types de pouvoirs présents dans les relations sur les événementiels sportifs internationaux. Le choix de s’appuyer sur Machiavel se justifie par la présence en Sciences de Gestion de modèles basés sur Weber (1947), lui-même inspiré par l’auteur du « Prince ». Les résultats obtenus sur le terrain des événementiels sportifs nous permettront d’accepter l’outil créé et de mettre à jour la présence de trois types de pouvoir distincts présents dans les relations."
"Internet a permis l’émergence d’un nouveau type d’entreprises qui ne fournissent plus elles-mêmes les services qui leur sont demandés, mais connectent les consommateurs à des particuliers-fournisseurs afin qu’ils répondent à leurs besoins. Cette étude menée sur 47 plateformes d’hébergement collaboratif actives en France, porte sur les services réellement fournis par ces plateformes et la manière dont elles parviennent à maîtriser la qualité de service fournie par des particuliers."
"L'arrivée de la technologie de la blockchain a conduit les acteurs en charge de la régulation du secteur financier français à prendre différentes décisions pour intégrer cette technologie potentiellement disruptive pour le secteur. L'analyse, sur la période 2008-2018, de l'évolution du champ organisationnel associé à ce secteur montre qu'un processus de changement institutionnel est en cours, avec des acteurs en charge de la régulation, actifs sur le sujet, qui se comportent en entrepreneurs institutionnels. Dans quelle mesure une technologie peut elle contribuer à initier un processus de changement institutionnel ? C'est cette question que propose d'éclairer ce cas empirique."
"La doctrine sociale de l’Eglise (DSE) fait référence aux trois formes de participation des travailleurs. La participation à la propriété de l’entreprise prend la forme de la détention de parts sociales ou d’actions de l’entreprise par les salariés. La participation à la gestion de l’entreprise se traduit d’une part par l’information et la consultation des salariés par l’entremise des institutions représentatives du personnel et d’autre part par la présence de salariés dans les instances de gouvernement de l’entreprise. Enfin, les salariés peuvent participer aux profits de l’entreprise grâce à un accord de participation (obligatoire en France pour les entreprises de plus de 50 salariés). En France, les systèmes de participation sont le fruit de plusieurs influences dont celle qu’a eu l’enseignement social de l’Eglise sur Charles de Gaulle qui fut à l’origine de l’essentiel des systèmes de participation encore en vigueur en France. La doctrine sociale de l’Eglise fait de la participation un de ses principes. Au travers des textes des encycliques sociales, nous constatons que la DSE promeut la participation pour atténuer le conflit entre le « monde du capital » et le « monde du travail » et édifier une communauté, pour favoriser une juste répartition des richesses et la prospérité."
"Notre recherche a pour objectif d'investiguer les effets de la transformation du travail en lien avec l'intelligence artificielle, en termes de manifestations de stress, de construction de sens au travail et d'adaptation des individus. Nous mobilisons pour cela les théories en lien avec les ressources et le stress au travail, ainsi que la notion de sensemaking. A ce stade de nos travaux, nous envisageons d'adopter une méthodologie en deux temps. Nous partirons d'une démarche exploratoire et qualitative, par le biais d'études de cas, afin d'identifier les ressources dans ce processus d'adaptation. Puis nous élaborerons un modèle de recherche, dans lequel nous interrogerons le rôle médiateur ou modérateur des ressources. Cette recherche pourrait contribuer à une meilleure compréhension des effets de la transformation liée à l'intelligence artificielle sur les individus au travail et de prévenir les risques psychosociaux."
"Les organisations publiques locales françaises sont secouées par un Big Bang territorial (i.e. vagues successives de réformes) : en 2016, le nombre de régions passe de vingt-deux à treize (excepté les territoires d’outre-mer) et, entre 2014 et 2018, vingt-deux métropoles sont créées. Les métropoles et les régions ont la charge de définir et de mettre en œuvre les stratégies d’attractivité territoriale. Les managers territoriaux et les élus locaux de ces organisations publiques sont ainsi contraints par la loi NOTRé à travailler ensemble. Se pose alors la question de l’intégration (plus ou moins forte) de ces relations entre des organisations (pour certaines) récemment formées et celle du processus de structuration des stratégies d’attractivité territoriale, au sens du développement économique et social. S’inscrivant dans une position épistémologique pragmatiste au sens de Dewey, ce travail mobilise la méthodologie de l’enquête, dont la situation indéterminée repose sur la compréhension de la complexité du défi, qui se présente aux acteurs métropolitains et régionaux. Dès lors, la problématique à laquelle nous proposons d’apporter des réponses est la suivante : dans un contexte de relations inter-organisationnelles publiques contraintes, en quoi l’intégration des relations entre les managers territoriaux et les élus locaux influence-t-elle le processus de structuration des stratégies d’attractivité territoriale ? En cohérence avec la philosophie pragmatiste, le cadre théorique et conceptuel relève d’une approche transdisciplinaire, qui permet de formuler des questions et des propositions de recherche. L’objectif est d’abord de traiter du pourquoi à travers une première question de recherche : quels sont les apports du management public collaboratif au renouvellement du management public ? La dimension du comment se traduit par une deuxième question de recherche : quel est le processus de structuration des stratégies d’attractivité territoriale ? Cette question se structure autour de trois propositions de recherche (P) qui caractérisent les stratégies d’attractivité de duales (P1), conflictuelles (P2) et discutées (P3). Le quoi est alors abordé dans une troisième question de recherche : quelles sont les caractéristiques des relations inter-organisationnelles dyadiques entre les managers territoriaux et les élus locaux au sein des organisations publiques locales ? Cette question se structure autour de trois propositions de recherche, qui appréhendent les relations inter-organisationnelles dyadiques contraintes (P4), dynamiques (P5) et territorialisées (P6). L’enquête exploratoire hybride mobilisée repose sur deux études menées conjointement. L’étude 1 analyse les discours institutionnels formalisant les stratégies d’attractivité territoriale des métropoles et des régions françaises, à partir de deux analyses statistiques textuelles. L’étude 2 caractérise les relations inter-organisationnelles entre les managers territoriaux et les élus locaux dans le processus de structuration des stratégies d’attractivité territoriale. Le recours à une étude de cas multiples (i.e. quatre couples métropole-région) enchâssés offre une stratégie de recherche holistique facilitant une analyse longitudinale, basée sur une approche processuelle, et ce à partir de deux vagues d’entretiens semi-directifs auprès de soixante managers territoriaux et de huit élus locaux, traités à partir d’une analyse de contenu thématique. Les résultats de ces études permettent d’enrichir le cadre théorique et conceptuel du management public, des stratégies d’attractivité territoriale et des relations inter-organisationnelles, en prenant soin de fournir des connaissances actionnables par les praticiens en management. Une fois que la situation indéterminée de l’enquête est devenue déterminée, de nouvelles conditions émergent et génèrent de nouvelles situations indéterminées. Ainsi, nous soulignons les apports, les limites et les voies de recherche de ce travail."
"Ce travail doctoral concerne la haine envers la marque ressentie par les consommateurs. Présentée dans cette recherche comme impliquant une attitude globale négative envers une marque, elle est composée de plusieurs émotions négatives et engendre toujours un comportement. A partir de trois recherches exploratoires et de quatre recherches quantitatives, (1) nous définissons le concept en cinq dimensions ; (2) nous mettons en évidence l’attitude négative envers la marque en tant qu’antécédent de la haine envers la marque et de ses dimensions, ainsi que le bouche-à-oreille négatif et l’évitement de la marque comme leurs conséquences ; (3) nous identifions les variables modératrices de ces relations que sont le locus de contrôle, la réactance psychologique, l’estime de soi et la propension du consommateur à résister ; et enfin (4) nous dressons quatre profils des consommateurs haineux à partir des dimensions de la haine envers la marque et des données sociodémographiques."
"Notre thèse consiste à étudier la pratique actuelle de la RSE dans le contexte des compagnies libanaises à caractère industriel, et à examiner la relation entre les pratiques RSE d’une part et le management des risques d’autre part, en utilisant des techniques de statistiques inférentielles, des analyses factorielles exploratoires et des modèles de régression linéaire multiple. C’est dans ce dernier cas que la contribution principale de cette recherche a été réalisée. Ainsi, cette recherche a permis de percevoir la RSE comme étant plus qu’un simple outil de marketing et de relations publiques mais aussi un vrai outil influant le risque dans les entreprises. Notre recherche élargit la base de connaissances dans ce domaine dans le contexte libanais, en mettant l’accent sur le management et les pratiques de l’entreprise en terme de gestion du risque, afin de mieux gérer par la RSE les impacts sociaux, environnementaux, et communautaires de leurs activités. Les résultats de cette étude permettront aux chercheurs de créer une base théorique et empirique plus forte sur laquelle les recherches futures sur le sujet de la RSE et du management des risques par la RSE peuvent être développées."
"Cet article étudie la performance du portefeuille d'épargne d'entreprise d'une cohorte d'environ 30 000 employés d'une banque française cotée. Nous disposons d'informations détaillées sur chaque poste de travail dans la banque, ce qui nous permet d'étudier les connaissances financières, les connaissances spécifiques des plans offerts et les informations privées des employés. Ces employés de banque mieux informés sont censés adopter un comportement qui est le plus proche de celui d'un investisseur rationnel averti. Nous étudions la performance du portefeuille des employés dans les plans d'épargne et montrons que l'expertise financière et la connaissance des plans sont liés à la participation aux plans offerts par l'entreprise. L'expertise financière améliore la performance individuelle du portefeuille d'achat d'actions des employés (ESPP) mais pas celle du plan d'épargne d'entreprise (CSP) et à la performance globale des deux plans proposés. Dans les deux cas, la participation est plus probable parmi toutes les catégories d'emploi (y compris les experts financiers), les femmes, les employés plus diplômés et les employés plus contraints financièrement. Nous mettons en évidence la comptabilité mentale liée à l'actionnariat salarié mise en évidence par Benartzi et Thaler (2001)."
"L’étude de l’action collective dans la gestion des biens communs par des communautés auto-organisées est un thème qui a suscité une littérature originale sur la gouvernance et les formes institutionnelles alternatives depuis cinq décennies. La richesse de ces développements a été confirmée par l’attribution du Prix Nobel d’Économie 2009 à E. Ostrom. Les recherches de l’école de Bloomington d’Ostrom ont permis de découvrir plusieurs principes de conception des systèmes de ressources communes qui, lorsqu’ils sont présents, favorisent une gestion collective pérenne de ces ressources. Ces résultats ont montré leur pertinence au-delà des seuls systèmes de ressources physiques. Hess et Ostrom ont ainsi appelé la communauté scientifique à s’intéresser à la gestion collective des connaissances vues comme une ressource partagée. Cet article propose de répondre à cet appel en explorant les possibilités d’adaptation du cadre d’analyse des biens communs à des situations de coopération dans lesquelles des acteurs gèrent des informations mutualisées. La question de recherche étudiée a pour objectif de comprendre comment des individus peuvent s’auto-organiser au moyen d’institutions durables pour gérer la production et l’utilisation de connaissances communes. Nous avons mené cinq études de cas en mobilisant une méthodologie de recherche qualitative. Les résultats montrent que le fonctionnement sur le long terme des collectifs auto-organisés s’inscrivant dans le cadre conceptuel d’Ostrom semble conduire à une gestion durable et efficiente de la ressource informationnelle."
"Cet article s’intéresse à l’attractivité d’un territoire : la région de Kalouga en Russie. L’objectif porte sur le lien théorique et empirique entre action publique locale et localisation des entreprises multinationales (EMN) pour en expliquer l’attractivité. L’approche choisie est celle des ressources échangées afin de mettre en perspective l’action publique en matière d’attractivité (Colletis et Pecqueur, 2005). L’étude empirique est fondée sur l’analyse de 5 processus d’implantation d’EMN européennes dans la région de Kalouga. Les résultats révèlent que les ressources territoriales décisives mises à disposition des EMN par les acteurs territoriaux sont de nature idéelle, informationnelle et relationnelle. La qualité des pratiques de management territorial devient donc un facteur d’attractivité territoriale non négligeable."
"La crise économique et financière de 2008 a eu de graves conséquences sur les finances publiques des États. Ainsi, à partir des années 2010, de nombreux pays se sont tournés vers des politiques d’austérité pour tenter de restaurer leurs grands équilibres budgétaires. En France, les collectivités territoriales ont été directement impliquées dans cet effort de redressement. Elles ont dû s’adapter à un contexte inédit pour elles, marqué par une raréfaction des ressources et de nouvelles contraintes budgétaires. Notre recherche étudie le processus de transformation du contrôle de gestion territorial en contexte austéritaire. Nous mobilisons la théorie du changement de Poole et V an de V en (1995) pour éclairer les phénomènes de changement de contrôle de gestion. Cela nous permet d’étudier le processus de transformation du package de contrôle de gestion des collectivités territoriales en contexte austéritaire. Nous menons des études dans quatre collectivités territoriales ayant dû réagir au contexte austéritaire. Nos résultats montrent que le management de l’austérité se traduit par des transformations importantes dans le package de contrôle ne se limitant pas aux seuls éléments budgétaires et financiers. Nous observons également de grandes tendances dans l’évolution du package de ces quatre organisations malgré des trajectoires de changement différentes. Cela nous permet d’en tirer des conclusions sur les modalités et les enjeux des phénomènes de transformation du package de contrôle."
"L’actionnariat salarié transforme l’exercice des relations de pouvoir dans l’entreprise en pacifiant les relations entre actionnaires, dirigeants et salariés (ou dit autrement en alignant les intérêts de ces mêmes acteurs). Il constitue, dans la trilogie proposée par Chassagnon (2011)1, une forme de pouvoir de jure que l’on retrouve historiquement au coeur de l’entreprise capitaliste (une sorte de version étendue et adogmatique de l’interprétation propriétaire, voir Chassagnon et Hollandts, 2014) et qui sert sa dynamique concurrentielle et ses conditions de création de valeur."
"Noël, Pâques, l'Epiphanie, la Toussaint, la Saint Nicolas, etc., le marché a depuis longtemps ajusté son offre au rythme des fêtes religieuses catholiques en France et dans bon nombre de pays occidentaux. Pourtant, l'émergence de l'islam comme deuxième religion de France soulève de nouveaux enjeux économiques (Pras et Vaudour-Lagrâce, 2007) désormais pris en compte par les entreprises - e.g., Carrefour, Casino, Doux, Nestlé, Quick et Yum!. Dès lors, se pose la question de l'importance du respect des prescriptions et des interdits religieux pour ce type consommateur, lorsqu'il est confronté à une situation d'achat. Fondamentalement inductive, cette étude repose sur une dynamique itérative d'analyse de 15 entretiens semi-directifs et d'une dizaine d'observations participantes en contexte d'achat. Le cadre théorique posé, les résultats seront ensuite discutés avant d'aboutir à la proposition du modèle de religiosité situationnelle."
"Cet article propose de ne plus voir la religiosité du consommateur comme une caractéristique stable. Plutôt, et en s'intéressant aux ensembles culturels minoritaires, cette recherche présente la religiosité comme une ressource mobilisée ou une contrainte à laquelle le consommateur fait face en environnement d'achat. Le contexte précédent la situation et le marché sont identifiés comme les causes de l'impact contextualisé de la religiosité dans la prise de décision du consommateur. L'objectif de cette recherche est d'explorer le phénomène afin d'identifier les facteurs intervenant le processus."
"Cet article questionne la place des enseignants-chercheurs qui, dans la société d’aujourd’hui, subissent les effets des réformes initiées par la stratégie de Lisbonne sur l’éducation (Conseil européen de Lisbonne en 2000). À partir de l’analyse de la littérature en Sciences de Gestion, cet article montre que l’environnement économique, géopolitique et financier modifie profondément le rôle des enseignants-chercheurs dans la société française. Une modélisation souligne les deux facettes du métier d’enseignant-chercheur que sont l’enseignement et la recherche. Elle montre également le subtil équilibre entre ces deux rôles et la capacité d’adaptation nécessaire pour entrer en relation avec des publics et des résultats différents. Finalement les indicateurs de performance retenus par l’État ambitionnent de montrer l’inefficacité des enseignants-chercheurs car ils s’appuient sur des critères inappropriés. Cet article offre des pistes de réflexion pour intégrer des indicateurs plus pertinents qui évaluent la performance des enseignants-chercheurs à l’échelle sociétale."
Cet article étudie sous un angle culturel les attitudes des directeurs des ressources hu-maines à l'égard de la diversité. Trente-cinq DRH de multinationales aux Pays-Bas et au Maroc ont participé à une enquête qualitative. Nous avons mobilisé l'approche GLOBE (Global Leadership and Organizational Behavior Effectiveness) comme base de comparai-son entre les deux pays. Les résultats confirment que les attitudes envers la diversité évoluent et montrent le rôle important de la culture dans ce domaine. Ils soulignent notamment l'importance de la tradition dans le contexte marocain.
"Cet article vise à comparer les bénéfices que les consommateurs retirent de la consommation nostalgique aux avantages différentiels que les entreprises dégagent d’une gestion de marque basée sur la nostalgie. Il s’appuie sur deux études qualitatives complémentaires, l’une auprès de 45 consommateurs, l’autre auprès de 10 responsables marketing d’entreprises. De cette comparaison découlent des évaluations tant qualitatives que quantitatives de l’efficacité opérationnelle de la nostalgie dans le management des marques."
"L’objectif de cette recherche est de comprendre le sens du pourboire pour les personnes qui en donnent et pour celles qui en reçoivent. Une étude qualitative menée auprès de clients et de personnels de plusieurs secteurs a permis de mettre au jour quatre interprétations différentes du pourboire, du point de vue des acteurs. Chaque interprétation inscrit le pourboire dans une dimension différente de la relation de service - la production du service, le contrat, la relation sociale ou la relation interpersonnelle - et construit le pourboire soit comme une indemnisation, soit comme un don. Cette compréhension approfondie du pourboire permet de formuler plusieurs recommandations managériales."
"Le présent chapitre vise à discuter l’importance et la valeur du don à l’université en général et, plus particulièrement, au sein du CNU. Le don de celui qui reçoit et qui a reçu d’une communauté, notamment en termes de formation, et qui donne à son tour. Un tel don souffre de non-reconnaissance car souvent l’intérêt calculé y est opposé. Dans une première section sont brièvement présentées les théories du don, et dans une seconde section est développé un parcours du don au sein de la section 06 du CNU."
"Cette recherche s’intéresse à la transparence dans un environnement numérique, examine la manière dont elle est perçue par les clients au travers de différentes évaluations (limpidité, objectivité, ouverture perçues), et étudie comment chacune de ces dimensions affecte la confiance et l’engagement des clients envers la marque. Elle souligne que les jugements de transparence diffèrent selon le rapport que les consommateurs ont personnellement développé vis-à-vis de leur environnement numérique (littératie, perspicacité, préoccupation pour la vie privée). A partir d’une étude empirique réalisée dans l’e-commerce (N= 445), les résultats montrent que la limpidité perçue des pratiques – contrairement à l’objectivité perçue – s’accompagne d’une baisse de confiance et impacte directement l’engagement. En revanche, l’ouverture perçue incite à l’engagement mais pas à la confiance. Cette recherche souligne combien la littératie et la perspicacité du client favorisent la perception de transparence alors que la préoccupation pour la vie privée la dégrade. Des implications théoriques et pratiques sont ensuite tirées de cette recherche."
"Cet article a pour objectif de comparer les réactions affectives cognitives et conatives aux annonces avec endosseur célèbre, par rapport aux annonces avec modèle inconnu et aux annonces avec produit seul. Les variables dépendantes sont l’attitude envers l’annonce, l’attitude envers la marque, la congruence avec l’image de soi et l’intention d’achat. Une quasi-expérimentation a été menée sur 2134 femmes. Les résultats montrent que les caractéristiques de la célébrité (crédibilité et congruence avec la marque) influencent l’efficacité de l’endossement. Les annonces avec endosseur célèbre sont plus efficaces que les deux autres types d’annonces que lorsque la célébrité est perçue comme crédible et congruente avec la marque."
"code3000.net, vous connaissez ? Mais si, rappelez-vous ! Vous vous êtes probablement entraîné dessus pour réussir votre code de la route, à moins que ce soit pour votre ASSR ; l’attestation de sécurité routière que l'on passe au collège, souvenez-vous ! code3000.net, start up leader et pionnière dans la préparation en ligne du code de la route, voit aujourd’hui sa position bousculée du fait des modifications du code, de l’arrivée massive sur internet des acteurs historiques du secteur en auto-école voire du développement des applications sur smart phones aux dépens des sites internet. Il est demandé aux étudiants de comprendre le modèle d’affaires (Business Model) et les opportunités entrepreneuriales saisies par code3000 pour évaluer sa position concurrentielle et suggérer des actions à mettre en place pour l’avenir."
"En mobilisant les notions « d'attention » et « d'apprentissages », le projet de cet article est de comprendre comment, à partir de la création commune d'un système informatisé d'échanges de données, des mutuelles d'assurance automobile ont construit un « réseau d'attention interorganisationnel », support d'échanges d'informations et de connaissances. Nous montrons, en particulier, que ce réseau est à la fois inscrit dans un processus historique de coopération entre ces mutuelles et résulte d'apprentissages liés à la mise en commun du système d'échanges de données."
"Entre acteurs isolés et dispositifs institutionnels globalisés, les réseaux sociaux constituent à la fois un niveau d'analyse et un mécanisme de transformation d'un champ organisationnel. Cet article décrit la structure et l'évolution de réseaux sociaux spécifiques, les réseaux d'administrateurs, en Europe entre 2000 et 2003. Ces réseaux revêtent des caractéristiques variées en termes de taille, de forme et de structure. Les résultats de cet article montrent que les interlocks régionaux restent marginaux. Ces résultats alimentent la controverse actuelle sur la convergence des institutions économiques en montrant la persistance des particularismes nationaux."
"Confrontés à des difficultés pour assurer la relève dans les cabinets d'audit, les experts-comptables et les commissaires aux comptes cherchent des solutions pour attirer les jeunes dans les filières d'études comptables. Intégrant l'orientation professionnelle dans le champ d'étude de la carrière et notamment de la socialisation organisationnelle, une étude qualitative auprès de 20 étudiants engagés dans des études supérieures en comptabilité vise à tester les cinq facteurs d'orientation synthétisés dans le modèle d'Allard et Ouellette (2002) et à faire émerger les stratégies d'orientation. L'orientation professionnelle apparait comme un processus de mise en adéquation des compétences (cognitives et professionnelles) d'un côté, et de la personnalité et les valeurs professionnelles de l'autre côté, ayant pour objectif l'épanouissement (imaginé) dans un milieu professionnel. L'orientation se déroule par étapes avec le souci de conserver des portes ouvertes le plus longtemps possible. Mots clés : Orientation professionnelle, socialisation organisationnelle, étudiants en comptabilité. Remerciements: L'auteur adresse ses remerciements à Nathalie Gonthier-Besacier avec qui le projet a émergé et les données ont été collectées."
"Cet article propose une synthèse des recherches ayant mobilisé le concept de distance psychologique autour des questions liées aux comportements prosociaux du consommateur, à la digitalisation de la consommation et aux pratiques marketing. Ce travail s’appuie sur une méta-synthèse de 584 articles issus de revues en psychologie et en marketing. Il permet une consolidation et une validation des connaissances du concept de distance psychologique dans ces différentes thématiques du marketing. Il montre également comment la distance psychologique peut être actionnée par les managers afin d’améliorer les réponses du consommateur. Les managers peuvent agir directement sur la distance en la réduisant ou l’augmentant pour profiter des bienfaits de la proximité ou de l’éloignement psychologique. Ils peuvent également ajuster les variables du marketing-mix en fonction de la distance psychologique expérimentée par le consommateur pour fluidifier le traitement de l’information et, in fine, améliorer les réponses de ce dernier."
"Notre étude cherche à identifier, dans un cadre théorique articulant les approches juridico-financières et cognitives de la gouvernance, le profil-type des investisseurs participant au financement d’un rachat d’entreprise par son propriétaire (owner buy out, OBO). En nous appuyant sur une grille d’analyse regroupant les facteurs explicatifs d’une prise de participation dans le cadre d’un OBO, nous avons procédé à une analyse typologique fondée sur une classification hiérarchique ascendante en deux étapes (two-step cluster analysis). Nos résultats, obtenus grâce à une enquête administrée auprès d’un échantillon de 42 investisseurs financiers spécialisés opérant en France, ont fait émerger deux classes d’investisseurs présentant des caractéristiques clairement différenciées. La différence entre les deux classes est d’ordre disciplinaire et cognitif et concerne essentiellement la perception qu’ont les investisseurs des enjeux stratégiques d’une opération OBO et la nature des indicateurs conduisant à la maîtrise du couple risque/rentabilité caractérisant un tel montage."
"En adoptant un cadre théorique articulant les théories juridico-financières et cognitives, notre recherche a pour ambition d’expliciter les mécanismes de gouvernance mobilisés par les investisseurs en capital dans le cadre des opérations de Owner Buy Out (OBO). Elle propose une étude qualitative exploratoire et inédite menée auprès de 17 investisseurs opérant sur le territoire français et spécialisés dans les opérations de capital développement et de transmission. Nos résultats mettent en évidence au travers d’une série de propositions le caractère contingent des mécanismes de gouvernance spécifiques aux opérations OBO, lequel dépend étroitement du profil dont se réclame le capital investisseur."
"Confrontés à des difficultés pour assurer la relève dans les cabinets d’audit, les experts-comptables et les commissaires aux comptes cherchent des solutions pour attirer les jeunes dans les filières d’études comptables. Intégrant l’orientation professionnelle dans le champ d’étude de la carrière et notamment de la socialisation organisationnelle, une étude qualitative auprès de 20 étudiants engagés dans des études supérieures en comptabilité vise à tester les cinq facteurs d’orientation synthétisés dans le modèle d’Allard et Ouellette (2002) et à faire émerger les stratégies d’orientation. L’orientation professionnelle apparait comme un processus de mise en adéquation des compétences (cognitives et professionnelles) d’un côté, et de la personnalité et les valeurs professionnelles de l’autre côté, ayant pour objectif l’épanouissement (imaginé) dans un milieu professionnel. L’orientation se déroule par étapes avec le souci de conserver des portes ouvertes le plus longtemps possible."
"La pandémie de Covid-19 conduit les organisations à accélérer les pratiques de travail collaboratif à distance. L’article propose de s’inspirer des organisations qui pratiquent l’Open Source pour manager ces activités. À partir d’une analyse de seize entretiens d’experts, la recherche identifie trois caractéristiques clefs transférables aisément à d’autres industries – le rôle de la communication, la reconnaissance des apports de chacun, l’utilisation d’outils et principes collaboratifs – et deux autres envisageables à moyen terme – la décomposition des tâches en modules et le développement d’un modèle d’affaire spécifique."
"Notre étude cherche à identifier, dans un cadre théorique articulant les approches juridico-financières et cognitives de la gouvernance, le profil-type des investisseurs participant au financement d’un rachat d’entreprise par son propriétaire (owner buy out, OBO). En nous appuyant sur une grille d’analyse regroupant les facteurs explicatifs d’une prise de participation dans le cadre d’un OBO, nous avons procédé à une analyse typologique fondée sur une classification hiérarchique ascendante en deux étapes (two-step cluster analysis). Nos résultats, obtenus grâce à une enquête administrée auprès d’un échantillon de 42 investisseurs financiers spécialisés opérant en France, ont fait émerger deux classes d’investisseurs présentant des caractéristiques clairement différenciées. La différence entre les deux classes est d’ordre disciplinaire et cognitif et concerne essentiellement la perception qu’ont les investisseurs des enjeux stratégiques d’une opération OBO et la nature des indicateurs conduisant à la maîtrise du couple risque/rentabilité caractérisant un tel montage."
"L’objectif de cet article est de comprendre les effets des pratiques de développement des compétences sur la performance adaptative, à travers la satisfaction des besoins fondamentaux. La technique des Partial Least Squares (PLS) a été appliquée aux données collectées lors d’une étude menée en 2015 auprès d’un échantillon de 161 salariés roumains de l’industrie automobile. Nos résultats montrent que le développement des compétences est positivement corrélé à la performance adaptative, avec une médiation partielle de la satisfaction des besoins fondamentaux. Le développement sur un poste de travail disposerait ainsi le salarié à mieux assumer des rôles inhabituels induits par la gestion d’un changement organisationnel."
"À partir de l’étude de cas unique d’une société d’accélération de transfert de technologie (SATT) performante, l’article vise à mettre en évidence la tension paradoxale entre valeur micro-économique et valeur territoriale, perçues, et la manière dont celle-ci est managée. Les résultats témoignent d’un processus d’apprentissage, au sein de la SATT investiguée, permettant la mise en œuvre d’une logique de combinaison alliant des pratiques organisationnelles paradoxales de différenciation et de dialogue qui se manifestent à un niveau stratégique, opérationnel et individuel."
"L’objectif de cet article est d’analyser la perception et l’acceptation de produits innovants dans le secteur végétal. Une étude exploratoire montre des perspectives envisageables en matière d’innovation à la fois sur ses caractéristiques intrinsèques (couleur) et sur ses caractéristiques extrinsèques (packaging). Les résultats mettent en évidence 1) les difficultés d’acceptation de modifications sur le végétal en raison de sa spécificité (nature vivante) ainsi que du comportement du consommateur à son égard et 2) une approbation de l’innovation au niveau du packaging si ce dernier valorise le produit végétal, respecte la naturalité et facilite la catégorisation."
"Croissance rapide de nouvelles formes de vente, stratégies de mutualisation des ressources logistiques, phygitalisation, plateformisation, politique de réenchantement des points de vente... Autant de bouleversements du paysage économique qui font désormais de la distribution une fonction clé de la gestion des entreprises. Comprendre et expliquer à la fois sa dynamique et ses enjeux revêt, à ce titre, une importance majeure. Rédigé par trois spécialistes reconnus du sujet, l'ouvrage se fixe pour objectif d'aborder de façon pédagogique les principales dimensions de la distribution : la relation avec l'acheteur final, la gestion des interfaces entre industriels et distributeurs, et les enjeux stratégiques liés à l'organisation du canal. En rapprochant les plus récentes recherches internationales de l'analyse approfondie des pratiques et stratégies du secteur, la troisième édition de ce livre, devenu un ""classique"" dès sa sortie, et abondamment cité depuis lors, propose une synthèse originale et ambitieuse sur un certain nombre d'interrogations très actuelles. Etayé de très nombreux exemples et illustrations, il s'adresse à un large public d'étudiants, de chercheurs et de praticiens qui souhaitent prendre connaissance des tendances d'évolution les plus significatives en matière de management de la distribution et de comportements de ses principaux acteurs."
"La participation des salariés à la gouvernance de l’entreprise fait l’objet d’un débat récurrent. D’un côté, certains soulignent l’affaiblissement potentiel de la fonction de contrôle du conseil d’administration alors que d’autres indiquent au contraire que la présence des salariés diminue le niveau d’asymétrie informationnelle. Au-delà de ce débat académique mais aussi managérial, nous revenons dans le cadre de cet article sur la fonction mais surtout les apports potentiels des salariés au sein de la gouvernance de l’entreprise. Dans le cadre de cet article nous tentons de dégager un cadre d’analyse global de la fonction et du rôle des administrateurs salariés. Nous suggérons notamment que l’efficacité de leurs mandats est conditionnée par trois séries de facteurs : des facteurs individuels, des facteurs endogènes à la gouvernance de l’entreprise concernée et des facteurs institutionnels relatifs au système national de gouvernance considéré. Nous avançons par la suite une série de propositions théoriques et opérationnelles permettant de rendre plus efficace la représentation des salariés au conseil."
"Ce Chapitre d'ouvrage soutient que, si la nouvelle Politique Agricole Commune a raison de vouloir laisser les marchés agricoles coordonner l’offre et la demande, elle a tort de délaisser les solutions de laisser faire en matière de production de biens alimentaires sains et d’aménités rurales de qualité.Le développement de cet argument s’organise autour de trois sections : la première évalue les résultats de l’ancienne PAC à l’aune de ses propres objectifs, la seconde présente les objectifs et les instruments de la nouvelle PAC et la troisième ébauche les solutions de laisser faire en matière agri-environnementale. Cet article dessine ainsi les contours d’une politique agricole alternative qui donnerait une place centrale à l’entrepreneur—agriculteur ou autre—dans la recherche de solutions aux attentes des consommateurs et des citoyens et stimulerait de nouvelles formes de coopération entre acteurs locaux."
"Cette recherche analyse l’effet des caractéristiques de la célébrité, en termes de crédibilité et de congruence avec la marque, sur l’image de marque. Une quasi-expérimentation a été menée sur un échantillon de 942 consommatrices. Les résultats confirment l’impact positif de la crédibilité et de la congruence de la célébrité sur l’image de marque. Le rôle médiateur de la congruence avec l’image de soi et le rôle modérateur de la familiarité avec la marque et de l’attitude envers la marque antérieure à l’exposition sont également étudiés."
"Face aux enjeux environnementaux et sociétaux de notre époque, le consommateur responsable fait figure de modèle. Cependant, il est, lui aussi, amené à transgresser consciemment ses normes personnelles. La présente recherche identifie les raisons et circonstances qui incitent les consommateurs responsables à enfreindre leurs normes personnelles et apporte un éclairage sur la façon dont ces consommateurs gèrent leurs comportements déviants de leurs convictions environnementales grâce aux stratégies de coping. Au terme d’une collecte de données empiriques s’appuyant sur la technique des incidents critiques, cette recherche propose une typologie des comportements allant à l'encontre des normes personnelles des consommateurs responsables (1) et met à jour l'influence de facteurs émotionnels, sociaux et situationnels dans l’apparition de ces déviances (2). Enfin sont abordées les différentes stratégies de coping mises en place par le consommateur responsable (3)."
"L’actionnariat salarié constitue un élément central des politiques d’épargne salariale développées par les entreprises contemporaines. Il contribue en général à soutenir la performance des entreprises tant sur le plan économique que social. Dans le cadre de cet article, nous suggérons, à rebours de la littérature, que la rotation du personnel et l’absentéisme peuvent inciter les entreprises à développer l’actionnariat salarié afin de limiter les comportements de retrait de leurs collaborateurs. Nous testons nos hypothèses de recherche au moyen d’une étude de cas économétrique portant sur un groupe français de construction coté. Nos données, mesurées tant au niveau individuel que des différentes filiales de ce groupe, portent sur 15000 salariés et sur cinq années consécutives. Nos résultats montrent notamment que la rotation et l’absentéisme du personnel sont significativement associés à l’investissement des salariés en actions de leur entreprise. Nos résultats apportent ainsi un nouvel éclairage sur les déterminants de l’investissement en actions de l’entreprise, en envisageant notamment qu’un lien de causalité différent puisse être envisagé."
"Cette recherche s'intéresse aux déterminants de la remise à plus tard d'un achat en ligne dans le cas particulier où ce dernier finit par être réalisé. Elle examine sept causes potentielles de ce comportement et montre l'effet significatif de trois d'entre elles : La non-urgence, l'incertitude du besoin et le risque lié à la réalisation immédiate de l'achat."
"A l’heure où la loi PACTE va peu à peu étendre la représentation des salariés à la gouvernance de l’entreprise, on peut considérer que la gouvernance des entreprises françaises est peu à peu en train de muter en s’ouvrant notamment aux salariés, ce qui pose de façon plus large la question de la potentielle réforme de l’entreprise. Se poser cette question c’est envisager un changement de paradigme. En premier lieu, c’est considérer que les décisions stratégiques de l’entreprises ne concernent pas seulement les dirigeants et les actionnaires les plus influents. C’est symboliquement accepter autour de la table (du conseil) des salariés, généralement relégués au rang de simples collaborateurs ou d’apporteurs de force de travail, pourtant indispensables au fonctionnement concret de nos entreprises contemporaines. C’est également ouvrir la porte à la diversité des profils au sein des conseils, s’éloignant incidemment du phénomène de petit monde, souvent utilisé pour décrire le milieu souvent fermé voire secret de la gouvernance des sociétés (Kogut 2012). Envisager la réforme de l’entreprise et la participation des salariés c’est également se replonger dans des débats qui furent vifs autour de la question de la démocratie industrielle et des relations sociales (Mouriaux 2013). Dans le cadre de cet article, nous proposons une lecture historique de la question de la possible participation des salariés à la gouvernance de l’entreprise, souvent envisagée comme un idéal de participation démocratique à l’intérieur de l’entreprise (Chapas et Hollandts 2017). Depuis la fin de la seconde guerre mondiale, c’est plus de soixante-dix ans d’histoire des relations sociales et de la gouvernance participative qui peuvent se lire autour d’un enjeu et de débats autour de la question essentielle de la réforme de l’entreprise. Notre analyse longitudinale et socio-historique s’appuie principalement sur les principaux jalons (schématiquement un par décennie) qui cristallisent à un moment donné les éléments du débat autour de la réforme de l’entreprise. Une attention particulière est portée sur le rapport Sudreau (1975) qui aurait pu constituer un tournant décisif pour la réforme de l’entreprise. Notre article entend contribuer sur plusieurs éléments. Sur le plan de la gouvernance, il retrace historiquement les textes fondateurs et les racines de la participation des salariés à la gouvernance et plus généralement des alternatives à la gouvernance actionnariale. Sur le plan de l’histoire des idées, cet article documente également la réduction du projet de réforme de l’entreprise qui s’étiole au fil des décennies pour se réduire au final à la question (technique) du nombre de salariés au conseil, bien loin des premières tentatives qui entendaient fonder de nouvelles sociétés et de nouvelles approches en matière de gouvernance. Enfin, notre article discute également de façon critique le cadre actuel d’association des salariés à la gouvernance et s’interroge sur la réforme de l’entreprise, en tant qu’institution centrale du capitalisme contemporain."
"L'ouvrage présente et illustre les 350 notions les plus représentatives du champ de la stratégie dans un style à la fois accessible et rigoureux. Il traite des différentes facettes de la stratégie : diagnostic, management stratégique, manoeuvres concurrentielles, innovation, changement, pilotage stratégique.. Chaque entrée propose la traduction anglaise du terme proposé, une présentation simple du concept et une illustration concrète. Une attention particulière est portée aux renvois entre les entrées et à leur complémentarité, afin de veiller à donner de la cohérence à l'ouvrage. L'équipe de quatre auteurs dispose de compétences et de champ d'intérêt très complémentaires."
"Dans un environnement concurrentiel prégnant, les universités font de leur rayonnement un levier de performance. Aussi, en tant qu’organisation professionnelle, leur performance repose en partie sur celle des enseignants-chercheurs qu’elles hébergent et donc, de leur propre rayonnement scientifique à un niveau individuel. A cet égard, la littérature tend à appréhender le rayonnement des enseignants-chercheurs uniquement à travers leurs publications et leur impact en termes de citation. Cette approche néglige la dimension sociale du métier d’enseignants-chercheurs et apporte une vision trop restrictive du rayonnement scientifique&nbsp;; d’autres pratiques de rayonnement ne sont pas prises en compte alors même qu’elles sont, d’une part, effectives, et d’autre part, reconnues et valorisées par des instances telles que le CNU ou l’HCERES. Cette recherche, inscrite dans une perspective RBV où le rayonnement est une source d’avantage concurrentiel, vise ainsi à enrichir la littérature en étudiant le concept de rayonnement scientifique des enseignants-chercheurs et en explorant les dimensions qu’il recouvre dans le cadre de sa mesure.&nbsp;"
"<p>Les SSII font face à une problématique de taille&nbsp;: fidéliser les consultants pour faire face à un taux de turnover élevé. Fonctionnant en équipesprojets, les SSII doivent également les mobiliser collectivement. Pour répondre à ce double enjeu, nous étudions en quoi l’engagement multiple permet d’affiner la compréhension du lien entre mobilisation collective et intention de quitter dans les SSII.</p>"
"L'utilisation croissante de Technologies de l'Information (TI) non autorisées a des conséquences qui dépassent le cadre organisationnel. Ces pratiques, non officielles et créatives, sont regroupées sous le nom de shadow IT (Information Technology), et échappent au contrôle des directions informatiques. Les recherches antérieures menées sur les répercussions du shadow IT ont souligné ses effets positifs sur la motivation et la performance des utilisateurs, sur le renforcement de l'innovation dans les organisations, et l'influence négative sur la stabilité comme sur la sécurité du système informatique officiel. Notre analyse, au travers des théories néo-institutionnelles, permettrait de considérer ce phénomène comme contribuant à la construction, à différents niveaux, de la légitimité de l'acteur métier, entrepreneur institutionnel. Ce phénomène serait également capable de se diffuser dans le champ institutionnel de l'organisation par l'entremise de processus isomorphiques, modifiant sa logique."
no abstract
no abstract
"Le rôle de la force de vente dans le contexte BtoB a évolué au cours des dernières décennies. Les commerciaux n’ont pas simplement pour mission de vendre, mais aussi de construire une bonne qualité de relation avec leurs clients professionnels pour mieux les fidéliser. A partir d’une revue de littérature complétée par une étude qualitative, les auteurs identifient 21 facteurs de performance relationnelle des commerciaux (FPR), qui contribuent à la qualité de relation avec les clients. Puis, ils hiérarchisent l’importance de ces facteurs dans l’amélioration ou la dégradation de la qualité de relation. Pour cela, ils introduisent la notion de poids fluctuants des FPR, dépendant de leur performance perçue, en parallèle de poids stables, dans la construction de la qualité de relation (modèle Tétraclasse : Llosa, 1997), et testent le modèle sur un échantillon de 202 commerçants et artisans. La classification des FPR en 4 catégories suggère des recommandations managériales en termes de recrutement, formation et management de la force de vente."
no abstract
"Objectifs de recherche : Les commerciaux B2B n’ont pas simplement pour mission de vendre, mais de construire une bonne qualité relationnelle avec leurs clients professionnels pour mieux les fidéliser. Cette recherche vise à identifier les leviers de la performance relationnelle des commerciaux et leurs contributions respectives à la qualité relationnelle. Méthodologie : A partir d’une revue de littérature, les auteurs dressent une liste de 21 éléments de performance relationnelle des commerciaux (EPR). Puis, à l’aide du modèle tétraclasse (Llosa, 1997) testé sur un échantillon de 202 clients artisans et commerçants, ils hiérarchisent la contribution de ces 21 éléments à la formation de la qualité relationnelle en les classant en 4 catégories : éléments basiques, clés, plus et secondaires. Résultats : Les résultats montrent que les comportements de vente orientés clients sont basiques, les éléments liés à la culture client sont clés, et ceux liés aux extra-rôles des commerciaux (rôle d’ami, rôle proactif) sont plus. Implications managériales/sociétales : Cette recherche permet de hiérarchiser les actions RH à entreprendre auprès des commerciaux (formation, culture client, recrutement) pour améliorer la qualité relationnelle, en fonction de la valeur client et du degré d’intensité concurrentielle. Originalité : Habituellement utilisé pour expliquer la construction de la satisfaction dans le domaine des services en B2C, tétraclasse porte ici sur la qualité de la relation avec les commerciaux dans le domaine B2B. Il élargit ainsi sa portée conceptuelle et sa validité externe."
"En comparant deux étapes de la chaine de valeur, deux niveaux de luxe, deux types de pays de délocalisation, cette recherche constitue une des premières validations empiriques des effets de la délocalisation pour les marques françaises du luxe. Menée sur un échantillon de 278 consommateurs de marques de luxe, les résultats montrent que le transfert du design ou de la fabrication de la France vers l’étranger affecte leur qualité perçue et leur évaluation globale. L’article permet aux gestionnaires des marques de luxe de savoir sous quelles conditions et dans quelle mesure leurs marques souffrent de la délocalisation."
no abstract
"Sous le contrôle de l’entreprise, l’expérience vécue par le client au contact direct du service représente une source précieuse d’image et de capital pour la marque. Cet article montre qu’au-delà du nombre d’expériences directes, le contenu et l’intensité avec laquelle le consommateur vit ces diverses expériences influencent fortement la force de l’image de marque. En s’appuyant sur de solides bases théoriques et empiriques cet article propose un ensemble de recommandations managériales qui placent l’expérience au centre des stratégies de la marque de service."
"La co-création du client a pris de l’ampleur ces dernières années, étant considérée comme un levier majeur pour générer de la valeur pour l’entreprise et le client. Néanmoins, pour que cette valeur supplémentaire existe, il faut que le client maîtrise son rôle. L’entreprise doit donc former ce dernier au script de service. Pour se faire, elle dispose de deux moyens : l’aide proactive, qui consiste à former le client en amont de la prestation, et l’aide réactive, qui est un appui pendant la rencontre. L’objectif de cette recherche est de comparer l’influence de ces deux aides sur la maîtrise du script, le montant d’achat et la satisfaction client. Se fondant sur une étude de 500 observations, cette communication montre que seule l’aide proactive améliore la maîtrise du rôle, l’aide réactive permettant uniquement de détecter les clients en difficulté. D’un point de vue managérial, ces résultats incitent à reconsidérer les systèmes d’aides existants qui se fondent principalement sur des aides réactives."
"La participation du client ne cesse de croître au sein des processus de services afin d'augmenter la rentabilité des prestataires. Pourtant, l'activité de cet employé temporaire que constitue le client peut se révéler une source de coûts, si ce dernier participe mal faute de compétences ou de motivation. Les auteurs abordent le concept de motivation uniquement sous un angle rationnel. Ils expliquent ainsi que pour bien participer le client doit percevoir le maximum de bénéfices et le minimum de coûts. En s'appuyant sur une étude quantitative auprès de 436 répondants, cette recherche montre qu'une deuxième route motivationnelle fondée sur les émotions apparaît quand le client est face à une situation différente de la situation attendue. Ainsi en fonction de situations vécues par le client et des routes motivationnelles stimulées, la réponse de l'entreprise devra être adaptée."
"Les expériences de magasinage sont des expériences vécues seul ou accompagné. Avec un accompagnant, un client passe plus de temps en magasin et dépense plus. Qu'en est-il selon le type d'accompagnant (ami, conjoint, famille) ? Quand la connaissance du magasin du client varie ? L'objectif de cette recherche est de mesurer, en fonction du type d'accompagnant, l'effet modérateur de la connaissance du magasin sur la relation entre accompagnement, temps passé en magasin et montant d'achat. Cette recherche montre que l'écart de temps passé en magasin entre client accompagné (conjoint ou membre de la famille) versus seul est d'autant plus fort que les clients connaissent bien le point de vente, le temps passé impactant alors le montant d'achat."
"Cette communication propose une revue systématique de la littérature en marketing portant sur l’économie du partage et de la consommation collaborative. Notre étude se base sur l’analyse bibliométrique de 126 articles publiés entre 2010 et 2019 dans 44 revues spécialisées en marketing, écrits par 299 auteurs travaillant dans 34 pays différents, et représentant un total de 6308 références. Elle témoigne de l’intérêt grandissant pour ces nouvelles formes d’échange et de consommation entre pairs. Différentes analyses (couplage bibliographique, co-citations et co-occurrences) nous permettent ainsi de (1) délimiter les contributions majeures provenant de la recherche en marketing et distinguer, à l’inverse, des axes de recherche sous exploités; (2) identifier les principaux acteurs de ce domaine (pays et revues scientifiques) ainsi que les fondements théoriques et les concepts mobilisés; enfin (3) appréhender l’évolution des différentes thématiques étudiées. Sur la base de l’ensemble de nos résultats, des pistes de recherche future sont proposées telles que la dimension internationale, les plateformes digitales, les fournisseurs de service, le développement durable et la gouvernance."
"Alors que le nombre de publications scientifiques traitant de l’économie du partage et de la consommation collaborative a augmenté de façon très significative ces dernières années, aucun consensus clair sur une définition commune ne semble se dégager. Le périmètre même du phénomène reste sujet à de nombreux débats. Notre objectif est de comparer, à travers une revue de la littérature, les définitions existantes en fonction d’une liste de différents critères. A partir du constat qu’il existe une multitude d’acceptions et interprétations différentes, nous optons pour une approche différente basée sur le caractère innovant du phénomène: quels types de pratiques ont émergé récemment, qui n’existaient pas auparavant ? Quelles sont leurs caractéristiques et leurs spécificités ? Nous proposons de nommer ce nouveau type d’échanges ‘services collaboratifs’."
no abstract
"Résumé : Les parcs naturels régionaux français (PNR) ont, parmi leurs objectifs, la mission d’expérimenter sur leur territoire des innovations et activités nouvelles. Ce sont ces expérimentations qui intéressent cet article, en particulier celles portant sur la valeur sociale. Il y est question d’investiguer le système d’intelligence collective qui émerge de l’engagement des parties prenantes lors du développement de ce type d’innovations. En d’autres termes, sous quelle forme se dessine l’intelligence collective territoriale lors de la création et le développement d’innovations sociales ? Pour ce faire, il est étudié deux structures, l’une française, l’autre allemande. La comparaison de ces parcs permet de mettre en exergue des différences techno-structurelles et culturelles en matière d’élaboration d’innovations sociales. Les résultats obtenus, montrent que si la technostructure française est, par son organisation, soumise à des injonctions, elle prône particulièrement la collaboration inter-sectorielle et forme un soutien solide. A contrario, la technostructure allemande ne préconise pas d’objectifs en faveur de l’innovation et semble présenter un niveau de distance hiérarchique plus élevé. Malgré ces distinctions, les résultats indiquent des innovations dans les deux cas étudiés mais donnent un avantage à la version française. Ils permettent de confirmer le rôle du PNR tantôt initiateur tantôt coordinateur (voire les deux) des systèmes d’intelligence collective inducteurs d’innovations sociales."
"Les contributions réunies ici s'intéressent à ce que pourraient être les conditions d'un multiculturalisme réussi, dont deux dimensions sont ici privilégiées : celle de conflits religieux qui n'ont cessé d'être instrumentalisés au service de causes moins saintes que profanes ; celle des modalités d'intégration dans un espace que les institutions internationales ne sont manifestement pas parvenues à unifier, laissant aux organisations privées la charge de reprendre à leur compte cette responsabilité sociale dont les sociétés civiles portent la demande pressante et qui paraît bien détenir la clé de la soutenabilité d'un avenir commun."
"Cette recherche s’intéresse aux motivations à la vente d’occasion entre particuliers (C to C). Trois études (focus- group, entretiens semi-directifs et questionnaire) montrent que les motivations ne sont pas uniquement écono- miques mais que ce qui donne du sens à la vente d’occasion supplante l’aspect financier : la volonté de s’opposer à la société de consommation (dimension protestataire), celle de retrouver le plaisir de l’échange (dimension récréationnelle) et celle de prolonger la vie des objets (dimension générative). La vente via Internet répond aux motivations économiques tandis que les vide-greniers et vide-dressing répondent davantage à des motivations récréationnelles, protestataires ou génératives."
"Cet article jette un éclairage sur une stratégie publicitaire adoptée par certaines marques grand public : l’utilisation de mannequins publicitaires séniors dans le but de séduire cette cible de consommateurs. Pour mieux comprendre les mécanismes d’action de l’âge du mannequin, une étude est menée auprès de 277 femmes de 60 à 70 ans. Elle établit que l’âge du mannequin publicitaire, lorsqu'il est appréhendé en fonction de l’âge subjectif des réceptrices, est à l’origine d’un sentiment de similarité. Celui-ci a des effets positifs sur l’évaluation de la marque présente dans la publicité pour deux catégories de produit (alimentaire et cosmétique)."
"Cet article analyse les attitudes et les représentations des publics vis-à-vis de l’art contemporain afin d’en appréhen- der les résistances et d’en inférer des implications marke- ting. Des études qualitatives menées auprès du grand public (entretiens semi-directifs et netnographie) ont permis d’ap- préhender les différents registres de résistance. Les résultats permettent d’identifier trois dimensions majeures de résis- tance : économique, affective et herméneutique. Les recom- mandations afin de répondre à ces résistances s’articulent autour de deux axes : redonner du sens et des émotions au public, restaurer la confiance entre les différents acteurs du marché."
"Cet article analyse les attitudes et les représentations des publics vis-à-vis de l'art contemporain afin d'en appréhender les résistances et d'en inférer des implications marketing. Des études qualitatives menées auprès du grand public (entretiens semi-directifs et netnographie) ont permis d'appréhender les différents registres de résistance. Les résultats permettent d'identifier trois dimensions majeures de résistance : économique, affective et herméneutique. Les recommandations afin de répondre à ces résistances s'articulent autour de deux axes : redonner du sens et des émotions au public, restaurer la confiance entre les différents acteurs du marché."
"Lemaitre N. et De Barnier V. (2015), Quand le consommateur devient commerçant : motivations, production d'expérience et perspectives, Décisions Marketing, 78, 11-28. Résumé Cette recherche s'intéresse aux motivations à la vente d'occasion entre particuliers (C to C). Trois études (focus-group, entretiens semi-directifs et questionnaire) montrent que les motivations ne sont pas uniquement écono-miques mais que ce qui donne du sens à la vente d'occasion supplante l'aspect financier : la volonté de s'opposer à la société de consommation (dimension protestataire), celle de retrouver le plaisir de l'échange (dimension récréationnelle) et celle de prolonger la vie des objets (dimension générative). La vente via Internet répond aux motivations économiques tandis que les vide-greniers et vide-dressing répondent davantage à des motivations récréationnelles, protestataires ou génératives. Mots-clés : revente C to C, dépossession volontaire, marché de l'occasion, motivations, canal de distribution. Abstract When consumers become merchants: motivations, experience production and prospects This research examines the motivations for secondhand sales between consumers (C to C). Three studies (focus group, semi-structured interviews and questionnaires) show that economic motivations are not the only ones and that motivations linked to giving sense to the secondhand sales supplant the financial aspect: the desire to oppose the consumerist society (protesting dimension), to find pleasure in the exchange (recreational dimension) and to extend the life of objects (generative dimension). Internet is preferred when the economic dimension prevails, while car boot sales respond to recreational, protesting and generative motivations."
"Cet article analyse les perceptions et les attitudes des enfants vis-à-vis des musées afin de mieux comprendre leurs attentes. L’objectif de cet article est de comprendre la manière dont les enfants perçoivent la dimension esthétique des musées. Des études qualitatives menées auprès d’enfants âgés de 7 à 12 ans (entretiens semi-directifs) ont permis d’appréhender les différentes approches de ces lieux culturels. Les résultats permettent d’identifier des perceptions et attitudes différenciées selon le degré de familiarité avec ce type de lieu. Les recommandations s’articulent autour d’un axe majeur : développer les aspects ludiques, expérientiels et sensoriels des musées afin que les enfants se sentent acteurs et parties prenantes de cet univers."
"Cet article est consacré à la consommation collaborative. Il est axé sur le comportement des consommateurs dans les services d’hospitalité en France. Après une cartographie des principales plateformes collaboratives, dans ce domaine, nous avons sélectionné quelques prestataires significatifs et avons mis en œuvre une approche « netnographique » en comparant les sites Web des fournisseurs et en analysant les contenus des blogs et forums où les consommateurs partagent leurs expériences."
"Jean-Daniel Reynaud, Professeur honoraire de sociologie du travail au Conservatoire national des arts et métiers, s’est éteint le 27 janvier 2019. À travers cet article collectif, les auteurs souhaitent lui rendre hommage en discutant l’apport de la théorie de la régulation sociale aux sciences de l’organisation et plus spécifiquement, à l’analyse des phénomènes de régulation dans les entreprises. La diversité des perspectives prises par les auteurs et des « terrains » qu’ils explorent montrent l’intérêt à convier la TRS pour appréhender la complexité des entreprises contemporaines et comprendre les enjeux qui les traversent."
"Ce travail propose de configurer l’idéologie sous jacente aux théories partenariales à travers les différentes écoles de pensée éthique. Focalisées sur la notion de bien commun ou d’intérêt général, les théories partenariales semblent avoir une grammaire commune qui, au delà des différences entre écoles de pensée, conduit à définir un mécanisme de prise de décision éthique fondé sur la notion d’empathie rationnelle d’Adam Smith. Nous suggérons que les tenants de cette approche partenariale devraient promouvoir une politique éducative favorable à des prises de décisions justes. Cette politique éducative repose sur des enseignements spécifiques dans lesquels la littérature et le cinéma devraient permettre de sensibiliser les dirigeants à la complexité de ce type de décision."
"En France, le conseil en lobbying est une activité à la fois récente et mal perçue, contrairement à ce qui se pratique aux États-Unis. Il est vrai que l'influence de la décision publique est une profession particulièrement sensible, tant sur le plan managérial que sur le plan sociétal. C'est pourquoi l'éthique (comprise dans son sens général comme la morale appliquée aux affaires) peut y jouer un rôle central. Cette communication s'interroge sur les pratiques et les enjeux de l'éthique dans le conseil en lobbying. Le terrain privilégié dans cette étude exploratoire du sujet est la France. Le cas d'un cabinet de conseil en lobbying est plus particulièrement développé, fruit d'une observation participante de trois mois. Il est complété par des données secondaires sur la profession en France et aux États Unis ainsi que sur les institutions françaises, européennes, américaines et québécoises. Les résultats de cette recherche sont développés autour de deux axes : 1. La pratique de l'éthique du lobbying diffère suivant l'âge et le degré d'institutionnalisation de la profession dans le pays ; en particulier en France on observe une éthique informelle au faible potentiel régulatoire et basée essentiellement sur l'exemplarité. 2.1. Les enjeux de l'éthique sont à la fois une structuration interne de la profession de conseil en lobbying mais également en externe une clarification des relations entre ces organisations et leurs différentes parties prenantes (notamment les clients, l'État et la société civile). Ce sujet suggère des opportunités de recherche future sur les principes et les procédures éthiques appliqués au lobbying, comme instrument de légitimation et de diffusion de bonnes pratiques, dans le cadre du développement de la responsabilité sociale des entreprises."
"L'attractivité et la compétitivité des territoires sont des préoccupations assez traditionnelles pour les responsables politiques. Elles ont néanmoins connu un grand essor et un surcroît de diffusion depuis le début des années 2010 et la détérioration de la situation économique en France. Nous les retrouvons donc au coeur des discours des décideurs publics, en particulier en ces périodes économiques difficiles, mais plus seulement. Ces thématiques sont aujourd'hui très largement reprises par des parties prenantes, représentant des intérêts privés (chefs d'entreprise, représentant de groupes industriels, etc.). Les récents mouvements de protestation bretons illustrent cela. Progressivement, les questions de compétitivité ont gagné la scène de l'intervention publique territoriale : comment les territoires, et les autorités qui en ont la responsabilité légale, peuvent-ils se positionner par rapport aux choix de localisation des activités publiques et privées ? Il s'agit d'être choisi pour accueillir des activités variées (production industrielle ou tertiaire, événementiel, tourisme, économie résidentielle, etc.), permanentes ou temporaires. En effet, la compétitivité des territoires désigne leur capacité à contribuer à la création et au développement d'activités économiques, à attirer et retenir les hommes comme les capitaux (Prager, 2005, p.33). Cette capacité à être choisi comme localisation d'activités et à répondre aux pressions de la « concurrence » qu'il subit, est considérée comme déterminante pour assurer un niveau de développement satisfaisant aux acteurs de ce même territoire (Hernandez, 2008). Certaines définitions insistent d'ailleurs davantage sur les conséquences sociales de la compétitivité territoriale. C'est souvent le cas des institutions internationales qui la définissent usuellement comme la capacité d'un espace à améliorer et accroître durablement le niveau de vie de ses habitants. La Déclaration de Lisbonne de l'Union Européenne (2000) complète cette définition avec la mention « avec des emplois plus nombreux et de meilleure qualité et une plus grande cohésion sociale » (Debonneuil, Fontagné, 2003). L’essor de la notion de compétitivité territoriale s’est accompagné de celle d’attractivité territoriale. Elle est rarement définie en tant que telle, car elle est le plus souvent envisagée comme une composante essentielle de la compétitivité territoriale. Généralement, elle recouvre l’ensemble des éléments tangibles et intangibles susceptibles d’attirer ou de conserver un acteur (personne physique ou morale et ses activités) ou les flux qu’il génère (par des productions, des consommations, des investissements , des transferts sociaux, des subventions, etc.) sur un territoire. Cette attirance a une vocation soit permanente (habitants, entreprises, centres de recherche, etc.), soit temporaire (touristes, visiteurs, etc.), en fonction des cibles établies (Hernandez, 2008). Parmi les champs d’intervention publics pouvant (devant ?) permettre aux territoires (et aux entreprises) de sortir victorieux de la course à la compétitivité, nous trouvons en bonne place l’événementialisation, dans les domaines sportifs ou culturels. Nous nous concentrons ici sur ce dernier. L’organisation d’événements, d’ampleur très variés, est aujourd'hui devenue une norme du management public territorial. Les attentes des acteurs locaux envers ces manifestations, dans toute leur diversité, sont nombreuses et pour dire, assez hétérogènes. Nous trouvons des externalités positives espérées, peu hiérarchisées et présentées parfois de façon confuse. Des recherches significatives ont été menées sur ces questions, dont quelques unes portent sur les événements territoriaux de Capitale européenne de la culture. Mais cette événementialisation n’est pas un phénomène spécifiquement européen ou occidental. En effet, en Afrique et en Asie, des événements assez similaires dans leur appellation, se déroulent également depuis 2005 : il s’agit des Capitales de la culture islamique. Trois villes obtiennent chaque année ce titre : respectivement pour l’aire arabe, africaine et asiatique. Ainsi, pour l’aire arabe, plusieurs villes méditerranéennes ont été l’hôte d’un tel événement, à l’image d’Alexandrie en 2008 ou plus récemment de Tlemcen en 2011. Nous avons déjà eu l’occasion de comparer le management territorial des métropoles marseillaise et tlemcennienne (Hernandez, Belkaid, 2013). Nous souhaitons aborder cette comparaison à présent sous l’angle de l’événementialisation culturelle : Quelles sont les attentes associées à l’événement mises en avant dans des documents publics (presse, parties prenantes organisatrices, observateurs scientifiques) ? Est-ce que celles associées à Marseille 2013 sont les mêmes que celles associées à Tlemcen 2011 ? Et est-ce que ce sont les mêmes que celles qu’on retrouve dans la littérature ? Pour répondre à ces interrogations, nous présentons tout d’abord une analyse de la littérature disponible sur ces questions, en insistant particulièrement sur les retombées espérées ou engendrées par les événements « Capitale de la culture ». Nous exposons ensuite la combinaison d’analyses statistiques textuelles (menées grâce au logiciel IRAMUTEQ) que nous avons mobilisée pour étudier deux de ces événements : Tlemcen 2011 et Marseille-Provence 2013. Enfin, nous révélons les résultats de notre étude empirique et les mettons en perspective avec les apports de la littérature dans la dernière partie de cette communication (discussion)."
"Le crowdsourcing et le design thinking sont des démarches d'innovation fortement médiatisées et mobilisées par les entreprises pour faire émerger des idées de nouveaux produits et services. Pourtant, on connaît encore mal leurs apports et leurs limites dans la phase d’idéation. Grâce à une étude de cas dans laquelle une entreprise a mobilisé ces deux démarches en parallèle, permettant une comparaison et une analyse détaillées, nous montrons que le crowdsourcing permet de générer un grand nombre d’idées au regard du temps investi et d’identifier les tendances du marché, sans être toutefois plus rapide ou plus économique. Le design thinking apparaît, quant à lui, comme une grille d’analyse pertinente pour donner du sens aux idées générées par le crowdsourcing, mais ne propose pas toujours de solutions opérationnelles."
"Le e-learning poursuit sa progression dans le paysage de la formation en entreprise. Cet ensemble de modalités multimédias de formation peut être considéré comme révolutionnaire, et impliquant des changements de méthodes de formation qui ne sont pas neutres (P. Gil, 2000). La question de la conduite du changement s’avère capital pour ce type de projets (B. Bachimont et al. 2002). Cette progression s’accompagne en effet de taux d’abandon qui peuvent rester important (C. Baujard, 2007, 2008 ; E. Bernardin, 2006 ; K. Frankola, 2001 ; M-D. Lytras et A. Pouloudi, 2001) et remettre en cause l’efficacité du e-learning dans les dispositifs de formation. Très souvent, ces projets sont mal gérés par les entreprises et engendrent ainsi des pertes appelés « coûts cachés » (H. Savall et V. Zardet, 2003). Les modes de déploiement des parcours e-learning auprès des salariés dans les grandes entreprises deviennent alors un enjeu primordial pour tenter de faire face à cette situation, le but étant alors de favoriser l’engagement des salariés dans leurs parcours e-learning.Quelles sont les différentes dimensions du déploiement d’un projet e-learning ? Comment influent-elles sur l’engagement des salariés ?Pour répondre à ces questions, nous avons mené quatre études de cas enchâssées dans un même contexte organisationnel, celui d’un constructeur automobile international. Quarante-huit entretiens avec des salariés ont été conduits. Les résultats ont permis de relever que les dimensions technologiques et pédagogiques d’un déploiement constituent le socle de la réussite de celui-ci. La dimension organisationnelle, avec l’environnement social, permet d’individualiser et d’adapter le déploiement aux salariés."
fr_abstract_s
Présentation
"I have a dream,Présentation"
"(À propos de Soc., 18 nov. 2020, n° 19-15.795, F-P+B)"
Le meilleur ou le pire des mondes ?
"Le développement des technologies de l'information dans ce que l'on appelle la « société de l'information » est à l'origine de nouveaux usages, de nouveaux échanges qui imposent, de manière coercitive, la mise en place de nouveaux modèles économiques et juridiques. Les contraintes du droit d'auteur et surtout son immatérialité étaient alors mal comprises voire ignorées par le corps social. Récemment les parlementaires français posaient dans des circonstances exceptionnelles le principe de l'illégalité du téléchargement entre internautes. Il faudra conjuguer : juste et équitable rémunération des auteurs, prise en considération des contraintes juridiques internationales avec la garantie de l'accès à la connaissance et à l'information pour tous, conformément à l'engagement de Tunis. La question de la rémunération des auteurs apparaît donc comme un des aspects majeurs de la gouvernance de l'Internet."
L'application de l'article L. 522-3 du code de justice administrative dans le contentieux de la voie de fait
"L'analyse des responsabilités des constructeurs en matière de risques du sol amène à deux constats.D'une part, l'attribution des responsabilités est parfois insatisfaisante, ceci est remarquable en responsabilités contractuelle et délictuelle : une plus grande rigueur dans les rôles dévolus à certains constructeurs est nécessaire. D'autre part, le déficit de communication entre le technicien et le juge est patent ; cet aspect intéresse avant tout la responsabilité décennale et les causes d'exonération de responsabilité : la qualité de l'information délivrée par le technicien est à améliorer.En matière contractuelle, la mission VISA de l'architecte nécessite une réévaluation. Les missions L et Av du contrôleur technique devraient être indissociables en secteur urbain. Le contrat de construction de maison individuelle serait amélioré par l'auto information du constructeur (consultation des documents géologiques disponibles), outre une assurance risques du sol. En matière délictuelle, le fondement réel retenu pour l'application de la théorie des troubles anormaux de voisinage aboutit parfois à des décisions iniques.Les causes d'exonération posent difficulté à travers la force majeure ; une démarche progressive intéressant l'extériorité, l'imprévisibilité, l'irrésistibilité pourrait constituer une voie de rationalisation. Concernant les responsabilités en jeu après la réception, il est souhaitable de se référer aux fonctions stabilité, protection, usage pour rationaliser les approches de l'ouvrage et de l'élément d'équipement, outre l'extension de la notion d'indissociabilité"
"En cas de contestation sérieuse d'une créance déclarée à la procédure collective et d'incompétence du juge-commissaire, l'article R. 624-5 du Code de commerce impose au magistrat de désigner dans son ordonnance la partie ayant la charge de saisir la juridiction compétente. Cette obligation suscite une abondante jurisprudence, tant sa mise en œuvre et ses conséquences sont parfois méconnues. Retour sur l'application délicate de cet article R. 624-5."
"Dans le cadre de la procédure administrative contentieuse, les recours ne sont pas pourvus d’un effet suspensif. Par conséquent, les autorités administratives ont la possibilité d’exécuter les décisions contestées jusqu’à la décision juridictionnelle. Cette situation, constitutive d’un principe, est le produit d’une philosophie contentieuse marquée par un déséquilibre à l’avantage de l’administration. Le principe de l’absence d’effet suspensif en est une des formes d’expression les plus directes tant il est susceptible de grever la protection des droits des requérants. Son étude permet indirectement de questionner la pertinence contemporaine des fondements idéologiques du droit et du contentieux administratif. Sa déconstruction fait apparaitre son caractère désuet au regard des évolutions en cours. Celles-ci, qu’elles soient propres au phénomène juridique ou qu’elles le dépassent, servent à déterminer les caractéristiques attendues des recours contentieux. La recherche d’une solution équilibrée, à mi-chemin entre efficacité administrative et protection des requérants, devient un impératif. Celui-ci sera atteint à partir d’une reconstruction de cet aspect épineux de la procédure administrative contentieuse en s’appuyant sur une analyse matérielle de la situation litigieuse."
"La résiliation du bail d’habitation prononcée par le juge-commissaire à la demande du liquidateur judiciaire doit respecter les conditions imposées par l’article L. 641-11-1, IV, du Code de commerce, relatif aux contrats en cours, et se conformer aux dispositions de la loi 6 juillet 1989 sur les baux d’habitation. Ces règles ne sont pas exclusives les unes des autres, mais se complètent."
"L’apparition d’un acte de Justice régulé selon les règles substantielles et processuelles adoptées par l’État façonne la construction d’une véritable activité professionnelle attachée à l’acte de Justice étatique : il s’agit de la professionnalisation du juge. Durant un certain temps, la mission de Justice devient l’affaire du peuple avec la création des juridictions de l’ordre judiciaire composées de citoyens en totalité ou en partie. Cependant, plusieurs problèmes sont observés dans l’exécution de l’acte de Justice étatique par ces citoyens. Face à ces difficultés, la conception de l’exercice de la mission de Justice étatique doit être modifiée: celle-ci ne peut pas être considérée comme une simple occupation. Il s’agit d’une activité dont la complexité implique une approche professionnelle dans sa mise en pratique. Il en résulte la création d’un parcours formateur à l’attention de ceux qui souhaitent faire acte de Justice étatique et l’établissement d’un chemin hiérarchique à l’attention de ceux qui souhaitent faire carrière dans la magistrature. Toutefois, face aux évolutions de la Règlementation et de la Société, des faiblesses dans la représentation initiale de l’exécution de l’acte de Justice étatique apparaissent : l’acte de Justice étatique ne doit plus seulement être juridiquement cohérent avec un rôle judiciaire précis attribué à chacun. Il doit être également spécialisé et surtout rapide. Il s’ensuit une confrontation entre l’acte de Justice étatique tel qu’il est exécuté conformément à ces nouvelles exigences et les nécessités de la mission de Justice telle qu’elle a été initialement apprise par les juges."
"Lors d'une procédure collective, la restructuration du capital peut être décisive pour sauver l'entreprise en difficulté, mais tous les associés n'y adhèrent pas nécessairement. À côté de la cession de droits sociaux et de l'augmentation de capital forcées, la conversion de créances en titres, mise en avant par la loi de 2015, peut être une solution pour restructurer la dette d'une société en passant outre l'opposition des associés."
"Le présent ouvrage vise à développer une analytique du ""concept d'arbitrage"" à partir de la mise en oeuvre de celui-ci dans le domaine sportif et ceci dans le but de développer notre compréhension du ""concept juridique d'arbitrage"". Le respect de la règle tout comme la transgression de la règle vont donc permettre de cerner le concept d'arbitrage sous une forme innovante puisqu'il s'agit d'analyser les rapports entre arbitrage sportif et arbitrage juridique et d'étudier le rôle de chaque intervenant, c'est-à-dire aussi bien de celui qui doit veiller à la bonne application de la règle mais aussi de celui qui est tenu au respect de la règle. Le champ de l'arbitrage gagne de manière continue du terrain et suscite, dès lors, de réels problèmes de définition. Entre les diverses formes de l'arbitrage n'existe bien souvent qu'un ""air de famille"". Etendre cette étude à l'arbitrage sportif a donc permis de cerner les contours du concept. Le respect de la règle, qu'elle soit juridique ou sportive, est donc le point d'achoppement entre ces deux disciplines. Ce rapprochement pouvait, certes, sembler quelque peu artificiel mais il est nécessaire à l'affirmation de la pertinence du concept ainsi qu'à la détermination de la structure interne de ce concept. C'est d'ailleurs ce qui a animé les débats, riches et divers, d'un colloque qui s'est tenu à la Faculté de droit de Toulon à l'initiative de deux doctorants. L'ouvrage transcrit la confrontation des débats entre droit public et droit privé, entre sportifs et juristes et entre spécialistes des questions d'arbitrage juridique et sportif."
"Il ne fait plus de doute que le divorce fait partie intégrante du paysage familial. Pour autant le droit au divorce a vacillé entre droit permissif et droit coercitif. La loi du 11 juillet 1975 a insufflé un mouvement de libéralisation qui va régner sur le droit du divorce. Mais c'est avec la loi du 26 mai 2004 que la libéralisation va prendre toute son ampleur. Même si elle s'inscrit dans une certaine continuité en maintenant la pluralité des cas de divorce, elle ouvre plus largement les portes de celui-ci. Les règles de fond et de forme sont étroitement liées dans le procès du divorce. La loi a donc supprimé de nombreuses barrières procédurales qui ont eu pour conséquence directe de simplifier la procédure et favoriser l'obtention du divorce. Les époux doivent respecter les obligations procédurales pour parvenir au prononcé du divorce. Les règles procédurales absorbent ainsi les règles substantielles. L'ouverture découle également d'une objectivation du droit du divorce. La cause de divorce trouve essentiellement sa source dans le constat d'échec du mariage. La loi a d'ailleurs consacré un véritable divorce faillite pour ne pas dire droit au divorce sur demande unilatérale qui n'exige qu'une cessation de vie commune pendant deux ans pour être prononcé. Elle fait également triompher la dimension individuelle sur la conception institutionnelle de l'union. Les arrangements constitutent la pierre angulaire du règlement du divorce. Les accords se retrouvent à tous stades de la procédure et dans tous les cas de divroce. L'ordre public conjugal connait donc un certain infléchissement corroboré par un relâchement de la faute dans le divorce et l'instauration d'un droit commun des effets du divorce. La réunion de ces différents facteurs contribue à l'émergence d'un droit subjectif au divorce."
"Depuis que Motulsky lui a consacré une réflexion décisive, le concept d'office du juge est couramment utilisé en doctrine.Toutefois, la juridiction administrative n'a eu recours à cette notion et ne l'a utilisée dans ses décisions que depuis quelques années. Nous retrouvons désormais régulièrement la notion d'office du juge dans le cadre des porcédures de référé. En effet, le contentieux administratif a connu une profonde réforme de ses procédures de référé avec la loi n°2000-597 du 30 juin 2000 relative au référé devant les juridictions administratives. D'anciennes procédures, comme le sursis à exécution devenu le référé-suspension, ou le référé-conservatoire, ont été modernisées ; et en parallèle le législateur a créé une nouvelle procédure - le référé-liberté - appelée à jouer un rôle de premier plan dans la protection des libertés fondamentales. D'autres procédures de référé ont également bénéficié de cette réforme pour connaître une certaine modernisation comme ce fut notamment le cas pour le référé-provision. La rénovation des procédures de référé d'urgence a ainsi marqué le point de départ d'une évolution de l'office du juge administratif des référés et le recours à la notion d'office du juge des référés a permis de justifier son intervention. Cette mutation de l'office du juge des référrés a tout de même préservé une certaine continuité jurisprudentielle. L'office du juge administratif des référés conditionne tous les aspects des procédures de référé. en effet, cette notion intéresse à la fois les conditions d'intervention du juge des référés et les mesures qu'il ordonne, mais également la façon dont sont contrôlées ses ordonnances."
Ce mémoire étudie les différents textes européens ayant institué le mécanisme de coopération policière opérationnelle que sont les équipes communes d'enquête ; comment ces dispositions sont prises en compte par les textes français et leur intégration à notre droit pénal national.
"Appliquée depuis le 1er janvier 2020, la réforme de la procédure civile issue de la loi du 23 mars 2019 de programmation 2018-2022 et de réforme pour la justice a des incidences en matière de procédures collectives. Si les modifications procédurales demeurent limitées, le recours obligatoire aux modes de résolution amiable des différends laisse en revanche apparaître des incidences conceptuelles plus considérables."
"2e édition</br> La 4e de couverture indique : ""Le présent ouvrage de droit constitutionnel est écrit par quatre auteurs dans un style direct, simple et détaillé. Il est tout d'abord destiné aux étudiants de Licence en Droit et AES 1re année en abordant la ""Théorie du droit constitutionnel"" et ""l'histoire constitutionnelle française"" généralement traités au 1er semestre ainsi que le ""Droit constitutionnel de la Ve République"" étudié au 2e semestre. Il est aussi susceptible d'intéresser des étudiants plus confirmés, des doctorants ou des spécialistes de la discipline avec des parties moins habituelles relatives aux ""Droits constitutionnels comparés"" et à des ""Éléments de droit constitutionnel européen"
"Notre système juridique laisse apparaître un déséquilibre entre droits et devoirs qui place le créancier dans une situation inconfortable face à un débiteur de mauvaise foi. L’ampleur de la crise économique mondiale, qui persiste depuis près d’une décennie, a institué un climat d’incertitude. Le manquement à l’obligation d’exécution des contrats en est une des conséquences principales, notamment l’inexécution du débiteur envers son créancier. De ce fait, le règlement est devenu la préoccupation majeure. Certes, l’inexécution est condamnée par le législateur, dans le but de conserver un équilibre entre les parties au contrat. Toutefois, malgré les nombreuses et fructueuses évolutions, une observation plus minutieuse de la situation fait émerger une surprotection sous-jacente du débiteur qui s’est vu octroyer de nombreux privilèges. A contrario, la place du créancier présente des signes d’affaiblissement dans notre système juridique et fonctionnement économique. Cette situation est préoccupante, dans la mesure où elle induit un ralentissement de l’économie nationale, dû à une fuite avérée des investissements en dehors de notre territoire, au profit d’autres pays européens. Cet état de fait se présente comme un problème subjectif mais également objectif, et il est nécessaire d’étudier les divers obstacles qui se dressent devant le créancier lors de la procédure de saisie du logement familial du débiteur. Le terme « obstacles » renvoie aux contraintes que le créancier doit prendre en compte, en amont mais aussi tout au long de la procédure de saisie, qui l’empêchent de recouvrer sa créance et par conséquent de réinvestir. Il est donc essentiel de proposer des solutions afin de répondre à un besoin de rééquilibrage des forces en présence, dans un contexte économique dégradé, qui suppose une adaptation du cadre législatif."
"Le contrat est un instrument juridique que tout individu utilise quotidiennement. Pour vendre, acheter, louer, prêter, fiducier, distribuer, arbitrer ou encore mandater, le contrat est incontournable. Cet instrument essentiel à la vie économique et sociale est envisagé par le droit français à deux niveaux : il existe ainsi une théorie générale, complétée par des régimes spéciaux. Pour appréhender la mise en œuvre de cette théorie générale, l’étude des différents régimes de contrats spéciaux est indispensable. Ces régimes spéciaux permettent de compléter, de déroger et d’adapter les règles contenues dans le droit des obligations. Cet ouvrage présente les principaux régimes de contrats spéciaux de façon didactique : les contrats opérant un transfert de propriété, les contrats portant sur l'utilisation de la chose, les contrats portant sur des services, les contrats portant sur la distribution et ceux portant sur la résolution des litiges."
"La réforme du 5 mars 2007 relative à la protection des majeurs a instauré un certain nombre de dispositifs permettant la reconnaissance des droits fondamentaux de la personne protégée. Applicable à plus de 700 000 personnes, la loi pose un certain nombre de difficultés notamment concernant l’articulation entre la nécessité de protéger et le besoin d’autonomie. L’étude du droit du majeur protégé à disposer de son corps pour en faire don illustre bien cette recherche de proportion, mais permet d’identifier les difficultés liées à la nécessité de consentement pour les actes médicaux. Ce travail présente l’analyse des dispositions spécifiques prévues pour le majeur protégé en matière de don de cellules hématopoïétiques, le don d’organes du vivant, le don « post-mortem », le don de sang, le don du corps à la science et le don de résidus opératoires. Il aborde enfin les différentes solutions juridiques permettant au majeur protégé de défendre ses intérêts et analyse leurs limites."
"La gestion du Cabinet (Partie 1), la vie au sein d'un cabinet Cabinet d'avocats (Partie 2), la défense des intérêts du majeur protégé (Partie 3)."
"L'Union européenne repose sur un ferme engagement à promouvoir et à protéger les droits de l'homme, la démocratie et l’État du droit dans le monde. Les droits de l'homme sont au cœur des relations de l'UE avec d'autres pays et régions. Cette thèse examine de manière approfondie les implications fondamentales résultant de la concurrence ou de la coopération entre les juridictions de Strasbourg et de Luxembourg à la lumière des derniers développements concernant le système de protection des droits fondamentaux au sein de l’Union européenne comme l’adhésion de l’Union européenne à la Convention européenne des droits de l’homme. La protection des droits de l'homme reste un sujet de discussion infinie. L’attitude générale à l’égard d’un catalogue plus exhaustif et contraignant des droits de l’homme est essentiellement positive, bien que les problèmes constitutionnels et juridictionnels d’un tel document contraignant ne puissent être ignorés."
"La filiation est un lien de droit dont la seule donnée biologique ne saurait rendre compte de la richesse et de la complexité. Le droit qui la régit forme un système de preuve qui varie dans le temps et dans l'espace en fonction de l'objectif qu'il poursuit, des contingences politiques et économiques, des influences sociétales et supranationales.Avec l'ordonnance du 4 juillet 2005, il s'est agi d'égaliser et de sécuriser les liens de filiation. Cependant, les rédacteurs n'ont pas instauré d'unification parfaite : il demeure des distinctions entre la maternité et la paternité parfois associées à des distinctions selon qu'il existe ou non un engagement conjugal entre les parents, de même qu'il y a toujours une différence de régime entre la filiation charnelle et la filiation par greffe. Parmi les divergences, il en est (particulièrement dans les domaines qui ont été exclus de la réforme) dont la suppression serait bienvenue au regard de la cohérence du droit. Il en est d'autres dont l'abolition est plus sujette à controverses. Enfin, il en est dont la disparition serait regrettable parce qu'elles témoignent de ce qu'est véritablement La Filiation. On rencontre ces distinctions dans le non contentieux, où leur réduction aurait pour conséquence de faire perdre à la filiation tout aspect symbolique, toute signification. C'est dans une analyse de ces règles qu'il faut rechercher la signification de ce lien. La filiation est un lien social reconnu par le droit, un lien social auquel il est donné valeur juridique."
"La rétractation de la promesse unilatérale de vente par le promettant avant la fin du délai d’option offert au bénéficiaire est sans effet. Cette sanction prévue à l’article 1124, alinéa 2, du Code civil est confirmée par la Cour de cassation qui refuse de renvoyer une question prioritaire de constitutionnalité relative à la validité de cette disposition."
"Le débiteur et l’administrateur judiciaire ne peuvent pas accepter conjointement une offre de transiger sans avoir obtenu au préalable l’autorisation du juge-commissaire. En l’absence d’ordonnance du juge-commissaire admettant une telle transaction, le pollicitant peut valablement rétracter son offre de transiger."
"Les réformes de l'enseignement du droit se succèdent depuis plusieurs années en France comme à l'étranger. Elles vont toutes dans le sens de l'efficacité. Mais que veut dire ce mot ? S'il s'agit de se référer à la pratique ou à l'utilité des enseignements, cette référence est équivoque. La pratique peut être la meilleure des choses, si elle sert à justifier la réforme de méthodes devenues obsolètes. Mais elle comporte aussi un risque de dérive, dès lors qu'au nom de l'efficacité on en viendrait à sacrifier certains contenus et à former de « simples techniciens » disposant d'une connaissance tronquée du droit positif. On entend dire parfois que la crise justifie l'urgence. Mais la crise ne justifie rien. Elle doit au contraire inciter à prendre le temps de la réflexion et de la distance critique au regard de certaines politiques."
"L’examen de la force normative de l’article 1964 du code civil, au-delà de développements propres au droit du contrat d'assurance, est l’occasion d’apprécier le rapport – en l’espèce paradoxal – entre la certitude de l’énoncé d’une disposition légale et la force normative de la norme qui s’en infère. Cette contribution met ainsi en évidence les liens entre la norme et sa force normative et insiste notamment sur le rôle du juge quant à la normativité d’un énoncé et quant à la force normative de l’interprétation donnée à celui-ci."
"Le patrimoine affecté à l’activité professionnelle de l’entrepreneur individuel doit être spécifiquement visé dans la déclaration de cessation des paiements et le jugement d’ouverture. Si tel n’est pas le cas, la procédure collective est ouverte à l’égard de tous les patrimoines détenus par le débiteur exerçant son activité par le biais d’une EIRL."
"Le contrat est un instrument juridique que tout individu utilise quotidiennement. Pour vendre, acheter, louer, prêter, fiducier, distribuer, arbitrer ou encore mandater, le contrat est incontournable. Cet instrument essentiel à la vie économique et sociale est envisagé par le droit français à deux niveaux : il existe ainsi une théorie générale, complétée par des régimes spéciaux. Pour appréhender la mise en œuvre de cette théorie générale, l’étude des différents régimes de contrats spéciaux est indispensable. Ces régimes spéciaux permettent de compléter, de déroger et d’adapter les règles contenues dans le droit des obligations. Cet ouvrage présente les principaux régimes de contrats spéciaux de façon didactique : les contrats opérant un transfert de propriété, les contrats portant sur l'utilisation de la chose, les contrats portant sur des services, les contrats portant sur la distribution et ceux portant sur la résolution des litiges."
"Cette thèse se veut à la fois critique et pragmatique. Critique de l’histoire récente du Mali lue à l’aune des outils procurés par l’analyse juridique, la sociologie et l’anthropologie du droit. Cette intention critique a conduit à interroger l’histoire du Mali des origines dans une perspective génétique qui a été détaillée dans les prolégomènes. Pragmatique, la thèse formule des propositions rattachées à la situation concrète du Mali, allant dans le sens d’un décloisonnement des consciences et de la restauration des normes constitutives de la République malienne. C’est à la satisfaction de ces deux impératifs que les analyses du langage ont été utiles. Elles ont également permis d’identifier certaines insuffisances de la dogmatique juridique généralement soucieuse de plaquer des concepts préfabriqués sur les réalités africaines, que de saisir au plus près ces réalités avec leurs nuances. Il est donc apparu opportun pour y parvenir de procéder à l’analyse des stratégies des acteurs ordinaires de la société politique malienne. La prise en considération des concepts manipulés par ces derniers explique le recours aux langues autochtones. La première partie de la thèse est consacrée à l’analyse du développement chaotique du Mali après l’abandon du projet des pères de l’indépendance sous la double pression des contraintes économiques et des déterminants culturels. La seconde partie en tire les conséquences du point de vue de la théorie constitutionnelle. C’est la raison pour laquelle la thèse s’achève sur la proposition d’une réforme du système de l’unité africaine inspirée, à l’image de tout ce qui précède, d’un dicton de la langue bambara : « Dormir sur la natte des autres, c’est comme si l’on dormait par terre »."
"L’image est la représentation symbolique du vivant ». Il s’agit d’une notion difficile à qualifier juridiquement étant donné que le droit notamment en France ne consacre pas de manière textuelle la liberté de l’image qu’il convient de rattacher à la fois à la libre création artistique, à la liberté d’information, à la liberté de communication (dont le terme n’est pas défini par le droit) ou plus largement à la liberté d’expression garantie par la D.D.H.C de 1789 à l’article 11 mais aussi en droit européen (article 10 de la C.E.D.H). L’image comme l’écrit est un véritable moyen d’expression et de communication dont la plupart des médias se servent aujourd’hui à travers ce qu’il convient d’appeler le « siècle de l’image ». La construction d’un statut juridique de l’image passe par l’intervention des juges. Cela a lieu au niveau européen (C.E.D.H, C.J.U.E) mais aussi en droit interne français avec, par exemple, la consécration du droit à l’image confronté au droit à l’information limitant celui-ci. L’image a donc de plus en plus besoin de droit, alors que ce dernier semble la délaisser ou l’ignorer. Cela tient au fait qu’elle est fondée en partie sur l’imaginaire donc sur l’irrationnel, alors que le droit prétend être une discipline rigoureuse et objective. Pourtant, l’image est un formidable outil de communication qui a toujours fasciné l’Homme et de tout temps ce dernier a cherché à contrôler les images, à les censurer voire à les utiliser à des fins de propagande, comme outil politique mais aussi comme outil économique. En effet, le pouvoir politique a longtemps eu le contrôle des images comme en atteste la pratique de la censure administrative des films en France ou le monopole de l’Etat sur l’audiovisuel public. Les écrits ont été mieux protégés grâce à l’importante loi de 1881 sur la presse. Les juges ont joué un rôle crucial dans l’émancipation de l’image et pour promouvoir sa libre circulation dans l’ « espace public » tout en protégeant les individus des dérives que celles-ci peuvent entrainer si elles ne sont pas convenablement régulées. Par ailleurs, l’avènement des nouvelles technologies de l’information principalement l’Internet, a contribué à libérer l’image des contraintes liées aux supports permettant sa circulation : l’apparition du numérique a favorisé sa fragmentation et donc sa démocratisation dans une perspective pluraliste. L’image est désormais à la portée de tous, elle est vulgarisée et circule sans considération de frontières, ce qui soulève à la marge un problème d’harmonisation du droit et des jurisprudences. Une régulation apparait cependant nécessaire dès lors que nous passons progressivement de la logique de l’image-pouvoir (contrôle) à la logique de l’image-savoir (démocratie). L’objet est rationnalisé alors que l’idée est vulgarisée. La thèse se propose de dresser une typologie des images existantes dans l’espace public démocratique, à l’aune des différents contentieux abordés, tout en recherchant chemin faisant, les solutions qui sont raisonnablement envisageables, en vue de favoriser ce processus de démocratisation du savoir par l’image (droit aux images), quitte à s’inscrire, pour ce faire, dans le cadre d’un champ disciplinaire nouveau que nous proposons d’appeler le « droit de l’environnement multi-communicationnel ». Ces propositions ont été formulées à la fin de l’ouvrage."
"Notre société contemporaine a évolué au gré du développement des techniques scientifiques, innovées constamment par l'être humain. La reconnaissance juridique du don d'organes, l'interdiction de l'euthanasie sont autant d'exemples récents démontrant une relation complexe entre le droit et la mort. Le chercheur se trouve ici confronté à des questions dépassant l'élaboration normative puisqu'elles intéressent également l'éthique. Les conflits qui se traduisent par des recours devant les juridictions mettent au défi le juge de trouver un équilibre entre science et autonomie personnelle, dans les zones d'incertitude entre la vie et la mort ainsi que la recherche des désirs exprimés par le mourant et l'ordre public. L'ouvrage vise à dégager une réflexion intéressant les contours de la mort. Afin d'introduire l'étude du contentieux de la mort tel qu'il est aujourd'hui, il présente une analyse approfondie sur la personne humaine et le cadavre."
"Le contentieux du mandat d’arrêt européen offre un éclairage sur la conciliation entre le respect des droits fondamentaux et l’effectivité de l’outil de coopération pénale européenne par la Cour de justice de l’Union européenne. L’articulation de ces impératifs contradictoires, néanmoins caractéristiques de la coopération pénale européenne, révèle les résistances à l’émergence d’un espace pénal européen. Après avoir opéré un premier travail de conciliation des intérêts en présence, la Cour devra dépasser ces contradictions pour mettre ce contentieux au service du processus d’intégration pénale européenne. Dans un contexte de crise des valeurs, ce contentieux, tantôt outil de résistance tantôt matériau résilient à la construction d’une Europe pénale, confère au juge un rôle déterminant dans la conduite de la politique pénale européenne. En effet, en répondant aux velléités souverainistes au moyen des principes de reconnaissance mutuelle et de confiance mutuelle, il contribue à la réalisation de l’objectif de rapprochement des législations pénales nationales imposé par les Traités. Le juge participe également au renforcement de la protection des droits fondamentaux par l’Union européenne et cristallise les prémices d’une harmonisation pénale européenne."
"Le droit des procédures collectives, autrement nommé droit des entreprises en difficulté, est une discipline fondamentale du droit des affaires. Grâce à l'arsenal juridique dont cette matière est dotée, les entreprises peuvent anticiper l'installation durable de leurs difficultés, négocier à l'amiable avec leurs partenaires économiques pour assurer leur sauvetage, demander l'ouverture d'une procédure judiciaire pour apurer leur passif, réaliser des opérations de cession d'activité et même mettre en oeuvre une liquidation judiciaire lorsque les difficultés sont devenues irrémédiables. Pour redonner de la vigueur à l'économie française, le législateur vient à nouveau de modifier la matière par, la loi PACTE n° 2019-486 du 22 mai 2019 relative à la croissance et la transformation des entreprises."
La clause compromissoire stipulée dans la convention s’applique malgré la procédure collective de l’un des contractants dès lors que le liquidateur exerce les droits et actions du débiteur dessaisi.
"La fiducie est un instrument introduit en droit positif par la loi du 19 février 2007. Mécanisme de sûreté ou de gestion, la fiducie trouve son utilisation privilégiée dans la garantie de paiement. En effet, face à l'inefficacité chronique des sûretés conférant un droit de préférence, aux crises systémiques régulières et au droit des entreprises en difficulté, la fiducie véhicule le rêve de la sécurité absolue. Ce rêve est d'ailleurs entretenu par le transfert temporaire de propriété dans un patrimoine affecté à la garantie de paiement. L'étude de l'efficacité du régime de droit commun de la fiducie sûreté, outil a priori redoutable, est donc indispensable. Cette étude est néanmoins insuffisante. Indéniablement, le développement de la fiducie-sûreté dépend tant de la cohérence de son régime de droit commun que de son efficacité lors d'une procédure collective. Le droit des sûretés et le droit des entreprises en difficulté sont deux matières indissociables. Si la première a pour fonction de protéger le créancier contre l'insolvabilité du débiteur, la seconde a pour but de traiter cette insolvabilité. Pour les sûretés, le droit des procédures collectives agit donc comme un révélateur d'efficacité. Alors, pour déterminer l'efficacité de la fiducie-sûreté, étudier le traitement de ce mécanisme dans le droit des entreprises en difficulté est incontournable."
"Le droit des entreprises en difficulté, autrement nommé droit des procédures collectives, est une discipline fondamentale du droit des affaires. Grâce à l’arsenal juridique dont cette matière est dotée, les entreprises peuvent anticiper l’installation durable des difficultés, négocier à l’amiable avec leurs partenaires économiques pour assurer leur sauvetage, demander l’ouverture d’une procédure judiciaire pour apurer leur passif, réaliser des opérations de cession d’activité et même mettre en œuvre une liquidation judiciaire lorsque les difficultés sont devenues irrémédiables. Au travers de 22 fiches thématiques, cet ouvrage propose d’explorer le droit des entreprises en difficulté et son actualité. Les différentes mesures et procédures seront envisagées selon la gravité des difficultés de l’entreprise, des plus légères aux plus graves. Les règles communes à toutes les procédures seront ensuite expliquées."
"Soc. 16 mai 2018, n° 16-25.898, à paraître au Bulletin"
"Le diagnostic de performance énergétique erroné qui n’a qu’une valeur informative, n’est pas sanctionné par la résolution de la vente de l’immeuble et la réparation intégrale du préjudice de l’acquéreur. Le préjudice subi par l’acheteur consiste simplement en la perte de chance de négocier un prix plus bas."
"Soc. 8 novembre 2017, n° 16-18.499, à paraître au Bulletin"
Le créancier auquel la déclaration notariée d’insaisissabilité est inopposable conserve son droit de poursuite sur l’immeuble du débiteur. Ce droit n’autorise cependant pas le créancier à contourner les règles de l’interdiction des paiements et de l’arrêt des poursuites individuelles pour faire condamner le débiteur en paiement.
"L'interdépendance communes-associations fait naître quelques interrogations sur le véritable lien qui les rapproche : comment sont articulées les relations entre les communes et les associations ? Quels sont les risques générés par la trop grande proximité entre les communes et les associations ? Quels sont les rapports des associations face à la commande publique ? Si les relations entre les associations et les communes peuvent être qualifiées de ""naturelles"" (Partie 1), nous verrons dans un second temps l'application du droit de la commande publique aux associations (Partie 2), et enfin que ces relations étroites entraînent des risques que les communes doivent tenter de mesurer (Partie 3)."
"[Deuxième édition] L'ouvrage de ""Droit constitutionnel"" est un manuel ayant pour objectif de présenter une vision à la fois scientifique et synthétique de la matière. Il est ainsi exposé la ""théorie de l'Etat"", le ""droit constitutionnel comparé"", ""l'histoire constitutionnelle de la France"", ""le droit de la Vème République"" et, de manière plus inédite, les ""rapports entre le système constitutionnel interne et ceux européen et international"". L'ouvrage est complété par des corrigés d'exercice pédagogique."
"L’article L.432-2 du Code de l’environnement réprime la pollution de l’eau qui porte atteinte au poisson et l’article L.216-6 du même code celle qui porte atteinte au milieu aquatique à l’exclusion du poisson. Dans l’arrêt du 16 avril 2019, la chambre criminelle de la Cour de cassation énonce que le champ d’application de ces deux infractions étant distinct, le cumul des poursuites ne méconnaît pas le principe non bis in idem."
"La 4ème de couv. indique :""Le présent ouvrage de droit constitutionnel est écrit par quatre auteurs dans un style direct, simple et détaillé. Il est tout d'abord destiné aux étudiants de Licence en Droit et AES 1ère année en abordant la « Théorie du droit constitutionnel» et « l'Histoire constitutionnelle française » généralement traités au lei semestre ainsi que le « Droit constitutionnel de la Ve République » étudié au 2e semestre.Il est aussi susceptible d'intéresser des étudiants plus confirmés, des doctorants ou des spécialistes de la discipline avec des parties moins habituelles relatives aux « Droits constitutionnels comparés » et à des « Eléments de droit constitutionnel européen »."
"La problématique du droit conventionnel en général et celui de l’invocabilité de la CESDHLF en particulier est simple à comprendre. Nous avons actuellement une grande demande de sécurité en France mais aussi dans d’autres pays faisant partie du Conseil de l’Europe –qui, plus est, pays confrontés à des problèmes de criminalité organisée avec une réelle porosité avec le terrorisme. Face à ce contexte, pour ce qui est de la France –ce qui n’est pas le cas d’autres pays– nous avons une tradition judiciaire de nature inquisitoriale/accusatoire et non de nature contradictoire –modèle proposé par la CESDLHLF– cette situation heurte notre fonctionnement procédurale qui procède de l’État et non du privé grâce à la jurisprudence. En outre, au-delà de cette difficulté spécifiquement française pour ce qui est de deux systèmes juridiques n’évoluant pas dans le même sens, on constate que les délinquants utilisent la ressource juridique de la Convention d’une manière dilatoire afin de se soustraire à leur responsabilité pénale encourue. Plusieurs positions en réaction à cette situation sont adoptées par les différents pays composant le Conseil de l’Europe ayant adhéré à un tel outil : ne pas tenir compte des condamnations (cas de la Russie), faire évoluer sa législation pour la mettre en adéquation avec les exigences posées par la CEDH (cas de la France jusqu’à présent en effectuant des réformes législatives d’opportunités sans une réforme majeure), sortir provisoirement ou définitivement de la Convention (plusieurs réflexions ont lieu en ce sens dans différents pays y compris la France concernée au premier chef par la menace du terrorisme islamique)."
"Cass.crim., 25 juin 2019, n°18-83.248 Attendu qu'il résulte de l'arrêt attaqué, du jugement qu'il confirme et des pièces de procédure que, le 2 février 2015, en forêt domaniale de Chinon, des agents de l'office national de la chasse et de la faune sauvage (ONCFS) ont constaté une action de chasse impliquant notamment deux traqueurs à pied et un cavalier identifié en la personne de M. X..., adjudicataire d'un lot de chasse ; que, poursuivi pour la contravention de chasse à l'aide d'un mode, moyen, engin ou instrument prohibé, en l'espèce un cheval utilisé comme moyen de poursuite et de rabat, M. X... en a été déclaré coupable par jugement du tribunal de police ; que M. X... et le ministère public ont relevé appel du jugement ; Attendu que, pour déclarer M. X... coupable, l'arrêt attaqué énonce que les agents de l'ONCFS ont observé que celui-ci a relayé, par cinq coups de trompe destinés aux chasseurs postés à tir, l'annonce de gibier faite par le meneur des chiens qui avaient levé de grands cervidés et s'est déplacé ensuite, à grande vitesse, à l'aide de sa monture, en poursuivant le gibier levé et en criant ""biche à la ligne, biche à la ligne"" ; que les juges ajoutent que, s'agissant de la question centrale de l'utilisation du cheval comme moyen de chasse, il résulte des procès-verbaux de l'ONCFS que M. X... a utilisé son cheval non pas comme un auxiliaire de chasse, mais comme un moyen de chasse permettant de forcer et de rabattre le gibier vers les lignes de tir, ce qui est prohibé ; Attendu qu'en l'état de ces énonciations procédant de son appréciation souveraine quant à l'usage du cheval comme moyen de rabat et de poursuite, et dès lors que, d'une part, l'alinéa 6 de l'article L. 424-4 du code de l'environnement prohibe tous les moyens de chasse autres que ceux autorisés même comme moyens de rabat, d'autre part, le cheval ne figure pas parmi les moyens autorisés, la cour d'appel a justifié sa décision ; D'où il suit que le moyen doit être écarté ; Et attendu que l'arrêt est régulier en la forme ; REJETTE le pourvoi ; Commentaire L'acte de chasse est défini par l'article L. 420-3 du Code de l'environnement comme « tout acte volontaire lié à la recherche, à la poursuite ou à l'attente du gibier ayant pour but ou pour résultat la capture ou la mort de celui-ci ». La chasse ne peut néanmoins se pratiquer de n'importe quelle manière, impératif de gestion et éthique obligent. C'est ce que vient de rappeler la chambre criminelle de la Cour de cassation dans l'arrêt rendu le 25 juin 2019. En l'espèce, le 2 février 2015, au cours d'une opération de chasse dans la forêt domaniale de Chinon, des agents de l'office national de la chasse et de la faune sauvage (ONCFS) ont constaté l'implication d'un cavalier qui a rabattu une biche vers deux traqueurs à pieds armés de fusils."
"Ce texte est la version manuscrite d’une intervention dans le cadre d’un colloque sur le thème « La révision des lois bioéthiques et l’évolution de la conception de la personne humaine » qui s’est tenu les 17 et 18 octobre 2019, à l’Université de Toulon."
"Les questions juridiques en ce qui concerne la robotique et l'intelligence artificielle représentent un enjeu et un intérêt majeur pour la société de demain. Elles permettent d'aborder le droit sous un angle prospectif et multidisciplinaire. Ce dernier n'étant plus conceptualisé sous la forme d'un bloc monolithique mais plutôt pensé de manière polymorphique. L'espace potentiel auquel se retrouve confronté la norme juridique aujourd'hui n'a de cesse de s'accroitre à mesure que de nouveaux domaines liées aux sciences et technologies se développent et impactent la société entière empreinte d'innovation et de créativité dans ce domaine. Le droit est de plus en plus amené à subir l'influence de la technique ou tout du moins adapter la législation pour la rendre compatible avec la technicité des données dans un espace potentiel mondialisé et hyper-connecté. Le juriste doit faire preuve ainsi d'un haut degré d'expertise pour évaluer l'impact de la technique sur le droit et faire évoluer ce dernier en conséquent. L'existence de cet espace potentiel conduit à transformer et décloisonner les sciences juridiques et normatives sous l'effet des données techniques mais il ne doit pas faire oublier le rôle de concert que doit jouer cet espace et le droit qui en découle avec celui de l'action politique (espace organisationnel) ou celui des considérations d'ordre éthique (espace matriciel) qui en matière de progrès technique doit bien évidemment être pris en compte. Le droit des robots doit en grande partie sa naissance à la prolifération des activités liées au numérique et à l'informatique et aux problématiques juridiques qui en découlent comme le droit à la liberté de communication, le droit au respect de la vie privée, à la protection des données personnelles, la lutte contre la cybercriminalité ou le terrorisme, le respect de la propriété intellectuelle. Il n'existe cependant pas encore à ce jour de statut spécifique qui ferait du robot une personne juridique comme le sont les personnes physiques ou les personnes morales jouissant de droits mais aussi de devoirs. D'ailleurs, il faudrait pour cela que les robots soient dotés d'une autonomie de penser et d'agir ou d'exprimer des émotions ce qui est impossible car ils dépendent d'un programme qui a été mis en place par les hommes"
"La personnalité juridique est attribuée à tout être humain au moment de la naissance. Si l’enfant à naître n’est pas une personne juridique selon la conception civiliste, le préambule de la Constitution de 1946 garantit néanmoins le respect de l’être humain dès le commencement de sa vie. Dans l’arrêt rendu le 12 juin 2018, la Cour de cassation a refusé de transmettre une question prioritaire de constitutionnalité aux motifs qu’elle n’était pas sérieuse, car le respect de l’être humain dès le commencement de sa vie n’impose pas une protection pénale des atteintes involontaires à la vie de l’enfant à naître, ce dernier étant protégé par un régime particulier."
"Lorsqu’au sein d’une famille le recours à la justice devient nécessaire, la spécificité du droit de la famille, les différents intérêts en présence laissent entrevoir un particularisme, le pluralisme juridictionnel. Ce pluralisme juridictionnel est apprécié comme la coexistence de plusieurs juridictions dans un même système de justice familiale. Ainsi, pourront intervenir, le juge aux affaires familiales, le juge des enfants, le juge des tutelles, ou encore le Tribunal de grande instance. Nous nous sommes demandé si cette pluralité était justifiée. D’une manière générale le pluralisme juridictionnel en droit de la famille paraît être conditionné par la recherche de la meilleure application de la règle de droit aux personnes. Le pluralisme juridictionnel traduit ainsi l’adaptation de la loi à l’évolution de la famille. Il est la traduction de l’évolution du pluralisme des modes de vie en famille. Dès lors le pluralisme juridictionnel permet différentes manières d’appliquer la règle de droit à l’ensemble des membres de la famille et réserve une protection plus spécifique à l’enfant."
"Soc. 20 décembre 2017, n° 16-19.517, à paraître au Bulletin"
"L’axe principal de mon rapport de stage portera sur le thème de la Gestion Prévisionnelle des Emplois et des Compétences. Ma mission principale a été la rédaction de l’accord GPEC et mise en place de ce dispositif. Cela m’a permis d’appréhender la problématique suivante : comment mettre en place un dispositif de GPEC en milieu associatif segmenté en plusieurs pôles de direction ? Il s’agit de présenter le lieu de mon stage l’Association AFPJR composée de 20 établissements et services (I) et d’aborder les différentes missions que l’on m’a confiées (II). Enfin, je développerai plus particulièrement la mission principale sur laquelle j’ai travaillée lors de ma période d’immersion (III), consistant à rédiger l’accord de Gestion Prévisionnel des emplois et des Compétences."
La pratique des départements
"(Décr. n° 97-360 du 17 avr. 1997, JO 18 avr. 1997, p. 5873)"
"(Décr. n° 96-793 du 10 sept. 1996, JO 13 sept. 1996 et Arr. du 22 oct. 1996, JO 30 oct. 1996, p. 15860)"
"(Circ. du ministre du travail et des Affaires sociales, DSS/4B/97 du 17 mars 1997, BOMTAS/MATVI n° 97-14, 10 mai 1997)"
"(Arr. du 28 mars 1997, JO 29 mars 1997, p. 4897 et 4922)"
"(CE 1re et 4e sous-sect. 18 mai 1998, req. n° 184313, Chambre syndicale nationale des services d'ambulances)"
"(Soc. 7 mai 1998, Trésorier principal du CHU Hôtel Dieu de Toulouse c/ CPAM de Haute-Garonne)"
"(CE 26 juin 1998, Confédération des syndicats médicaux français et autres)"
"(CE 3 juill. 1998, req. n° 188004, Syndicat des médecins de l'Ain et autres)"
"(Soc. 9 avr. 1998, M. Jean-Paul Vasseur et al. c/ CPAM Calvados et autres, arrêt n° 2055 P, Juris-Data n° 001665)"
"(Soc. 12 mars 1998, CPAM Hauts-de-Seine c/ M. Rouleaud, arrêt n° 1451 P, Juris-Data n° 001238)"
"(Décr. n° 98-275 et Arr. du 9 avr. 1998, JO 15 avr. 1998 ; cf. supra, p. 496, l'article de M. Harichaux)"
"(Circ. DSS/AT n° 98-397 du 1er juillet 1998, BO min. Solidarité et Santé, n° 98/30, 8 août 1998, p. 219)"
"(Circ. CNAMTS ENSM n° 32/98, n° 66/98 du 23 juin 1998, non publiée)"
"(Soc. 25 juin 1998, arrêt n° 3272 P)"
"(Soc. 19 mars 1998, CPAM de l'Ariège c/ Bouichou, Juris-Data n° 001210)"
"(Décr. n° 98-271 du 9 avril 1998 relatif à la carte d'assurance maladie et modifiant le code de la sécurité sociale, JO 12 avr. 1998, p. 5714 ; Décr. n° 98-275 du 9 avr. 1998, JO 15 avr. 1998, p. 5799 ; Arr. du 9 avr. 1998 relatif aux spécifications physiques et logiques de la carte d'assurance maladie et aux données qu'elle contient, JO 15 avr. 1998, p. 5802 ; Arr. du 9 avr. 1998 relatif aux conditions d'émission et de gestion des cartes d'assurance maladie, JO 15 avr. 1998, p. 5801)"
(CE 12 juin 1998)
"(Circ. CNAMTS CABDIR n° 8/98 du 15 oct. 1998, non publiée)"
"(Arr. du 7 oct. 1998, JO 14 oct. 1998)"
"(Décr. n° 98-899 et 98-900 du 9 oct. 1998, JO 10 oct. 1998)"
"(Civ. 1re, 7 oct. 1998, n° 1489 P, Mme P. c/ SA Generali France)"
"(Circ. CNAMTS CABDIR du 30 sept. 1998, non publié)"
(Circ. DSS/AT n° 397 du 1er juill. 1998)
"(Circ. CNAM/CABDIR, n° 6/98 du 17 sept. 1998)"
"(L. n° 98-657 du 29 juill. 1998, JO 31 juill. 1998)"
"(Arr. du 4 déc. 1998, JO 5 déc. 1998)"
"(Circ. DSS/4B/99 du 9 mars 1999, à paraître au BOMES)"
"(Circ. CNAMTS DDRI n° 4/99, ENSM n° 8/99, 12 mars 1999, non publiée)"
"(Arr. du 11 févr. 1999, JO 16 févr. 1999, p. 2425)"
"(L. n° 99-477 du 9 juin 1999, JO 10 juin 1999, p. 8487)"
"(L. n° 99-641 du 27 juill. 1999, JO 28 juill. 1999, p. 11229)"
"(Circ. CNAM, CABDIR n° 2-97 du 15 janv. 1997, non publiée)"
"(Convention dentaire du 18 avr. 1997, non publiée)"
"(Arr. du 23 janv. 1997, JO 26 janv. 1997, p. 1365)"
"(Civ. 2e, 2 avr. 1996)"
"(Soc. 11 avr. 1996, CPAM de Saint-Etienne c/ Mme X.)"
"(Circ. DSS/DAC n° 97-521 du 23 juill. 1997, BOMES 97/33 du 4 sept. 1997)"
"(Circ. CNAMTS DGR n° 72/97, ENSM n° 35/97, 9 juill. 1997 et n° 76/97, ENSM n° 38/97, 18 juill. 1997)"
(Circ. DAS/DSS/SDF n° 97/319 du 23 avr. 1997)
"(Circ. du ministère de l'Emploi et de la Solidarité, DSS/SDAS/2B n° 97-735 du 21 nov. 1997, non publiée)"
"(Circ. CNAMTS-DGR n° 91/97 du 12 nov. 1997, non publiée)"
"(Circ. CNAMTS-CADDIR n° 15/97 du 4 déc. 1997, non publiée ; Arr. du 28 mars 1997 portant approbation nationale du médecin généraliste, JO 28 mars 1997 ; Arr. du 17 oct. 1997 portant approbation de l'avenant n° 1 à la Convention nationale des médecins généralistes, JO 18 oct. 1997)"
"(Rapport de la Commission instituée par l'art. 30 L. n° 96-1960 du 27 déc. 1996 de financement de la sécurité sociale pour 1997, ministère de l'Emploi et de la Solidarité)"
"(CE 1re et 4e sous-sect. 1er déc. 1997, req. n° 184546, Union des professions de santé libérales, SOS action santé et autres ; Décr. n° 96-925 du 18 oct. 1996 relatif au carnet de santé institué par l'art. L. 162-1 c. séc. soc.)"
"(CE 1re et 4e sous-sect. 1er déc. 1997, req. n° 185361, Syndicat général des médecins du travail et autres)"
"(Soc. 15 mai 1997, Caisse primaire d'assurance maladie de la Seine-Saint-Denis c/ c. et autres, RJS n° 6, juin 1997, p. 471, n° 731)"
"(Soc. 10 juill. 1997, CPAM du Béarn et de la Soule c/ Lauwers)"
"(Soc. 19 juin 1997, CPAM du Lot c/ M. Faurie, arrêt n° 2569)"
(Circ. CNAM/DDRI/DAR/AC n° 112/2002 du 12 août 2002)
"(Arr. du 14 août 2002, JO 28 sept. 2002)"
"(Arr. du 26 août 2002, JO 30 août 2002)"
"(Arr. du 26 août 2002, JO 30 août 2002)"
(Circ. CNAM/DDRI/DSM n° 124/2002 du 6 sept. 2002)
"(CE 1re et 2e sous-sect. ré. 5 juin 2002, Ministre de l'Emploi et de la Solidarité c/ Mme Rose, req. n° 239757)"
"(Soc. 27 juin 2002, arrêt n° 1942 FS-P)"
"(Soc. 11 juill. 2002, arrêt n° 2407 FS-P)"
"(Soc. 11 juill. 2002, 2 arrêts, n° 2405 FS-P+B+I et n° 2400FS-P+B+I)"
"(Décr. n° 2003-314 du 4 avr. 2003 prévu à l'art. L. 1142-1 c. santé publ. JO n° 81, 5 avr. 2003, p. 6114)"
"(Décr. n° 2003-462 du 21 mai 2003, JO n° 122, 27 mai 2003)"
"(CE sous-sect. réun. 12 févr. 2003, Caisse primaire d'assurance maladie de Bayonne)"
"(Soc. 31 oct. 2002, arrêt n° 3114 FS-P+B)"
"(Soc. 25 mars 2003, arrêt n° 1021 FS-P)"
"(Soc. 25 mars 2003, arrêt n° 1016 FS-P)"
"(Circ. DGS/DS 6 D n° 2003-71 du 13 févr. 2003, BOMASTS, n° 2003/ 09 du 15 mars 2003)"
"(Délibération CNIL, 19 nov. 2002)"
"(Décr. n° 2002-1463 du 17 déc. 2002, JO 10 déc. 2002)"
"(Lett. min. DGS/SD S C du 4 avr. 2003, BOMASTS n° 2003-17 du 10 mai 2003)"
"Actes du Colloque de l'AFDS mars 2008, Numéro hors-série de la RDSS, 138 pages, 2008"
"Dossier RDSS 3-2008, 60 p."
"Christian BYK (dir.),Cahiers de droit de la santé du Sud-Est, n° 9, Colloque du Palais du Luxembourg du 3 avril 2008"
"Bérengère LEGROS,2008, 395 pages, Etudes hospitalières"
"Note sous Tribunal administratif de Besançon, 10 nov. 2009, Mme B., n° 0900299 (D. 2009, p. 2807, obs. S. Lavric, AJ famille 2009, p. 489, obs. F. Chénedé)"
"Stéphanie HENNETTE-VAUCHEZ,Coll. Repères, Ed. La Découverte, 126 pages, 2009"
"Guillaume ROUSSET,Collection Thèses, Les études hospitalières, 635 p., 2009"
"Hector GROS ESPIEL, Jean MICHAUD, Gérard TEBOUL (avec la coordination de Laurence AZOUX BACRIE),Analyses et commentaires, 674 pages, 2010, Economica"
(Circ. CNAM/DDRI 1999-2001 du 2 août 2001)
(Circ. CNAM/DDRI n° 116/2001 du 13 août 2001)
(Circ. DSS/DACI n° 349/2001 du 17 juill. 2001 ; lett.-circ. ARRCO n° 2001-42 du 16 août 2001)
"(Circ. DGS/DS 6 C n° 2001-318 du 5 juill. 2001, à paraître au BOMES)"
"(Décr. n° 2001-833 du 13 sept. 2001, JO 15 sept. 2001, p. 14700)"
"(Décr. n° 2001-963 du 23 oct. 2001, JO 24 oct. 2001)"
(Circ. DHOS/E1/2001/503 du 22 oct. 2001)
"(L. n° 2001-647 du 20 juill. 2001 relative à la prise en charge de la perte d'autonomie des personnes âgées et à l'allocation personnalisée d'autonomie, JO 21 juill. 2001 et Décr. n° 2001-1084, 1085, 1086 et 1087 du 20 nov. 2001, JO 21 nov. 2001, p. 18485 et s.)"
"(L. n° 2002-303 du 4 mars 2002 relative aux droits des malades et à la qualité du système de santé, JO 5 mars 2002, p. 4118 à 4159)"
"(Soc. 28 févr. 2002, 6 arrêts)"
"(Lett. min. DSS/2A du 28 janv. 2002 et circ. DSS/2A/2002, n° 110 du 22 févr. 2002, BOMES, 9 mars 2002.321 et 333)"
"(Let. Min. DSS/2 du 30 janv. 2002, BOMES n° 2002/8 du 5 mars 2002.263)"
"(Décr. n° 2002-793 du 3 mai 2002, JO 5 mai 2002)"
"(Arr. du 30 avr. 2002, JO 4 mai 2002)"
"(Circ. DHOS/02/DGS/DCAJ/SD2C/DSI/IA, n° 2002-22 du 16 avril 2002)"
"(Décr. n° 2001-437 du 16 mai 2001, JO 23 mai 2001)"
"Numéro hors série RDSS, août 2010, 151 pages"
"Numéro hors série RDSS 2011, 141 pages"
"Didier TABUTEAU (dir.),Rapport 2011, avr. 2011, 128 p., Presse de Science Po"
"Marc DUPONT, Annick MACREZ,Presses de l'EHESP, 4e édition, 480 pages, déc. 2012"
"Note sous CE, 30 avril 2014, n° 357900, Département du Loir-et-Cher"
"Note sous CE, ordonnance, 5 novembre 2014, Mme C, n° 385431"
"Florence FABERON (dir.),624 pages, 2015, PUAM"
(Soc. 11 avr. 1996)
"(Décr. n° 96-377 du 30 avr. 1996, JO 7 mai 1996, p. 6853)"
(Soc. 29 févr. 1996)
(Soc. 29 févr. 1996)
(Soc. 17 avr. 1996)
"(Rapport sur les « mourants en prison », Observatoire international des prisons et Aides-Provence, juin 1996 et Rapport de l'inspection générale des affaires sociales, 1995)"
(Soc. 30 mai 1996)
"(Arr. du 22 juill. 1996, JO 26 juill. 1996)"
"(Ord. n° 96-345 du 24 avr. 1996, JO 25 avr. 1996, p. 6311)"
"(Circ. DAS n° 96/644 du 17 oct. 1996, non publiée)"
"(Circ. DGS/DS 2 n° 96 du 8 janv. 1996, non publiée)"
"(Arr. du 10 avr. 1995, JO 22 avr. 1995, p. 6262)"
"(Décr. n° 95-361 du 31 mars 1995, JO 7 avr. 1995, p. 5537)"
"(Décr. n° 95-564 du 6 mai 1995, JO 7 mai 1995, p. 7371)"
"(Circ. DGS/SP2 n° 88 du 1er déc. 1994 relative aux actions de santé en faveur des personnes en difficulté, non publiée)"
"(L. n° 93-1027 du 24 août 1993, art. 36 à 40, JO 29 août 1993, p. 12196 ; Décr. n° 94-294 du 15 avr. 1994, JO 16 avr. 1994, p. 5634 ; Décr. du 21 sept. 1994, JO 23 sept. 1994, p. 13544)"
"(Circ. DSS/AAF/A-1/95/11 du 17 févr. 1995, non publiée)"
"(Circ. min. Aff. soc. DSS/ AT n° 95/22 du 3 mars 1995, non publiée)"
(CE 15 mai 1995)
"(Soc. 16 févr. 1995, D. 1995.J.474, note Y. Saint-Jours)"
(Circ. min. Santé publique et Intégration n° DAS/DSS/DIRMI/95 53 du 19 juill. 1995)
(CE 12 juill. 1995)
"(Economie et statistique, n° 282)"
"(Soc. 23 nov. 1995, Mme Frésil c/ CPAM de la Creuse, Juris-Data n° 003352)"
"(Circ. du secrétaire d'Etat à la sécurité sociale, DGS/DS 2 n° 96-10 du 8 janv. 1996, non publiée)"
"(Décr. n° 96-91 du 31 janv. 1996, JO 7 févr. 1996, p. 1964)"
"(Soc. 29 juin 1995, arrêt n° 3048 p. non publié)"
"(Soc. 23 nov. 1995, CPAM d'Arras c/ Syndicat département des chirurgiens dentistes, Juris-Data n° 003351)"
"(Arr. du 1er mars 1999, JO 2 mars 1999, p. 3119)"
"(L. du 27 juill. 1999, JO 28 juill. 1999, p. 11223)"
"(Décr. n° 99-1004, 99-1005, 99-1006 du 1er déc. 1999, JO 2 déc. 1999 et Décr. n° 99-1012 et 99-1013 du 2 déc. 1999, JO 3 déc. 1999)"
"(Décr. n° 99-1035 du 6 déc. 1999, JO 11 déc. 1999, p. 18440)"
"(Décr. n° 200-495 du 2 juin 2000, JO 7 juin 2000)"
"(Circ. DSS/4 B n° 2000-45 du 26 janv. 2000, BO Solidarité et Santé n° 2000/8, 11 mars 2000, p. 511)"
"(Décr. n° 2000-157 du 23 févr. 2000, JO 27 févr. 2000)"
"(Circ. CNAMTS DRP n° 18/2000, ENSM n° 18/2000, 31 mars 2000)"
(Circ. DSS/2A/4 C/2000/250 du 9 mai 2000)
"(Arr. du 23 févr. 2000, JO 25 mars 2000, p. 4585 ; Circ. CNAMTS-DDRI n° 41/2000, ENSM n° 12-2000, 17 mars 2000)"
"(Circ. DSS/5A/5B n° 2000-21 du 12 janv. 2000, BO Solidarité et Santé n° 2000/4, 12 févr. 2000, p. 177)"
"(Circ. DSS/2A/DAS/DPM/2000/239 du 27 avr. 2000, à paraître au BOMES)"
"(Ord. n° 2000-548 du 15 juin 2000 relative à la partie législative du code de la santé publique, JO 22 juin 2000, p. 9340)"
"(Décr. n° 200-763 du 1er août 2000, JO 6 août 2000, p. 12231)"
"(Circ. DSV/2 A du 27 avr. 2000, à paraître au BOMES ; Circ. CNAMTS-DDRI n° 66/2000, ENSM n° 28/2000-AC n° 24-2000, 12 mai 2000, non publiée)"
"(Décr. n° 2000-638 du 7 juill. 2000, JO 9 juill. 2000 ; Arr. du 3 juill. 2000, JO 16 juill. 2000, p. 10903)"
"(Circ. DSS/DALI/2 A/5 A n° 2000-419 du 24 juill. 2000, à paraître au BOMES)"
(Cass. 14 mai 1998 ; Circ. n° DSS/AT/2000/178 du 31 mars 2000)
(Circ. DSSL/A/4 C/2000/250 du 9 mai 2000 et Circ. CNAM DDRI n° 104/2000 du 11 août 2000)
"(Décr. n° 2000-1005 du 16 oct. 2000, JO 18 oct. 2000, p. 16541 et circ. CNAMTS-CABDIR n° 1/2001 du 20 févr. 2001, non publiée)"
"(Circ. intermin. du 14 mars 2001, ministère de la Solidarité et ministère de la Santé)"
"(Décr. n° 2000-1004 du 16 oct. 2000, JO 18 oct. 2000, p. 16541)"
"(Etude CNAM, Direction des statistiques et des études/CNAM, Point Stat. 28 sept. 2000)"
(Ass. plén. C. cass. arrêts n° 460 à 464 du 22 déc. 2000)
Première partie
"Recherche jurisprudentielle et institutionnelle,Deuxième partie"
"(TASS Haute-Vienne, 18 mai 1993)"
(Soc. 20 janv. 1994)
(Soc. 31 mars 1994)
(Soc. 31 mars 1994)
(Soc. 3 févr. 1994)
(Soc. 19 mai 1994)
"(Décr. n° 94-842 du 26 sept. 1994, JO du 1er oct. 1994)"
(Circ. DH/SDAF n° 94-30 du 24 août 1994 relative au financement des soins dispensés aux détenus en milieu hospitalier. BOMASSV n° 94-33 du 6 oct. 1994)
"(Circ. DSS/AT n° 94-06 du 21 janv. 1994, transmise par Circ. CNAMTS-DGR n° 19-94 du 3 mars 1994, BOMASSV n° 94-6, 6 mai 1994, p. 187)"
"À propos de l'avis du Conseil d'État du 21 juillet 2009, Mme Idjihadi"
(Première partie)
À propos des articles 16 et 64 du projet de loi relatif à la croissance et à la transformation des entreprises
"Note sous CE, Sect., 3 juin 2019, n° 415040, 419903, 422873, 423001 (4 espèces)"
"Le trust est une institution redoutée par le droit français qui l’a longtemps ignoré. Pourtant, son utilisation n’a cessé de croître ces dernières années. Le législateur a donc été forcé de le reconnaître a minima. Retour sur la saga de cette lente reconnaissance, dont le dernier épisode date du 15 décembre 2017."
"Entre pragmatisme et désillusion idéologique, l'Union européenne ressemble aujourd'hui à un espace politique balbutiant, tant la complexité de cette ""construction"" a éloigné les peuples de l'idée européenne originaire, basée sur la paix mais aussi et surtout sur la solidarité entre les Hommes. Le système inédit de distribution des pouvoirs qui a été mis en place par les traités, à l'intérieur duquel des institutions reflétant des intérêts différents et/ou contradictoires se font face, suscite aussi de multiples interrogations sur l'organisation politique de l'Union. La prise de décision y relève à la fois de l'exploit technique mais aussi de la négociation, du consensus et des compromis réalisés par les multiples acteurs en présence : acteurs étatiques (à tous les niveaux des États), acteurs institutionnels, acteurs privés. Le système mis en place a multiplié les procédures d'adoption et de contrôle de et sur l'action de l'Union européenne, tout en tentant de les rendre ""plus efficaces et plus démocratiques"". Le concept de gouvernance qui a vu le jour dans ces conditions n'est pas exempt de critiques et suscite divers questionnements : la gouvernance est-elle un révélateur d'une incapacité (relative) de l'Union sur le plan politique et/ou un système de résolution des conflits ? Comment ce système fonctionne-t-il dans les situations de crises ? Enfin, la multiplicité des discours suscités par cette nouvelle forme de gouvernementalité, sert-elle uniquement à la justifier et en expliquer le sens ou encore à lui donner un contenu en accord avec certaines pratiques. En d'autres termes, les discours sur la gouvernance font-ils partie de la gouvernance ?"
no abstract
"Le paquet de modernisation électorale du 25 avril 2016 » qui comprend deux lois, la loi organique n° 2016-506 de modernisation des règles applicables à l'élection présidentielle et la loi n°2016-508 de modernisation de diverses règles applicables aux élections, met en oeuvre une réforme souhaitée depuis longtemps par le Conseil constitutionnel et par les Autorités administratives indépendantes concernées par le contrôle de l'élection. Si les mesures contenues dans cette réforme (notamment la réforme de l'équité - et non plus l'égalité - appliqué aux temps de parole réservés aux candidats pendant une période dite « intermédiaire » de la campagne audiovisuelle) ont été vivement critiquées, particulièrement par les « petits » partis politiques ; c'est néanmoins la décision du Conseil constitutionnel du 21 avril 2016, qui mérite commentaire. En effet, et pour la première fois, il semble que ce dernier ait consacré ses observations comme une nouvelle norme de référence intégrée au bloc de constitutionnalité en matière de contrôle des élections présidentielles. Le Conseil constitutionnel peut-il être à la fois « conseiller » et « censeur » ?"
"Ce volume rassemble enquêtes et études menées par le Centre d'études et de recherches sur les contentieux sur certaines ""questions sensibles"" en droit pénal et procédure pénale."
"La prestation compensatoire peut être demandée par tous les époux, dans tous les divorces. Lorsque l’équité le commande, elle peut être refusée, soit en considération des critères prévus à l’article 271 du Code civil, soit lorsque le divorce est prononcé aux torts exclusifs de l’époux qui demande le bénéfice de cette prestation, au regard des circonstances particulières de la rupture. Dans un arrêt du 5 décembre dernier, la Cour de cassation a précisé que l’équité peut permettre de refuser d’attribuer une prestation compensatoire, mais pas d’en limiter le montant."
"Si l’autonomie de la personnalité morale des sociétés d’un groupe oblige à apprécier les conditions d’ouverture d’une procédure collective individuellement pour chaque membre, tel n’est pas le cas pour l’élaboration des solutions. Par une approche globale, le tribunal peut prendre en considération la cohérence des solutions dans le groupe."
"Si le marché de l’art en Europe existe depuis très longtemps, il n’apparaît pas en tant que tel dans les Traités sur l’Union européenne, leurs règles ayant plutôt tendance à exclure les œuvres d’art du marché et à affirmer l’exclusivité de la compétence des États membres en ce qui concerne la culture. Toutefois, le droit du marché de l’Union européenne n’a jamais laissé tout à fait de coté les biens (et les services) culturels. En effet, en raison de leur dimension économique, ces derniers ont vocation à entrer dans son champ d’application, notamment en ce qui concerne l’un des principes majeurs du marché intérieur : l’élimination de toutes les entraves aux échanges. Pour autant l’objet d’art est-il une marchandise ? Comment peut-il s’émanciper des lois du marché alors que jamais les liens n’ont été aussi puissants entre le « champ culturel » de manière générale et le « marché des transactions », autrement dit le domaine de l’économie réelle et des flux de marchandises ? Comment le marché de l’art – qui n’a jamais été un marché comme les autres – fonctionne-t-il aujourd’hui, comment a-t-il été influencé et comment a-t-il influencé la règlementation européenne ?"
Le droit de rétention portant un immeuble qui appartient à une société placée en liquidation judiciaire se reporte pleinement sur le prix de vente. Tel est le revirement de jurisprudence opéré par la chambre commerciale de la Cour de cassation qui applique désormais l’article L. 642-20-1 du Code de commerce au droit de rétention immobilier.
"L’exemple de l’action sociale démontre que la logique des blocs de compétence est dénuée de logique, elle a contribué à assembler de manière hétéroclite des politiques pourtant sous‑tendues par des besoins différents, même si elles relèvent toutes de l’assistance. Derrière ce vocable se dissimule en réalité une mosaïque d’interventions qu’il serait temps d’agencer différemment pour ne plus nier l’imbrication des différentes politiques publiques. Si les intercommunalités peuvent concurrencer le département dans certaines de ses compétences en ce domaine, un rapprochement des politiques sociales et des politiques économiques conduiraient à valoriser le rôle de la région. La création d’un « cinquième risque » couvert par la Sécurité sociale ne semble pas à l’ordre du jour."
"Le tribunal peut exceptionnellement prolonger la période d’observation à la demande de l’administrateur, malgré l’opposition du ministère public, sans commettre d’excès de pouvoir."
"La 4e de couverture indique : ""Cet ouvrage, pédagogique et clair (avec de nombreux tableaux, schémas et cartes) présente, conformément au programme des facultés de droit : les différentes juridictions françaises : fondements et organisation des ordres judiciaire et administratif ; les principes et règles essentiels de procédure : déroulement de l'instance, voies de recours et d'exécution des décisions ; les diverses professions judiciaires : avocat, notaire commissaire de justice... ; la justice européenne et internationale. Cette 11e édition, complétée et refondue, s'adresse à ceux qui doivent assimiler ou mobiliser rapidement et efficacement des connaissances fondamentales : étudiants en 1re année de licence de droit et d'AES, en capacité de droit, en IUT carrières juridiques ; candidats aux concours administratifs de catégorie A ou B de la fonction publique d'État et territoriale ; candidats à l'examen d'accès à la profession d'avocat, de greffier, de policier... Ce manuel sera également utile aux non juristes, justiciables ou citoyens, souhaitant faire valoir leurs droits et/ou se repérer dans un dédale de textes et de procédures."
"Tout en se livrant, avec toute la compétence d'un juriste averti, à des analyses très fines du droit positif, et cela en dialogue avec les grands constitutionnalistes classiques, l'auteur refuse d'en rester là et entend poser, dans une perspective explicative et critique, ""une pluralité de regards"" sur son objet, permettant d'""ouvrir une voie"" nouvelle et de ""se défaire d'une tradition doctrinale vieille de plus de deux siècles"". Ceci implique à ses yeux, d'une part, de prendre appui sur les ressources de sciences humaines, telles que l'histoire, la sociologie, la science politique, voire l'anthropologie, mais aussi, d'autre part, de recourir aux lumières de la philosophie, comme le suggère notamment l'expression ""droit politique"", empruntée à Rousseau qui en faisait une ""branche de la philosophie politique"". C'est armé de tels outils que l'auteur n'hésite pas à exercer de manière constante sa liberté de jugement et, comme il l'affirme souvent, ainsi que le suggère à lui seul le premier terme - ""pour"" - du titre de l'ouvrage, à ""prendre parti"", à s'engager."
no abstract
no abstract
no abstract
no abstract
no abstract
"Encore présente dans le projet d'ordonnance de 2015, la cause étrangère fait ses adieux avec l'ordonnance n° 2016-131 du 10 février 2016 portant réforme du droit des contrats, du régime général et de la preuve des obligations. Son départ va de pair avec l'adoption d'une définition subjective de la force majeure reflétant l'irrésistible attraction de la faute à laquelle n'échappe pas le nouveau droit des obligations."
"Tout comme les fonctionnaires nationaux, les fonctionnaires de l’Union européenne sont soumis à des devoirs relevant de la déontologie, cette science de la moralité forgée par Jeremy Bentham dans un ouvrage paru en 1834 . Selon cet auteur, la déontologie, qui est la science des devoirs, entretient une proximité avec l’éthique vue comme la science de la morale. Ces règles de comportement ont souvent un contenu moral et appliquées aux fonctionnaires, elles sont à la fois prescriptives et obligatoires. En ce sens, elles sont distinctes de l’éthique qui a une vocation plus réflexive. D’une certaine façon l’éthique est bien présente dans les règles de déontologie, tout comme la morale, mais pas avec les mêmes référentiels : les référentiels éthiques ou moraux sont marqués par des obligations et des interdictions, alors que ceux de la déontologie sont d’avantage gouvernés par l’expérience. C’est donc bien dans le contexte de l’utilisation de ces mots que la déontologie se différencie de l’éthique."
Le séisme judiciaire ne devrait pas avoir lieu pour les parquets français (à propos des arrêts de la Cour de justice de l’Union européenne du 27 mai 2019).
"Dans une affaire où un incendie, consécutif à un court-circuit dans une carrosserie, a endommagé l’appartement voisin, les propriétaires de celui-ci recherchèrent la responsabilité des propriétaires et des locataires de l’atelier pour trouble du voisinage et communication d’un incendie. Leur action fut totalement rejetée. La responsabilité pour trouble du voisinage fut écartée car seule la responsabilité pour communication d’un incendie pouvait être invoquée. Et cette dernière fut écartée en l’absence de faute du détenteur de la chose dans laquelle le feu avait pris."
"Pour faire appel du jugement statuant sur le plan de cession, le débiteur doit démontrer son intérêt personnel à agir. En cas d’irrecevabilité de cet appel, le débiteur ne peut pas former pourvoi en arguant d’un excès de pouvoir négatif des juges du fond. Tels sont les deux revirements que la Cour de cassation vient d’opérer."
"Si la guerre est, selon Carl Clausewitz, « la continuation de la diplomatie par d'autres moyens » 1 , l'exécution de la décision de justice est-elle, en matière de contrats, la continuation de leur exécution par d'autres moyens ? En effet, au-delà et en l'état du droit positif, une guerre semble être déclenchée entre les différents instruments d'exécution contractuelle. Un contrat, quelle que soit sa nature, suppose le libre consentement de chacune des parties, ainsi que le respect d'un accord mutuel en vertu duquel elles engagent des actions qui dépendent des clauses fixées. D'une certaine façon, le contrat est donc une manière d'éviter l'affrontement : une forme d'entente négociée entre deux instances possédant chacune un bien convoité par l'autre. Dès lors que l'on s'est imposé conjointement une interrogation, comment gérer l'inexécution de l'une des parties ? Dans cette hypothèse, que devient le contrat lorsqu'il est l'enjeu d'une décision judiciaire ? On connaît les prémisses doctrinales de cette interrogation, le contrat 2 , qui selon le professeur Le Tourneau, « est destiné à assurer la création et la circulation des richesses, à échanger des biens et des services ; il n'a pas pour finalité de réparer des dommages » 3. En cas d'inexécution, la sanction attribuée au débiteur joue un rôle fondamental. Le débat ne vaut que vis-à-vis de la nature de la sanction : on peut viser par la sanction soit l'exécution du contrat, soit la réparation du préjudice dans la mesure où le contrat n'a pas été"
"La présente étude se propose de plonger au coeur de la manifestation la plus évidente du phénomène juridique : le règlement des conflits. Plusieurs générations de juristes représentant diverses sensibilités doctrinales et spécialités proposent ici un dialogue fructueux sur cette question. En confrontant une multitude de points de vue, le présent ouvrage s'adresse à tous ceux qui souhaitent comprendre et approfondir leurs connaissances sur ce mécanisme élémentaire de la vie en société. C'est ici l'ensemble du processus juridique de règlement des conflits qui est « dépoussiéré »"
"Jusqu’à l’insertion d’un titre spécifique sur la culture, le domaine artistique sera simplement abordé au travers de politiques et de réglementations communes et souvent, il justifiera des exceptions aux principes régissant le marché commun, notamment en matière de libre circulation des marchandises ou en ce qui concerne la propriété intellectuelle. Cette antinomie s’explique très bien. La référence au respect de la diversité signifie que l'identité culturelle européenne est moins considérée comme une richesse commune que comme une menace pour la spécificité culturelle propre à chaque État membre. En ce sens, l’objectif de l’Union n’est pas de créer un véritable espace artistique commun, mais de protéger ce qui existe déjà."
"« Il est plus facile de briser un atome que de briser un préjugé » Albert Einstein Le libellé du sujet que nous nous proposons de traiter – « Réflexions épistémologiques sur le concept d'exécution en droit »-appelle quelques précisions. Nous n'entendons pas évoquer – de manière directe – la problématique, maintenant classique, du passage du droit au fait par l'exécution d'un titre exécutoire… Ce qu'il faut retenir a déjà été mis en évidence, notamment, par Kojève, lorsqu'il énonce que « le droit séparé de l'exécution construit une utopie sans attache avec la réalité qui, par suite, n'arrive pas à se réaliser et entraine dans sa ruine l'autorité qui l'a produite […] » 2. Par ailleurs, il n'est pas non plus de notre propos de parler des difficultés que peut rencontrer le créancier dans le recouvrement de sa créance ou encore le débiteur dans l'exécution de la décision de justice 3. Bien que ces questions soient essentielles, nous avons néanmoins décidé de mettre de côté la description de ces enjeux au profit d'une étude moins conventionnelle mais conceptuellement tout aussi nécessaire. En effet, l'inscription du droit dans la réalité passe fatalement par la capacité des juristes à se doter d'instruments susceptibles de penser cette liaison entre le droit et le fait. Une réflexion épistémologique 4 sur le concept d'exécution en droit impose dès lors une clarification de ces instruments accordant au droit sa capacité de performer le réel."
fr_abstract_s
"Les œuvres de Clément Marot (1496-1544) font partie des best-sellers du premier siècle de l’imprimerie. Dès le début des années 1530, les éditions se multiplient à un rythme effréné d’Anvers à Avignon, de Paris à Poitiers. Refusant d’abdiquer son autorité face à l’incurie des imprimeurs, Marot est l’un des tout premiers à prendre en main l’édition de ses œuvres pour façonner un recueil « en belle forme de livre ». Fruit d’une dizaine d’années de travail, cette bibliographie critique propose au lecteur un parcours guidé dans le maquis éditorial marotique, complété par une description minutieuse de toutes les éditions parues du vivant du poète et juste après sa mort (jusqu’en 1550). Les quelque deux cents notices, systématiquement illustrées, ouvrent la porte des ateliers d’imprimerie du début du xvie siècle, au moment où s’invente le livre de poésie moderne."
"Cette thèse examine la question des rapports de classes et des relations sociales dans la ville de Bristol au XIXe siècle. A cette époque, la ville se dessine comme une ville relativement peu industrialisée, à l'économie très diversifiée, aux secteurs d'emploi variés et conservant des modes de production préindustriels. Nous nous interrogeons sur les conséquences sociales d'un développement si particulier. En nous inspirant des travaux révisionnistes, nous suggérons que des phénomènes de continuité ont également joué un rôle sur les relations sociales. L'étude d'une tradition locale philanthropique exceptionnelle ainsi que celle d'un attachement marqué à la religion nous permettent de démontrer de quelle manière ces traditions ont pu conditionner les rapports entre les classes. Nous analysons ensuite les mécanismes de contrôle social utilisés afin d'institutionnaliser les rapports de classes. Nous étudions également le développement du syndicalisme et évaluons la popularité du mouvement travailliste avant de nous interroger sur l'émergence d'une conscience ouvrière à Bristol. Nous cherchons donc à comprendre de quelle manière la combinaison de tous ces paramètres a pu façonner les rapports entre les classes et nous tentons de déterminer leurs conséquences sur la nature des relations entre les groupes étudiés. Nous aspirons donc mettre en lumière les situations de domination, de conflit, de contrôle social mais aussi de consentement, de coopération et de consensus."
no abstract
no abstract
no abstract
"À travers l’étude d’un certain nombre de discours prononcés par des députés de la majorité comme de l’opposition ainsi que par la garde des sceaux lors des débats parlementaires autour du mariage pour tous qui se sont déroulés les 17 et 18 avril 2013, ce mémoire s’attache à étudier le cadre et les acteurs de la situation d’énonciation ainsi que les éléments rhétoriques et argumentatifs utilisés par les uns et les autres au sein de l’hémicycle. Notre étude visera à montrer que l’éthos se construit dans une interaction spécifique (un cadre communicatif contraint par le contexte spatio-temporel et le but visé) déterminant les actes de langage produits, cependant que le sujet du débat oriente les différents types d’arguments avancés par les uns et les autres. Quelles sont les stratégies mises en œuvre par les locuteurs pour que, dans la situation de communication qui est celle du débat parlementaire, l’image qu’ils projettent d’eux-mêmes soit suffisamment convaincante et aboutisse au ralliement de l’auditoire ? Quels actes de langage leur permettent-ils de préserver leurs faces ou de menacer celles de leur adversaire politique ? Quels mécanismes vont-ils mettre en place dans leur acte d’énonciation pour que leur éthos discursif entre en résonance avec les imaginaires de connaissances et de croyances de l’auditoire visé ? En répondant à ces questions, nous chercherons à définir les différents éthos à partir de l’usage préférentiel de tel ou tel type d’argument."
"Dans ce mémoire, je mène une analyse globale et éclectique du discours de Benoît Hamon prononcé à la bourse du travail à la Seyne-sur-Mer (en fonction de catégories d’analyse diverses et variées : linguistique énonciative, argumentation, sémantique discursive). Le contexte particulier oblige le locuteur à adopter une stratégie argumentative et discursive qui lui est propre et que j’analyse. À travers cette étude, je montre la dualité de ce discours. Tout d’abord, je m’attache à montrer la maîtrise et le travail de préparation du locuteur pour sa prise de parole. Je démontre que le discours suit une structure solide défendant une idéologie forte à l’image des discours politiques traditionnels. Je montre alors la technicité et l’aspect pédagogique du discours. Cette préparation en amont est aussi visible par les discours convoqués par Benoît Hamon que je détaille. Dans un second temps, j’analyse la part d’improvisation dans le discours. L’oralité a un impact non négligeable sur la prise de parole du locuteur. Je démontre alors l’impact des phénomènes d’élaboration du discours oral (phénomènes d’hésitation, ratés de la parole, variations, commentaires…). Je prend aussi en compte l’auditoire et le comportement théâtral de Benoît Hamon : en effet, il se met en scène à la fois au niveau physique et discursif. Je m’attèle alors à montrer que l’ethos de Benoît Hamon se construit à travers son discours."
no abstract
"RESUME Cet article s'intéresse aux valeurs modales et évidentielles de devoir et must. La difficulté principale est que l'évidentialité, contrairement aux langues amérindiennes telles que le tuyuca ou le Bosavi, n'est pas clairement grammaticalisée ni en anglais ni en français. Ces dernières années ont toutefois fait émerger l'hypothèse que les auxiliaires de modalité, plus particulièrement épistémiques, encoderaient une valeur non seulement modale mais aussi évidentielle. Une autre difficulté est due au fait que la plupart des analyses entretiennent une certaine confusion conceptuelle qui empêche de cerner avec précision le rôle joué respectivement par l'évidentialité et la modalité et les liens susceptibles de s'établir entre les deux. Par exemple évidentialité inférentielle et modalité épistémique sont souvent utilisés de façon interchangeable. C'est la raison pour laquelle cette étude propose une approche méthodologique simple qui consiste à appliquer le concept guillaumien de chronologie notionnelle aux trois notions que sont l'évidentialité, l'inférence et la modalité. Ces trois notions sont à la fois distinctes et étroitement reliées et l'inférence occupe une position centrale dans la chaîne de relations causales. Ce lien montre que si la modalité implique l'évidentialité, l'évidentialité ne conduit pas systématiquement à la modalité. Il permet ultimement d'apporter quelques éléments de réponse à la question centrale que pose la recherche actuelle, celle de savoir si l'évidentialité est encodée ou non dans le sémantisme même de la forme modale."
"La dédicace fictive du Cymbalum mundi, dont les circonstances de publication sont des plus mystérieuses, peut être lue comme une défense précoce du français. Présentant l’œuvre comme une traduction, le prétendu Thomas du Clevier affiche, avant Étienne Dolet, le choix de la langue vulgaire contre le latin. L’argument de la traduction, probablement inventé, permet d’introduire l’idée d’une adaptation de la langue écrite à l’époque contemporaine. Le style facétieux de l’épître dédicatoire illustre le principe rhétorique du naturel que l’auteur appelle implicitement de ses vœux. Le titre de l’œuvre confirme malgré son caractère énigmatique la coloration morale et spirituelle de la réflexion linguistique."
"Cet article réfléchit aux raisons qui rendent problématique l’analyse et le classement des modalités, et examine si le concept plus récent de modalisation peut permettre de clarifier notre compréhension des opérations mises en jeu. Il montre que, pour distinguer autant que faire se peut plusieurs lieux d’inscription de la subjectivité énonciative l’on peut s’appuyer pour cela sur deux critères : d’une part, la distinction entre le marquage des relations interlocutives et le marquage du seul point de vue de l’énonciateur ; d’autre part, la distinction entre, d’une part, la subjectivité inhérente à l’énoncé primaire, responsable des schématisations (Grize 1996) ou représentations construites par le discours, et d’autre part, les commentaires distanciés apportés à cet énoncé pris comme un tout, à son énonciation ou à telle forme linguistique utilisée dans l’énoncé, commentaires auxquels on pourrait réserver le terme de « modalité », ou, si l’on préfère, celui de « modalisation »."
"Cet article se compose d'une partie théorique où, après une définition de l'énonciation, sont confrontés les deux concepts de style et d'éthos et leur intérêt épistémologique respectif, et d'une partie où, à travers l'analyse de poèmes de Jean Follain et Marcel Migozzi,est étudiée la diffraction de la subjectivité dans la poésie contemporaine et la valeur heuristique de la notion linguistique de point de vue pour rendre compte de l'interaction auteur-lecteur et du débat interprétatif mis en scène par le texte."
"L’article s’intéresse aux seuls livres français du libraire Guillaume Boullé, tous publiés en 1534 : une édition des deux recueils de Clément Marot (L’Adolescence clementine et la Suite de l’Adolescence clementine) et une traduction de Plutarque due à Geoffroy Tory (déjà publiée par l’auteur lui-même à Paris en 1532). Cette exception française du catalogue de Boullé semble s’expliquer par l’intervention directe d’Antoine Du Saix, qui coiffe l’un des deux volumes marotiques de pièces de sa plume alors inédites et rend hommage à Tory, le modèle invoqué à la fin de l’Esperon de discipline qui vient de décéder, à travers la réédition du Plutarque que les deux hommes admirent également."
"L’article propose d’étudier l’objet textuel et bibliographique que constitue la série d’étrennes conçues sur le même moule métrique et offertes par Clément Marot aux dames de la cour à l’occasion du nouvel an 1541. À travers l’étude de leurs nombreuses variations, il essaie de montrer comment les différentes publications auxquelles donne lieu l’événement, d’une autorité douteuse, sont autant de « genèses éditoriales », c’est-à-dire d’œuvres de libraire dont le rapport avec le coup d’éclat courtisan est, sinon incertain, du moins problématique."
"Partant de la découverte dans une bibliothèque italienne d’une édition inconnue du Second livre de la Metamorphose d’Ovide traduit par Clément Marot, publiée par Sulpice Sabon à l’« Enseigne du Rocher » (nous ne connaissions jusqu’alors aucune édition imprimée en dehors des œuvres complètes du poète), l’article propose de réexaminer la carrière de Sulpice Sabon et la mystérieuse « Enseigne du Rocher » à qui nous devons plusieurs textes essentiels de la littérature française des années 1540. À travers l’étude du matériel typographique connu et du catalogue des publications, l’article essaie de mieux mettre en lumière le rôle de relais qu’a joué Sabon entre l’activité finissante d’Étienne Dolet et les débuts remarquables de l’atelier de Jean de Tournes."
"Cette publication (2010) est apparue comme le résultat d'une communication faite lors d'un colloque traitant de pédagogie et d'enseignement (Atlanta, 2007). Cette communication (faisant intervenir des Graduate Teaching Assistants d'origine allemande, française, argentine ou chinoise et responsables des cours de composition en anglais et techniques de rhétoriques auprès de natifs anglophones) avait voulu engager une réflexion didactique sur le rôle de "" médiateur culturel "" dans l'enseignement et l'apprentissage de l'anglais spécialisé. Traitant de la position du locuteur non-natif face aux locuteurs natifs apprenants, la question centrale était de savoir ce que l'on pouvait entendre par locuteur natif ou non-natif. Pour Adam Chomsky, par exemple, le simple fait d'être humain donne la capacité de discerner entre le vrai et le faux grammatical. Le locuteur natif pour Chomsky n'a pas de réalité sociale. Pour d'autres, au contraire, l'environnement social joue un rôle même inconscient. Dans notre cas, le problème n'est pas d'être intégré dans une culture étrangère ni même de perdre sa culture source mais plutôt d'être accepté et reconnu pour ce que l'on est par l'autre communauté de langue. Plutôt que d'essayer de se rapprocher d'un locuteur natif, l'enseignant de langue étrangère (le non-natif) va pouvoir devenir un médiateur entre sa propre culture et celle de l'autre et inscrire son enseignement dans un ensemble langue-discours-culture."
"Résumé: Cette communication se veut comme une ouverture à la littérature sudiste (ses plus grands écrivains, ses caractères, ses problématiques, les questionnements qu'elle suscite, etc..., etc...) pour aider les Greeniens à mieux sentir ses liens avec l'inspiration greenienne. Cette communication tache d'amorcer des comparaisons entre les auteurs du Sud des Etats-Unis et Green, comme un moyen de susciter des réactions chez les Greeniens."
"Cette étude interroge le tourisme balnéaire tel que les espaces littéraires de Marcel Proust et de Paul Morand en acclimatent la représentation. Représentatifs des mutations sociologiques qui marquent le long XIXe siècle, ces lieux saisonniers de villégiature où fréquentent tant l’aristocratie qu’une classe nouvelle de bourgeois aisés, et dont la cartographie est arrêtée à la Belle Époque, sont le lieu d’une épiphanie qui remet en jeu les hiérarchies sociales les plus établies. Ce sont ces mutations, en perpétuelle recomposition, que suit cet article, qui montre le caractère opératoire de la métaphore picturale et du motif impressionniste venus des Marines d’Elstir, qui gouvernent en sous-main l’univers référentiel de La Recherche du temps perdu."
"Cet article étudie les énoncés nominaux dans un corpus de cent poèmes de Pierre Reverdy. Ces énoncés réduits à des séquences nominales précédées ou non d'un Gprép peuvent être caractérisés comme des phrases existentielles. Ils ne sont pas descriptifs mais présentent des séries d'évènements servant de toile de fond à ceux qu'évoquent les énoncés à prédicat verbal. La plupart des énoncés peuvent être analysés comme des nexus : le nom est le noyau d'un énoncé entièrement rhématique, évènementiel, mais il sert de support à un deuxième apport prédicatif formulé dans un groupe participial ou prépositionnel ou dans une relative évoquant des propriétés transitoires. Bien que très littéraires, ces énoncés peuvent être inclus dans la grande famille des constructions présentatives avec prédication seconde. La dernière partie de l'article analyse leur rôle pragmatique et stylistique."
"Dans les textes poétiques, les termes d'adresse sont souvent précédés du mot ô, qui apparaît ainsi comme un soutien de l'apostrophe et comme un marqueur générique : les invocations en ô apparaissent dans des textes de registre soutenu et à tonalité lyrique, où la relation énonciative est mise au premier plan. Mais le ô lyrique apparaît aussi en contexte exclamatif. La relation de coréférence entre l'entité qui fait l'objet d'une apostrophe et certains pronoms ou déterminants du reste de l'énoncé n'est pas systématique et l'étude du corpus d'Apollinaire montre que les frontières entre emploi exclamatif et emploi en apostrophe sont floues. Nous partirons donc dans cette étude d’une description syntaxique de l’apostrophe où nous nous interrogerons sur le rôle de ô : puisque ô permute librement devant les apostrophes avec une absence de marqueur, on peut se demander s’il se borne à ajouter plus d’expressivité à l’énoncé ou s’il remplit d’autres fonctions. Nous essaierons ensuite de définir quels sont les contextes favorisant l’ambiguïté entre interprétation vocative et interprétation exclamative des syntagmes ô + SN. Enfin, nous nous interrogerons sur ce que pourrait être le signifié commun aux emplois exclamatif et vocatif de ô : l’expressivité attachée à ô et formulée dans l’article du TLF en termes d’emphase épuise-t-elle la description de ce signifié commun ?"
"Dans le droit français, l'exposé des motifs accompagnant un projet de loi doit « indique[r] de manière simple et concise, les raisons pour lesquelles ce projet est soumis au Parlement, l'esprit dont il procède, les objectifs qu'il se fixe et les modifications qu'il apporte au droit existant ». Cet article étudie les exposés des motifs des lois de 2003, 2010 et 2014 sur les retraites afin de déterminer les caractéristiques linguistiques conférant à ce genre de texte institutionnel une valeur d'autorité sans préjuger de l’effet réel produit sur les parlementaires. Nous nous interrogeons tout d'abord sur la conduite discursive qui préside à ce genre de discours, explicatif mais aussi foncièrement argumentatif, en raison d'un cadrage et de choix lexicaux qui présentent le projet de loi comme la seule réponse possible au problème du « déséquilibre » des systèmes de retraite. Nous montrons ensuite comment cet aspect se trouve conforté par l'effacement énonciatif caractéristique des textes étudiés, et par un usage des modalités (aléthique, déontique) et du futur qui en renforce la valeur d'évidence et la visée perlocutoire, de sorte que le débat parlementaire se trouve textuellement évacué du processus législatif."
"Etude systématique des œuvres d'Henri Troyat consacrées à sa patrie d'origine, la Russie. Celles-ci sont composées, d'une part, de la représentation sous forme romanesque de la vie en ancienne Russie, et, d'autre part, de nombreuses biographies de souverains et personnages russes célèbres. La première partie de la thèse est consacrée à la description de l'univers franco-russe. La seconde partie présente l'homogénéité et i'hétérogénéité du texte littéraire. Henri Troyat peut être à la fois auteur, narrateur, personnage principal dans des romans où la fiction se mêle à la réalité. Naturalisé Français à l'âge de vingt-et-un ans, il entreprend aussitôt une carrière romanesque couronnée très vite par le prix du roman populiste et le prix Concourt, il se consacre à un dialogue de deux cultures dont il est peut-être le seul à posséder la clé. C'est ce que l'on s'est efforcé d'analyser dans cette seconde partie de la thèse. Dans la troisième partie sont présentées les éminiscences russes précises se référant à ses textes et à ceux des auteurs classiques russes les plus célèbres. Les russismes, les citattens, les chansons constituent une sorte de florilège que Ton a essayé de présenter dans sa grande diversité et sa grande beauté. La lecture de l'œuvre considérable d'Henri Troyat donne grâce à la fiction romanesque, servie par une remarquable rédaction et contrôlée en permanence par des références familiales précises, une vision incomparable de la Russie éternelle et de son histoire. L'annexe mérite mention, car elle constitue un important glossaire où l'on retrouve des mots français russifiés en abondonnant leur signification initiale."
"L’article propose d’examiner de près les quatre éditions marotiques publiées par François Juste qui portent l’emblème de Rabelais (1533-1535) afin de caractériser leur spécificité par rapport aux éditions officielles du poète imprimées à Paris. L’analyse de la totalité des pièces ajoutées permet d’infirmer l’hypothèse parfois formulée d’une collaboration directe ou indirecte du poète, qui aurait fait des éditions Rabelais-Juste les éditions officieuses de l’œuvre marotique. L'article est complété par la transcription intégrale des pièces ajoutées des éditions considérées."
"L’article constitue une étude de l’épigramme imitée de Martial adressée par Marot à Rabelais, unique trace d’une amitié littéraire insaisissable. Publié de manière posthume en 1550, l’émouvant quatorzain fait peut-être écho à l’utopie thélémite tout en reflétant l’amertume diffuse qui imprègne les dernières années du règne de François Ier, alors que le poète erre sur les chemins de l’exil et que le romancier aspire à une retraite dédiée à l’otium."
"Clément Marot (1496-1544) est l’un des premiers auteurs français à s’impliquer de façon décisive dans la publication de ses écrits, refusant d’en abandonner la diffusion aux seuls libraires. L’entreprise est pour lui l’occasion de s’interroger sur la nature de l’œuvre littéraire, sur l’intention qui lui donne sens, de l’écriture du texte à sa correcte « mise en livre ». Entre 1532 et 1544, Marot met donc au point une réflexion originale sur ce que doit être un recueil poétique, qu’il accompagne d’un discours publicitaire et théorique novateur, auquel un succès sans précédent assurera une enviable postérité."
"Comment lire les textes ? Alors qu’on reproche parfois à la stylistique un défaut de réflexion sur ses procédures et ses outils ou un certain atomisme descriptif, cet ouvrage collectif place les méthodes d’analyse textuelle au coeur de sa problématique. Il s’emploie à définir et à articuler les paliers et les unités d’analyse pertinents, en soulignant la dépendance des faits locaux à l’égard de déterminations globales (discours, genre, texte). Certaines contributions explorent des formes codifiées (paragraphe, chapitre, strophe), d’autres des zones de localité à construire selon un projet herméneutique. Toutes explorent, dans des corpus variés selon les genres et les époques, les rapports entre spatialité du texte et temporalité des parcours de lecture dans la sémiosis verbale. La perspective intégrative de l’ouvrage apporte un éclairage nouveau sur des objets traditionnels de l’étude stylistique : figure de rhétorique, construction syntaxique, phénomène énonciatif… Ce renouvellement intéressera aussi bien les linguistes du texte que les spécialistes de littérature."
no abstract
"Se fondant sur une approche pluridisciplinaire et comparative entre histoire littéraire, histoire culturelle et histoire religieuse, ce volume présente une cartographie contrastée de la prédication chrétienne autour de la mort dans l’Europe des deux Réformes, marquée par les dissensions religieuses."
"Comment lire les textes ? Alors qu’on reproche parfois à la stylistique un défaut de réflexion sur ses procédures et ses outils ou un certain atomisme descriptif, cet ouvrage collectif place les méthodes d’analyse textuelle au coeur de sa problématique. Il s’emploie à définir et à articuler les paliers et les unités d’analyse pertinents, en soulignant la dépendance des faits locaux à l’égard de déterminations globales (discours, genre, texte). Certaines contributions explorent des formes codifiées (paragraphe, chapitre, strophe), d’autres des zones de localité à construire selon un projet herméneutique. Toutes explorent, dans des corpus variés selon les genres et les époques, les rapports entre spatialité du texte et temporalité des parcours de lecture dans la sémiosis verbale. La perspective intégrative de l’ouvrage apporte un éclairage nouveau sur des objets traditionnels de l’étude stylistique : figure de rhétorique, construction syntaxique, phénomène énonciatif… Ce renouvellement intéressera aussi bien les linguistes du texte que les spécialistes de littérature."
"Nous proposons dans cet article de comparer l’éthos de deux poètes contemporains, James Sacré et Antoine Émaz et, ce faisant, de mettre à l’épreuve notre conception de l’éthos discursif. Nous faisons l’hypothèse que celui-ci est construit par le lecteur sur la base d’une série de marques matérielles : structure du livre, dispositif énonciatif, choix lexicaux, syntaxiques et prosodiques créateurs d’un rythme spécifique, lequel peut être relié à une corporalité de l’énonciateur textuel. Sa construction est également influencée par les textes métapoétiques des auteurs ou par leurs adresses au lecteur au sein même des œuvres étudiées. Le lecteur peut alors adhérer à ce monde éthique ou se tenir à distance de la relation qui lui est proposée."
Communication au colloque « Geoffroy Tory : arts du livre et création littéraire (1523-1533) » organisé par Olivier Halévy (Paris 3-Sorbonne nouvelle) et Michel Jourde (ENS de Lyon) le 10 juin 2011
"Le présent article analyse l’utilisation faite au cours du xxe siècle de la mythologie irlandaise et plus particulièrement du cas des fêtes dites « celtiques » par deux auteurs irlandais, William Butler Yeats et Brian Friel. Cette utilisation, que l’on pourrait imaginer au premier abord dictée par des raisons purement artistiques, se révèle plus profonde et symptomatique de la culture irlandaise, cette Irishness, souvent étroitement liée à des valeurs politiques et/ou religieuses. La thématique de la « science » et son impact sur la culture et les mœurs de l’Irlande moderne sera un élément qui permettra d’enrichir notre étude, la problématique ayant été abordée directement par les deux auteurs en complément, et parfois en opposition, à la « tradition » irlandaise."
"Nous étudions les diverses représentations de la fête du premier mai, dite fête de Beltaine (ou Bealtaine), en Irlande et en Grande-Bretagne à travers les âges (folklore et culture populaire des XIXe-XXIe siècles, fête du Moyen-Age, origines a priori celtiques) afin de comprendre sa symbolique et ses implications sociales, politico-religieuses et identitaires."
"L'étude des fêtes est fructueuse entre toutes comme préparation à l'étude des mythes, parce que c'est dans les fêtes que la pensée et l'action religieuses sont le plus intimement liées. 1 L'année irlandaise ancienne était, traditionnellement, ponctuée par quatre grandes célébrations : Samain, Imbolc, Lughnasa et Beltaine. Peut-être convient-il de rappeler que la fête de Samain correspondait à l'entrée dans la saison hivernale et était le plus souvent célébrée à la fin octobre ou dans la première quinzaine de novembre (si l'on se permet d'adapter un calendrier ancien à notre calendrier moderne, julien puis grégorien) ; Imbolc était quant à elle célébrée aux environs du premier février du calendrier moderne, Beltaine, du premier mai, Lughnasa, du premier août. Comme le fait justement remarquer Máire MacNeill dès les premières pages de son étude de la fête de Lughnasa, étude séminale s'il en est, nous ne savons ni quand, ni comment ces quatre fêtes furent identifiées à ces dates précises 2. Ces quatre fêtes irlandaises (aujourd'hui encore des dates 1 Henri Hubert, cité dans M. MacNéill, The Festival of Lughnasa. Dublin, 1982, p. viii. 2 Ibid., p.1. Toutes les données à notre disposition, depuis les premiers textes chrétiens jusqu'aux recueils folkloriques les plus récents, assimilent bien Imbolc, Beltaine, Lughnasa et Samain respectivement aux mois de février, mai, août et novembre des calendriers modernes."
"La "" trilogie du Chiapas "", constituée par deux romans Balún Canán (1957), Oficio de tinieblas (1962) et un recueil de nouvelles Ciudad Real (1960) illustre l'affrontement entre les dominants blancs, héritiers de la Conquête espagnole et les dominés, les Indiens, dépossédés de leurs terres ancestrales. Aux yeux de la critique littéraire pratiquement unanime, Rosario Castellanos (1925-1974) donne une vision de l'Indien "" de l'intérieur "" très novatrice dans le courant littéraire indigéniste. Selon cette perspective, la trilogie apparaît comme un hymne à la parole indigène en lutte contre le silence et l'oubli. Notre travail effectue une nouvelle lecture qui interroge l'ambiguïté constitutive de la trilogie, comme preuve non pas de l'adéquation, mais de la fracture existante entre l'univers indigène et sa représentation littéraire. En confrontant l'histoire du Chiapas, la réalité ethnologique des Tzotzil-Tzeltal, la place de Rosario Castellanos au sein de l'indigénisme mexicain des années cinquante et sa production littéraire, nous démontrons que l'auteure offre une vision ethnocentrique de l'Indien qui véhicule l'idéologie indigéniste officielle. Les stratégies narratives mises en place par le recours à une perspective ethnique fictive, aux intertextes indigènes, aux mythes d'apparence maya ne servent pas à valoriser la culture indienne. Rosario Castellanos ne parvient pas à (re)connaître l'Indien dans son altérité, car, par delà sa dénonciation des injustices sociales, elle engage avant tout une réflexion sur l'intégration de l'Indien à la nation mexicaine et sur sa nécessaire acculturation."
"Cet article étudie le lexique et l’argumentation des discours d’investiture des Premiers ministres portugais de 2002 à 2011. Il montre qu’il existe des différences significatives entre discours de droite et discours de gauche et des enjeux argumentatifs importants dans l’emploi de lexèmes tels que Estado, modernização et abertura (État, modernisation et ouverture). Il met aussi en évidence les reprises dialogiques à des fins critiques des discours des prédécesseurs. Mais sur d’autres plans – image du Portugal, euphémisation de la crise et rapport aux citoyens –, les convergences l’emportent et brouillent les différences gauche/droite, ce qui peut encourager le scepticisme et le désintérêt des électeurs."
"Nous étudions dans cet article deux séries de poèmes publiés en 1991 et 1996, qui se caractérisent par un très fort effacement énonciatif. Les poèmes se donnent comme des poèmes sans locuteur et sans destinataire. Leur valeur illocutoire est difficile à discerner car ils contiennent très peu de lexèmes subjectifs. Ce sont des descriptions dépourvues apparemment de parti pris. Nous les envisageons – dans une perspective historique où une partie de la poésie a été longtemps dévolue à la célébration du monde – comme la forme contemporaine, modeste, retenue, d’une célébration qui ne pourrait se dire que dans l’effacement de la subjectivité, et qui consisterait à laisser entrevoir des rapports entre différents éléments du monde. Il nous semble cependant possible de lire certains d’entre eux comme des allégories du processus d’écriture, sans que pour autant le premier niveau de lecture perde sa pertinence."
"On se propose dans ce travail d'étudier les rapports qu'entretiennent la modalité et le conditionnel dit de rumeur ou journalistique, en espagnol et en français. Dans une première partie, je définirai le conditionnel de rumeur, puis tenterai dans une seconde partie de présenter les critères permettant de distinguer celui-ci des autres types de conditionnel. Enfin, j'examinerai la valeur modale -épistémique ou évidentielle- qui peut lui être associée. ******************* This paper aims at discussing the problem of the modality and the so called conditional of 'rumor' in Spanish and French. First, I shall explain how to define the conditional of 'rumor', then I shall examine how it can be distinguished from the other kinds of conditional, and at the end, I shall discuss the possible epistemic or evidential modality of this conditional."
"Une étude comparée du début des journaux de Leiris, Gide et Léautaud. Une réflexion sur la pertinence des notions de narratologie créées par Gérard Genette pour l'étude des journaux d'écrivains."
"Cette étude est consacrée à l'oeuvre poétique de Gigi Damiani, grand nom du journalisme anarchiste italien et compagnon de route d'Errico Malatesta. Si le parcours de vie de cet autodidacte commence et se termine à Rome (1876-1953), il le conduit dans de nombreuses métropoles, notamment celles où s'est installée une forte communauté italienne : São Paulo, Marseille, Paris, Bruxelles, Tunis. Partout il compose des poésies de lutte et d'espoir, surtout dans les moments les plus troubles de l'histoire italienne et internationale, dont des pans entiers apparaissent ainsi sous un angle inédit, tandis que se dessine également le parcours d'un militant qui, quels qu'aient été les sacrifices à accomplir, n'a jamais suivi d'autres chemins que les chemins de la liberté."
"Etude de la notion de curiosité dans les trois versions de la Justine de Sade, notamment à partir de la base de données FRANTEXT"
"Le dossier de ce numéro comprend quatre articles qui portent tous sur la période la plus récente de la vie politique portugaise, mais qui s'intéressent à des productions sémio-linguistiques différentes : discours d'investiture des Premiers ministres de 2002 à 2009, voeux présidentiels en 2008, 2009 et 2010, affiches électorales en 2002 et 2011, et émission de télévision pendant la campagne électorale pour les législatives de 2009."
"Les récents modèles d’intonation du français (Philippe Martin) ont mis en évidence l’existence d’une macro-syntaxe du discours à l’oral (Blanche-Benveniste).L’application de ces modèles à la tournure rhétorique appelée « apostrophe » apporte plusieurs éclairages sur le fonctionnement de cette tournure. Notamment, elle permet d’expliquer la distinction entre apostrophe référentielle (du type de l’appel) et apostrophe prédicative (du type de l’insulte).Fort de ces résultats, l’étude de la parole parlée fournit alors un compendium d’observations et d’outils applicables au matériel de la linguistique historique."
"Les pastiches-Zola constituent un signe de réception de l’œuvre zolienne. Le style de l’auteur des Rougon-Macquart est toujours ciblé plus d’un siècle après sa mort par les auteurs de ce type de réécriture. Notre travail de recherche pose le problème de l’appropriation mimétique et/ou différentielle des pastiches-Zola postérieurs à sa mort. La question centrale à traiter est celle de savoir comment Zola est imité dans ces pastiches. Le pastiche implique le phénomène de reprise textuelle qui est caractérisé par l’existence de deux types de textes : l’hypertexte (texte dérivé) et l’hypotexte (texte imité). Une approche comparative associée aux théories de l’intertextualité et de l’hypertextualité nous a permis de découvrir que les pasticheurs de Zola s’inspirent de ses romans et des œuvres d’autres auteurs célèbres. Ils produisent ainsi des pastiches à hypotexte simple, double ou multiple. Les pastiches-Zola se distinguent aussi par leur visée interne (littéraire) ou externe (politique, publicitaire). Les pastiches à visées externes montrent que leurs auteurs documentent les faits sociaux de leur époque comme le faisait Zola. Certains pasticheurs font preuve d’engagement (littéraire) en s’attaquant, mais de manière indirecte, aux personnalités politiques de leur époque. En plus de leurs intentions, leur représentation du style de Zola diversifie leurs productions écrites. Chaque pasticheur obéit à certaines règles d’imitation liées de manière générale aux principes de ressemblance et de différence. Certains pastiches sont ainsi plus inventifs et/ou plus représentatifs du style zolien que d’autres. L’étude des manières imitantes des pasticheurs permet de mener une réflexion sur la notion du style dont les définitions paraissent floues. Les pasticheurs ne s’attardent plus seulement sur la thématique zolienne qui a été très critiquée de son vivant et même après sa mort. Ils allient les thèmes aux procédés stylistiques et expriment, de ce fait, une reconnaissance des qualités poétiques de l’œuvre du chef de file du naturalisme. Les pastiches-Zola produits au XXe et à l’aube du XXIe siècles apparaissent donc, non seulement comme une analyse critique de cette œuvre, mais aussi comme une critique des critiques partielles ou partiales qui ont précédé le renouvellement de la lecture des textes de l’auteur de Médan. L’imbrication des procédés stylistiques et des thèmes zoliens repris et adaptés aux contextes des pasticheurs montre que le style littéraire convoque à la fois les dimensions idiosyncrasiques et sociologiques des façons d’écrire propres à un écrivain, le style n’étant pas un phénomène abstrait mais une série de choix formels et thématiques historiquement situés et dont l’imitation est grandement déterminée par des représentations."
"La politique urbaine a pour objet d‟agir sur la gestion des villes afin d‟améliorer le bien-être des résidants. En Angleterre, l‟Etat est intervenu dès le XVIIème siècle afin de soulager les mauvaises conditions de vie des plus pauvres en agissant sur l‟hygiène et le logement. Puis la ville s‟agrandissant, les désordres liés au surpeuplement se sont multipliés, spécialement dans les zones les plus dépourvues des centres-villes. C‟est à Londres que ceux-ci ont été les plus criants et les besoins de solutions les plus urgents. Ainsi, pour permettre une gestion plus efficace des délits urbains, la gouvernance de la capitale britannique a été réformée. C‟est ce à quoi se sont attelés les Travaillistes dès leur arrivée au pouvoir en 1997, et ils ont en même temps donné une nouvelle impulsion à la politique en direction des villes en favorisant les aspects sociaux, environnementaux de la rénovation urbaine, tout en comptant sur l‟investissement des communautés. Et l‟adoption de la politique dite de la Troisième Voie fut un pas essentiel en ce qu‟elle ne rejetait plus le secteur privé mais l‟acceptait, au contraire, comme partenaire privilégié. Islington, l‟un des boroughs les plus petits de Londres, a la particularité de jouxter une des zones les plus prospères du monde : la City de Londres. Cependant, il présente des caractéristiques sociales communes aux zones les plus pauvres du pays. Les programmes qui y ont été lancés ont pris en compte cette spécificité. L‟analyse de ces projets nous permet de passer en revue la politique de rénovation telle qu‟elle a été voulue par les Travaillistes de 1997 à 2010, d‟essayer d‟en tirer des bilans et de tenter de pointer les défis auxquels les villes vont être confrontées."
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"En 1548, la réédition de L’Oraison de Mars de Claude Colet donne à lire plusieurs nouveaux poèmes de circonstance, dont un huitain latin « ΑD LECTOREM F. R. CARMEN » et à sa suite un sizain grec sans indication d’auteur. Plusieurs arguments permettent de proposer l’hypothèse d’une attribution de la pièce néo-latine à François Rabelais, dont le Tiers livre fut publié en 1546 chez le même Wechel. C’est aussi l’occasion de faire la lumière sur Claude Colet, personnage méconnu."
"Développant une conception de l’effacement énonciatif comme stratégie argumentative, l’article confirme la validité de l’approche énonciative pour des textes apparemment « sans locuteur ». Il montre, en étudiant trois recueils de poésie contemporaine (textes de Jean Grosjean, Marcel Migozzi et Lorand Gaspar), que cet effacement n’est jamais complet et qu’il fait sens par contraste avec des poèmes à la subjectivité plus marquée. La description précise de la subjectivité « résiduelle » dans ces poèmes qui frappent au premier abord par leur ton neutre et objectif est suivie d’une analyse des enjeux pragmatiques de cet effacement énonciatif tant au niveau de la construction du référent que de celle des relations entre scripteur et lecteur."
"Le recueil Grenet (Lausanne, BCUL, M 1016) est un manuscrit poétique personnel compilé au milieu du XVIe siècle par un marchand auvergnat réfugié à Genève. La collection rassemble des textes célèbres ou rares de Marot, des pièces sur l’apprentissage de l’écriture et des poésies satiriques et militantes en faveur de la Réforme. Cette étude propose la première analyse complète de la conception matérielle et intellectuelle du livre, ainsi qu’une interprétation stylistique et littéraire des textes recueillis, afin d’examiner les enjeux socio-culturels du geste collectionneur chez un lecteur du XVIe siècle."
"Lorsque Jean de Boyssoné célèbre par une épitaphe la mémoire de Clément Marot, c’est le souvenir du poète des « amours tendres » (teneros amores) qu’il évoque – celui des larcins et des jeux de Cupidon ou encore des ébats sexuels de Martin et Alix. Pour Boyssoné, l’affaire semble donc entendue : l’auteur de L’Adolescence clementine est bien le délicat poète de Vénus. Or depuis la fin du XXe siècle, la critique a eu tendance à infléchir cette image pour lui substituer celle plus austère du chantre de l’amour « Ferme », résolument (voire religieusement) opposé aux séductions faciles de « Cupido, le mol filz de Venus ». Je voudrais donc analyser l’imaginaire érotique du recueil de l’adolescence du poète (l’âge de Vénus) afin d’en dessiner la cohérence ou les contradictions. Suivre Pétrarque et jouer les amoureux, c’est parfois risquer de s’amollir comme les courtisans moqués par l’Arétin, qui arborent le petit Pétrarque de poche en même temps que les ornements les plus féminins. Dans L’Adolescence clementine, le « disciple estimé de Pétrarque » (comme il se nomme lui-même) semble vouloir conjurer cette menace de l’amour amollissant à travers un diptyque que composent dès l’ouverture du recueil le « Jugement de Minos » et le « Temple de Cupido », un dialogue humaniste adapté de Lucien et un « petit traicté d’amourettes » par lesquels le jeune rimeur entend apprendre à un roi tout aussi jeune que l’amour bien compris n’est pas un risque pour la virilité d’un prince. À partir de l’étude de ces deux textes, j’aimerais montrer en quoi L’Adolescence clementine parvient à se construire comme un recueil de la masculinité triomphante. L’analyse des deux seules pièces du livre que l’on peut qualifier de burlesques (avant la lettre) permettra de confirmer a contrario cette lecture, par la mise en scène exceptionnelle d’un apprentissage inversé qui voit l’adolescent perdre sa virilité au contact des femmes et des efféminés."
"Dimension centrale du processus historique d’assignation des corps féminins à leur fonction reproductrice et sexuelle, le sein devient à partir du XIXe siècle l’emblème de la lutte des femmes pour la conquête de leurs droits (Yalom, 1997). Au XXe siècle, du célèbre mythe des « bra-burning » aux mouvements pour la « libération des seins », l’exhibition des seins nus se décline dans de multiples espaces et reflète l’évolution des théories et stratégies féministes. À partir d’un corpus filmique, ce travail porte sur l’analyse des répertoires d’action collective (Tilly, 1986) de l’activisme aux seins nus, et notamment du mouvement Femen. L’exhibition guerrière de seins nus ornés d’un marquage linguistique devient l’espace de la restauration symbolique de la parole confisquée des femmes, à travers sa « mise en performance » (Butler, 1999), source de nouveaux modes de réalité qui « excèdent » et retravaillent la norme de genre."
"Depuis 1996, on observe la forte croissance des pratiques de télétravail dans les pays de l'Union Européenne. Cette évolution peut sembler paradoxale si l’on se souvient que de nombreux analystes de la société industrielle ont développé l’idée selon laquelle une dimension centrale de la modernité se trouvait dans la séparation entre le domicile et le lieu du travail. Même si Marx n’a pu, à son époque, se représenter le télétravail, l’analyse qu’il réalise du travail à domicile apporte un éclairage fécond à l’étude de ses formes modernes. Ses écrits sur le general intellect mettent en lumière certaines mutations du procès de travail qui accompagnent la montée en puissance de la technologie, et méritent d’être repensés à l’ère d’Internet et du télétravail. Enfin, l’approche marxiste, dans son souci répété de dévoiler l’étroite imbrication existant entre les champs de la famille et du travail, pose les jalons d’une interprétation du télétravail dans sa double relation à la production et à la reproduction."
"Si, depuis 1993, Internet connaît un rythme de diffusion exemplaire, la distribution mondiale des serveurs demeure profondément inégale, puisque 95 % d'entre eux sont concentrés en Amérique du Nord, en Europe Occidentale, au Japon et en Australie, qui représentent à peine 15 % de la population mondiale. La diffusion d'Internet dans le monde est étroitement corrélée à l'indice de développement humain (IDH), et différentes recherches révèlent que la pénurie des capitaux constitue le principal obstacle à l'introduction des technologies de l'information et de la communication (TIC) dans les pays en développement. Ces observations empiriques contribuent à valider une approche déductive de l'innovation, qui s'attacherait à recenser les facultés de génération et d'attraction des innovations des différents pays, en fonction de leurs atouts économiques, sociaux et culturels. Elles s'inscrivent aussi dans le modèle diffusionniste de l'innovation, incarné par E. Rogers, qui met l'accent sur le rôle déterminant des ""attributs perçus"" d'une innovation pour expliquer la diversité des taux de diffusion observables. Le premier objectif de ce travail est d'appréhender la diffusion d'Internet en France, à travers l'analyse statistique des sites ""officiels"" créés entre 1997 et 1999 par un groupe d'acteurs spécifiques, les municipalités. En effet, les sites exigent d'importants investissements humains, techniques et financiers, qui renvoient à des comportements dynamiques et créatifs . Ils fournissent également des informations relatives aux contenus proposés par leurs créateurs. L'analyse effectuée laisse apparaître que la diffusion spatiale des sites Internet tend à renforcer les dynamiques locales préexistantes. Elle démontre aussi que le processus de diffusion d'internet n'est pas seulement l'expression d'un transfert technologique réussi : il réconcilie deux logiques en apparence contradictoires: celle du réseau global et celle du territoire local."
"En dépit de l'accroissement notable de la pratique sportive féminine depuis les années 1960 1 , d'importantes disparités genrées se maintiennent : la pratique sportive des femmes est plus irrégulière 2 et adopte des formes moins institutionnelles-les femmes ne détiennent qu'un tiers du total des licences et seulement 10% d'entre elles participent à des compétitions. En outre, l'inégalité d'accès au sport selon le sexe est accentuée par des facteurs sociaux : 32 % des jeunes filles âgées font du sport en ZEP 3 contre 51 % hors ZEP 4 • Les disparités entre les genres concernent aussi la nature des pratiques sportives : les femmes sont quasiment absentes de certains sports (football, rugby) et investissent majoritairement des pratiques dites de ""la forme"" (gymnastique, danse ...) 5. Pourtant, leur exclusion de certains sports n'arbore plus, dans la France d'aujourd'hui, de formes légales. Au delà de la faiblesse des subventions et de l'offre sportive dédiée aux filles, la prégnance des modèles traditionnels de féminité et de masculinité peut être lue comme un facteur déterminant dans la reproduction de ces inégalités les qualités associées au sport-le courage, l'agressivité, l'esprit de compétition-sont traditionnellement considérées comme ""masculines"" ·et contraires à la ""nature"" féminine. Ces représentations collectives 1 9% de femmes ""sportives"" en 1968, source : Sondage SOFRES pour Le Pélerin (Février 1968), 79% en 2000 , Proportion de femmes ayant déclaré pratiquer au moins une activité sportive, Source: enquête MJS/INSEP juillet 2000. 2 55% des hommes et 40% des femmes disent pratiquer une activité physique et sportive (APS) « au moins une fois par semaine », Ibid. 3 Zone d'éducation prioritaire. 4 Contre 63 % des garçons, source : enquête Pratique sportive des jeunes, ministère des sports, novembre 2001. 5 Louveau, Catherine, La ""forme"" ou ""les formes""?, in Pociello, Christian (dir.), Sport et société Approche socioculturelle des pratiques, Paris, Vigot, 1981."
"Les transformations qui ont affecté la sphère familiale et professionnelle et l’influence croissante des normes européennes ont contribué à faire évoluer les modèles et politiques familiales. La première partie de cet article s’attache à comparer l’évolution récente des modèles de conciliation français et britannique, le modèle suédois se voyant également ponctuellement sollicité au titre de référence emblématique. L’étude des pratiques concrètes d’articulation entre la famille et le travail laisse apparaître le maintien de distances notables entre d’une part la Suède et la France, et, d’autre part, le Royaume-Uni, et ce en dépit des mutations de sa politique familiale. Dans la deuxième partie de ce travail, est réalisée l’exploitation secondaire d’un panel de questions issues de l’enquête longitudinale EVS (2001-2008). L’analyse statistique révèle que les écarts entre les pratiques se retrouvent dans les représentations et les attitudes à l’égard de la division sexuelle des rôles de chaque pays. En outre, la situation des femmes ne suit pas un tracé linéaire : les retours en arrière sont fréquents et révèlent la force de résistance des stéréotypes de genre."
"Le concept de frontière demeure un thème encore peu exploré en sciences sociales. Pourtant, il constitue un outil fécond, notamment dans sa capacité à capturer la dimension relationnelle des processus identitaires et à analyser les modes de construction des inégalités sociales, « raciales » et sexuées. La notion de frontière ouvre aussi l'analyse sur la question du «nous » et des « autres » et sur la dynamique des processus de catégorisation et d'étiquetage. La grande étendue des champs ouverts par la notion de frontière nous conduit à limiter ici l'analyse à quelques unes de ses dimensions exemplaires. Trois principaux points seront distingués. En premier lieu, nous analyserons comment les discours et pratiques de ségrégation et de domination trouvent une légitimité dans l'exclusion de « l'autre » hors des frontières de l'humanité. La nature de la frontière qui sépare l'humain du non-humain est toujours en débat, objet de polémiques et de controverses-qu'est-ce qu'être « humain » ? Posséder une « âme » ? Être doté de raison ? Au fondement de sa critique de l'ethnocentrisme, c'est le critère de la participation à une « commune humanité » qui se verra retenu par l'anthropologie classique. La frontière est donc une zone en mouvement que de multiples acteurs travaillent à redéfinir : l'intensité des débats et combats qui se déroulent révèle l'étendue des enjeux à l'oeuvre et leur dimension stratégique. Enfin, le thème des frontières d'humanité ressort questionné par les progrès des connaissances scientifiques et de leurs applications médicales, qui déterminent l'apparition d'une logique de dissociation croissante entre la personne, le corps humain et ses différents éléments. Cette évolution ouvre vers de nouvelles incertitudes et questionnements bioéthiques sur le statut de l'humain."
"Problématiques liées à la diffusion de créations lexicales 'complexes' : divergences interprétatives autour de l'unité sociodiversité dans les discours scientifiques Résumé Cette étude s'intéresse à la diffusion de la lexie sociodiversité dans le discours scientifique. La présentation des emplois de la lexie en contexte multidisciplinaire nous permet d'analyser les divergences interprétatives de l'unité en discours, en l'absence de définition de référence. L'article apporte une réflexion sur les problématiques définitoires de la lexie et la complexité de son sémantisme dans le discours des sciences depuis une vingtaine d'années."
"Le cas des Primeiras Estórias de João Guimarães Rosa Michèle Monte, avec la collaboration d'Inês Oseki-Depré João Guimarães Rosa 1 est considéré au Brésil comme l'inventeur d'une nouvelle langue littéraire, qui mêle des mots régionaux et des néologismes savants, et conjugue une certaine oralité avec une grande sophistication. Le dépaysement qu'on ressent à la lecture de ses textes provient à la fois de ses inventions langagières et de l'entrelacement qu'il opère entre une réalité ancrée dans le monde rural de l'intérieur (Sertão) du Minas Gerais et une réflexion métaphysique qui fait de ses personnages des émules de Don Quichotte, de Faust, des héros homériques ou des moines bouddhistes. Primeiras estórias est un de ses derniers livres, paru en 1962, composé de 21 courtes nouvelles, très différentes les unes des autres par le ton, le sujet, le style 2. Le terme estória est une forme archaïque de história mais il a été remis à l'honneur au XX e siècle dans la littérature lusophone brésilienne ou africaine pour désigner des nouvelles inspirées de la tradition orale 3. Voici ce qu'en dit Rosa dans une des préfaces de son recueil de nouvelles Tutaméia paru en 1967 : L'estória ne veut pas être une histoire. En toute rigueur, elle doit s'opposer à l'Histoire. L'estória, parfois, se rapproche de l'anecdote. L'anecdote, étymologiquement et dans sa visée, exige d'être totalement inédite. Une anecdote est comme une allumette : on la frotte, elle s'enflamme, et adieu son utilité. Mais même si elle a déjà servi, elle peut encore avoir un autre usage, comme inducteur, par exemple, ou instrument d'analyse, dans les rapports de la poésie et de la transcendance. (Nous traduisons) Cette réflexion sur le genre de l'estória montre que, derrière l'apparente simplicité des récits, peut se cacher une grande complexité sémantique. Plus encore que son grand roman Grande sertão : veredas 4 et que les longues nouvelles qui composent Sagarana et Corpo de baile, ces courtes histoires se caractérisent par une écriture singulière qui fait surgir le mystère d'un monde apparemment familier. Elles ont été traduites en français une seule fois, par Inês Oseki-Depré, et le livre a été publié en 1982 5 , vingt ans après la parution au Brésil. Dans ce travail, nous présenterons tout d'abord les caractéristiques les plus saillantes de la langue et du style des Primeiras estórias, puis nous commenterons certains des choix de la traductrice dans une des nouvelles, « La troisième rive du fleuve », en les confrontant à la version revue par l’éditeur lors d’une parution ultérieure dans une anthologie."
no abstract
"L’édition du Pro Archia de Cicéron procurée par Charles de Sainte-Marthe et publiée à Lyon en 1540 chez le libraire Pierre de Sainte-Lucie est restée jusqu’ici inconnue. L’article décrit l’édition en la situant dans son contexte pédagogique, lié à l’enseignement de Sainte-Marthe au collège de la Trinité à Lyon, et humaniste, dans le sillage de l’analyse du Pro Archia par Melanchthon. Il s’achève par la transcription et la traduction du paratexte de l’édition qui comprend une épigramme latine de Jean Tegulanus de Rouen et une épître de Sainte-Marthe à Maurice Scève fournissant quelques indications sur l’amitié des deux hommes en 1540."
no abstract
"Le problème de l’origine concerne évidemment autre chose que l'origine du langage : son essence, son rapport à la pensée et à la société, l'organisation de la pensée humaine (empirisme vs rationalisme) sa différence à la pensée animale et plus généralement la position de l’humanité dans l’ensemble du règne animal ». (S. Auroux). Comme le rappelle Sylvain Auroux dans le texte qui inaugure ce numéro, la question de l’origine des langues est irrémédiablement liée à celle de l’origine de l’homme, et a de tout temps suscité nombre de débats passionnés et tentatives d’explications diverses. La question de l’interdit renvoie en réalité à celle de l’impossibilité scientifique. Cette question de l’interdit, qui est posée de façon spécifique par rapport à l’essence même de l’objet de la linguistique chez Saussure, M.-A. Cruz l’examine à son tour plus spécifiquement sous l’aspect du rapport système/temps. L’étendue de la problématique multiplie les questions : il s’agit de s’interroger à la fois sur les manières dont les langues modernes sont le reflet de ce qui a existé d’abord puis évolué, tout autant que de se demander comment le langage a évolué ? et qu’est-ce qui a évolué exactement ? Mais il s’agit aussi, de distinguer fondamentalement entre la problématique liée à l’émergence du langage même et celle de l’évolution des langues (R. Nicolaï). Pour R. Nicolaï, la recherche sur l’origine des langues est sans aucun doute un mythe qu’il convient d’aborder en termes de « constructions intersubjectives ». Cette thématique du « mythe de l’origine des langues » est, on le verra, commune à plusieurs auteurs, dont A. Szulmajster-Celnikier qui la reprend à l’occasion d’une interrogation plus vaste sur les modalités et les raisons de la naissance et de la mort des langues. Quel est le rôle de la « transmission », du contact des langues et comment traiter de la « variation » dans cette problématique ? (S. Mufwene) ? Quelles sont les différentes approches de classification des langues (A. Szulmajster-Celnikier) et qu’en est-il d’une corrélation possible entre évolution génétique des populations et diversification des langues sur la planète (L.-L. Cavalli- Sforza) ? L. Métoz expose ici les problématiques de la « nouvelle synthèse » et comment un certain nombre de travaux (dont ceux, précisément de L.-L. Cavalli-Sforza, mais aussi de Greenberg, Ruhlen, Renfrew) recourent à la linguistique, l’archéologie et la biologie pour appréhender le problème dans sa complexe globalité. La question centrale, toujours présente et rappelée par S. Auroux est : l’origine des langues est-elle inaccessible à l’histoire ? D’après Schumann & al., elle n’est plus aujourd’hui inabordable, en raison plus particulièrement des découvertes relativement récentes sur le cerveau et sur les processus impliqués dans l’évolution. L’exploration des fondements biologiques du langage et les questionnements afférents sont du moins devenus possibles et légitimes sous certains de leurs aspects, plus résolument grâce aux découvertes et réflexions venant d’autres disciplines que de la linguistique, comme la biologie, la paléoanthropologie, l’éthologie, la neurolinguistique (Schumann & al. ; E. Salzen ; M. Corballis ; B. Fracchiolla). Les contributions de G. Chapouthier & S. Robert, mais aussi de J.-L. Dessales mettent en regard, à des degrés divers, les données observées en termes de « construction par juxtaposition » au niveau à la fois des langues et du vivant, et reprennent respectivement l’analogie entre mosaïque « génétique », constitutive du vivant et mosaïque « syntaxique » constitutive du sens. Partant du principe qu’il est illégitime de discuter d’éléments qui ne peuvent être selon lui, ni confirmés ni infirmés, F. Rastier, reprend à son compte la question de l’interdit comme impossibilité scientifique, et réfute pour sa part la question même de l’origine du langage pour développer ce qui fait selon lui la spécificité des langues. Or, sur l’origine du langage et des langues – et comme le laisse supposer la conclusion de Sylvain Auroux – c’est sans aucun doute dans le débat lui-même – qui participe de la volonté tout autant que du besoin humain de comprendre le monde – qu’il convient de chercher la légitimité, et non dans les thèses visant à expliquer l’un et l’autre de façon définitive. Anne Szulmajster-Celnikier remarque en conclusion de son article : « l’attention portée à la pluralité linguistique a en outre l’avantage de mettre en relief, sur fond d’universalisme (l’unité écologique et psycho physiologique du genre humain, et le fait que l’on puisse traduire d’une langue à l’autre) un relativisme linguistique fondamental. Faire voir ce que, selon la formule de Jakobson reprise par Hagège, les langues « obligent » à dire ou «empêchent de dire », c’est aussi prendre conscience que les structurations spécifiques des idiomes imposent aux sujets parlants des représentations diversifiées du monde qu’il importe d’analyser dans leur richesse, non de réduire à l’unique ». Aussi est-ce par l’hétérogénéité des approches que nous avons voulu refléter, à l’échelle de ce numéro et des auteurs qui nous ont fait l’amitié d’y participer, la difficulté d’approcher le vivant et l’humain dans leurs grandes diversité et complexité, en tenant compte des apports récents de l’interdisciplinarité. Comme à la fin du XIXe siècle les débats restent vifs, sans doute parce qu'ils engagent aussi des visions de l'homme différentes, et que des désaccords profonds existent. C’est pourquoi, sans doute, il convient d’envisager ce numéro plus sous l’angle des questions qui y sont posées, que sous celui de réponses qui y seraient données."
"Problématique L'énonciation dans la classe : l'histoire, la langue, le contexte. L'un des rôle du professeur est de faire le lien entre les différents apprentissages et savoirs et de former à l'esprit critique, c'est-à-dire, d'enseigner aussi à avoir du recul par rapport à ces savoirs. Pour cela, il ne peut lui-même faire l'économie d'une mise à distance de sa propre culture, qui n'a de vérité que relative et de références partagées avec les élèves que partielles : « On aperçoit alors le danger opposé, aussi redoutable que l'ethnocentrisme, au moins sur le plan pédagogique : négliger, gommer, annuler tout ce qui est le ""vécu"" dans le culturel. Être indigène dans une culture, c'est aussi cette expérience irremplaçable qui consiste à la percevoir de l'intérieur, et, en outre, à l'intérieur de soi-même. Une pratique culturelle est toujours aussi une subjectivité. » 1 Le propos vise ici à remettre ces questions en perspective par rapport à la nécessité d'une conscience épistémologique. L'enseignant d'aujourd'hui construit en effet son enseignement à partir d'un savoir appris et construit, qui n'est plus partagé a priori en tant que « capital culturel » commun comme il pouvait l'être aux 19 e et 20 e siècles, où la plupart des références culturelles, religieuses, littéraires, 1 Porcher et alii, 1986, p. 28."
"En raison des nombreuses variantes offertes par les textes qu’il contient, le manuscrit BnF fr. 4967 a le plus souvent servi de simple répertoire pour l’élaboration d’éditions de grands auteurs comme Marot ou Saint-Gelais. L’examen plus approfondi du manuscrit dans sa globalité permet pourtant d’apprécier d’un œil nouveau la circulation des textes et des idées au début de la Renaissance. On sait que la querelle Marot-Sagon constitue un événement littéraire et éditorial majeur de la décennie 1530 qui assure profit aux libraires et publicité aux belligérants, mais on ignore généralement qu’elle s’est aussi développée par une circulation manuscrite dont la mesure reste à déterminer. À cet égard, le volume que nous proposons d’étudier comporte des pièces pour la plupart inédites qui nous invitent à repenser la notion même de querelle littéraire. Au sein des milieux lettrés de cour, le différend semble se pratiquer comme un divertissement, écho de la fièvre éditoriale qui s’empare au même moment des centres d’imprimerie. C’est cette sociabilité polémique originale, qui repose autant sur l’affrontement individuel que sur l’invention collective et le jeu de société, que nous donne à voir de manière exemplaire le manuscrit Bnf. fr. 4967."
"Selon l’approche psychosystématique guillaumienne, les formes verbales imperfectives du français et de l’anglais présentent une image sécante du temps d’événement, avec résolution de la partie non encore accomplie en accompli. Pour adapter, dans certains cas, les contraintes de l’expérience à la représentation invariante de langue, plusieurs linguistes, tels R. Valin, A. Joly ou D. O’Kelly, proposent une représentation de discours, faisant varier le signifié de puissance en langue, c’est-à-dire la quantité d’accompli et de non encore accompli du temps d’événement. Ainsi, l’imparfait narratif fait passer, en discours, l’accompli (décadent), du positif indiscuté à un positif discuté, avoisinant zéro d’aussi près que possible. De même, la variable ω (accompli) est réduite à zéro (ω = Ø ) et α occupe l’intégralité de l’événement (α = 1) dans les imparfaits à valeur modale, c’est-à-dire dans les énoncés du type « Un instant après, le train déraillait » (interprétation contrefactuelle). Un examen de la représentation des formes verbales anisogènes de l’anglais révèle, lui aussi, une déformation similaire de l’image-temps dans les énoncés en BE+-ING à sens futur (« My train’s leaving tomorrow »). Comment des emplois aussi différents que l’imparfait narratif, l’imparfait modal, et BE+-ING à sens futur peuvent-ils présenter la même variation de leur temps d’événement ? Si cette similarité aspectuelle justifie le rapprochement opéré ici entre ces trois formes, elle remet aussi en cause le principe même qui consiste à introduire une représentation de discours pour rendre compte des fluctuations de la réalité d’expérience."
"En composant le blason « du beau tétin » depuis son exil ferrarais, Marot habille d’un respectable nom français une nouveauté italienne (le capitolo de Baldassare Olimpo da Sassoferrato degli Alessandri), sans que l’objet littéraire qu’il crée alors ne corresponde exactement à ce que les Italiens pratiquent ni à ce que les Français ont connu jusque-là. Le poète banni orchestre habilement l’événement littéraire sous la forme d’un concours afin de s’ériger en lanceur de modes par-delà les monts et de convaincre François Ier qu’il est celui qui saura le mieux acclimater la littérature française à l’esprit italien."
"Trois concepts mettent en relief les interactions paysagères dans la région d'Essaouira : le géosystème, la désertification et les modes d'anthropisation. Effectuée sur stéréoscopes Bosch et Lomb au CEREGE, la cartographie des dunes est réalisée à partir de clichés aériens IGN au 50 000e, mission n° 17 du 10 août 1953 dans le domaine du visible pour la première et de clichés IFN au 20 000e, mission n° 852 de décembre 1984 dans le domaine de l'infrarouge pour la seconde. Elle indique une partition ternaire de la dynamique spatiale du massif dunaire, morphologie générale, types de formes dunaires et répartition des dunes. A cette partition ternaire s'ajoute l'expansion urbaine qui entraine une évolution du contact avec la ville. Exceptés les graffitis phéniciens de ""Mogador"", les premières sources écrites remontent à Pline l'Ancien. La présence d'espaces beaucoup plus boisés qu’aujourd’hui est attestée. Au XIème siècle, Al-Bakr décrit le site d'Amogdul comme un mouillage très sûr. C'est donc au moins à partir du XIème siècle qu'un seuil démographique est franchi. Les processus menant à la désertification apparaissent dès le début du XIXème siècle. Ils sont intégrés aux relations géosystémiques entre trois interfaces : l'articulation entre phases climatiques et phases anthropiques, les relations hommes-milieu et les mutations spatiales biotiques et abiotiques. Nature et culture sont toutes deux contenues dans les trois notions de départ et s'inscrivent au centre du géosystème."
"Cet article propose une étude de l’œuvre journalistique et littéraire de Gigi Damiani (1876-1853) qui offre de nombreuses représentations de figures d’émigrés et d’exilés. Comme chez d’autres anarchistes en exil, et sans qu’il y ait en cela la moindre contradiction, on trouve chez Damiani un très fort attachement à l’Italie (et à la langue-culture italienne) et de virulentes attaques contre l’exploitation de cet attachement, au sein des communautés italiennes émigrées, à des fins politiques, économiques ou autres. L’étude de ces textes permet de comprendre certains des mécanismes par lesquels se construit la culture d’une communauté émigrée, souvent non reconnue, voire méprisée, aussi bien par les sociétés qui reçoivent ces étrangers que par celles qu’ils ont, géographiquement, quittée."
"Ce Roman d’un chercheur est un document de synthèse, rédigé pour l’obtention de l’habilitation à diriger des recherches. Il retrace les étapes et la progression de mes travaux que sous-tend une thématique générale : les mouvements migratoires dans l’Italie des XIXe-XXIe siècles, (émigration italienne ou immigration en Italie). Par nature, le thème de mes recherches comporte une composante interculturelle, les Italiens dont il est question étant en contact soit avec la culture des pays qui les accueillent, soit avec celle des pays d’origine des migrants qu’ils côtoient en Italie. Ce thème se prête donc particulièrement bien à la pratique interdisciplinaire de la recherche et se décline de plusieurs façons : autour de l’approche littéraire adoptée pour plusieurs travaux et articles, autour de l’approche politique et de l’histoire des anarchistes italiens en exil, autour de la reconstruction de la mémoire de l’émigration italienne (au Brésil, en Lorraine, dans le Sud-Est de la France notamment) ou enfin lorsque que la question de l’émigration italienne se confronte à celle d’autres déplacements de populations, celles qui arrivent en Italie depuis quelques décennies maintenant. Dans la plupart des travaux cités, les thématiques se croisent, signe de la cohérence de la démarche adoptée et de la dynamique qui anime les thèmes étudiés"
"De nombreux militants anarchistes ont été entraînés dans la vague d'émigration italienne au Brésil à la fin du XIXe siècle. L'expérience de la colonie Cecilia, fondée par Giovanni Rossi au Paraná en 1890 est la première manifestation de cette présence anarchiste. Mais c'est à São Paulo que les anarchistes italiens ont essentiellement développé leurs activités, en particulier dans le domaine de la presse. Entre 1890 et 1920, ont paru une trentaine de journaux anarchistes rédigés en italien, numéros uniques et périodiques à la publication irrégulière, mais aussi hebdomadaires qui ont suivi les événements italiens et brésiliens pendant plusieurs années. L'étude de ces journaux met en relief un aspect de l'émigration italienne au Brésil et permet d'évaluer la contribution des Italiens à la naissance du mouvement ouvrier brésilien."
"Cet article traite du roman L'invasion publié en 1907, dont l'auteur Louis Bertrand accumule les clichés xénophobes et racistes sur les immigrés italiens à Marseille, qui constituaient alors un cinquième de la population de la ville. Le roman exprime non seulement la peur de l'""invasion"" du territoire français par des étrangers, mais reflète aussi la progression d'idées que Bertrand – ultranationaliste et antidémocrate, considérait comme dangereuses pour le bon peuple travailleur et respectueux."
Ouvrage d'Ettore Cinnella traduit de l'italien
"Cet article, paru à l'occasion de la sortie du recueil de textes et témoignages Enfants d'Italiens quelle(s) langue(s), parlez-vous? éditions GEHESS, Toulon, 2009, offre, à l'intention du grand public, des réflexions en lien avec la thématique du rapport à la langue chez les descendants de populations immigrées, dans le cas spécifique, des descendants d'Italiens."
"Entretien avec Amedeo Bertolo conduit par Mimmo Pucciarelli, traduit de l'italien par Isabelle Felici Je suis né en 1941 à Milan. Ma mère était couturière et mon père mosaïste : il faisait des mosaïques artistiques et décoratives. Ils étaient tous les deux frioulans. Ma mère était arrivée à Milan à l'âge de cinq ans et mon père à vingt-cinq. Mon père a commencé à travailler en tant que salarié puis, au milieu des années cinquante, il a monté une petite entreprise artisanale. Je possède encore quelques tableaux qu'il a exécutés pour moi vers la fin de sa vie. Mes parents n'ont eu que deux enfants : j'ai un frère né cinq ans après moi. Ma mère a travaillé dès l'âge de douze ans, d'abord dans un atelier de couture puis, quand elle a eu des enfants, elle a travaillé à domicile, jusqu'au milieu des années cinquante. Ni elle ni mon père n'ont jamais eu d'activité sociale ni politique ; d'ailleurs mon père travaillait dix ou douze heures par jour, y compris le samedi… J'ai peu de souvenirs de mes premières années et ils ne sont pas significatifs de ma « carrière » d'anarchiste. Il y a peut-être un épisode significatif : quand j'avais neuf ou dix ans, j'ai fabriqué à la main trois affiches illustrées signées « Mouvement apatride mondial » et je les ai collées sur les murs de mon quartier..."
"L’auteur de l’article met à jour dans une série de lettres extraites des Lettres persanes de Montesquieu un prétexte narratif : la visite de la bibliothèque d’un couvent. Elle montre que Montesquieu prône un retour au texte pour éviter la perversion des textes sacrés dans l’Exégèse et les traités théologiques, et absout ironiquement les traités de morale dont l’érotisme est à peine voilé. La bibliothèque révèle également le désir d’organiser et de maîtriser le monde."
Une étude lexicale des mots pathos et pathétique dans la littérature du XVIIIe siècle avec la base de données FRANTEXT
Etude des références à Racine dans l'œuvre de Rétif de la Bretonne
"L'incipit de La Religieuse est un texte rempli de paradoxes et de surprises. Il suscite un lecteur singulier qui doit prendre pour modèle le premier destinataire du texte, le marquis de Croismare, et qui entre en concurrence avec lui. En s'appuyant sur les catégories de la réception définies par Wolfgang Iser dans L'Acte de lecture, on constate que le rôle du lecteur ainsi mis en place est radicalement nouveau dans l'histoire de la littérature et préfigure les recherches modernes de la fiction contemporaine."
"Ce mémoire est une étude comparative des discours de remerciements à la Cérémonie des Oscars de trois réalisateurs : Clint Eastwood, Steven Spielberg et Ang Lee. Cette étude permettra d'analyser un discours peu étudié, celui des remerciements à la cérémonie des Oscars."
"Les slogans sont importants dans la campagne publicitaire des fournisseurs automobiles. Ce sont des outils utilisés pour impressionner, séduire, frapper l'esprit des clients et leur donner l'envie d'achat. Ils contiennent des caractères particuliers dans la forme, dans le contenu et dans les figures stylistiques. Avec un corpus qui regroupe des slogans publicitaires du domaine automobile, cette étude apporte des analyses détaillées sur les particularités linguistiques du slogan automobile. Les résultats devraient être bien utiles pour les publicitaires."
"Bernard Werber est un auteur contemporain incontournable et néanmoins décrié par le milieu littéraire. Son oeuvre traite de thèmes fondamentaux comme l’origine de l’humanité ou l’histoire des religions. La trilogie des Dieux présentement étudiée met en scène un apprenti dieu qui va subir un vrai parcours initiatique. Chaque lieu qu’il va traverser sera porteur de sens et prodiguera un enseignement qui ne pourra être pleinement appréhendé qu’à la fin du Mystère des dieux, à la manière d’un puzzle dont les héros ramasseraient les pièces disséminées dans l’objet-livre. Ce mémoire s’attache à examiner les différentes places de la trilogie en tâchant de comprendre comment elles sont liées, enrichissent et mettent au coeur des interrogations le concept de création artistique ou divine, thème récurrent si ce n’est symptomatique de l’oeuvre werbérienne. Cette étude cherchera à définir une poétique de l’espace attachée au symbolisme de la création, en se posant notamment la question : est-ce le lieu qui crée les personnages qui le peuplent ou si ce sont les personnages qui entraînent la création du lieu dans l’esprit de l’écrivain ?"
"À l'exemple du roman <i>M. Ripley</i> de Patricia Highsmith et des deux adaptations cinématographiques très divergentes <i>Plein soleil</i> de René Clément et <i>Le talentueux Mr. Ripley</i> d'Anthony Minghella, cette étude comparative s'attache à démontrer la complémentarité de la littérature et du cinéma, l'enrichissement d'un archétype et son inscription dans la sphère mythique."
"Cette recherche se concentre sur le classement syntaxique des titres de presse en français avec leurs valeurs sémantiques. Elle met aussi en évidence les figures de style utilisées dans les titres pour attirer l'attention du lecteur en créant le style du journal. D'autre part, la comparaison des titres entre les journaux, entre les rubriques et entre journaux papier et en ligne aidera les étudiants à avoir une vue générale de leurs différences. Cette recherche a l'ambition de faciliter la compréhension et la création des titres de presse pour les étudiants qui veulent travailler dans ce domaine."
"Quel est l'attrait de la fantasy ? Il s'agit de répondre à cette question à travers deux trilogies : <i>Le Livre des étoiles</i> d'Érik L'Homme et <i>La Quête d'Ewilan</i> de Pierre Bottero. Dans un premier temps, nous verrons le motif de la quête, central à ces œuvres. Dans un second temps, à travers la découverte d'autrui, de l'altérité, les héros effectuent un véritable apprentissage. Enfin, dans un dernier temps, il faut questionner les mondes imaginaires créés."
"Le Hezbollah libanais est un parti qui a marqué et qui marque toujours la scène politique nationale et régionale arabe en ce qu’il est un « mouvement de résistance » à Israël qu’il pense avoir vaincu lors de la guerre de 2006. Au moment de l’éclatement du conflit en Syrie, ses adversaires considèrent qu’il s’est transformé en « une force occupante » : il a lâché sa résistance à Israël au profit de la défense du régime Assad allié. L’épisode du 7 mai 2008 en interne et son intervention en Syrie rendent les armes du Hezbollah illégitimes selon beaucoup. De là, nous nous sommes posés la question pour savoir dans quelle mesure sa rhétorique de résistance est une fin en soi contribuant à la survie du parti dont les actions et les armes ne font plus l’unanimité."
"L’analyse des magazines féminins américains et britanniques des années 1950, 1960 et 1970, les représentations de la femme et de leur rôle dans la société, ainsi que l’interprétation des messages qui ressortaient de ces publications sont les objectifs fondamentaux de la thèse ici présentée. Ces magazines seront décortiqués dans le but d’analyser les représentations des femmes et de l’image mythifiée de la « femme parfaite » et leur impact sur les lectrices, ainsi que questionner cette vision idéaliste des femmes, comprendre la transformation de ces ménagères en féministes engagées à la fin des années 1950 et, finalement, établir une comparaison entre les mouvements féministes américains et britanniques des années 1950 aux années 1970. Cette étude examinera les magazines en tant que participants à l'évolution des valeurs culturelles qui redéfinissaient la vie domestique américaine et britannique après la Seconde Guerre Mondiale. L’analyse ira enquêter sur le multiple rôle des magazines – entreprise, conseillère pour des questions sociales et familiales, diffuseur d’idéologies ou, tout simplement, source de divertissement et information. L’interaction de ces publications avec d’autres éléments culturels de deux pays, dont la politique, l’économie, les nouvelles technologies et les études psychologiques et sociales feront également partie de la recherche. Le corpus d’analyse se constitue des magazines mensuels qui avaient une part importante dans la construction de l’idéalisation de la femme : Ladies’ Home Journal, Good Housekeeping et McCall’s aux Etats-Unis et Housewife, Women’s Day et My Home au Royaume-Uni, ainsi que les magazines féministes créés au début des années 1970, Ms aux Etats-Unis, et Spare Rib au Royaume-Uni."
"Certaines histoires font froid dans le dos. Un vieil homme apparaît dans l’étude d’un avoué pour réclamer l’identité d’une gloire de l’Empire de Napoléon Bonaparte : le colonel Chabert. Or, celui-ci est mort ; les vivants le disent sur la foi de l’état civil qui le dit sur la foi de témoins. A ces vérités, le vieux monsieur oppose le réel de sa vie et fait vaciller l’évidence. Dans le silence de la nuit, le récit de ce revenant fait apparaître les faibles contours de l’image du glorieux colonel. Il demande à être reconnu, cet homme qui va apprendre qu’une identité ne se réclame pas, elle se construit. Il avait pourtant bien commencé, amenant son corps mutilé et, avec lui, le récit ; il aurait pu continuer en acceptant la seule voie possible pour décrocher le Colonel Chabert du tableau des morts illustres : la transaction. Etre Chabert mais abandonner certains éléments de sa trajectoire. L’identité du vieux monsieur ne nous est donnée ni par le roman qui l’a créé ni par le droit qui a tué Chabert parce que le droit et la littérature proposent des solutions pour construire une vérité qui ne sera toujours qu’incertaine. L’identité d’un individu ne se situe pas ailleurs que dans les récits d’une culture, et le jeu consiste à s’ajuster sans cesse dans la rencontre entre tous les discours qui désignent. L’un d’eux est celui du droit, il pose les critères qui font exister et qui individualisent. Le droit et La Comédie humaine de Balzac se rejoignent alors en un point qui les confond pour raconter ces histoires saisissantes où l’identité se joue entre les deux, faisant ainsi du droit et de la littérature des domaines propices au regard de l’anthropologue."
"L’objectif de cette thèse est de proposer une description unifiée de la modalité poétique, qu’elle apparaisse dans des textes en vers ou en prose. L’angle d’approche adopté est celui de la pragmatique linguistique. Renonçant à la catégorie générique de poésie, nous redéfinissons la poéticité à partir de la notion d’effets, que notre enquête se propose donc de définir. Afin de dégager des pistes théoriques, nous passons d’abord en revue un certain nombre de notions issues des approches (post-)jakobsonienne, énonciatives et évocatives, puis nous précisons nos hypothèses en menant des analyses de corpus sur des textes de Michaux et Roubaud. De cette première étape, il ressort que les effets poétiques s’accompagnent de parcours interprétatifs concomitants, de statuts différents. Afin de proposer une formulation linguistique de ces phénomènes, nous avançons des arguments justifiant le choix de la théorie des actes de langage comme cadre descriptif général. Réexaminées dans ce cadre conceptuel, les données rassemblées jusqu’ici nous permettent de définir les effets poétiques comme des effets perlocutoires anticipés, et parallèles à la visée illocutoire principale. Enfin, nous évaluons la robustesse de cette définition à travers une série d’études de corpus portant sur une gamme étendue de faits de langue : des phénomènes syntaxiques (parallélismes), des faits énonciatifs (polyphonie, valeurs du présent de l’indicatif) ou textuels (allégorie) sont ainsi examinés. Ces analyses confirment la plausibilité de nos hypothèses. En outre, elles permettent d’envisager l’étude de la poéticité dans des textes plus contemporains ainsi que dans la prose romanesque de certains auteurs."
"Une trace est une suite d’empreintes, laissées par le passage d’un être ou d’un objet – c’est donc avant tout l’indice d’un chemin parcouru. C’est à ce déplacement dans le temps et dans l’espace qu’invitent mes travaux de recherche dont l’objectif est de suivre Edith Wharton « à la trace ». La trace, c’est d’abord, pour ce qu’elle nous apprend sur le voyageur et son rapport au monde, cette croisière en Méditerranée qu’elle entreprend en 1888 à bord du Vanadis. C’est également l’empreinte qui subsiste de cette expérience du voyage : un manuscrit dactylographié qui retrace le périple et rend compte du rapport particulier d’Edith Wharton à l’écriture.La trace – ce qu’on suit (« suivre à la trace ») – renvoie donc à une double activité : d’une part au voyage lui-même, d’autre part, à l’exploration de toutes les pistes que j’ai cru bon d’ouvrir à partir du document originel : sur la vie et l’œuvre d’Edith Wharton, sur son environnement socio-culturel et sur le genre de la littérature de voyage – toute une série d’empreintes, donc de signes conduisant à de nombreux signifiés. La question demeure toujours, en dernier ressort, de savoir si les signifiés que croit avoir découvert le chercheur sont bien ceux de l’écrivain."
"Cette recherche, menée dans deux établissements, m’a permis de constituer un corpus de textes d'élèves de CM2 et de 6e, accompagnés de leurs brouillons ou autres états intermédiaires, qu'ils aient été exigés ou non des enseignants. A partir de la méthodologie de la critique génétique, une base de données a été créée afin de recenser les quatre opérations du brouillon de certains élèves (ajouts, suppressions, remplacements et déplacements). L'étude propose ensuite une analyse plus détaillée de l’opération de suppression. Alors que dans les précédentes recherches, l’opération de suppression reste encore majoritairement associée à un désaveu ou à un échec dans les copies des élèves, les analyses quantitative et qualitative de cette thèse permettent d’envisager la suppression comme le fruit d’une véritable stratégie engagée par l’élève. Enfin, des dispositifs pédagogiques de relectures par les pairs ont été mis en place dans les classes afin d’étudier d’une part les conseils de suppressions émis par les pairs et d’autre part l’impact de ces remarques dans les réécritures des élèves."
"« Qui suis-je ? » cette question récurrente posée dans la plupart des textes de l'écrivain et scénariste américain, Paul Auster, est celle qui donne à penser l'identité de tout individu. Cette identité plurielle, parfois si difficile à saisir, n'est pourtant pas étrangère à toutes les empreintes qui marquent chaque parcours existentiel. Empreintes certes génétiques et généalogiques, mais aussi culturelles, familiales et sociales qu'il appartient à l'individu non seulement de découvrir et d'interpréter, mais de comprendre afin de donner sens à son histoire. Dans cette approche est mise en question la part de liberté qui échoit au sujet, et qui le rend en partie du moins l'acteur de sa propre identité. Un acteur en dette de tous les héritages reçus, mais qui peut en transmettant à son tour découvrir qui il est. C'est cette philosophie de l'identité inscrite dans les écrits austériens que cet article se propose de questionner."
"Entre le XIIIe siècle et le XVIIe siècle, se constitue un vocabulaire nautique spécifique, celui de la marine du Levant, différent de celui de la marine du Ponant utilisé en Atlantique. Ce vocabulaire est né du voyage dans le temps des mots provenant de l’indo-européen, du latin et du grec, mais sa spécificité vient des échanges, du voyage des mots dans tous l’espace méditerranéen qui permet à toutes les langues méditerranéennes de s’enrichir mutuellement. Les récits de voyage à Jérusalem constituent une source importante car ils sont rédigés par des hommes qui n’ont jamais auparavant vu la mer et qui vivent leur première expérience de navigation ; ils ont tout à apprendre pour pouvoir transmettre leur expérience à leurs lecteurs. Le métissage linguistique méditerranéen est le fait des marins qui transmettent aux marchands et aux voyageurs qui sillonnent la Méditerranée. Quand un mot domine et se répand dans d’autres langues cela signifie qu’il appartient à une langue d’une nation dominante soit dans ses techniques de navigation, soit par sa puissance maritime.Les écrivains voyageurs racontent leur navigation en montrant leurs sentiments, en exprimant leur admiration pour les spectacles nouveaux qu’ils découvrent. Comme la mer est un élément inconnu, elle engendre très souvent la peur qu’il faut surmonter, le voyage est une épreuve nécessaire et enrichissante. La constitution d’un vocabulaire spécifique montre qu’au delà des guerres, de la course, des fortunes de mer, des hommes se sont parlés, se sont entendus, ont construitensemble les outils nécessaires pour nommer les chose, c’est-à-dire les comprendre et se comprendre."
"La réalité peut être recrée virtuellement pour être manipulée cognitivement en lieu et place de l'entité ou du processus simulés. Cette relation d'homologie, poussée à l'extrême dans l'apprentissage sur simulateur, favorise le processus d'immersion dans l'univers mimétique ainsi représenté. La puissance du mimème en tant qu'inducteur d'immersion dans la réalité virtuelle ne dépend pas uniquement de la fidélité mimétique, mais également de la dimension affective suscitée par la réalité mimée. Ces deux critères conjuguent leurs effets respectifs pour créer une "" illusion référentielle "", propice à la motivation et donc, in fine, à la qualité de l'apprentissage. De plus, la recherche en didactique professionnelle a montré comment l'individu accroît son potentiel cognitif dans l'exercice de son métier. Sa compétence se construit par la multiplicité des confrontations aux situations. Le simulateur permet précisément de multiplier les situations à loisir, mais aussi de fractionner l'apprentissage, le rendre plus moins difficile ou plus ou moins ciblé, selon le but recherché."
"En nous invitant à considérer son œuvre d’un point de vue esthétique, André Gide souhaite dépasser les considérations morales et psychanalytiques plaçant un pan de la sexualité en dehors de la norme. L’auteur s’attache à présenter les défauts de tout ce qui est défini a priori comme étant la normale. Ainsi, le paraître auquel ont recours certains personnages de L’Immoraliste, des Caves du Vatican ou des Faux-monnayeurs permet de repenser le modèle représenté par la famille bourgeoise. Il ne convient plus de traiter certaines perversions comme de simples déviances. En jouant avec les apparences, les personnages oscillent entre le monde du spectacle et le travestissement. Tous ces jeux rhétoriques et esthétiques incitent à une redéfinition de la perversion. Ajoutons enfin que cette volonté de dépasser l’idée selon laquelle l’homosexuel se trouve en dehors de la normale permet à Gide de proposer une véritable réflexion sur l’homosexualité et la place de l’homosexuel dans notre société. C’est notamment grâce à cette réflexion que va émerger dans les pays anglo-saxons et plus tardivement en France une nouvelle approche de la sexualité appelée : Gender Studies"
"L’enjeu de la présente étude est de montrer que l'ordre des mots acquiert un rôle métalinguistique important dans le système des opérations internes de la langue : il connote la manière de penser le monde phénoménal. Pour cette étude sur le coréen, la théorie que nous avons choisie est la systématique énonciative. Cette théorie met en œuvre une analyse qui ne sépare pas le Discours des conditions linguistiques de sa production. Dépassant le cadre d’une systématique des langues, elle rappelle qu’un énoncé n'est jamais isolé du contexte linguistique et situationnel où se trouve le sujet parlant. Nous commencerons par l’observation de l'énoncé fondamental, afin de dégager ses unités constitutives fonctionnelles ; en d’autres termes, nous adopterons la démarche sémasiologique, mais à partir des conditions d'énonciation et de la situation de production de l'énoncé. Nous examinerons ensuite l'ordre interne des éléments de l'unité constitutive fonctionnelle, qui relève aussi bien du domaine morpho-syntaxique que sémantique. Nous verrons que l’ordre à l’intérieur du syntagme et le choix du mot formel en coréen constituent un mécanisme majeur de la syntaxe coréenne, mécanisme qui dépend de la visée d’effet du locuteur. Si chaque langue a sa manière d’organiser les éléments au sein d’une unité donnée, c’est que chaque langue analyse à sa façon la perception du monde expérientiel. Quel est alors l’ordre prescrit par le système linguistique du coréen, au niveau du mot, du syntagme et de la phrase ? Quelle est la liberté de manœuvre du locuteur au moment de la construction de la phrase dans l’acte de langage ? C’est à ces deux questions que ce travail a tenté d’apporter une réponse. La présente étude comporte quatre parties. La première partie propose d’examiner la structure de l’énoncé : de l’énoncé au syntagme. La deuxième partie explique la disposition des constituants dans l’énoncé. La troisième partie étudie l’ordre des éléments au sein du syntagme nominal, en fonction de la place du déterminant. Cela concerne la logique combinatoire du mot matériel et du mot formel qui relève essentiellement de la syntaxe interne d’une unité constitutive fonctionnelle de l'énoncé. Enfin, la quatrième partie se consacre à une syntaxe de l’adverbe, basée sur sa mobilité au sein de l’énoncé, mobilité qui affecte l’incidence adverbiale."
"Internet a modifié nos comportements en matière de communication et notre manière de consommer l'information. Parmi les nouveautés apportées par le média on compte les pratiques du podcasting (dans lesquelles nous insérons les pratiques du streaming et du vlog) qui ont remis en avant la scène de la production amateur. Dans ce mémoire, nous aspirons à montrer la place de cette pratique amateur sur Internet dans l'histoire du journalisme et de la communication en évoquant ses spécificités par rapport aux médias traditionnels."
"La Renaissance, ou Quattrocento, c'est le terme qui représente le mouvement intellectuel et artistique qui s'est développé en Italie au quinzième siècle. Nombreux sont ceux qui estiment que la Renaissance constitue une période de modernité. Par conséquent, je voudrais étudier la renaissance est un phénomène culturel qui est fortement lié à l'émergence de la société urbaine."
"Cette thèse propose une analyse contrastive de la notion de défini telle qu’elle est exprimée dans le système de l’article en anglais et en arabe moderne standard. L’ensemble des notions associées au défini et à l’indéfini sont examinées d’un point de vue sémantique et d’un point de vue syntaxique, afin de découvrir la manière dont les deux langues traitent ces concepts; les différences et les ressemblances sont répertoriées dans le contexte d’une étude détaillée de corpus. Le récit, The Brook Kerith de l’écrivain irlandais George Moore a été choisi pour des raisons géo-historiques et littéraires: les événements racontés se déroulent en Terre Sainte à l’aube de l’ère chrétienne. Les occurrences du syntagme nominal en anglais et en arabe analysées dans le premier chapitre permettent de dégager les convergences et les divergences des deux systèmes. Les résultats sont soumis à une analyse quantitative et statistique. Il en ressort que la valeur de l’article défini en anglais (“the”) et en arabe (“al”) correspondent dans 76% des emplois. La ressemblance entre la valeur de l’article indéfini (“a / an”) en anglais et son équivalent en arabe s'élève à 96%. Cependant, dans la mesure où l’arabe est une langue sans article indéfini, le fonctionnement de l’article zéro en anglais est sans équivalence; on découvre que l’arabe choisit selon le contexte, soit la marque du défini (al), soit la marque sémiologique de l’indéfini. En dernière analyse, on constate une grande ressemblance entre les mécanismes cognitifs sous-jacents; les différences concernent les transformations sémiotiques de la structure profonde."
"Vers 1537 paraît à Toulouse un intrigant recueil d’épîtres en vers relatant un voyage à Sumatra effectué par trois Français. Parfaitement fictifs, les récits mêlent l’imaginaire des grandes découvertes à celui du mythique pays de Cocagne. En s’embarquant en rêve pour ce véritable Éden, les trois aventuriers font de la description des usages insulaires l’image inversée d’une Europe déclinante, qui s’en va à vau-l’eau. D’inspiration marotique, cette subversive fantaisie poétique est restée inconnue des spécialistes des récits de voyage et de la poésie du XVIe siècle, faute d’avoir été rééditée depuis 1537. Notre édition propose de la redécouvrir dans toute sa verve simple et efficace, en l’accompagnant d’une introduction et d’une dense annotation, qui en restituent le contexte culturel, éditorial et littéraire."
"Dès la deuxième moitié du XIXe siècle, nombreux sont les Italiens qui ont immigré dans le département du Var, géographiquement proche de la péninsule. Pourtant, le Var n’a pas suscité le même intérêt chez les historiens de l’immigration que les départements limitrophes des Bouches-du-Rhône et des Alpes-Maritimes. Ce constat nous a menée à l’étude du phénomène migratoire transalpin dans le Var. Majoritairement élaborée à partir de sources de première main qui ont nécessité un important travail dans les Archives Départementales, cette thèse est caractéristique de la présence italienne dans le Var à la fois d’un point de vue démographique, socio-économique et mémoriel. En d’autres termes, elle révèle l’ampleur de l’exode italien dans le département, le profil des immigrés et leur répartition géographique sur le territoire. Ce travail montre également l’impact de l’arrivée massive des travailleurs transalpins sur l’économie varoise et définit les secteurs d’activités dans lesquels ils sont majoritairement embauchés. Enfin, il s’intéresse aux aspects socioculturels de l’immigration, traités notamment à l’aide de la reconstitution de la mémoire par des descendants d’Italiens établis dans le département du Var qui racontent l’histoire de leurs ancêtres. Il étudie alors le parcours migratoire des Transalpins du Var de manière individuelle et tente d’échapper aux clichés, notamment concernant le déracinement, l’adaptation, la discrimination ou encore la transmission des origines italiennes. Construite selon une démarche positiviste et constructive, cette étude prend en compte différents types de sources qui se complètent et qui donnent accès à l’immigration italienne de la manière la plus réaliste."
"Dans l'échange verbal entre plusieurs personnes, des termes en particulier sont souvent employés pour s'adresser à autrui : il s'agit des termes d'adresse. Ces termes peuvent être considérés comme créateurs de liens entre le langage et la société par le fait qu'ils fournissent des informations non sans intérêt sur la relation unissant les interlocuteurs. Il existe divers moyens de communiquer et de traduire une certaine forme de politesse, de familiarité, de distance, d'affection, de haine, etc., dont ces termes font partie. En plus de leur valeur sémantique, ils possèdent une valeur sociale et culturelle non négligeable.Les huit romans retenus dans cette étude, écrits par quatre grands écrivains de l'époque victorienne (Charlotte Brontë, Charles Dickens, George Eliot et Anthony Trollope), offrent au lecteur curieux un aperçu privilégié de la deuxième moitié du XIXème siècle. Outre l'aspect littéraire, les thèmes abordés en relation avec les événements de l'époque intéresseront le lecteur historien et le mode de vie et les mœurs décrits fourniront de précieux détails au lecteur sociologue, tous ces aspects étant également facilement accessibles au lecteur ordinaire. Cet examen synchronique des termes d'adresse permet, non seulement de montrer la pluralité des termes pouvant être utilisés en adresse directe, mais aussi de souligner l'importance des facteurs linguistiques, paralinguistiques et extra-linguistiques dans toute étude de cette nature."
"La littérature «moderne» s’est essentiellement développée dans les cours européennes du XVIe au XVIIIe siècles. Afin de mieux cerner les rapports entre hommes de lettres et gens de cour, le Centre d'étude sur l'Etat, la Société et la Religion en Europe, Moyen-Age et Temps modernes"" (ESR) de l'université Versailles Saint-Quentin-en-Yvelines et le Centre d’Etude de la Lan­gue et de la Lit­té­ra­ture Fran­çai­ses des XVIIe et XVIIIe siè­cles (CELLF 17-18) de l’uni­ver­sité Paris Sorbonne-Paris IV (Centre natio­nal de la recher­che scien­ti­fi­que) ont organisé, fin septembre 2008, un colloque à Versailles. Ce col­lo­que a eu pour but de mener une enquête sur le sta­tut, les fonc­tions et le rôle (sym­bo­li­ques ou réels) que les hom­mes de let­tres ont pu avoir dans le cadre des cours moder­nes. Au-delà d’une sim­ple appro­che socio­lo­gi­que de la lit­té­ra­ture, comme le fit naguère A. Viala en étudiant la « nais­sance de l’écrivain », en cen­trant le pro­pos sur l’espace spé­ci­fi­que de la cour, ces tra­vaux tirent parti des avan­cées actuel­les sur l’his­toire des cours, pour mieux com­pren­dre l’un des cadres majeurs où s’est déve­lop­pée la « lit­té­ra­ture » moderne. De Ronsard, ""poète du Roi"", à Voltaire, conseiller du Prince, à Paris comme à Berlin, en pas­sant par les « his­to­rio­gra­phes » Racine et Boileau, et sans oublier Saint-Évremond, qui occupa des fonctions auprès de la cour anglaise durant son exil, tou­tes ces figu­res ont déter­miné des confi­gu­ra­tions dif­fé­ren­tes de la fonc­tion de l’homme de let­tres auprès du monar­que. En France, la carrière de Benserade auprès de Louis XIV est exemplaire à bien des égards, mais la figure des prédicateurs de cour et celle des dra­ma­tur­ges pro­té­gés par le Prince est aussi évoquée. Il est également question des courtisans devenus écrivains, comme Saint-Simon, voire du monarque lui-même lorsqu’il se pique d’écrire, comme le mon­tre le cas exem­plaire de Frédéric II de Prusse. L’évocation de la cour de Catherine II ren­d sen­si­ble aussi le cosmopolitisme des écrivains qui la fréquentent. De fait, une pers­pec­tive com­pa­ra­tiste est néces­saire, car le sta­tut des ""écrivains-cour­ti­sans"" n’est pas homo­gène dans l’Europe moderne, des cours ita­lien­nes de la Renaissance aux cours alle­man­des des Lumières."
"La place de la laideur dans certaines productions de l'art moderne et contemporain | Post-Scriptum (post-scriptum.org) RÉSUMÉ Dans l'art moderne et contemporain, on assiste à l'émergence d'une émancipation de la laideur. Expurgée de toute idéalité, l'oeuvre donne à la laideur un rôle de premier plan. Cet affranchissement n'est pas sans entraîner un bouleversement dans l'ordre des valeurs. L'alliance du beau, du vrai et du bien, tout comme celle du laid, du mal et du faux se fissurent de telle sorte que la laideur, affranchie de son nécessaire rapport au mal et au faux, tend dans certaines oeuvres à être le signe du vrai ou du bien. La laideur n'est plus cantonnée à figurer ce qui ne devrait pas être mais à transcrire l'apparaître de ce qui est. Porteuse ainsi d'une certaine véracité, elle est l'expression de ce que voit l'artiste, de ce qu'il ressent, de ce qui le traverse. Dans les travaux plastiques, les installations, les performances, la laideur devient par la même le moyen de dénoncer l'ignominie propre à certains types de rapports intersubjectifs. Ces oeuvres se veulent alors moteur de changement, de transformation et d'inspiration pour le respect de la dignité de tous les individus. Nullement inintelligible comme jadis on la qualifia, la laideur, par-delà la « significabilité » de l'oeuvre, donne à penser et par-là même à résister. Cependant, dans d'autres types de productions, la laideur délestée de tout rapport à la bienséance et à la morale va être utilisée comme outil de transgression qui en générant du dégoût va conduire à questionner la frontière entre art et non-art."
"Cet article est une étude du premier roman de Younis Tawfik, poète et écrivain d’origine iraquienne, installé depuis plusieurs décennies à Turin, où il a vu arriver et s’installer différents flux de population étrangère, notamment marocaine. La straniera (1999) met en scène la lutte pour la vie et la dignité d’une jeune femme marocaine fraîchement arrivée en Italie, et les interrogations du protagoniste masculin, immigré de longue date et parfaitement intégré, que la rencontre avec la jeune femme renvoie à son passé et à son statut d’étranger. L’article met en lumière la dimension didactique du roman, qui, à sa façon, explique l’« Orient » aux Italiens, analyse les croisements qui s’opèrent entre deux parcours immigrés et examine les modalités qui, selon l’auteur, viennent définir le dialogue interculturel."
"Nous nous proposons, en nous appuyant sur les théories sémantiques de la métaphore (Rastier, Cadiot et Visetti, Kleiber), d'étudier le rôle du contexte dans un texte poétique contemporain de Lorand Gaspar intitulé ""Monastère"" : le contexte joue un rôle décisif, soit pour motiver la catégorisation incongrue, soit pour permettre de comprendre quel point de vue subjectif a déclenché le choix d’un lexème actualisant un certain « motif » indépendamment d’un domaine thématique précis. Nous établirons tout d’abord les réseaux sémantiques structurant le texte indépendamment des métaphores qu’il contient, puis nous nous intéresserons à la façon dont les métaphores se signalent à l’attention du récepteur et au processus interprétatif dont elles font l’objet et dans lequel le contexte joue un rôle crucial."
"Ce recueil contient une série de témoignages, poésies, enquêtes, entretiens… qui ont tous pour point de départ ces quelques questions : Enfants d’Italiens, quelle(s) langue(s) vous a-t-on transmise(s) ? Avez-vous eu des difficultés avec la langue du pays d’accueil (le français en principe)? Quels efforts avez-vous fournis pour récupérer une « autre » langue, peut-être perdue au fil des générations, éventuellement pour la transmettre à votre tour ? Si de la langue de vos ancêtres, il ne reste que des traces, quelles formes prennent-elles et dans quelles circonstances se manifestent-elles ? Parmi les auteurs des textes, figurent des enseignants et des étudiants, d’italien ou de français pour la plupart. Tous sont des enfants, ou petits-enfants, d’Italiens et nous racontent, dans certains cas par des biais détournés, leur histoire d’amour, parfois d’amour-haine, avec la (les) langue(s) qu’ont parlée(s), ou n’ont pas parlé(es), leurs ancêtres."
"« Homme libre, toujours tu chériras la mer ! », posait, on le sait Baudelaire. Mais laquelle ? Pour s’en tenir au seul cas français, c’est peu dire, en effet, que l’air de la mer que l’on respire en Méditerranée n’a que peu en commun avec celui qui souffle sur l’Atlantique. Et cela ne tient pas aux seules considérations climatiques : Ouest contre Sud, sauf à convoquer une théorie des climats au sens où l’entendait le XVIIIe siècle, quand l’Atlantique a souvent été associé au Nord, dans une co..."
"Dans l’épisode central du Piège diabolique (1962), le dessinateur belge Edgar P. Jacobs dépeint le monde tel qu’il l’imagine en l’an 5060, alors que l’un de ses héros, le professeur Mortimer, voyage dans le temps avec une machine inventée, puis déréglée par l’un de ses rivaux, avide de vengeance. La planète, après avoir été ravagée par une guerre nucléaire, à la fin du XXIe siècle, est alors sous la férule d’un gouvernement totalitaire et cet épisode montre ainsi les conséquences d’une « guerre future », synonyme de régression pour l’humanité entière. Outre l’indéniable influence de romans d’anticipation, principalement ceux de H. G. Wells, la confiscation du progrès technique par une poignée d’hommes et le spectre d’une guerre nucléaire en constituent les principaux traits. Cependant, Jacobs tire également profit de la peinture de cet univers cauchemardesque pour transmettre un message d’espoir et d’humanisme, comme le montre la victoire finale des populations « assujetties », à laquelle Mortimer contribue grandement."
"Corso, personnage-lecteur de El club Dumas d’Arturo Pérez-Reverte assiste impuissant à la contamination de son univers référentiel « réel dans le roman » par des êtres de fiction issus des Trois mousquetaires d’Alexandre Dumas. Gagné peu à peu par le syndrome de don Quichotte, le détective de livres interprète sa réalité fictionnelle à travers le prisme de son imaginaire. L’infortuné Corso, dupé par un narrateur-personnage d’une mauvaise foi aussi efficace que redoutable et manipulé par un auteur expert en stratégie narrative, vit une aventure « métaleptique ». Par un jeu spéculaire, le comportement irrationnel du protagoniste renvoie le lecteur réel à sa propre responsabilité dans l’acte de lecture. La fable, traitée de façon ludique, illustre une métaphore de la lecture et donne lieu à un questionnement vertigineux sur les frontières entre réalité et fiction."
"Le lyrisme de Camus dans Noces et dans L’été est double : il proclame la fusion de l’homme et de la nature, avec la joie qui l’accompagne, tout en exprimant la conscience aiguë de la condition mortelle. L’article analyse les modalités d’écriture de ce lyrisme paradoxal dans les descriptions de paysage, en le comparant à celui de Philippe Jaccottet et de Francis Ponge. Il met notamment en évidence la façon dont le lecteur est associé à l’émotion du locuteur à travers la syntaxe, le lexique, et la priorité donnée au toucher et à la vibration mais aussi le choix d’une écriture volontairement simple qui atteint à une certaine impersonnalité et suggère plus qu’elle n’explique."
"L’article étudie l’importance du paysage dans l’œuvre d’Azorín (1873-1967), en particulier dans le Paysage espagnol vu par des Espagnols, datant de 1917. On y décèle la méthode du Levantin, « flâneuse et capricieuse, à la fois intense, subtile et épurée, sachant ne jamais faire long, consciemment ingénue et excessivement cultivée ». On y remarque « l’obsession descriptive et phénoménologique, le réflexe pictural, photographique et cinématographique du paysage, parfois abstrait ou géométrique ». L’auteur conclut qu’avec Azorin, on a affaire « à un Espagnol et à un Levantin, non plus ibéro-latin mais gréco-arabe, résolument, et même eurafricain, frappé de gallicisme mental »."
"Au cours du long congé qu’il obtient de 1836 à 1839, Stendhal, consul en Italie – la patrie de son cœur – depuis 1830, redécouvre dans la mère patrie une France profondément transformée par le tournant de la croissance pris sous la monarchie de Juillet. C’est de ce véritable choc que témoignent les « voyages en France », où le voyage de reconnaissance s’accompagne d’un retour sur soi quand les choses vues renvoient Stendhal au point aveugle de sa vision, de la France et du monde. À revoir la France, Stendhal revoit aussi son système axiologique pour aboutir, entre étude sur le vif et médiation de la fiction, à un partage sémantique entre civilisation et culture qui suit assez fidèlement la ligne de partage des eaux entre Méditerranée et Atlantique."
"Dans la dernière pièce de la « trilogie du Chiapas », Oficio de tinieblas (1962), Rosario Castellanos juxtapose trois époques : le soulèvement des Indiens chamulas au XIXe siècle, la Présidence de Lázaro Cárdenas, héritier de la Révolution mexicaine (1934-1940) et le présent de l’écriture. Par un travail de recréation de l’Histoire dans la fiction, l’auteure contrecarre le discours hégémonique et monologique des vainqueurs en nous plongeant dans la dimension magico-religieuse du mouvement messianique indien. Cependant, Rosario Castellanos véhicule l’idéologie de la politique indigéniste des années soixante selon laquelle les croyances mythiques sont un frein à la libération des Indiens, condamnés à rester en marge de l’Histoire."
"À partir des années 1860, l’immigration massive d’origine européenne bouleverse la constitution démographique et culturelle de l’Argentine. Parallèlement au développement d’une pensée nationaliste et xénophobe qui rejette l’influence des nouveaux venus, José M. Ramos Mejía et Raúl Scalabrini Ortiz contemplent à trois décennies de distance – le premier à travers la perspective positiviste, le second à travers celle avant-gardiste – les effets de ce mouvement de fond pour réfléchir en termes positifs à la constitution d’un homme nouveau, issu du métissage entre les vieux créoles et les Européens récemment débarqués."
"Cette étude poursuit les objectifs suivants : retrouver la vérité, ou ce qui reste de vérité ou de vraisemblance historique autour du personnage, d’autant que le mythe infiltre constamment ce qui nous est présenté de quatrième ou dixième main comme l’histoire d’Inês, y compris par des ouvrages de référence de type scolaire ou universitaire, tant en France qu’en Ibérie, de façon à mieux mesurer l’écart qui sépare le mythe (saisi en sa fondamentale variabilité) de ce que l’on peut retrouver ou inférer avec prudence de cette vérité historique ; appliquer une démarche mythocritique actualisée au thème d’Inês de Castro."
"Avant-propos C’est peu dire que les paysages méditerranéens ont fait l’objet d’une célébration constante, depuis cette Campania felix dont la bonne fertilité éblouit Pline l’Ancien, pays de cocagne que Nicolas Poussin retrouve dans l’Arcadie où il campe ses bergers et dont Claude Gellée suit l’épiphanie, de Délos à Carthage, d’Égypte en Galilée, d’Italie en Libye, sur tout ce pourtour méditerranéen qui sert de cadre à ses compositions où nature et culture se fondent idéalement, et jusqu’à l’h..."
"Médiateur par excellence entre le divin et le terrestre, l’ange – et l’épisode biblique de l’Annonciation en particulier, stimulent l’imaginaire contemporain, le mystère de l’incarnation ne pouvant être représenté mais figuré. S’appuyant sur le monde de références du spectateur – des textes source aux thèmes iconographiques médiévaux et renaissants (par exemple Giotto, Fra Angelico, Limbourg, del Cossa, Crivelli, della Francesca), L’Évangile selon saint Matthieu de Pasolini a principalement recours au son. De la musique sacrée (Bach, Misa Luba, Negro Spiritual) et profane (Mozart, Prokofiev, Blind Willie Johnson) aux chœurs qui fonctionnent comme des voix over, le film fait ‘entendre’ la présence angélique au spectateur. L’hybridité même des codes cinématographiques interroge le statut de l’image, tout en évoquant la présence immanente de l’ange –métaphore de l’indicible et l’inintelligible."
"Dans le domaine des politiques sociales, David Cameron a principalement développé deux points : le concept de Big Society et la nécessité d’effectuer des coupes budgétaires. Il s’agit ici de voir quel a été leur poids dans le domaine des politiques de retour à l’emploi. Ces points ont souvent été relayés dans la presse comme la manifestation de l’influence américaine en matière sociale, mais si cette source est indiscutable elle n’est certainement pas la seule. En effet, entre 1997 et 2007, alors que les conservateurs britanniques se trouvaient dans l’opposition, leurs homologues australiens étaient au pouvoir. La possibilité que les conservateurs britanniques se soient inspirés des réformes mises en œuvre en Australie mérite analyse. Les similitudes entre les propositions de David Cameron et les politiques de la coalition libérale-nationale australienne sont en effet remarquables."
"Dans le Club Dumas ou l’ombre de Richelieu, l’écrivain Arturo Pérez-Reverte, fervent lecteur, rend un hommage ludique au Livre. Deux manuscrits sont au cœur de sa construction labyrinthique, nouent l’intrigue et surtout dessinent, révèlent et déterminent les personnages. Héros d’Alexandre Dumas et créatures romanesques, filles de Conan Doyle et de Jacques Cazotte envahissent la fiction, poursuivent le protagoniste et l’amènent aux limites du fantastique. Mêlant les structures génériques du roman d’aventures et celles du roman policier, l’auteur entraîne le lecteur dans une quête herméneutique dont la fin est dans les livres. L’intertextualité y devient matière littéraire et facteur structurant du récit. Héritier de Borgès et de Cortazar, Arturo Pérez-Reverte. bouleverse les relations auteur-personnages, et redéfinit les liens qui unissent auteur et lecteur, attribuant à ce dernier un rôle prépondérant dans la genèse de l’oeuvre."
"La création d'une revue n'est en rien une obligation du parcours universitaire. On le sait, les revues naissent laborieusement, vivent dans l'incertitude et s'éteignent rapidement. Babel espère tenir, éterniser l'engouement d'une jeune équipe de collaborateurs, être plus qu'un éphémère ronronnement tintinnabulant. Babel, revue annuelle, entend donc jouer un triple rôle : contribuer au renforcement et à l'épanouissement d'un jeune pôle universitaire, celui de la Faculté des Lettres et Sciences..."
"Entre 1787 et 1851, les relations des ascensions au Mont-Blanc réalisées par des alpinistes britanniques présentent, en dépit de formes différentes, une « véritable unité sur le plan du contenu », à savoir : une expérience extraordinaire, la description détaillée d’un accident ou des souffrances des participants, des éléments précurseurs de la mesure d’une performance, et « une part non négligeable réservée aux expériences et aux observations scientifiques »."
"La disparition des opposants fut une méthode de répression employée de façon massive par les dictatures latino-américaines entre les années 1960 et 1980. Elle devait soutenir une politique de terreur totalitaire qui à bien des égards ressemblait à un autodafé honteux. Par ailleurs, le concept de disparition et les arguments de défense qu’il inspira aux bourreaux ne sont pas sans rappeler le discours des négationnistes sur la Shoah. Le traumatisme collectif provoqué par cette répression se lit dans de nombreuses œuvres de fiction qui révèlent toutes la diversité des choix esthétiques qui traversent la littérature contemporaine."
"Le regard hiératique et frontal de l’ange dans l’art chrétien byzantin, puis dans l’art roman, s’estompe à mesure que l’humanisme triomphe à la Renaissance : le regard de l’ange se détourne du spectateur, à l’image de l’évolution des pratiques religieuses et idéologiques et la réception des images cultuelles. Dans Les Ailes du désir de Wim Wenders, l’identification d’un ange passe par d’autres détails (iconique, pictural ou particolare). Le « regard à la caméra » adressé au spectateur et les jeux de regards à l’écran rétablissent l’interaction avec l’image (dé-figurée) de l’ange. Le rapport entre image et propagande, dans l’économie byzantine comme dans l’Occident moderne et ses médias, se voit transformé par le réalisateur allemand en une interrogation sur l’Histoire, non dénuée d’ « émerveillement », au sens médiéval du terme."
"Un pastiche à la fois moins connu et plus original que l’archétype satirique de Daudet dans son Tartarin sur les Alpes, se trouve dans une liste de récits de course écrits par Etienne Bruhl « à la manière » d’un grand nombre d’écrivains. L’article analyse un pastiche attribué à Marcel Proust, récit en trois pages, trois paragraphes et trois phrases écrit par un auteur lui-même alpiniste de haut niveau."
"Il s’agit d’une réflexion autour du motif de la guerre présente et future sur le mode de l’uchronie selon un schéma habituel dans les romans de Jules Verne. Les Cinq cents millions de la Bégum correspondent dans la fiction à l’héritage miraculeux d’une grand-tante éloignée, ladite Bégum, épouse en secondes noces d’un lointain parent commun aux deux héros selon une généalogie fictive. L’héritage fabuleux se trouve partagé entre deux ennemis, parents par le hasard d’un autre mariage : d’une part Herr Schultze, professeur allemand passionné de chimie, d’autre part François Sarrasin, docteur français préoccupé par les problèmes de santé et d’hygiène. Et c’est à l’occasion d’un Congrès international d’Hygiène à Londres en 1871 que le docteur français lance le projet philanthropique d’une Cité modèle. L’histoire, comme intrigue romanesque, est une transposition de l’Histoire, du conflit franco-prussien, sous forme d’une uchronie projetée vers la marge nord-ouest des États-Unis, une utopie située géographiquement mais en une sorte de non-lieu produit par l’éloignement. Dans la fiction, l’héritier allemand riposte en annonçant au même Congrès devoir fonder une Place forte, Stahlstadt, la Cité de l’Acier. Mais cette dialectique manichéenne structurant le roman serait faussée par une absence, cette fonction vacante révélée par le chaos du conflit entre la Cité de l’Acier et ladite Cité de la Paix."
"Existe-t-il une littérature de montagne ? C’est la question que posait Bernard Amy, à la fois alpiniste, chercheur et écrivain dans un article publié en 2005. Comme la littérature de voyage, dont elle est sans doute un avatar, il en existe de nombreuses variantes et d’autres genres sont sans doute encore à naître. Au-delà de l’exploit ou de l’aventure, les voyages et les courses en montagne ont également un horizon verbal. S’agit-il d’écrire ou de décrire les sommets ? Sans chercher à répon..."
"Henri Ghéon a composé un Œdipe chrétien renvoyant largement à la Bible et particulièrement aux œuvres de saint Paul. La découverte du dieu inconnu, dieu d’amour, par Œdipe s’accompagne d’une mise en scène christique – et astrologique – de son destin."
"Est ici évoqué le mirage de Tanger, cette ville portuaire du Maroc maritime à la situation privilégiée sur le détroit de Gibraltar, entre Atlantique et Méditerranée, et qui a connu un engouement particulier durant le dernier demi-siècle, lorsque les trois apôtres de la Beat Generation, Burroughs, Ginsberg et Kerouac vinrent y séjourner, précédés par l’Américain Paul Bowles, compositeur, écrivain et voyageur qui fait figure de précurseur en venant s’installer définitivement à Tanger au lendemain de la Seconde Guerre Mondiale. De ces rencontres artistiques, littéraires et psychédéliques entre expatriés à Tanger naîtront à la fois le chef d’œuvre avant-gardiste de Burroughs, Le Festin nu, où Tanger se dissout dans une expérience intérieure et, parallèlement, une grande œuvre de la littérature marocaine classique, Le Pain nu, de Mohamed Choukri, traduit en anglais par Paul Bowles, et qui restitue la cité portuaire avec le réalisme d’un récit autobiographique traditionnel. Deux œuvres frappées par la censure dans leur pays d’origine, qui illustrent différemment l’envers du décor tangérois fait de transgressions, d’expériences qui ne sont pas étrangères aux démons de la contre-culture américaine comme de la misère du Maroc illustrée par le roman de Choukri, qui a été traduit par Ben Jelloun."
"Le présent article s’appuie sur un texte méconnu d’Eugène Sue, Les Mystères du peuple, entrepris au lendemain de la révolution de 1848 et qui suit, sous l’antagonisme millénaire qui affronte une famille de prolétaires à une maison aristocratique, une lutte des classes que Sue reconnaît aussi millénaire. Un roman dont l’intérêt tient aussi à ce que Sue en appelle, pour fonder sa démonstration, à un système de représentations entériné par la Révolution française, qui fait des Gaulois les représentants du peuple, porteurs des valeurs de la démocratie et du droit. Une pente qui s’accusera tout au long du XIXe siècle, jusqu’au triomphe que leur consent la Troisième République. Avant, ironie de l’histoire, à moins qu’il ne s’agisse du principe de l’Histoire, qui puise au conflit même, que ces Gaulois porteurs de liberté et de civilisation ne soient instrumentés au service de causes moins nobles, avec la colonisation."
"A travers quelques écrits d’alpinistes austro-allemands (Guido Lammer, Emil Zsigmondy, Leo Maduschka, Hermann Buhl, Walter Pause), et par l’analyse de la politique de la jeunesse du Club alpin austro-allemand de l’entre-deux-guerres, il est démontré que l’initiation à la montagne est également acceptation, voire souhait, de la mort. Mais l’évolution de l’alpinisme vers plus de sécurité et les changements de génération s’opposent désormais à cette tendance morbide."
"Dans cet entretien, l’auteur interroge le poète grec Théo Crassas sur ses raisons d’écrire en français. Le contexte familial - un oncle poète et francophone, ainsi que ses parents, éclaire ce choix. S’il est vrai que le français n’est plus la langue impériale qu’il fut longtemps, il n’en demeure pas moins une école universelle, de par ses auteurs classiques qui ont consacré son universalité. À ce titre, le français demeurera aussi capital dans l’histoire des humanités que le grec ancien ou le latin. Crassas se définit comme un cas singulier, car il n’écrit ses textes qu’en français."
"De tout temps, les déplacements de l’homme furent naturellement influencés par les limites de sa condition physique et les facteurs climatiques propres à chaque latitude. De cette constatation naquit la célèbre « théorie des climats » qui prétendait définir, par une généralisation parfois péjorative, le volkgeist de chaque nation. Si Ford Madox Ford partageait l’essence de cette idée, établissant une distinction nette entre Nord et Sud, de par leurs spécificités et leur interaction à travers l’Histoire, il émit aussi la possibilité d’un rapport complémentaire. L’historique Route de la soie devint pour lui le symbole de cette symbiose, unifiant les nations de part et d’autre de sa courbe. L’antique système commercial serait-il la clé d’un monde meilleur ? Serait-il le remède à cette nouvelle ère industrielle où la surproduction règne et le petit producteur se meurt ?"
"Avec Le Roi s’amuse (1832), Hugo tire des oubliettes de la mémoire collective le personnage de Triboulet, mais au prix d’une relecture volontairement infidèle de la figure du fou de cour, faisant de celui-ci l’impitoyable observateur des futilités courtisanes, dissimulant sous le sarcasme le sentiment de son infériorité sociale et de sa difformité. Les nombreux témoignages littéraires qui couvrent les XVe et XVIe siècles et portent sur trois Triboulet successifs, régulièrement confondus par la suite, rendent un son bien différent, même s’ils permettent d’observer une mutation du statut des fous de cour. Ceux-ci, des simples idiots qu’ils étaient la plupart du temps jusqu’au XVe siècle, paraissent acquérir au tournant du Moyen Âge et de la Renaissance une intelligence nouvelle. L’étude de la mise en scène de leurs réparties les plus spirituelles dans les textes d’époque évoquant les trois Triboulet permet ainsi de poser la question de la nature du fou de cour, précisément au moment où l’humanisme rhénan s’empare de la figure à marotte pour en faire le miroir des folies humaines."
"Réduire le poids du motif ? J’ai appris à me méfier des poèmes travaillés à bout portant sur le motif, et dont le poids me condamne plus ou moins à la prose, lieu de la précision référentielle, prose parfois longue, très longue, difficile à maîtriser parce qu’il faut la garder poétique, la construire, alors qu’elle est plus ou moins guettée par l’universel reportage du sujet. Le maximalisme opérant fidèlement sur le motif présente quelque chose d’ingrat, d’inégal – ce qui ne veut pas dire que..."
"Écrivain français nourri de culture russe, Henri Troyat est l’auteur d’une œuvre profondément marquée par l’influence conjuguée de la littérature tant russe que française. Cette influence se reflète dans le cœur et l’esprit de ses personnages qui balancent, au gré des lectures que l’auteur leur prête, entre deux mondes littéraires, le russe et le français. C’est à l’exploration des bibliothèques imaginaires d’Henri Troyat qu’est consacré cet article."
"Existe-t-il un type Catulle (poète fiévreux, sensuel, insolent, mort jeune à trente ans, de goût au besoin oriental à l’exemple de Callimaque, le maître libyen et alexandrin de notre auteur) ? Peut-être. Pessoa, toujours en quête de doubles majeurs, place même, abusivement, sous l’invocation de Catulle son ami Sá-Carneiro, suicidé à près de 26 ans, et tous les génies exploratoires aimés des dieux donc condamnés à une mort précoce de l’esprit. Il y en a en tout cas beaucoup de Catulles dans la littérature universelle (latine, arabe avant et après l’Islam, ibérique, française), certains de ces poètes ou écrivains français morts parfois jeunes ayant eux-mêmes traduit ou adapté Catulle au tournant du siècle (Jean de Tinan, Jean-Marc Bernard et quelques autres Poètes de l’École Fantaisiste ici reproduits)."
"Lorsque intervient la révolution américaine, c’est désormais doublement que les États-Unis font figure de Nouveau Monde : non plus seulement géographiquement, comme lointaine frontière, mais bien aussi politiquement, porteurs qu’ils sont des promesses d’un contrat social plus équitable et d’un modèle de développement, peut-être appuyé sur la conquête, mais sur celle de la liberté. Ce sont ces principes originels, portés sur les fonts baptismaux de la jeune république, dont le présent article s’attache à suivre, dans une vieille Europe qui se regarde au miroir d’outre atlantique, la progressive remise en cause tout au long de ce XIXe siècle qui a vu le fonctionnement de la référence américaine accuser un douloureux divorce entre le contenu symbolique qu’elle revêt et l’expression pratique qu’elle reçoit. Un miroir progressivement éloigné et de plus en plus terni à mesure que l’image qu’il renvoie prend la forme d’un expansionnisme brutal. Alors, le prisme américain laisse place à des spectres."
"Cet article envisage, à partir du cas symptomatique d’Edgar Quinet, les stratégies de contournement qu’impose l’exil pour continuer de peser sur l’histoire intellectuelle de son temps et d’alimenter la circulation des idées. Privé de l’audience que lui donnait son titre de professeur au Collège de France, Quinet retrouve une chaire virtuelle en continuant de se faire entendre par son œuvre et au travers de ses disciples, souvent d’anciens étudiants, qui assurent la diffusion de la parole du maître à l’échelle du continent européen comme au-delà, où elle sert les combats pour la démocratie aux Amériques, prouvant que le décentrement n’empêche pas le recentrement quand cette dissémination pose les bases d’une Internationale de la résistance qui se constitue en réseau."
"En 1981, Jean Ricardou écrivit une étude intitulée « Bien faire, et laisser dire », qui se penche sur les rapports entre le prologue du Jardin aux sentiers qui bifurquent et certaines des nouvelles de ce recueil de Jorge Luis Borges. Afin de rendre hommage au critique français disparu en 2016 et d’engager un débat avec ses écrits, nous rééditons cet article en annexe, à la suite d’un commentaire où nous nous efforçons d’évaluer, près de trente-cinq années plus tard, certaines des implications de son analyse."
"Cahier de verdure est le premier ouvrage de Philippe Jaccottet réunissant à la fois des proses et des poèmes. Cette première forme d’hétérogénéité se double de différences typographiques et énonciatives entre trois sortes de textes composant ce recueil. Après les avoir décrites, nous essayons de montrer que ces différences correspondent à trois façons d’approcher le réel : par notations au fil des jours, par retour sur un événement marquant et reformulations successives de l’impression ressentie, par décantation et condensation. Cahier de verdure est à cet égard à la fois un prolongement de la recherche poétique antérieure de Philippe Jaccottet où l’on observait déjà ces trois modes d’écriture, et une étape nouvelle dans la mesure où de leur mise en regard naît un dépassement de l’opposition continu/discontinu caractérisant respectivement, selon Jaccottet, la prose et la poésie."
"Marqué par le triple sceau de la théologie, de la littérature et de la critique, le mythe de Babel engendre une réflexion sur les fondements de la pensée occidentale. Le texte de la Genèse XI est une source considérable de création et de questionnement qui, de saint Augustin à Joyce ou Perec, demeure particulièrement féconde."
"Dans cet article, l’auteur retrace l’histoire de l’une des bibliothèques les mieux fournies au monde en livres sur la montagne et l’alpinisme : The Alpine Club Library. En vertu de la richesse et de la qualité de ses cartes géographiques, de ses revues spécialisées et de ses livres, la collection du club constitue une mémoire vivante de l’alpinisme et représente une source incontournable pour le chercheur en quête de la geste de l’Âge d’or de cette discipline."
"L’étude de plusieurs manuscrits médiévaux dits ‘de théâtre’ fait apparaître que la mise en recueil des textes au Moyen Âge est généralement sous-tendue par un projet édifiant. Tandis que l’ordonnancement des Mystères de la procession de Lille, conservés dans le ms. Guelf. 9 Blankenburg, témoigne du projet didactique et édifiant du maître d’œuvre du recueil, l’étude du ms. 57 de la Bibliothèque Municipale de Rodez, qui mêle le Mystère de l’Ascension de la Vierge à des textes non dramatiques, montre que le volume a été conçu comme un livre de méditation. Enfin, l’analyse du ms. Condé 616, qui ne contient que le Mystère de la Conception de Nostre Dame par parsonages, tend à démontrer que le travail de compilation qui est à l’origine de cette pièce peut être envisagé comme une forme particulière de mise en recueil propre à l’écriture dramatique."
"Cet article invite à envisager la relation complexe entre les Européens et leurs cousins du Nouveau Monde, notamment à travers le tourisme. Dès le milieu du XIXe siècle, les progrès dans les moyens de transport font de l’Europe un lieu de prédilection pour le touriste américain. Entre 1830 et 1900, c’est plus particulièrement vers Paris que les jeunes bourgeois Américains (et quelques privilégiés de la classe moyenne), en quête de connaissances et d’apprentissage, affluent en masse. Ce voyage représente, pour cette nouvelle génération, l’opportunité unique d’acquérir une expérience personnelle et professionnelle, alors sans égale dans le Nouveau Monde. Ce groupe de jeunes immigrés américains joue un rôle majeur dans l’évolution du climat intellectuel et artistique de Paris, à la fin du XIXe siècle. C’est à cette même époque que la Côte d’Azur – Menton, Nice et Cannes surtout – devient une des destinations préférées des Américains durant les longs mois d’hiver. Les côtes françaises de la Méditerranée attirent de riches familles américaines qui réussissent à donner un nouvel essor à la French Riviera, comme ils la surnomment alors."
"Les pratiques culturelles et les traditions constituent le socle de l’identité d’un peuple ou d’une ethnie. Ces pratiques sont diverses et concernent tous les domaines de la vie sociale. Dans cet article, nous nous intéresserons plus particulièrement aux pratiques liées à la conservation de la virginité des jeunes filles dans les pays du Maghreb et à la manière dont elles sont traitées d’un point de vue littéraire dans le roman de Leïla Marouane La jeune fille et la mère (2005). Dans quelle mesure la littérature participe-t-elle d’une remise en question de certaines pratiques ancestrales autour du bassin méditerranéen ? Et quels sont les enjeux de la mise en mots de ces pratiques ? Pour répondre à ces questions, nous ferons tout d’abord un tour d’horizon des différents rites liés à la virginité et à l’importance sociale et familiale qu’ils revêtent, puis nous mettrons ces pratiques en perspective avec le discours littéraire de la romancière franco-algérienne en tentant de montrer quel rôle peut jouer la littérature dans la rupture d’avec un héritage culturel. Enfin, nous traiterons plus particulièrement du rôle du matriarcat dans la perpétuation de l’idéologie machiste que véhiculent certaines de ces pratiques."
"Ovide dans les Fastes affiche le projet d’abandonner son éthos de poète badin, éthos endossé dans les œuvres précédentes de registre élégiaque. Il entend adopter la posture officielle, celle du uates, présent chez Virgile dans l’Énéide : il serait le poète inspiré des dieux qui chante les valeurs nationales, glorifiant la famille du prince et adhérant à la propagande augustéenne. Mais on note des éléments de discordance, notamment dans l’origine de son inspiration : ce n’est pas la Muse épique mais le doctus princeps qui l’inspire, le transformant en poète de cour destiné à flatter. De plus, Ovide délègue souvent sa parole à une divinité quand ses connaissances lui font défaut. Ce procédé, fréquent dans les œuvres étiologiques, lui permet de se dissocier du discours tenu comme n’étant pas le sien et c’est justement dans ces passages que l’on retrouve une certaine moquerie à l’égard des héros fondateur (Romulus notamment), qui fondent la propagande augustéenne. Cette discordance entre l’éthos affiché et l’éthos en sourdine ainsi que le choix du mètre élégiaque montrerait qu’au contraire Ovide renoue dans cette œuvre avec sa Muse légère et taquine. Elle s’explique également par un contexte littéraire où les auteurs élégiaques réfléchissent à leur liberté de création, notamment par rapport à la politique et aux genres traditionnels."
"La mise en scène d’un texte dramatique du Moyen Âge implique aujourd’hui de nombreux choix dont le présent article rend partiellement compte, en s’appuyant sur celle du Jour du Jugement par Bénédicte Boringe en 2007 dans l’église de Veyrier-du-Lac. Les trois principaux aspects envisagés sont la nature du texte joué, l’organisation de l’espace scénique en relation avec la place qu’y occupe le meneur du jeu et la prise en compte du public visé pour la représentation. Il apparaît ainsi que refaire l’expérience d’un théâtre populaire permet d’éviter l’écueil d’un théâtre expérimental pour lequel la représentation n’est finalement qu’un laboratoire d’analyse théâtrale."
"L’article étudie tout d’abord la façon dont Roubaud se distancie du genre élégiaque par un positionnement décalé du locuteur et une hétérogénéité générique qui contestent la possibilité de la poésie. Il étudie ensuite la façon dont s’inscrit dans le texte la femme aimée et disparue, entre présence et absence. Il étudie ensuite dans les détails la fragmentation syntaxique et la répétition caractéristiques de ce texte : d’abord signes de chaos et de sidération, elles changent peu à peu de forme et indiquent la réapparition progressive d’une parole poétique qui traverse l’aphasie et la sidération et parvient à bâtir sur le vide."
"Le réalisateur français de cinéma d’animation Michel Ocelot a réalisé une trentaine de courts métrages avant son premier long métrage au succès retentissant, Kirikou et la sorcière (1998), suivi de Kirikou et les bêtes sauvages (2005). Dans cet entretien, il s’exprime sur deux de ses œuvres proches du conte de fées, faisant la part belle au Moyen Âge : Princes et princesses (1999), film d’animation dont le montage suit la technique des ombres chinoises et Azur et Asmar (2006), film d’animation en 3D aux décors somptueux inspirés de l’iconographie de la fin du Moyen Âge. L’entretien met à jour la fascination exercée par les traditions visuelles et narratives, tant occidentales qu’orientales, de l’imaginaire médiéval."
"À travers son film Le Destin (1997) et le portrait d'Averroès (ibn Rushd), grand juge, philosophe et conseiller du calife al-Mansur, le réalisateur égyptien Youssef Chahine (1926-2008), évoque l'Andalousie du XIIe siècle et plus particulièrement la ville de Cordoue, capitale des Almohades et lieu d'affrontements entre extrémistes musulmans et savants soucieux de la diffusion des connaissances. Le recours au Moyen Âge est, pour le réalisateur, un moyen de «mieux saisir la complexité du présent »."
"L’usage que Ridley Scott fait des sources dans Gladiator a été plus que décrié par la critique, qui se gausse des anachronismes, et il n’est jusqu’à ses conseillers mêmes qui ne lui reprochent de ne s’être entouré de spécialistes que pour mieux renoncer à une lecture puriste de l’Histoire. Voilà pour la vulgate, qu’il s’agira ici de dépasser, en montrant, à partir d’un parallèle inattendu avec L’Histoire Auguste, que le fonctionnement du film pourrait s’avérer infiniment plus retors qu’on ne le croit généralement, dans un commun recours aux vertus du storytelling mobilisées contre la servilité factuelle de la vérité historique récusée comme la seule propre. Contre ses faux prestiges, qui fossilisent le passé, Scott joue des pouvoirs de la fiction et du mentir-vrai qu’elle autorise pour lui redonner toute son actualité au point que, loin d’assassiner l’Empire, c’est un empire des signes redivivus qu’il donne, dans toute sa complexité et son ambivalence."
"Il s’agit dans cet article d’explorer la méditerranéité de Philippe Jaccottet qui s’avère plus culturelle que géographique. Un parcours de Paysages avec figures absentes nous permettra de mettre en évidence la fécondité de la référence à l’Antiquité gréco-latine dans l’œuvre de Jaccottet, en particulier pour penser une poésie qui se propose d’inscrire dans le langage l’expérience du sacré offerte par certains lieux privilégiés."
"De la critique de l’adaptation… Ce numéro de la revue Babel-Littératures Plurielles entend rouvrir le dossier des relations entre cinéma et littérature pour rendre compte des interpénétrations entre leurs langages respectifs, sans pour autant restreindre le corpus des œuvres étudiées aux seuls films qui ont emprunté leur sujet à une œuvre littéraire. Les rapports entre littérature et cinéma ne se réduisent pas au seul phénomène de l’adaptation. Loin de pouvoir rendre compte de toutes les form..."
"Dans cet article d’ouverture, nous proposons une brève présentation du genre élégiaque dans son contenu thématique, sa tonalité et son rapport au chant. Dans l’élégie, la poésie entendue comme rythme et forme canalise l’expression de la douleur au risque de la faire paraitre artificielle. L’élégie oscille ainsi entre l’écueil d’une trop grande maitrise et celui d’un abandon à l’informe, avec des arbitrages très différents entre ces deux risques selon les esthétiques et les époques, comme le montrent la variété des textes étudiés qui vont de l’Antiquité latine à l’extrême modernité en passant par André Chénier, le romantisme, Baudelaire, Verlaine et Laforgue. Sur le versant énonciatif, on observe aussi une grande diversité depuis des textes qui visent à faire communier dans la plainte locuteur et récepteur et d’autres qui cultivent une distance ironique ou visent à une disparition du sujet. L’élégie n’est pas dépourvue non plus d’une dimension communautaire et politique dans certaines de ses manifestations et le volume lui fait justice en montrant à la fois la pérennité de ce genre et sa plasticité."
Les recueils manuscrits du Moyen Âge ont longtemps été considérés par les médiévistes eux-mêmes comme des objets hétéroclites et sans autre mérite que celui d’avoir porté à notre connaissance un grand nombre de textes. Au mieux ces manuscrits volumineux pouvaient-ils faire figure de bibliothèque. Mais l’hétérogénéité des œuvres rassemblées par les copistes et l’absence apparente (ou supposée) de principes organisateurs dans les recueils ont souvent déconcerté les plus grands spécialistes de l...
"En tant que sous-genre spécifique, le récit d’enquête respecte des prescriptions assez constantes qui participent à l’élaboration d’un univers fictionnel particulièrement bien réglé. Agatha Christie offre par exemple un modèle très efficace où chaque élément participe à imposer un système qui, du fait de son renforcement par la répétition, atteint un caractère rituel. L’écrivain argentin Rodolfo Walsh a composé une œuvre qui joue habilement avec ces règles, soit pour les réduire à une épure abstraite, soit pour les trahir."
"Après avoir consacré un précédent volume de la revue Babel – Littératures plurielles à l’étude d’une œuvre emblématique située à l’époque charnière qui voit le monde occidental passer du Moyen Âge à la Renaissance, l’équipe Monde Médiéval & Renaissant, rattachée au laboratoire Babel (EA 2649), a choisi de poursuivre ses recherches sur la période médiévo-renaissante dans une perspective plus large en abordant le thème de la folie. Continuant de privilégier dans ses travaux une approche pluridi..."
"Depuis l’Evangile de la beat generation avec la publication en 1957 d’On the Road de Jack Kerouac à l’Apocalypse figurée par la mise en fiction d’un fait divers tragique avec le Voyage au bout de la solitude paru sous le titre original d’Into the Wild en 1996 sous la plume du journaliste écrivain Jon Krakauer, relatant l’histoire vraie d’un jeune homme ayant trouvé la mort pour avoir abandonné la civilisation pour la nature sauvage du Grand Nord, c’est toute une parabole du désenchantement qui s’inscrit dans un demi-siècle de littérature et de cinéma américains. Après le rêve d’une fuite hors de la société matérielle par le recours à des spiritualités autres ou des paradis artificiels, tentés par les trois apôtres de la beat generation, Kerouac, Ginsberg et Burroughs, le désenchantement au sens fort d’éloignement des dieux même illusoires de la précédente génération se concrétise par la mort dans la nature hostile de l’Alaska et par le symbole fort d’un bus rouillé transformé en abri puis en sépulcre."
"Fruit d’une lente évolution, le « paysage » fait irruption dans la peinture européenne au cours du XVe siècle, en particulier dans de somptueux manuscrits enluminés. L’inscription du paysage dans le programme iconographique du manuscrit fr. 5594 ne sert pas seulement un discours de propagande chrétienne d’exhortation à la croisade, mais donne également lieu à la création d’un paysage méditerranéen hybride, entre Occident et Orient. Ce décor enluminé, naturel ou urbain, est un espace fantasmé qui mêle éléments réalistes et exotiques, architecture gothique, mais est également chargé de symbolique. La forteresse, le rocher, la mer ou le « désert-forêt » des romances construisent une vision poétique et politique de l’espace, dans un programme où l’image demeure avant tout au service d’un discours idéologique dynastique et militaire."
"Le metteur en scène et acteur français Denis Llorca a dirigé le Centre dramatique national de Franche-Comté (1982-1991), avec lequel il a monté d’audacieux spectacles au texte dense, à la durée inhabituelle pour le spectateur moderne : Kings (1978) d’après Shakespeare [dix heures] ; Eux, les possédés (1982) d’après Dostoïevski [huit heures], ou encore Quatre Saisons pour les Chevaliers de la Table Ronde (1989) d’après la légende arthurienne [12 heures]. Avec sa propre compagnie, il a ensuite porté à la scène un projet de sept spectacles s’inspirant des mystères médiévaux, avec des textes empruntés à la Torah, aux Évangiles et au Coran, où le sublime se mêle au grotesque. Dans cet entretien, le réalisateur revient sur ses multiples visions du Moyen Âge dans sa recréation du mythe arthurien au théâtre, mais aussi à l’écran, dans le film qu’il a tiré de la pièce: Les Chevaliers de la Table Ronde (1990) avec Maria Casarès, Alain Cuny et Michel Vitold."
"Le bilinguisme de Supervielle est déterminé, selon l’auteur de cet article, par la naissance du poète en Uruguay et ce qu’il nomme les « Routes de l'Hispanité » : les correspondances d'émigrés béarnais des Amériques retrouvées à ce jour, provenant des pays de la Plata et en priorité de Montevideo. Supervielle serait « le plus français des poètes espagnols », selon un mot de Claude Roy. Ces routes sont également les « Routes du Drame » car à l’âge de 8 mois, le poète perd ses parents, qui s’empoisonnent alors qu’ils venaient juste de revenir en Béarn. Le poète retourne alors en Amérique du Sud où il est élevé par son oncle et sa tante. L’auteur soutient que bien des traits de la poésie de Supervielle proviennent de cet écart entre l’Amérique du Sud et la France, quand l’écriture poétique s’apparente à une source de guérison."
"La critique claudélienne, de façon récurrente, au fil des générations, a multiplié commentaires et remarques touchant les aspects religieux du théâtre de Claudel, avec une prédilection logique pour son opus majeur, Le Soulier de satin. Toutefois, cette production critique semble fournir davantage de perspectives à portée générale que des explications précises et déterminantes. Il conviendrait en somme de recourir à un point de vue résolument théologique. Maintes fois, Claudel a d’ailleurs pri..."
"Avec la naissance de l’alpinisme sportif se popularisent les récits de course, que leur modestie et minimalisme font passer pour des productions sans grand intérêt, selon les critiques initiées par John Ruskin. Mais au-delà de l’aspect factuel et descriptif, il s’agit bien d’une véritable geste et d’une épopée où il est question tout autant d’une histoire culturelle des élites victoriennes que de l’affirmation de la puissance impériale britannique."
"Cet article invite à découvrir, dans une optique comparative, deux récits de voyage : le premier The Cruise of the Vanadis, de la romancière américaine Edith Wharton, carnet de bord d’un tour de la Méditerranée, entrepris en 1888, en compagnie de son mari Teddy et publié posthumément en 1992, le second, Sea and Sardinia (1921), de l’écrivain anglais D. H. Lawrence qui décrit un bref séjour de Taormine en Sicile jusqu’en Sardaigne avec sa femme, Frieda. Ces deux regards croisés seront l’occasion de comparer non seulement deux manières bien différentes de concevoir un voyage d’agrément, mais aussi deux manières de s’adresser au lecteur. Si le regard de Wharton est porté presque exclusivement sur la nature et les vestiges de l’antiquité, dans un monde où les êtres humains sont étrangement absents, celui de Lawrence, en revanche, est filtré par ses propres états d’âme et par sa relation houleuse avec Frieda."
"Denis de Rougemont dans son essai La Part du Diable s’est efforcé de prouver que la Tour de Babel est exemplaire de l’action du « diable dans nos dieux et dans nos maladies ». Plus proche des analyses contenues dans les Mythologies de Barthes que de celles rencontrées au fil des traités de démonologie, Denis de Rougemont dénonce le modernisme qui a, de fait, consacré Babel « grand mythe de notre temps » (p. 146). La thèse avancée a pour fondement « la babélisation des cadres matériels de notr..."
"C’est par la figure du prisme, qui renvoie à la vision d’une réalité déformée, qu’il serait possible d’envisager les représentations littéraires des Amériques depuis cinq siècles. Il y a d’abord l’invention au sens fort de l’Amérique, par la geste de Colomb, soucieux d’évangéliser le Nouveau Monde en une croisade dont il rend compte aux Rois catholiques. Les noms de baptême des anciennes Antilles traduisent cette vision chrétienne d’une Terre de Promission, qui devient vite Terre de Perdition..."
"La Collégiale de Clans (1137) offre un exemple de « géographie sacrée ». Le bâtiment de la Collégiale s’inscrit dans un paysage qu’il christianise. Les fresques médiévales et, plus tard, la Collégiale baroque, entourée de son chapelet de chapelles, illustrent les liens entre paysage, zodiaque et symbolique chrétienne. En outre, il est manifeste que la culture populaire des villageois et celle des clercs trouvent en la Collégiale un aboutissement commun : le bâtiment de pierres devient une humble et insistante prière."
"Née à l’instant du passage entre Moyen Âge et Modernité, La Celestina (1499-1502) de Fernando de Rojas épouse la dynamique de rupture au sein de laquelle elle voit le jour. Un examen des divers modes d’écriture du personnage, mais aussi des visages de l’humain qu’ils façonnent, permettra de saisir ce mouvement fait de ruptures et dissidences qui emporte La Celestina de la fin du Moyen Âge jusqu’à la dernière modernité."
"Avec la publication de La cuestión palpitante, série d’articles avec lesquels elle introduisit le naturalisme en Espagne, Emilia Pardo Bazán fut très critiquée par ses contemporains. Cette rupture par rapport à la norme sociale, vers laquelle ni son statut ni son sexe n’auraient dû la mener, fut accompagnée d’une autre rupture : celle d’avec les conventions littéraires. En effet, dans de nombreux récits courts, le milieu rural est dépeint sans aucune idéalisation mais est au contraire le théâtre de nombreux crimes. Pour Emilia Pardo Bazán, la cruauté ne fut pas simplement un moyen de s’imposer dans un monde littéraire masculin mais aussi un formidable outil pour dénoncer l’ignorance, la cupidité d’une société viciée."
"Ce vingt-sixième volume de la revue Babel recueille vingt contributions explorant le monde hispanique, divers et multiculturel mais entretenant depuis de nombreux siècles des liens d’amour et de haine, liens célébrés ou décriés dans une langue elle aussi marquée par de multiples singularités, mais restant commune à l’ensemble de ce monde. Réunies autour d’une réflexion sur la notion de rupture par rapport à une norme artistique ou non artistique – distinction qui guidera cet avant-propos –, c..."
"Dans ce numéro de la revue Babel, nous nous proposons de faire un état des lieux, en ce début de XXIe siècle, sur le Moyen Âge comme objet de représentation et d’interprétation au théâtre comme au cinéma. Ses différentes mises en scène convoquent à des degrés divers les sens du spectateur et, selon leurs conditions de production, provoquent l’imaginaire de chaque individu ou requièrent la participation collective du public. Nous avons souhaité confronter des perspectives variées, en ouvrant c..."
"S’intéressant au parcours méditerranéen que dessine Mathias Sandorf dont les héros sillonnent de part en part le bassin méditerranéen, cet article s’attache à comprendre la logique qui gouverne cet espace jusque dans la création d’une île fictive au fond du golfe de Tripoli, Antékirtta, où, pour un comte Sandorf hongrois devenu l’apatride docteur Antékirtt, construire une utopie à même de ravauder les blessures de l’histoire, lesquelles doivent beaucoup à la violence des impérialismes d’État, secoués par le mouvement des nationalités. Ce faisant, il s’agira de replacer le roman et sa signification dans les enjeux contemporains et de le relire, moins comme une réinterprétation fictionnelle des croisières de 1878 et 1884 au cours desquelles Jules Verne découvre ces espaces pour lui exotiques, que comme un dialogue avec les théories que Renan vient de professer dans son essai Qu’est-ce qu’une nation ? (1882)."
"Dans Le Mystère de saint Vincent (Angers, 1471 et 1476 ?) tel qu’il nous est parvenu dans le ms. BnF, fr. 12538, les formes traditionnelles de la folie sont représentées de manière marginale et souvent allusive. L’étude du paradigme morphologique du mot folie dans la pièce permet cependant de définir trois formes de folie qui, au-delà du critère religieux qui opposerait diamétralement le christianisme au paganisme, se distinguent et se complètent d’une manière asymétrique. La détermination de ces différents types de folie dans le mystère révèle dans le discours du Mystère de saint Vincent une forme de conservatisme que l’étude d’autres mystères plus tardifs vérifierait sans doute. Elle permet du moins faire l’hypothèse que le discours des mystères assure jusqu’au cœur du XVIe siècle la permanence de la pensée médiévale sur la question de la folie."
"En amont de la French Theory, véritable source d’inspiration pour les chercheurs sur les études de genre, il semble à certains égards que l’on puisse parler d’influence directe de l’œuvre gidienne. En effet, les différentes théories liées à la sexualité qui ont pu voir le jour à partir des années 1960 en France puis quelques années plus tard Outre-Atlantique sont à divers égards inspirées par André Gide. Comme l’auteur le reconnaît lui-même, le travail du littéraire exige du lecteur qu’il fas..."
"Rousseau, à la fin de sa carrière, prêt à renoncer à toute communication, se nourrit apparemment sans s'en rendre compte d'un tuf littéraire qui renvoie à l'enfance, malgré la cruauté de ses images, mais on sait depuis La Fontaine que "" cet âge est sans pitié "". Ce n'est pas la dimension la moins émouvante de cette reconquête éperdue et presque impossible d'une identité défaite dans le discours des autres, de cette recherche d'une parole éclatée mais vraie. Si "" son cœur transparent comme le cristal ne peut rien cacher de ce qui s'y passe ""), son écriture fonctionne comme une lanterne magique où les images se font et se défont pour acquérir valeur d'argument, comme un kaléidoscope où, parmi les éclats de voix, se refait le visage d'un écrivain défiguré par le chagrin, et se ressaisit une unité toujours menacée."
"L'usage que le libertin fait du pathétique est toujours marqué au coin de la duplicité et s'inscrit dans une stratégie qui consiste à utiliser momentanément le langage de la victime pour la battre sur son propre terrain. L'illusion de la Présidente de Tourvel est de convertir par les larmes le libertin aux bons sentiments. Or quand le libertin se montre sensible au pathétique qui émane de sa victime, son émotion est de nature esthétique, voire érotique, mais ne constitue en aucun cas une adhésion aux valeurs morales qui le sous-tendent. Le libertin sait distinguer le spectacle esthétique de son intention moralisatrice, alors que la victime cherche à les confondre dans l'effusion lacrymale. Cette ambiguïté du pathos, constitutive du roman de Laclos, est radicalisée par Sade dans l'histoire de Justine."
"Si la transgression est intimement liée à l’insoumission, serait-ce dire pour autant, comme on le prétend habituellement, qu’elle porte en elle une charge négative pour le devenir des Hommes ? Ne faudrait-il pas plutôt considérer que la transgression n’est en elle-même ni positive ni négative mais qu’elle le devient en fonction de l’interdit fixé et de la valeur qu’on donne à celui-ci ? Dans cette perspective, cet article visera à déterminer les critères à partir desquels la transgression peut être considérée comme illégale et illégitime, légale et illégitime, illégale et légitime. En fonction de ces paramètres, sera envisagée la question des limites qui échoient à la liberté de s’exprimer, d’agir et de créer. Mots-clés : Transgression, frontière, légal, légitime, droits fondamentaux, liberté, aliénation, art, pouvoir, résistance."
"Quelle place pour la biographie dans les études marotiques ? La question semble oiseuse tant le poète de Cahors se met en scène dans une œuvre qu’il va jusqu’à orner partiellement de son propre nom avec L’Adolescence clémentine. Et pourtant, le rapport de la vie à l’œuvre est biaisé par la subtilité d’une écriture souvent médiatisée (jeu intertextuel, influence de la tradition, etc.) dont tout l’enjeu est d’entretenir l’illusion de la confidence. Difficile dans ces conditions de ne pas se laisser prendre au jeu marotique comme l’ont fait bien des lecteurs depuis le XVIe siècle, des légendes facétieuses du XVIIe qui font du poète un nouveau Till l’Espiègle aux lectures critiques modernes, plus soucieuses certes de vérité historique, mais qui négligent parfois la richesse d’un texte confondu avec un simple témoignage."
"Calvy de la Fontaine (actif de 1534 à 1561) est l’un des auteurs les moins bien connus de la querelle Marot-Sagon. L’article retrace sa carrière et reconstitue sa production afin de montrer comment sa trajectoire littéraire est symptomatique des années 1530-1550 : enthousiasme collectif autour du renouveau marotique et tentatives singulières de se faire une place au soleil, professionnalisation des traducteurs au service des libraires, évolution du goût sous l’influence de la jeune Brigade."
"L’article étudie la façon dont les imprimeurs du XIXe siècle se sont progressivement emparés des codes typographiques des débuts de l’imprimerie, mettant fin à l’hégémonie des Didot. Les Perrin et Jannet les imposent d’abord pour l’édition des textes anciens avant que Poulet-Malassis et Lemerre ne tentent d’imprimer des auteurs modernes en habit d’autrefois. L’analyse des rêveries typographiques de Huysmans montre enfin comment les hommes du XIXe ont réinterprété ces codes de façon ludique."
"L’article montre l’important écho dans la littérature satirique (épigrammes et coq-à-l’âne) qu’eurent la destitution spectaculaire du chancelier Poyet (août 1542) et son procès (1544-1545). Le rappel des données historiques et le retour aux sources manuscrites, parfois éditées avec des erreurs de transcription qui en empêchaient la compréhension, permettent d’élucider plusieurs passages considérés comme obscurs et de rendre à Clément Marot un dizain dont l’attribution était jusqu’ici repoussée."
"Le recueil Grenet (Lausanne, BCUL, M 1016) est un manuscrit poétique personnel compilé au milieu du XVIe siècle par un marchand auvergnat réfugié à Genève. La collection rassemble des textes célèbres ou rares de Marot, des pièces sur l’apprentissage de l’écriture et des poésies satiriques et militantes en faveur de la Réforme. Cette étude propose la première analyse complète de la conception matérielle et intellectuelle du livre, ainsi qu’une interprétation stylistique et littéraire des textes recueillis, afin d’examiner les enjeux socio-culturels du geste collectionneur chez un lecteur du XVIe siècle."
"Cet ouvrage réunit trois arts de poésie française écrits vers la fin du xve siècle, l'Instructif de seconde rhétorique, l'Art de rhétorique de Molinet et un Traité de rhétorique. Ces ouvrages dressent un état des lieux du savoir poétique dont bénéficiaient les poètes de la première Renaissance et leurs lecteurs."
"Le treizième numéro de la revue Babel-Civilisations a pour objet la notion de « croyances » et son application à un domaine spatio-temporel précis, à savoir celui du « monde anglophone contemporain », en adéquation avec les thématiques traditionnelles de notre groupe de recherche. Un colloque organisé en 2016 avait pour objet cette idée de croyance(s) et a permis d'amorcer la réflexion de fond que l'on pourra retrouver dans ces lignes. Entre 2016 et 2017, de nouveaux collègues ont manifesté leur intérêt pour ce sujet et ont permis d'étoffer ce numéro, qui présente les réflexions de pas moins de treize contributeurs-quatorze si l'on inclut cette présentation. La gestation du projet fut inhabituellement longue et complexe ; il est d'ailleurs intéressant de noter que les réactions initiales, lors de la première formalisation des titres provisoires, furent assez contrastées, pour dire le moins. A l'enthousiasme des uns se juxtaposait un silence courtois des autres, que l'auteur de ces lignes eut d'abord du mal à interpréter. Ce n'est que lors de discussions informelles entre collègues"
"Le treizième numéro de la revue Babel-Civilisations a pour objet la notion de « croyances » et son application à un domaine spatio-temporel précis, à savoir celui du « monde anglophone contemporain », en adéquation avec les thématiques traditionnelles de notre groupe de recherche. Un colloque organisé en 2016 avait pour objet cette idée de croyance(s) et a permis d'amorcer la réflexion de fond que l'on pourra retrouver dans ces lignes. Entre 2016 et 2017, de nouveaux collègues ont manifesté leur intérêt pour ce sujet et ont permis d'étoffer ce numéro, qui présente les réflexions de pas moins de treize contributeurs-quatorze si l'on inclut cette présentation. La gestation du projet fut inhabituellement longue et complexe ; il est d'ailleurs intéressant de noter que les réactions initiales, lors de la première formalisation des titres provisoires, furent assez contrastées, pour dire le moins. A l'enthousiasme des uns se juxtaposait un silence courtois des autres, que l'auteur de ces lignes eut d'abord du mal à interpréter. Ce n'est que lors de discussions informelles entre collègues"
Ce document a été généré automatiquement le 6 décembre 2020. Revue française de civilisation britannique est mis à disposition selon les termes de la licence Creative Commons Attribution-Pas d'Utilisation Commerciale-Pas de Modification 4.0 International.
Nous allons nous interroger sur le sens que l’écrivaine donne au terme « mythes ». Nous verrons ensuite à quel point la troisième partie de ce tome un est fondamentale pour la compréhension des analyses beauvoiriennes sur les femmes et leurs conditions d’existence. Nous terminerons en montrant que l’analyse beauvoirienne des cinq cas d’écrivains fonde les études littéraires de genre.
"Table ronde avec les organisatrices, Xavière Gauthier, Sheila Mallovany-Chevallier, Constance Borde."
"Avoir le droit d'apprendre à lire et à écrire a été un des premiers combats féministes en France. Les femmes ont conquis progressivement ce droit à l’instruction, quoique sur un malentendu. En effet, il s’agissait, pour les réformateurs de la IIIe République, de donner aux mères les moyens d’éduquer et de suivre les progrès de leurs fils, futurs citoyens et soldats. Le corps des professeur.e.s et des instituteurs et des institutrices est devenu un des piliers du régime républicain. Nous retrouvons cette figure tutélaire dans de nombreux romans d’apprentissage, tant au masculin qu’au féminin. Nous aimerions nous interroger sur la représentation de l’instruction donnée aux filles dans cinq romans qui traversent le vingtième siècle : Claudine à l’école (Colette, 1900), Ariane, jeune fille russe (Claude Anet, 1920), Adieu mes quinze ans (Claude Campagne, 1960), Les Enfants de la brume (Claude Campagne, 1970) et Djinn la malice (Jacqueline Cervon, 1972). Instruire et éduquer les filles dans une école va-t-il de soi ? Que disent les discours sur l’école, produits dans un temps, un lieu et une sensibilité d’écrivain.e précis ? Quelle est la place des écoles (classiques, mais aussi de la vie, à travers diverses expériences) dans la libération et/ou l’asservissement des héroïnes ? Ces éducations sont-elles véritablement libératrices ?"
"Les liens qui se tissent entre Henri Perron et Nadine Dubreuilh dans Les Mandarins, et entre Luc et Dominique dans Un Certain sourire sont de l’ordre de la liaison amoureuse, liaison à première vue classique entre un homme âgé, plus expérimenté, et une jeune fille moins experte. Mais est-ce vraiment le cas ? Et n’y a-t-il pas une remise en question des rapports traditionnels de domination masculine ? Tout d’abord, nous voudrions étudier la manière dont sont présentés les deux couples, en nous focalisant sur la manière dont les auteures présentent les relations des hommes aux femmes, dans l’optique d’un rapport de domination de type Pygmalion. Puis, nous voudrions étudier la façon dont les jeunes filles se positionnent par rapport aux hommes dont elles deviennent les maîtresses. Ensuite, nous voudrions voir si la masculinité traditionnelle est ou non remise en question dans ces deux ouvrages."
"Simone de Beauvoir (1908-1986) est connue comme écrivaine, journaliste, essayiste, dramaturge et “mère” du féminisme de la deuxième vague. Mais sait-on qu’elle est une globe-trotteuse et que l’écriture du Deuxième Sexe (1949) est inspirée de ses nombreux voyages ?"
"Le texte suivant est l’adaptation française d’un travail pour un cours d’anglais effectué pendant mon Master 2 (2008-2009). Il s’agissait de présenter n’importe quel sujet en cinq minutes et en anglais. Comme nous avions précédemment évoqué la série Desperate Housewives, l’angle d’attaque de ma présentation m’est naturellement venu, avec – je l’espère – le dynamisme des femmes de Wisteria Lane."
"Dans L'Héritier de Robinson d'André Laurie, les femmes ne sont jamais seules à partir sur les traces du héros de Defoe, - elles sont toujours accompagnées d’un homme -, puisque les romans d’aventures sont conçus et destinés à un public masculin, occidental, blanc et hétérosexuel. Qu'en est-il dans le détail du roman ? comment sont-elles présentées et décrites ?"
"Compte-rendu de l'ouvrage de Yann Hamel, pour le carnet de recherches Chère Simone de Beauvoir."
"Le lien entre alcool et amour, nourriture et assouvissement du désir, est ancien et reconnu par les écrivains depuis l'Antiquité. Au vingtième siècle, Simone de Beauvoir et Françoise Sagan ont abondamment mis en scène des personnages qui boivent, mangent, se séduisent et font l'amour après avoir quitté la table. Pour leurs héroïnes, l'alcool et les mets sont le passage obligé avant l'amour. Par ailleurs, le voyage et le dépaysement sont l'occasion de rencontres multiples."
"« Sororités comblées – Interview de Typhaine D », Tiphaine Martin, Typhaine D, Voyages autour de mon cerveau, janvier 2021. URL : https://vadmc.hypotheses.org/1139"
"« Dans un jardin qu’on dirait éternel : suivre la route du thé », Voyages autour de mon cerveau, septembre 2020. URL : https://vadmc.hypotheses.org/633"
"Publication et présentation d’un texte retrouvé de Simone de Beauvoir, « “La Princesse de Clèves” à Belleville »"
"« “Les gens ne sont pas que des odeurs” : nez ou museau ? », Voyages autour de mon cerveau, août 2020. URL : https://vadmc.hypotheses.org/563"
"« Est-ce ainsi que les femmes chantent ? : genre et musique de films », in Philippe Gonin et Jérôme Rossi (dir.), Le Cinéma populaire et ses musiciens en France de 1930 à nos jours, Dijon : Presses Universitaires de Dijon, p. 201-214."
"« Snow Therapy : guérir le couple ou se guérir du couple ? », in Nicolas Balutet et André-Alain Morello, Corps, Genre, Santé, Toulon : Effigi , p. 93-109."
"« Interview de Stéphanie Guyot-Nourry », Tiphaine Martin, Stéphanie Guyot-Nourry, Voyages autour de mon cerveau, novembre 2020. URL : https://vadmc.hypotheses.org/836"
"« Quatre Mariages et un enterrement : tristes clichés », Voyages autour de mon cerveau, février 2021. URL : https://vadmc.hypotheses.org/?p=1196"
"« Margaret Douthat Blossom, Un amour de la route », Simone de Beauvoir Studies, n°31 (2020), p. 339–357."
"« Tristes cocktails : faire la fête chez Scola, Lubitsch, Duvivier et Camerini », Voyages autour de mon cerveau, février 2021. URL : https://vadmc.hypotheses.org/1244"
"« Bim, bam, boum : femmes debout contre les coups », Colloque Les femmes et leurs corps, Médiathèque, Orléans."
"« Alliance, solidarité, sororité, la ligne de crête – Interview de Martine Storti », Tiphaine Martin, Martine Storti, Voyages autour de mon cerveau, mars 2021. URL : https://vadmc.hypotheses.org/?p=1368"
"« VADMC sur les réseaux sociaux ! », Voyages autour de mon cerveau, août 2021. URL : https://vadmc.hypotheses.org/2000"
"« Guide du doctorat », Voyages autour de mon cerveau, octobre 2020. URL : https://vadmc.hypotheses.org/?p=662"
"« Y a-t-il des héritières Beauvoir ? Pour une arborescence féministe » , Voyages autour de mon cerveau, avril 2021. URL : https://vadmc.hypotheses.org/1471"
"« Bertrand Tavernier est mort, vive la cinéphile et le cinéma », Voyages autour de mon cerveau, mars 2021. URL : https://vadmc.hypotheses.org/1450"
"« Alain Geismar – Beauvoir au masculin », Tiphaine Martin, Alain Geismar, Voyages autour de mon cerveau, juin 2020. URL : https://vadmc.hypotheses.org/?p=361"
"« Joséphine Baker par Simone de Beauvoir, de revue en revue », Voyages autour de mon cerveau, novembre 2021. URL : https://vadmc.hypotheses.org/?p=2240"
"« La musique de l’écriture – Interview de Katherine Pancol », Tiphaine Martin, Katherine Pancol, Voyages autour de mon cerveau, novembre 2021. URL : https://vadmc.hypotheses.org/?p=2018"
"« À bord de l'Aire avec Zaza - Jour 8 », Voyages autour de mon cerveau, mai 2021. URL : https://vadmc.hypotheses.org/?p=1708"
"« Debout les femmes ! Filmer des exploitées », Voyages autour de mon cerveau, novembre 2021. URL : https://vadmc.hypotheses.org/?p=2171"
"« Aline, de Valérie Lemercier : le souffle du cœur », Voyages autour de mon cerveau, novembre 2021. URL : https://vadmc.hypotheses.org/2156"
"« Eugénie Grandet, religion ou voyage ? », Voyages autour de mon cerveau, octobre 2021. URL : https://vadmc.hypotheses.org/?p=2118"
"« Mourir peut attendre, surtout pour les femmes », Voyages autour de mon cerveau, octobre 2021. URL : https://vadmc.hypotheses.org/?p=2107"
"« Chercher l’Histoire - Interview de Björn Larsson », Tiphaine Martin, Björn Larsson, Voyages autour de mon cerveau, novembre 2021. URL : https://vadmc.hypotheses.org/?p=2130"
"Tout d'abord, nous nous intéresserons à la vie de Germaine Acremant, existence traditionnelle bouleversée par la gloire littéraire avant l'indifférence des années cinquante et l'oubli contemporain. Puis, nous nous pencherons sur le cas de Ces dames aux chapeaux verts : à quoi tint le succès à l'époque de sa parution, que nous dit ce roman sur la nouvelle condition féminine, quel charme et quel intérêt peut y trouver un lecteur ou une lectrice d’aujourd’hui ? Enfin, nous nous attacherons au reste de son œuvre des années vingt et trente, afin de montrer qu'il est aussi digne de curiosité que son roman le plus connu."
Articolo su il ristorante Le Jardin Gourmand.
"La bande dessinée mue depuis plusieurs années en roman graphique, rejoignant sa consœur la littérature, quand l’explosion des cases la fait toucher au Septième Art. Cette confusion des genres scripturaux rejoint la multiplication de ses publics : enfantins, adolescents, adultes, communient dans un amour identique des rayons BD. Les lecteurs sont variés et ne lisent pas uniquement les ouvrages réservés à leur tranche d’âge. Parents et enfants se penchent ensemble sur les désormais classiques Tintin, Astérix, Yakari, Alix…, tandis que les adolescents n’hésitent pas à dévorer les différents tomes du Persépolis de Marjane Satrapi ou ceux de L’Arabe du futur de Joann Sfar. Cependant, cette effervescence multicolore, multiculturelle et multigénérationelle ne doit pas faire oublier le machisme encore très présent dans le monde de la bande dessinée. Les personnages féminins ne sont pas forcément au premier plan de l’intrigue, et restent encore trop souvent cantonnés au rôle de faire-valoir, de substitut maternel ou de repos du guerrier, pour le valeureux enquêteur, aventurier, explorateur des mondes intergalactiques. Nous allons voyager dans l’univers des bandes dessinées et des romans graphiques francophones - avec quelques coups d’œil vers le non-francophone. Nous nous interrogerons sur la représentation des femmes : comment sont-elles dessinées ? quels rôles leur sont attribués ? peuvent-elles devenir des sujets ?"
Interview de Yann Hamel pour le carnet de recherches Voyages autour de mon cerveau.
"« Henri et Luc, ou le mythe de Pygmalion revisité dans Les Mandarins et Un certain sourire », Voyages autour de mon cerveau, mai 2020. URL : https://vadmc.hypotheses.org/67"
"« Présentation de Claudine Monteil », Voyages autour de mon cerveau, juillet 2020. URL : https://vadmc.hypotheses.org/?p=488"
"« Interview Gina Opiniano - Beauvoir Seminar », Tiphaine Martin, Gina Opiniano, Voyages autour de mon cerveau, novembre 2020. URL : https://vadmc.hypotheses.org/694"
"« Simone de Beauvoir et la nature à Paris », Voyages autour de mon cerveau, juillet 2020. URL : https://vadmc.hypotheses.org/428"
"Nous voudrions nous interroger sur le sens à donner aux recompositions familiales qui sont présentées dans ces deux romans d’Hector Malot. Comment retrouver sa famille ? Qu’est-ce qu’avoir une famille ? Peut-on distinguer plusieurs catégories de famille ? Quel modèle familial est proposé par Malot dans les deux romans ? Comment arriver jusqu’à sa famille biologique ? Nous verrons comment les héros passent d’une famille à une autre, à quels apprentissages ils sont confrontés jusqu’à leurs retrouvailles avec leur famille biologique, et comment ils voyagent en famille jusqu’à leur famille."
"« Ecrire, surprendre – Interview de Sarah Maeght »,Tiphaine Martin, Sarah Maeght, Voyages autour de mon cerveau, novembre 2020. URL : https://vadmc.hypotheses.org/?p=812"
"« Pour l’amour et des lettres – Interview de Judith Coffin »,Tiphaine Martin, Judith Coffin, Voyages autour de mon cerveau, décembre 2020. URL : https://vadmc.hypotheses.org/926"
"« À bord de l'Aire avec Zaza - Jour 13 », Voyages autour de mon cerveau, mai 2021. URL : https://vadmc.hypotheses.org/?p=1806"
"« Tout ce que le Ciel permet : sortir du cercueil », Voyages autour de mon cerveau, février 2021. URL : https://vadmc.hypotheses.org/?p=1205"
"Tiphaine Martin, Priscilla Leclerc, « Repartir de plus belle – Interview de Priscilla Leclerc, Les Covoyageurs », Voyages autour de mon cerveau, février 2021. URL : https://vadmc.hypotheses.org/1226"
"« Toucher pour (se) connaître : Simone de Beauvoir en voyage », in Nicolas Balutet et André-Alain Morello, Corps, Genre, Santé, Toulon : Effigi , p. 31-48."
"« #TotallyMegalo - Interview Elisabeth Ndala », Tiphaine Martin, Elisabeth Ndala, Voyages autour de mon cerveau, novembre 2020. URL : https://vadmc.hypotheses.org/?p=745"
"La mondialisation économique de la seconde moitié du vingtième siècle a accentué les déplacements, qu’ils soient voulus ou pas. Les humains voyagent, qu’ils soient - très schématiquement - dans la société des loisirs occidentale, ou pris dans les flots migratoires du sud de l’Europe et de l’Amérique. Ces flux et reflux sont présents dans la chanson, et ce, depuis l’explosion viatique de la société des loisirs de l’après-Première Guerre mondiale. Dans cette communication, nous voudrions explorer les modalités chantées du voyage, du vingtième siècle à nos jours, en nous interrogeant sur les permanences et les altérités du récit de voyage. Nous analyserons tout d’abord ce qui relève du pittoresque, vision biaisée d’un lieu visité ou vu par rapport à un autre lieu - par exemple l’anti-américanisme de « Où est-il donc ? » (Vincent Scotto, 1937), de « Coin de rue » (Charles Trenet, 1954) et d’autres chansons, tributaires de la montée en puissance de l’empire américain. Nous étudierons ensuite comment le pittoresque devient progressivement critique - par exemple dans « J’ai rêvé New-York » et « Manhattan » (Yves Simon, 1974), malgré une fascination qui reste profonde pour l’ailleurs. Nous terminerons par l’approfondissement de chansons totalement critiques par rapport aux clichés, comme, par exemple, dans « L’Agneau » (Bénabar, 2011) et « Le Complexe du sédentaire » (Bénabar, 2018), qui rejoint « Roar » (Katy Perry, 2013). Notre époque n’est plus aux dénonciations sans nuances d’un ailleurs vu par le petit bout de la lorgnette. Il s’agit plutôt de prendre l’autre comme révélateur de nos conduites pas toujours très honorables. Notre but est de montrer la distance prise par les chanteurs et chanteuses au fil du temps, par rapport au mirage touristique et à des analyses politiques à courte vue."
"La bande dessinée a acquis tardivement ses galons de Neuvième Art. Elle reste encore peu étudiée, malgré certaines reconnaissances publiques, dont témoignent la longévité du Festival d’Angoulême. Un nombre croissant d’expositions lui sont consacrés dans de grands musées, des musées lui sont entièrement dédiés, ainsi que des documentaires sur ses créateur.trices. Elle a également gagné sa place dans les programmes scolaires officiels du français, loin de la distinction entre genres « mineurs » et genres de la « culture légitime ». La bande dessinée mue depuis plusieurs années en roman graphique, rejoignant sa consœur la littérature, quand l’explosion des cases la fait toucher au Septième Art. Cette confusion des genres scripturaux rejoint la multiplication de ses publics : enfantins, adolescents, adultes, communient dans un amour identique des rayons BD. Les lecteurs sont variés et ne lisent pas uniquement les ouvrages réservés à leur tranche d’âge. Parents et enfants se penchent ensemble sur les désormais classiques Tintin, Astérix, Yakari, Alix…, tandis que les adolescents n’hésitent pas à dévorer les différents tomes du Persépolis de Marjane Satrapi ou ceux de L’Arabe du futur de Joann Sfar. Cependant, cette effervescence multicolore, multiculturelle et multigénérationelle ne doit pas faire oublier le machisme encore très présent dans le monde de la bande dessinée. Les personnages féminins ne sont pas forcément au premier plan de l’intrigue, et restent encore trop souvent cantonnés au rôle de faire-valoir, de substitut maternel ou de repos du guerrier, pour le valeureux enquêteur, aventurier, explorateur des mondes intergalactiques. Nous allons voyager dans l’univers des bandes dessinées et des romans graphiques francophones - avec quelques coups d’œil vers le non-francophone. Nous nous interrogerons sur la représentation des femmes : comment sont-elles dessinées ? quels rôles leur sont attribués ? peuvent-elles devenir des sujets ?"
"« J’ai rêvé… : récits de voyage en chanson », Voyages autour de mon cerveau, juin 2021. URL : https://vadmc.hypotheses.org/?p=1873"
"« À bord de l’Aire avec Zaza – Jour 2 », Voyages autour de mon cerveau, mai 2021. URL : https://vadmc.hypotheses.org/?p=1605"
"Dans cette communication, nous aimerions nous intéresser à la forêt comme espace labyrinthique, où animaux et humains se croisent, se fuient et s’éloignent les uns des autres. Nous envisageons l’espace silvestre comme lieu d’apprentissage pour les personnages de certains films des Studios Disney comme Blanche-Neige, Bambi, et des films Perceval le Gallois, L’Ami de mon amie, d’Eric Rohmer, et La Forêt de Quinconces, de Grégoire Leprince-Ringuet. Nous nous demanderons quelles sont les valeurs attribuées à la forêt par les cinéastes choisis et comment les cinéastes occupent l'espace silvestre. Nous verrons tout d’abord comment la forêt est le lieu de multiples rencontres. Nous nous intéresserons ensuite à la forêt comme espace pour les rêves et de rêve. Nous analyserons enfin le lieu forestier comme accès, étape, de l’apprentissage de l’existence, tel le labyrinthe construit par Dédale."
"« Amanit et Fosca, l’aventure de la traversée des siècles », Tiphaine Martin, Irina Durnea, Voyages autour de mon cerveau, juin 2021. URL : https://vadmc.hypotheses.org/1942"
"« Lutter contre le patriarcat avec une poêle : Patraque et Cie », Colloque Sorcières, êtes-vous là ?, Médiathèque, Orléans."
"Depuis les années trente jusqu'aux années cinquante et soixante, Simone de Beauvoir a voyagé à la fois en Espagne, républicaine puis franquiste, et en Italie, fasciste puis démocratique. Le destin politique inverse de ces deux pays a encadré les périples beauvoiriens, leur donnant aujourd'hui une orientation politique qui n'était pas celle qui était désirée au départ par Beauvoir. Son engagement dans la vie politique de son temps, qui a eu des répercussions importantes sur sa vie privée mais aussi dans sa manière de regarder le monde autour d'elle, a-t-il profondément modifié la façon dont elle se déplace ? Dans un premier temps, nous nous interrogerons sur l'imaginaire beauvoirien à propos de l'Italie et de l'Espagne, afin de cerner ses attentes par rapport aux deux pays. Dans un deuxième temps, nous étudierons sa manière de narrer, a posteriori, ses pérégrinations en terre espagnole et italienne, afin de discerner si une orientation politique est donnée par l’auteure à ses comptes-rendus viatiques, et si oui, comment. Dans un troisième temps, nous nous interrogerons sur le choix des destinations de Simone de Beauvoir : à quel désir profond correspond-il ? L'Italie a-t-elle vraiment pris la place de l'Espagne dans son cœur ? Ou - même si la guerre civile n'avait pas éclaté en Espagne - la relation de Beauvoir à ces deux pays serait-elle restée profondément tributaire de son imaginaire ?"
"« À bord de l'Aire vec Zaza - Jour 12 », Voyages autour de mon cerveau, mai 2021. URL : https://vadmc.hypotheses.org/?p=1790"
"Danielle Darrieux est un des symboles de la jeune fille émancipée dans les années trente. Cette actrice, qui a commencé sa carrière à l’âge de quatorze ans, s’épanouit dans des comédies trépidantes, mais également dans des drames ou des mélodrames. Dans notre communication, nous aimerions nous intéresser à la persona de Darrieux dans les films de cette période, en focalisant nos analyses sur certains films, tels que Quelle drôle de gosse (Léo Joannon, 1935), Abus de confiance (Henri Decoin, 1937), Mademoiselle ma mère (Henri Decoin, 1937), La Coqueluche de Paris (Henry Koster, 1938). Quel.s rôle.s les réalisateurs (tous des hommes) lui font-ils jouer ? Ces années trente sont des années dures. Après le « jeudi noir » de 1929, le chômage est galopant, les ligues d’extrême- droite et les mouvements de gauche pour la paix et contre le fascisme surgissent et s’affrontent, la crise sociale s’accroît. Cette exaspération des tensions de classe se traduit plus dans le cinéma français que dans le cinéma nord-américain, grâce au « réalisme poétique », mais on la trouve également dans des films populaires, de manière plus ou moins voilée. Dans une première partie, nous analyserons les espaces où se meut Darrieux, alors que les femmes sortent de plus en plus du foyer, mais où elles sont peu acceptées et très souvent en danger. Dans une deuxième partie, nous analyserons le langage corporel de l’actrice et la manière dont elle est filmée : cadrages, mimiques, tenue debout / assise, gestes et surtout vêtements, qui disent beaucoup sur la modernité, la transgression ou le conformisme du personnage et de la construction du mythe « D.D. ». Dans une troisième partie, nous analyserons les discours : ceux du personnage et ceux tenus par les autres personnages. Cet ensemble fait-il de Darrieux une avocate féministe ou une timide garçonne ?"
"« À bord de l'Aire avec Zaza - Jour 10 », Voyages autour de mon cerveau, mai 2021. URL : https://vadmc.hypotheses.org/?p=1748"
"Voyageuse infatigable, elle parcourut tous les continents, à l'exception de l'Australie et les récits de voyage sont très présents dans son autobiographie depuis les Mémoires d'une jeune fille rangée (1958), jusqu'à Tout compte fait (1972). Ces narrations constituent la structure même des volumes mémoriels de Simone de Beauvoir, mais cette thématique du voyage tient également une place importante dans ses ouvrages de fiction ou dans son abondante correspondance."
"« Exposition Simone de Beauvoir, médiathèque Olympe de Gouges - Joigny », Voyages autour de mon cerveau, mai 2020. URL : https://vadmc.hypotheses.org/213"
"« Eric Levéel, Interview d'Eric Levéel - Beauvoir au masculin », Tiphaine Martin, Eric Levéel, Voyages autour de mon cerveau, juin 2020. URL : https://vadmc.hypotheses.org/310"
"« Frédéric Maget - Beauvoir au masculin », Tiphaine Martin, Frédéric Maget, Voyages autour de mon cerveau, mai 2020. URL : https://vadmc.hypotheses.org/201"
"« Un prénom de.... - Interview de Sarah Sauquet », Tiphaine Martin, Sarah Sauquet, Voyages autour de mon cerveau, novembre 2020. URL : https://vadmc.hypotheses.org/697"
"Les femmes politiques oubliées sont nombreuses, trop souvent confinées en soutien et inspiratrices de telle figure politique masculine, à l’instar des muses des écrivains ou artistes. Parmi elles, Jeanne Deroin, ouvrière lingère devenue institutrice, révolutionnaire en 1848. Jeanne Deroin est une militante socialiste, puis féministe, même si le terme n’existe pas encore en ce milieu du dix-neuvième siècle. Il vient plus tard, d’abord en 1872, sous la plume d’Alexandre Dumas fils, dans un sens péjoratif. Comme souvent, ce mot est repris dans une acception positive par les femmes et les hommes qui luttent pour les droits des femmes. C’est la grande Hubertine Auclert, journaliste, institutrice et écrivaine, qui donne à « féministe » le sens très large de « partisans de l'affranchissement des femmes », en 1882, dans une lettre adressée au préfet de la Seine. C’est bien à ce groupe qu’appartient Deroin qui fonde le journal L’Opinion des femmes,. L’objetcif est d’obtenir que le suffrage soit véritablement universel, et non pas uniquement masculin, qui lui se pense et se veut universel. Nous désirerions nous attacher à cette figure méconnue du combat pour le suffrage féminin. Nous retracerons d’abord son existence, en insistant sur sa postérité, avant de nous attacher à ses combats, socialistes et féministes. Nous finirons par nous interroger sur le contenu et la portée de L’Opinion des femmes, digne ancêtre de La Fronde de Marguerite Durand."
"« La marquise de Blocqueville, diariste, peintre et écrivaine voyageuse », Voyages autour de mon cerveau, novembre 2020. URL : https://vadmc.hypotheses.org/836"
"« À bord de l'Aire avec Zaza - Jour 15 », Voyages autour de mon cerveau, mai 2021. URL : https://vadmc.hypotheses.org/?p=1835"
"L’idée de cet article à quatre mains nous est venue à la suite du colloque Réceptions contemporaines de Simone de Beauvoir (Nice, 2018) où nous avons chacune présenté un pan de la réception de La Femme rompue. Nous reprenons ici plus brièvement nos analyses et renvoyons à nos articles publiés dans Les Cahiers Sens Public, n°23-24, décembre 2019."
"« Y a-t-il des héritières Beauvoir ? Pour une arborescence féministe », Colloque international Constellations créatrices : héritages et réseaux féminins, Les Jaseuses."
"« À bord de l’Aire avec Zaza – Jour 1 », Voyages autour de mon cerveau, mai 2021. URL : https://vadmc.hypotheses.org/?p=1597"
"Nous aimerions nous interroger sur les représentations de Mercédès comme Pénélope de l'époque romantique. Que nous dit Alexandre Dumas des attentes masculines en termes de fidélité amoureuse féminine ? Quels sont les liens entre la Pénélope d'Homère et la Mercédès de Dumas ? Comment Dumas fait-il de Mercédès une Pénélope déchue ? Y a-t-il d'autres Pénélope dans Le Comte de Monte-Cristo, qui elles ne sont pas déchues par le narrateur ? Qu'est-ce que cela nous dit des représentations de la fidélité/infidélité féminine de Dumas, du courant romantique auquel il appartient et de son époque ? Nous verrons comment Le Comte de Monte-Cristo est une réécriture de l'Odyssée. Nous analyserons les liens entre les deux oeuvres, à travers la figure de Pénélope. Nous soulignerons les liens entre Mercédès et Pénélope. Nous montrerons ensuite comment Mercédès est une Pénélope déchue, à la fois héroïne romantique, femme glacée par l'ascension sociale, puis femme sacrificielle qui trouve sa rédemption dans la maternité et la solitude. Enfin, nous tracerons le portrait des autres Pénélope présentes dans le roman, en les opposant au personnage de Mercédès. Correspondent-elles à la Pénélope antique, qui reste patiemment auprès de sa toile en attendant Ulysse ?"
"Si les voyages hors de Paris sont l’expression même de cette libération, il ne faut pas oublier de prendre également en compte les voyages dans la nature parisienne. Nous opposerons l’espace naturel parisien à l’espace naturel hors Paris, tant dans l’enfance que dans la jeunesse et la vieillesse de Simone de Beauvoir. Nous verrons quels attributs possède la nature dans Paris."
"Simone de Beauvoir est la théoricienne majeure au XXème siècle de l’égalité des sexes. Dans son essai le plus célèbre, Le Deuxième Sexe, elle démontre comment la société créé l’infériorité des femmes, tant de manière juridique qu’au quotidien. Beauvoir analyse également les écrits des hommes, à la fin du tome 1 de son essai, dans la partie « mythes ». Dans cette communication, j’aimerais explorer cette partie du Deuxième Sexe, afin de définir quelle théorie de la littérature s’en dégage : est-elle matérialiste, existentialiste, féministe ou les trois? Tout d’abord, je présenterai la structure et les postulats théoriques de cette partie, qui condense les parties consacrées à la biologie, au matérialisme historique, à l’anthropologie et à l’Histoire et qui ouvre sur de nouvelles analyses et approfondissements des mythes du féminin. Ensuite, j’examinerai le lien entre contenu et contenant : comment Beauvoir applique-t-elle ses théories dans les cinq chapitres consacrés à Montherland, D.H. Lawrence, Claudel, Breton, et Stendhal ? Enfin, je me pencherai sur la réception de cette partie par les critiques : qu’en ont-ils retenu et comment l’ont-ils analysée."
"Dans cet article, nous désirerions montrer que l’image que Simone de Beauvoir construit dans ses volumes autobiographiques, et tout particulièrement lorsqu’elle voyage, est loin des clichés de raideur et de tristesse qu’une certaine critique voudrait attacher à sa personne. Dans un premier temps, nous examinerons son rapport très fort à la terre. Dans un deuxième temps, nous nous demanderons si l’âge change le rapport qu’elle entretient avec les voyages. Dans un troisième temps, nous nous interrogerons sur l’ouverture aux sensations du corps beauvoirien."
"« Rosa Bonheur, le château-atelier », Tiphaine Martin, Katherine Brault, Voyages autour de mon cerveau, août 2020. URL : https://vadmc.hypotheses.org/574"
"« La Bonne Epouse, vive la révolution féminine et féministe ! », Tiphaine Martin, Martin Provost, Voyages autour de mon cerveau, juillet 2020. URL : https://vadmc.hypotheses.org/521"
"Les voyages occupent une place considérable dans l’œuvre de Simone de Beauvoir, qu’il s’agisse de la fiction, des essais, ou des écrits personnels. Il s’agit de périples dans la nature, mais aussi d’excursions dans l’espace urbain. Les randonnées dans et hors Paris, tant dans les provinces françaises qu’en Europe, sont l’occasion de séjours « au féminin », c’est-à- dire de pérégrinations où le fait d’appartenir à un sexe précis acquiert de l’importance. Le voyage au féminin, pour Beauvoir, est notamment l’occasion de se confronter au regard et au corps de l’Autre, et particulièrement aux personnes qui vivent du commerce sexuel. Si elle a consacré un chapitre du Deuxième Sexe aux prostituées et à leurs conditions de vie, elle n’évoque pas les prostitués mâles, de même qu’elle ne parle pas de sa propre expérience en tant que spectatrice de la prostitution dans le cadre de son essai, ni de sa fascination pour ce milieu vivant en marge de la société bien-pensante dont elle est issue. Dans cette communication, nous nous proposons d’interroger la dimension de genre dans le cadre des récits de voyages autobiographiques beauvoiriens, du moins ceux qui se déroulent dans un décor urbain. Dans ceux-ci, Beauvoir construit-elle un espace urbain sexué? Nous voudrions montrer la manière dont, après la fascination de l'adolescence et du début de l'âge adulte pour le spectacle des gens vivants aux marges de la société, elle prend ses distances avec cette vision pittoresque du monde « interlope »."
"De son enfance jusqu’à son décès, Simone de Beauvoir a été une grande voyageuse. Elle a transcrit cette expérience viatique dans ses volumes de mémoires (Mémoires d’une jeune fille rangée, La Force de l’âge, La Force des choses, Tout compte fait), ainsi que dans deux récits de voyage, l’un consacré aux Etats-Unis (L’Amérique au jour le jour) et l’autre à la Chine (La Longue Marche). Ses périples en Grèce couvrent trois de ses cinq ouvrages autobiographiques, car elle a effectué des séjours en terre hellénique dans les années 1930, 1950 et 1960. Cependant, elle n’a pas été une voyageuse naïve, car elle a été au contact non seulement des récits de l’Antiquité, mais elle a également lu les narrations viatiques – fictions et essais- de Chateaubriand, Maurice Barrès, Paul Morand, George Duhamel: les auteurs de la modernité du début du vingtième siècle autant que les auteurs du dix-neuvième siècle. Dans cette communication, nous voudrions interroger le statut narratif des récits de voyage de Simone de Beauvoir, tels qu’elle les a insérés dans un cadre mémoriel. Simone de Beauvoir imite-t-elle ses devanciers, ou crée-t-elle un récit de voyage singulier ? Tout d’abord, nous étudierons l’influence des lectures sur la manière dont Beauvoir voit la Grèce avant de s’y rendre. Puis, nous étudierons la manière dont elle copie ses devanciers, en centrant ses récits de voyage sur le pittoresque antique, et en ayant une vision peu politisée de la Grèce. Enfin, nous centrerons notre analyse sur l’originalité du discours beauvoirien, qui transmet sa vision d’un lieu par le biais de son corps, qui n’a pas besoin de recourir aux références à l’Antiquité pour être une chair heureuse. Nous ne nous interdirons pas quelques allusions à ses séjours hellènes ultérieurs, quoique notre intervention soit concentrée sur son périple effectué dans les années 1930."
"Le neuvième art regorge de personnages féminins, mais il existe peu d'héroïnes, et surtout quasiment aucune héroïne qui donne son nom à une série. Laureline, inventée par Jean-Claude Mézières et Pierre Christin, intervient dès la première bande dessinée de la série Valérian. Cependant, son prénom n'est associé à celui de Valérian que tardivement, en 2007, soit quarante ans après la première publication. Et le réalisateur Luc Besson n’a pas jugé bon de reprendre les deux noms dans le titre de son adaptation de la bande dessinée. Seul reste Valérian. Juste avant les héroïnes Cellulite et Agrippine de Claire Bretécher, il y a eu l'électronicienne Yoko Tsuno, dont les aventures sont racontées et dessinées par le Belge René Leloup. Depuis sa première apparition dans le journal Spirou en mai 1971, Yoko n'a cessé de fasciner son lectorat. Ses aventures la conduisent dans différents pays : son Japon natal, mais également la Chine et l'Indonésie, où elle a une partie de ses racines. Elle se rend également en Allemagne, depuis la Belgique où elle travaille. Sa toute première aventure la conduit, avec ses acolytes Vic Vidéo et Paul Pitron, dans une autre dimension, sur la planète Vinéa. Elle y revient régulièrement, créant ainsi un espace différent mais familier aux lecteurs. Dans cette communication, nous aimerions interroger son statut d'héroïne : est-elle définie comme fille de… son père, un prénom (Yoko), ou est-elle une individualité de premier plan qui maîtrise les évènements, qui résiste aux clichés de l’exotisme asiatique, et à celles et ceux qui se mettent sur son chemin ? Tout d'abord, nous nous pencherons sur sa généalogie : comment nous est-elle présentée, dans le premier album, puis dans les suivants ? Ensuite, nous interrogerons son statut d’étrangère : Orientale parmi des Occidentaux, Terrienne chez les Vinnéens, femme et scientifique dans un monde d’hommes. Puis, nous analyserons son rapport aux sciences, dans ce qu’il a de novateur dans les rapports femmes-hommes."
"Le recueil de nouvelles La Femme rompue est paru juste avant les évènements de Mai 68, en janvier de cette année-là. Le récit éponyme avait paru dans le magazine ELLE à l’automne précédent, illustré par la sœur cadette de Simone de Beauvoir, Hélène de Beauvoir. Malgré une mauvaise réception critique, le succès est au rendez-vous. La fortune populaire du recueil ne s’arrête pas là. Dix ans plus tard, en 1978, la réalisatrice Josée Dayan adapte « La Femme rompue » pour la télévision. Simone de Beauvoir participe à l’écriture du scénario, co-signé par Françoise Verny, éditrice et écrivaine. En 2015, l’actrice Josiane Balasko, connue pour ses rôles dans des films grand public, adapte « Monologue » pour le théâtre. Elle présente son spectacle en tournée, gagnant un public d’un milieu différent, qui la connaît peu ou partiellement. Entre-temps, très peu d’articles et de travaux universitaires ont été menés sur le recueil en France. Dans cette intervention, nous aimerions analyser les réceptions médiatiques et critiques des adaptations successives des nouvelles du recueil. Y a-t-il eu des changements dans l’accueil réservé à La Femme rompue ? L’ensemble des récits a-t-il enfin conquis la critique ? Nous étudierons tout d’abord la réception critique et publique du volume lors de sa pré- publication, en 1967, puis lors de sa publication en recueil, en janvier 1968. Nous examinerons ensuite les deux types de réceptions, pour les adaptations télévisuelles et théâtrales, avec ce que cela suppose de différence de genre, de public et d’époque. Nous terminerons en nous interrogeant sur le manque d’intérêt universitaire français pour ce recueil, au regard de son succès auprès d’un public plus élitiste que par le passé."
"Introduction au documentaire de Dominique Gros, Simone de Beauvoir, femme actuelle, 8 mars 2008, Maison des Étudiants Suédois, Cité Internationale Universitaire de Paris."
"Présentation de mes recherches sur Simone de Beauvoir et le récit de voyage, depuis mon Master 2 jusqu’à aujourd’hui. Et je n’ai pas fini…"
"« La Bonne Epouse, pour un monde mixte et joyeux », Tiphaine Martin, Séverine Werba, Voyages autour de mon cerveau, juillet 2020. URL : https://vadmc.hypotheses.org/?p=511"
"« À bord de l'Aire avec Zaza - Jour 11 », Voyages autour de mon cerveau, mai 2021. URL : https://vadmc.hypotheses.org/?p=1763"
"« À bord de l'Aire avec Zaza - Jour 14 », Voyages autour de mon cerveau, mai 2021. URL : https://vadmc.hypotheses.org/?p=1815"
"« Le spectre du Deuxième Sexe – Interview d’Amirpasha Tavakkoli », Tiphaine Martin, Amirpasha Tavakkoli, Voyages autour de mon cerveau, avril 2021. URL : https://vadmc.hypotheses.org/?p=1516"
"« À bord de l'Aire avec Zaza - Jour 7 », Voyages autour de mon cerveau, mai 2021. URL : https://vadmc.hypotheses.org/?p=1694"
"« Changer ce monde – Interview de Florence Montreynaud », Tiphaine Martin, Florence Montreynaud, Voyages autour de mon cerveau, mars 2021. URL : https://vadmc.hypotheses.org/?p=1435"
"« Simone de Beauvoir à Joigny », Voyages autour de mon cerveau, mars 2021. URL : https://vadmc.hypotheses.org/?p=1165"
"Pendant son enfance, Simone de Beauvoir a été influencée par la vision de son père sur les femmes, et notamment par sa définition du devoir féminin, qui provenait de ses lectures. Georges de Beauvoir a donné en exemple à ses filles, selon les propos rapportés par Simone de Beauvoir dans les Mémoires d’une jeune fille rangée, les héroïnes de la romancière Colette Yver. Ces personnages féminins plaisent au père de Simone de Beauvoir : « (...) il admirait la sagesse des romans de Colette Yver où l’avocate, la doctoresse, finissent par sacrifier leur carrière à l’harmonie du foyer (...) ». Ni Simone ni sa soeur cadette Hélène n’ont suivi cette voie paternaliste. Cependant, Simone de Beauvoir s’est inspirée d’un des romans de Colette Yver, Princesses de science, pour une de ses nouvelles publiée en 1968, « La Femme rompue ». Nous aimerions analyser la manière dont Simone de Beauvoir change totalement le sens de l’oeuvre de Colette Yver, après l’avoir prise comme base de sa nouvelle. Nous étudierons, dans un premier temps, Princesses de science comme l’intertexte de « La Femme rompue », afin d’en dégager les différences et les ressemblances, ce qui nous permettra de répondre, quoique incomplètement, à notre question de départ : s’agit-il d’un simple duplicata de Colette Yver ou Simone de Beauvoir réécrit-elle véritablement Princesses de science, même dans les ressemblances avec son modèle ? Dans un deuxième temps, nous comparerons les deux fins choisies par les auteures, dans le dessein de distinguer quelle conclusion est la plus libératrice pour les héroïnes, et, par conséquent, pour les lectrices. Dans un troisième temps, nous verrons de quelle façon Simone de Beauvoir dépasse le roman de Colette Yver, en ajoutant une dimension d’oppression économique à son tableau d’un couple en crise, dimension que Colette Yver veut ignorer, au moins après le mariage."
"« Diffuser les films féminins et féministes – Interview de Nicole Fernandez-Ferrer », Tiphaine Martin, Nicole Fernandez-Ferrer, Voyages autour de mon cerveau, décembre 2020. URL : https://vadmc.hypotheses.org/968"
"Co-organisation, introduction et animation des débats lors de la conférence de Claudine Monteil, « Le Féminisme américain », Cercle Condorcet, Maison Paul Bert, salle Anna, Auxerre."
"« Gérard Bonal - Beauvoir au masculin », Tiphaine Martin, Gérard Bonal, Voyages autour de mon cerveau, mai 2020. URL : https://vadmc.hypotheses.org/191"
"L’autobiographie, récit rétrospectif écrit à plus ou moins longue échéance, fourmille de documents très divers. Le lecteur pourrait imaginer que ces pièces sont uniquement extraites des fonds personnels de l’écrivain qui retrace et se concentre sur sa propre existence, tout en élargissant parfois son propos au portrait de son époque, ce qui est faux. L’écrivain en utilise beaucoup d’autres. Mais tous ces documents utilisés par l’auteur, quels sont-ils ? D’où viennent-ils ? Quels effets la multiplicité des voix narratives produit-elle sur le lecteur ? Nous avons choisi de nous focaliser sur le cas des récits de voyage autobiographiques beauvoiriens, présents dans les Mémoires d’une jeune fille rangée (1958), La Force de l’âge (1960), La Force des choses (1962), Tout compte fait (1972). Ces narrations viatiques ne constituent pas un ensemble qui peut être séparé du reste du texte autobiographique, mais, au contraire, elles constituent la structure même des volumes mémoriels. Dans un premier temps, nous analyserons la multiplicité des sources utilisées par Simone de Beauvoir dans l’écriture des récits de voyage. Dans un deuxième temps, nous étudierons la manière dont l’ensemble des documents ayant trait au voyage permet la construction de la structure autobiographique. Dans un troisième temps, nous nous interrogerons sur les effets produits sur le lecteur par cette construction particulière."
"« “Le cinéma, ça vient de soi” – Interview des Sœurs Barbault », Tiphaine Martin, Emilie et Sarah Barbault, Voyages autour de mon cerveau, mars 2021. URL : https://vadmc.hypotheses.org/1395"
Récit de ma J.A.P.D. (Journée d'Appel de Préparation à la Défense).
"Qu’ont en commun Benoîte Groult, Mylène Demongeot, Dian Fossey et Virginia Woolf ? Tout d’abord, leur existence a été mise en roman graphique par une femme et un homme : Catel Muller pour les deux premières, Bernard Ciccolini pour les deux anglo-saxonnes. Ensuite, ce sont toutes des femmes à forte personnalité, dont l’histoire personnelle et familiale a traversé et s’est entremêlée à l’histoire politique internationale plus que troublée du vingtième et vingt-et-unième siècle. Enfin, leur reconnaissance publique a été longue. La mise en roman graphique de leur trajectoire entretient donc un lien très fort avec l’ouverture à de nouveaux publics, plus larges, plus jeunes et plus féminisés que celui de la bande dessinée traditionnelle. Il ne s’agit pas d’autobiographie dessinée, comme celle de Marjane Satrapi dans Persépolis (2000-2003) ou celle de Riad Sattouf dans L’Arabe du futur (série commencée en 2014). Les romans graphiques dont nous faisons état ont comme point de départ des mémoires, souvenirs, autobiographies personnelles, mais elles font également intervenir des témoignages directs, voire, pour Benoîte Groult, l’expérience de la dessinatrice, qui dialogue avec ses proches, la féministe et sa famille. L’ensemble des romans graphiques se situant dans des époques de conflits et de combats sociaux, le parcours au féminin en est fortement influencé et diverge de vies au masculin. Le rapport à l’autre est également différent pour Woolf, Demongeot, Groult, Fossey, car elles ont vécu dans des sociétés ultra-patriarcales et ont réussi à s’en détacher. Pour autant, elles ont pleinement profité de la vie, ainsi que les autres femmes présentes dans leur entourage. Dans cette communication, nous aimerions analyser comment l’expérience nutritionnelle est forcément reliée aux fêtes (bals bourgeois, fêtes familiales et amicales, fêtes publiques), au sexe et au voyage. Nous commencerons par étudier la mise en images de ces femmes et de leurs proches féminines, ainsi que de leurs expériences festives, liées à la nourriture et à la sexualité, dans ces quatre romans graphiques. Nous verrons ensuite comment ce lien a un impact sur leur vie, liée aux tourments de l’Histoire. Nous terminerons en examinant dans quel rapport aux autres s’inscrivent ces femmes voyageuses, sexuées et s’alimentant comme elles le peuvent, au gré des vicissitudes de leur existence. N’est-ce pas là que la fête remplace le pain et prend tout son sens d’oubli ?"
Qu'est-ce qu'un roman beauvoirien ?
"« Simone de Beauvoir, l’aventure d’être soi – Interview de Fabrice Gardel », Tiphaine Martin, Fabrice Gardel, Voyages autour de mon cerveau, mars 2021. URL : https://vadmc.hypotheses.org/?p=146"
"Tout au long de son existence, Simone de Beauvoir a été confrontée à la nourriture. Dès son enfance, les repas ont fait partie des moments les plus importants de son quotidien. Nous pouvons nous demander si elle a gardé, une fois adulte, cet appétit de nourritures bien terrestres. Dans ses mémoires, son attention aux mets la porte à noter tout ce qui relève d’une pratique culturelle, voire politique de l’alimentation. Dans un premier temps, nous ferons un rapide relevé de l’attention personnelle à la nourriture de Beauvoir. Dans un deuxième temps, nous nous interrogerons sur une éventuelle pratique de classe de la nourriture. Dans un troisième temps, nous verrons comment la faim peut être un moyen de pression politique, voire personnel."
"Simone de Beauvoir, écrivaine engagée, philosophe, romancière, est aussi une voyageuse infatigable. Elle a parcouru des kilomètres de routes, escaladé bien des monts, a osé se glisser dans quelques eaux. Pourtant, elle est toujours revenue à Paris. Pourquoi ? Nous aimerions nous interroger sur les raisons qui la poussent à regagner la capitale, ainsi que sur la manière dont elle transmet ses souvenirs de voyage dans ses mémoires. L'odyssée peut parfois se transformer en exil définitif, comme pour Arthur Rimbaud ou Aurélie Tidjani, mais elle peut aussi être simple aller-retour entre deux points, pause dans le quotidien, sans incidences sur l’existence. Pour Simone de Beauvoir, le périple ne s’arrête pas au retour. Les sensations corporelles et mentales sont transformées en souvenirs écrits. Voyager n’est pas uniquement partir de Paris puis y revenir, c’est faire un trajet intérieur qui peut conduire à transcrire ses expériences, à continuer à garder un lien avec l’ailleurs et les personnes rencontrées. Lors de cette transcription, Simone de Beauvoir inscrit son corps dans le monde. Son corps n’est pas qu’un simple mécanisme qui lui permet de se mouvoir, la mémorialiste le place au centre de son rapport au monde. Tout d’abord, nous nous pencherons sur les modalités du voyage, sur les raisons qui poussent Simone de Beauvoir à quitter Paris, puis à y revenir. Ensuite, nous retracerons les étapes du souvenir qui aboutissent à l’écriture de la chronique viatique, où le corps joue un rôle, depuis les achats (de vêtements, de souvenirs) pendant ses pérégrinations, jusqu’à la narration des séjours insérée dans l’autobiographie, sans oublier les rencontres que Beauvoir fait pendant ses expéditions. Puis, nous nous interrogerons sur les choix narratifs beauvoiriens, sur les raisons qui poussent l'écrivaine à s’attarder et à revenir sur certains lieux dans ses souvenirs, alors qu’elle ne fait qu’en mentionner d'autres."
"Selon ce que déclare Simone de Beauvoir dans Le Deuxième Sexe, « la femme est vouée à la magie », et, selon les propos que François Truffaut prête à son héros Bertrand Morane dans L’Homme qui aimait les femmes « toutes les femmes sont magiques ». Le Grand Meaulnes, héros voyageur, éternel errant, comme Corto Maltese, ne reste pas indifférent aux femmes qu’il rencontre. Mais Augustin Meaulnes en épouse une, Yvonne de Galais, tandis que Corto passe de femme en femme, comme si chaque pays se dessinait à travers un visage et un corps féminins. Dans l’univers d’Hugo Pratt, et surtout dans les aventures qu’il fait vivre à son héros le marin Corto Maltese, qu’en est-il de l’élément féminin et du rapport à la magie ? Hugo Pratt inscrit-il son personnage dans une filiation bien précise, celle des écrivains voyageurs du dix-neuvième et du début du vingtième siècle, tels Flaubert, Loti, Barrès, Montherlant, Morand ? Pour ceux-ci, la conquête d’une femme étrangère leur permet de dominer l’espace où ils se trouvent."
"Dans les années trente, Beauvoir a-t-elle été une exploratrice, une voyageuse, une aventurière ou une touriste ? Nous nous interrogerons sur le sens de ces mots en général, à cette époque et pour Beauvoir, à travers les récits de ses voyages au Maroc, en Espagne, en Grèce, en Italie… Nous élargirons ensuite notre propos en plaçant Beauvoir au centre de la constellation des écrivaines voyageuses et des grandes voyageuses. Nous soulignerons enfin les difficultés notées par Beauvoir d’être une femme qui voyage à cette époque de l’entre-deux-guerres. Ces difficultés sont-elles niées ou la mémorialiste dépasse-t-elle la dichotomie femme/ homme pour n’être plus qu’un individu qui arpente la planète ?"
"« Le labyrinthe forestier : Walt Disney, Eric Rohmer, Grégoire Leprince-Ringuet », Voyages autour de mon cerveau, juin 2020. URL : https://vadmc.hypotheses.org/71"
"Mon intérêt pour Simone de Beauvoir provient de la lecture du Deuxième Sexe, qui a été une découverte. J’ai ensuite lu le reste de son œuvre. En Master 1 j’ai étudié La Femme rompue, en comparant les thèses féministes du Deuxième Sexe avec leur application pratique dans les trois nouvelles de La Femme rompue. J’ai été alors frappée par la dénonciation virulente du pittoresque par Muriel, l’héroïne de « Monologue », la deuxième nouvelle de La Femme rompue. Pour mon sujet de Master 2, j’ai donc choisi un sujet sur les voyages. J’ai étudié la place prépondérante des récits de voyage dans les mémoires de Simone de Beauvoir. Ils envahissent peu à peu l’espace autobigraphique, avant de former la majeure partie du dernier tome autobiographique, Tout compte fait. J’approfondis cette étude dans le cadre d’un doctorat. Ma communication portera sur trois points de mon travail de Master 2. Je montrerai tout d’abord comment la mémorialiste inscrit par petites touches son amour pour les voyages, avant d’effectuer le voyage. Dans un deuxième temps, je présenterai la manière dont le corps beauvoirien est mis en œuvre, notamment à travers les moyens de déplacement empruntés par Simone de Beauvoir, la nourriture, ses rencontres amicales et amoureuses. Dans un troisième temps, j’étudierai quelles sont les traces matérielles et inconscientes laissées par le voyage."
"Nous avons choisi de nous intéresser principalement au cinéma, ce qui ne nous empêchera pas de faire des contrepoints avec la littérature. Nous sommes partis du Bal d’Ettore Scola (1983), qui rend hommage aux bals de quartier, de 1900 à 1983, mais aussi aux cinémas français, italien, et américain, d’où notre attention pour Les Hommes, quels mufles !, de Mario Camerini (1932), 14 Juillet de René Clair (1933), La Veuve joyeuse, d'Ernst Lubitsch (1934), La Belle Équipe, de Julien Duvivier (1936), Monsieur Max, de Camerini (1937) et Ninotchka, de Lubitsch (1939). Nous nous intéresserons d’abord aux raisons de faire la fête en ces temps d'après la crise de 1929, dans un bal ou ailleurs. Puis, nous interrogerons le rite festif comme pratique de classe : qui vient au bal populaire et pourquoi ? Enfin, nous nous demanderons si ces films proposent d’autres formes de fêtes, notamment pour les couples de héros : y a-t-il un maintien du bal populaire dans leur quotidien, ou celui-ci n’est qu’une parenthèse ? Sortent-ils transformés du bal ? À quelles autres formes d'amusement sont-ils ou non conviés ?"
"Dans Le Deuxième Sexe, Simone de Beauvoir établit des parallèles entre la situation des femmes, celles des Afro-Américains et celle des Algériens, Marocains et Tunisiens alors colonisés. Cependant, l’écrivaine pousse-t-elle ses parallèles jusqu’au bout, de manière stricte ? Nous verrons la signification pour Beauvoir de ces trois termes qui définissent sa théorie des parallèles : caste, race, sexe. Nous examinerons jusqu’où va Beauvoir dans les parallèles. Enfin, nous analyserons les applications pratiques de cette théorie."
"Les femmes politiques oubliées sont nombreuses, trop souvent confinées en soutien de/ inspiratrices de telle figure politique masculine, à l’instar des muses des écrivains ou artistes. Parmi elles, le cas Jeanne Deroin, ouvrière lingère devenue institutrice, puis révolutionnaire en 1848. Jeanne Deroin est une militante socialiste, puis féministe, même si le terme n’existe pas encore en ce milieu du dix-neuvième siècle. Elle fonde le journal L’Opinion des femmes, pour lutter pour le vote des femmes. Nous désirerions nous attacher à cette figure méconnue du combat pour le suffrage féminin. Nous retracerons d’abord son existence, avant de nous attacher à ses combats, socialistes et féministes. Nous finirons par nous interroger sur le contenu et la portée de L’Opinion des femmes, digne ancêtre de La Fronde de Marguerite Durand."
"En 1919, Robert Garric fonde les Équipes Sociales. Le but affiché est de combler, du moins en partie, le fossé entre les classes. Les termes choisis – Équipes sociales – définissent son projet : travail en commun et préoccupation pour les questions sociales. À l’époque où Simone de Beauvoir utilise ses nouvelles connaissances dans le cadre familial, en apprenant à sa sœur les rudiments du français, des mathématiques et de l’histoire, Robert Garric pense à faire se rencontrer ceux que la société a éloignés."
"Conférence à l'Université pour tous de Bourgogne, ESPE d'Auxerre."
"« Parler au plus grand nombre – Interview de Séverine Hettinger », Tiphaine Martin, Séverine Hettinger, Voyages autour de mon cerveau, mars 2021. URL : https://vadmc.hypotheses.org/1295"
"Beauvoir, dite « Notre-Dame-des-Sartre », dite « la Grande Sartreuse », dite aussi « La Sartreuse de charme », a volontairement diminué son originalité philosophique, ce qui fait grincer bien des dents féministes. Il en est de même pour sa théorie de la littérature, que l’autrice ne présente jamais comme telle. Nous aimerions lui rendre justice. Tout d’abord, nous rappellerons les sources d’inspiration théorique de Beauvoir (Marx, Sartre, Dos Passos et les autres). Puis, nous analyserons ses propos sur la littérature (mémoires, conférences et entretiens). Enfin, nous mettrons en relation ses propos et son œuvre, à travers l'étude de cas particuliers."
"Nathalie Lemel ou plutôt Le Mel, grande figure de la Commune de Paris, est désormais inconnue hors des cercles d’historien.ne.s. Sa participation active au restaurant communautaire « La Marmite » est minorée au profit du fondateur Eugène Varlin, autre héros de la Commune. Quant à son existence après l’amnistie et son retour de déportation, elle est expédiée en quelques lignes dans le peu de biographies qui lui ont été consacrées."
"Simone de Beauvoir s’est déplacée tant par terre que par mer et dans le ciel, de Paris à Pékin, en passant par Rome, Ismaïlia, Stockholm, les Cyclades, New York, Tokyo, Prague, Bamako, Manaus, ou La Havane... Retour sur cette écrivaine voyageuse, castor existentialiste et nomade à la découverte du monde."
"« Interview de Delphine Lannaud », Tiphaine Martin, Delphine Lannaud, Voyages autour de mon cerveau, décembre 2020. URL : https://vadmc.hypotheses.org/?p=875"
"« Interview de Grégoire Leprince-Ringuet », Tiphaine Martin, Grégoire Leprince-Ringuet, Voyages autour de mon cerveau, juin 2020. URL : https://vadmc.hypotheses.org/121"
"« Emilie Desjeux, peintre et photographe icaunaise laïque », Voyages autour de mon cerveau, décembre 2020. URL : https://vadmc.hypotheses.org/?p=875"
"« Des Bouches vraiment inutiles ? Voyage et féminisme chez Beauvoir dramaturge », in Dominique Bréchemier, Femmes en scène, femmes de théâtre, Paris : L'Harmattan, p. 243-254."
"« Les Points communs – Interview d’Aurélie Le Floch », Tiphaine Martin, Aurélie Le Floch, Voyages autour de mon cerveau, novembre 2020. URL : https://vadmc.hypotheses.org/?p=796"
"« Le Syndrome du Grand Meaulnes », Voyages autour de mon cerveau, novembre 2020. URL : https://vadmc.hypotheses.org/779"
"« À bord de l'Aire avec Zaza - Jour 5 », Voyages autour de mon cerveau, mai 2021. URL : https://vadmc.hypotheses.org/?p=1672"
"« Une radio à écouter avec plaisir tout le temps », Voyages autour de mon cerveau, juillet 2020. URL : https://vadmc.hypotheses.org/457"
"« L’enjeu existentiel de Beauvoir – Interview de Géraldine Gourbe », Tiphaine Martin, Géraldine Gourbe, Voyages autour de mon cerveau, avril 2021. URL : https://vadmc.hypotheses.org/?p=1533"
"« Diffusion de La Philosophie marseillaise de Simone de Beauvoir », Voyages autour de mon cerveau, septembre 2020. URL : https://vadmc.hypotheses.org/611"
"« À bord de l'Aire avec Zaza - Jour 3 », Voyages autour de mon cerveau, mai 2021. URL : https://vadmc.hypotheses.org/?p=1631"
"« “Etouffée par cette blancheur” : l’amie géniale des Inséparables », Voyages autour de mon cerveau, octobre 2020. URL : https://vadmc.hypotheses.org/?p=652"
"« À bord de l'Aire avec Zaza - Jour 4 », Voyages autour de mon cerveau, mai 2021. URL : https://vadmc.hypotheses.org/?p=1648"
"« The Perfect Candidate : lueurs féministes en Arabie Saoudite », Voyages autour de mon cerveau, septembre 2020. URL: https://vadmc.hypotheses.org/?p=642"
"« À bord de l'Aire avec Zaza - Jour 9 », Voyages autour de mon cerveau, mai 2021. URL : https://vadmc.hypotheses.org/?p=1727"
"« Héroïnes – Interview de Catel Muller », Tiphaine Martin, Catel Muller, Voyages autour de mon cerveau, avril 2021. URL : https://vadmc.hypotheses.org/?p=1550"
"« Sortir de l’ombre ? June Carter dans Walk The Line », Voyages autour de mon cerveau, octobre 2021. URL : https://vadmc.hypotheses.org/1976"
"« Transmettre l’histoire du cinéma – Interview des Sœurs Kuperberg », Tiphaine Martin, Clara Kuperberg, Julia Kuperberg, Voyages autour de mon cerveau, octobre 2021. URL : https://vadmc.hypotheses.org/?p=2078"
"« Lutter contre le patriarcat avec une poêle : Patraque et Cie », in Dominique Brechemier, Carole Laforêt et Adeline Muret (dir.), Sorcières, êtes-vous là ?, Paris : L’Harmattan, p. 299-311."
"« Entrevue Sartre-Beauvoir sur fond d’Inséparables », Voyages autour de mon cerveau, mars 2021. URL : https://vadmc.hypotheses.org/1324"
"« Vers un féminisme planétaire – Feminist in the City », Tiphaine Martin, Julie Marangé, Voyages autour de mon cerveau, septembre 2020. URL : https://vadmc.hypotheses.org/604"
"Table ronde du Festival International des Écrits de Femmes, organisé les 12-13 octobre 2013, Saint-Sauveur-en-Puisaye."
Récit de ma J.A.P.D. (Journée d'Appel de Préparation à la Défense).
Suite du récit de ma J.A.P.D. (Journée d'Appel de Préparation à la Défense).
"Que reste-t-il du mâle occidental aujourd’hui, quand il est sorti de son élément urbain, tout portable éteint ? Comment gère-t-il les évènements inattendus ? A quelles décisions est-il confronté ? Quelle place laisse-t-il à sa moitié et comment celle-ci s’y sent-elle ? La femme a-t-elle la parole au sein du couple et de la famille ? A-t-elle également besoin d’une thérapie individuelle, ou pense-t-elle la cure comme celle de son couple ? Qu’est-ce que les évènements imprévus remettent en cause du couple et de la famille, et de la perception extérieure de celui-ci ? Y a-t-il des remèdes proposés par d’autres personnages, pour d’autres personnages, d’autres couples ?"
"L’écriture mémorielle est un récit personnel qui accorde, selon la définition de Philippe Lejeune une large place à l’Histoire. Mais de quelle manière les écrivains rapportent-ils leurs expériences des événements historiques qu’ils ont traversées, ou qu’ils ont vécues ? Attendent-ils plusieurs années ou écrivent-ils sur le vif ? En effet, les mémoires ne sont pas uniquement la narration du passé lointain, l’auteur y inscrit parfois l’instant présent. Il en est ainsi d’Alexandre Dumas (Mes Mémoires) et de Chateaubriand (Mémoires d’Outre-Tombe) qui concluent leur œuvre autobiographique en décrivant leur passé proche. Chez Simone de Beauvoir, la rédaction de son œuvre mémorielle se rapproche du temps présent au fil des volumes. Si les Mémoires d’une jeune fille rangée (1958) rapportent une période lointaine - l’enfance et l’adolescence de Beauvoir entre 1908 et 1929, il n’en est pas de même pour les autres volumes, jusqu’à Tout compte fait (1972), récit des années 1962 à 1971, qui couvre la période de la signature de l’indépendance de l’Algérie jusqu’aux combats féministes. Dans cet article, nous aimerions nous interroger sur la manière dont Beauvoir se met en scène comme témoin de son époque, sur les raisons qui la poussent à décrire immédiatement les évènements de son temps, dépassant donc le cadre singulier de l’autobiographique pour atteindre l’universel singulier des mémoires. Dans un premier temps, nous nous demanderons dans quelle mesure la mémorialiste prend du recul par rapport aux événements historiques. Dans un deuxième temps, nous scruterons la manière dont Simone de Beauvoir traite de l’Histoire. Se contente-t-elle de transcrire les gros titres des journaux, ou insuffle-t-elle un accent personnel à ce qui se déroule hors de sa vie privée ? Dans un troisième temps, nous nous intéresserons à la manière dont la mémorialiste réagit en tant que personne aux conflits et aux évènements (deux guerres mondiales, Front Populaire, instauration des Quatrième et Cinquième Républiques, mouvements contestataires à travers le monde…). Qu’en retient-elle, et pourquoi ? Souci de décrire son époque en vue de la postérité, ou volonté de prendre position ici et maintenant sur telle ou telle question, mais d’une manière différente que par des articles de journaux ?"
Exposition sur les voyages de Simone de Beauvoir.
"« Appel - Florilège auteurs et autrices », Voyages autour de mon cerveau, mai 2020. URL : https://vadmc.hypotheses.org/241"
"« Ma J.A.P.D. », Voyages autour de mon cerveau, juillet 2020. URL : https://vadmc.hypotheses.org/481"
Présentation de la vie et de l'œuvre de Simone de Beauvoir.
"Nous voudrions nous demander quelle est l'originalité de la vision du conflit de ce couple d'instituteurs. Tout d'abord, nous verrons comment le conflit divise ou non la famille Mauny. Puis, nous étudierons la manière dont Émile rapporte la guerre qu'il vit quotidien. Enfin, nous verrons comment le couple reste un instituteur et une institutrice, guerre mondiale ou non."
"Conférence à la Médiathèque Olympe de Gouges, Joigny."
"Simone de Beauvoir, grande figure du féminisme français, écrivaine dont le roman Les Mandarins a reçu le prix Goncourt en 1954, a fait représenter sa pièce Les Bouches inutiles en octobre 1945, au théâtre des Carrefours, ancien nom du théâtre des Bouffes du Nord. Dans le contexte de la Libération et de la découverte des camps de concentration et de déportation, la trame tragique de cette pièce prend une résonance particulière, qui a été analysée par Danièle Fleury, avec la réception de la pièce. Pour notre part, nous nous intéresserons aux liens entre Les Bouches inutiles et l’univers du voyage, qui est consubstantiel à la persona beauvoirienne, sans négliger la dimension féministe de la pièce. Nous mettrons en corrélation le monde viatique beauvoirien avec le cadre flamand des Bouches inutiles. Nous interrogerons également le dépaysement provoqué par ce cadre étranger, choisi par l’autrice pour parler des choix de son propre pays pendant l’Occupation et pour « manifester une vérité humaine générale », selon ses propres termes. Nous établirons ensuite des parallèles entre la pièce et le roman Tous les hommes sont mortels (1946), qui suit immédiatement Les Bouches inutiles. Nous terminerons en examinant avec précision les caractéristiques féministes de la pièce."
"« Y a-t-il des héritières Beauvoir ? Pour une arborescence féministe » , Voyages autour de mon cerveau, avril 2021. URL : https://vadmc.hypotheses.org/1471"
"« Interview de Martin Provost – Beauvoir au masculin », Tiphaine Martin, Martin Provost, Voyages autour de mon cerveau, juillet 2020. URL : https://vadmc.hypotheses.org/536"
"« À bord de l'Aire avec Zaza - Jour 6 », Voyages autour de mon cerveau, mai 2021. URL : https://vadmc.hypotheses.org/?p=1683"
"« Sortir du sexisme - Interview de Sylvain Frécon », Tiphaine Martin, Sylvain Frécon, Voyages autour de mon cerveau, mars 2021. URL : https://vadmc.hypotheses.org/1316"
"« À la redécouverte d’Antonia Bird », Tiphaine Martin, Guillaume Banniard et Muriel Cinque, Voyages autour de mon cerveau, janvier 2022. URL : https://vadmc.hypotheses.org/2437"
"Dans quelle mesure écrire, c’est pour la romancière franco-algérienne Leïla Marouane, prendre les armes ? Ma thèse propose d'abord une analyse contextuelle, historique et sociologique des romans de Leïla Marouane avant d’analyser les rapports entre violences et écriture dans le contexte postcolonial. Il s’agit de montrer les rapports entre Histoire de l’Algérie et littérature postcoloniale d’expression française, et comment la romancière se réapproprie cette Histoire à la fois par l’écriture de la violence subie mais aussi par l’écriture du renversement de la violence. Dans quelle mesure la nécessité d’écrire a partie liée avec la violence ? Quelles en sont les manifestations stylistiques et narratives ? Nous avons choisi de mettre le mot « violence » au pluriel dans notre titre, car elle est polymorphe et multifactorielle. En plus du fait qu’aucune thèse n’a encore été entièrement consacrée aux œuvres de Leïla Marouane, ce sont les manifestations d’une violence féminine récurrente qu’il était pertinent de comprendre. Les héroïnes victimes d’abord, puis criminelles à leur tour qui jalonnent ce corpus témoignent de la mise en mots de phénomènes de renversement de la violence. À travers une langue crue et cynique symptomatique d’une reprise de pouvoir, la romancière dénonce les traumatismes, la confiscation de l’identité et de la féminité, l’obscurantisme religieux et le poids des traditions, mais elle dit aussi la nécessité de tendre toujours vers un « ailleurs », géographique ou fantasmé."
"« À bord de l’Aire avec Zaza – Jour 16 », Voyages autour de mon cerveau, janvier 2022. URL : https://vadmc.hypotheses.org/?p=2393"
"« Ernest et Célestine, le film : amère déception », Voyages autour de mon cerveau, décembre 2021. URL : https://vadmc.hypotheses.org/?p=2339"
"En mars 2010, Barack Obama fait adopter par le Congrès une réforme visant à élargir au maximum la couverture maladie des Américains. Parmi les points de la réforme, dans le cadre des plans collectifs, les employeurs ont l’obligation de fournir à leurs salarié(e)s un panier de soins préventifs dont le contenu, édicté par le ministère de la santé, inclut les moyens de contraception. Le ministère de la Santé a écarté la prise en charge des soins contraceptifs par les employeurs religieux au sens strict comme les Eglises, puis les organisations religieuses sans but lucratif dont les universités et les hôpitaux. Mais ces exemptions ne vont pas empêcher une levée de boucliers au sein de l'épiscopat catholique qui dénonce le contraceptive mandate comme une atteinte à la liberté religieuse et en appelle à la désobéissance civile. Il est bientôt rejoint par de nombreuses organisations ´évangéliques, avec lesquelles il finit par faire cause commune pour défendre le droit à l’objection de conscience des chrétiens à accepter la contraception comme un soin pris en charge. Ce droit recouvre également leur refus de la légalité de l’avortement et du mariage homosexuel. Depuis 2010, plusieurs Etats, sur la base de cette posture et grâce à des majorités républicaines ont adopté des textes protégeant prioritairement la liberté religieuse incluant l’objection en cas de conflit de conscience face à la loi. Cette présentation s’attachera à mettre en évidence les racines de l’objection de conscience dans le contexte religieux américain ainsi que sa direction actuelle. Elle dressera un premier état des lieux de l’efficacité, légale et judiciaire, de cette stratégie d’objection religieuse."
"Les contributions réunies ici s'intéressent à ce que pourraient être les conditions d'un multiculturalisme réussi, dont deux dimensions sont ici privilégiées : celle de conflits religieux qui n'ont cessé d'être instrumentalisés au service de causes moins saintes que profanes ; celle des modalités d'intégration dans un espace que les institutions internationales ne sont manifestement pas parvenues à unifier, laissant aux organisations privées la charge de reprendre à leur compte cette responsabilité sociale dont les sociétés civiles portent la demande pressante et qui paraît bien détenir la clé de la soutenabilité d'un avenir commun."
no abstract
no abstract
"Dans cette contribution, est examinée la figure décriée du pléonasme, sous deux de ses réalisations possibles, le pléonasme discursif (type «Une fille chiante, c'est un pléonasme») et le pléonasme dérivatif (du type «harmonie harmonieuse»). Ces deux configurations sont étudiées dans une perspective énonciative et pragmatique afin de mettre en perspective les rapports entre pléonasme et «style simple», mais aussi de réfléchir au simple, à la fois comme catégorie discursive et comme processus de simplification."
"Du 3 au 5 octobre 2013, s’est tenu à Nice un colloque international sur le thème « Figures du discours et contextualisation ». Longtemps, les figures de rhétorique ont été conçues comme des écarts par rapport à une norme et comme des ornements du discours. Plus récemment, d’autres approches ont proposé d’y voir plutôt des configurations du discours à la fois nécessaires et fonctionnelles qui disent quelque chose de la relation entre le locuteur et l’interlocuteur, entre le locuteur et le monde. C’est dans ce courant innovant et pluridisciplinaire, où se rencontrent linguistique, pragmatique, philosophie, sciences cognitives, que s'est situé le colloque dont on lira les actes sur ce site. Nous avons proposé de mettre en dialogue ces différentes approches pour faire progresser l’analyse des figures en contexte. Dans le domaine linguistique, les approches énonciatives et pragmatiques considèrent les figures dans le discours, et donnent un sens au déplacement terminologique illustré par les dénominations concurrentes de « figures de rhétorique »,« figures de style » et « figures du discours ». En effet, les études actuelles déplacent l’approche typologique vers une approche discursive qui essaie de rendre compte des figures, non seulement dans leur forme, mais aussi dans leur mise en œuvre discursive. Sans oublier ni l’archéologie des figures, ni leur ancrage rhétorique ni même leurs déterminations formelles, les approches discursives mettent l’accent sur la question de la production du sens dans les figures. Dans la dynamique des travaux qui ont conduit ces dernières années à repenser l’approche des figures du discours, parmi lesquels on peut citer l’approche praxématique de Catherine Détrie (Cahiers de praxématique 35, 2000 ; Du sens dans le processus métaphorique, Champion, 2001), l’approche rhétorico-pragmatique de Marc Bonhomme (Semen 15, 2002 ; Pragmatique des figures du discours, Champion 2005), l’approche énonciative en termes de points de vue d’Alain Rabatel (Langue française 160, 2008 et Le Français moderne t. 79, 2011), nous avons souhaité poursuivre l’analyse discursive des figures en posant la question de leur contextualisation. Le présent colloque se donnait plus précisément comme objectif de faire le point sur les interactions entre figures et contexte(s) : il s’est agi d’envisager d’une part comment et dans quelle mesure l’analyse prend en compte le contexte dans l’étude des figures et de réfléchir d’autre part sur la manière dont les figures – conditionnées par le contexte – conditionnent elles-mêmes le contexte."
"Dans la dynamique des travaux qui ont conduit ces dernières années à repenser le champ des figures du discours, parmi lesquels on peut citer ceux de Catherine Détrie en praxématique, ceux de Marc Bonhomme en pragmatique, ou encore ceux d’Alain Rabatel abordant les figures en termes de point de vue, nous souhaitons poursuivre l’analyse discursive des figures en posant la question de leur contextualisation. Les articles réunis dans ce numéro, relevant d’approches linguistiques et théoriques diverses, sont des études de corpus et partagent l’attention au contexte dans l’analyse des figures du discours. Ces articles proviennent d’une sélection de communications présentées lors du colloque Figures du discours et contextualisation (Nice, 3-5 octobre 2013), enrichie de trois articles inédits, dont deux à visée didactique. Le présent numéro se donne pour objectif de faire dialoguer ces différentes approches en soulignant ce que ce dialogue apporte à l’étude des figures en contexte."
no abstract
"Si le paysage maritime forme la toile de fond de nombreux récits de l’auteur ligure, cette présence devient fonctionnelle dès lors que ses personnages s’immergent dans le paysage pour nager : des premiers récits des années 40 et 50 jusqu’au volume Palomar, en passant par les Cosmicomiche, la nage constitue dans l’œuvre de Calvino un mouvement récurrent qui, loin de représenter une simple activité physique ludique et agréable permettant de se déplacer d’un lieu à un autre, acquiert une fonction poétique de plus en plus évidente. Mouvement du corps mais aussi de l’esprit, la nage devient pour les personnages calviniens le moyen d’accéder à une dimension autre, et de se fondre dans un horizon poétique."
"Dollari e vecchie mondane fait le récit de situations typiquement représentatives de la période historique faisant suite au débarquement des soldats américains en Italie, mais l’angle d’approche de la figure du soldat – que Calvino étudie déjà dans le roman Il sentiero dei nidi di ragno – est totalement novateur. À l’intérieur d’un cadre narratif a priori banal se développe un texte qui flirte avec la prose poétique, composé de récurrences formelles, de mouvements, de tableaux, qui se répondent en des structures symétriques ou en des jeux d’échos. Calvino expérimente ici – dans l’un de ses premiers textes clefs – une méthode d’écriture structurée autour de ce que l’on peut nommer de véritables mots clefs, décelables en filigrane dans l’ensemble de son oeuvre."
"Dans son Cantico del gallo silvestre, Giacomo Leopardi s'inspire d'un mythe lié à différents cultes, celui du Tarnegòl bar : dans sa réinterprétation, il donne la parole à cet animal mythique, coq géant touchant ciel et terre qui annonce, par son chant, la fin des ténèbres et la naissance du jour. Les représentations de ce mythe sont, à notre connaissance, extrêmement rares, mais on retrouve ses traces dans le dessin et la peinture avec les productions de Giorgio De Chirico et Marc Chagall. Nous étudierons donc les représentations du Tarnegòl bar chez ces trois artistes, leurs valeurs symboliques et les variations qu'ils proposent à partir de ce mythe originel."
"On trouve une dimension hypertextuelle dans l’œuvre de deux auteurs majeurs de la littérature italienne, Giacomo Leopardi (1798-1837) et Italo Calvino (1923-1985), et cette dimension constitue même l’un des principaux traits communs de leur écriture. Si leur écriture est influencée par un rapport constant à la lecture, la relecture, la réécriture, voire à la compilation des textes d’autrui, des connexions fortes s’établissent aussi à l’intérieur de leur propre œuvre : les textes de Calvino et ceux de Leopardi, tous ensemble, construisent quelque chose. Ils s’agencent, ou mieux certains de leurs points, de leurs lignes, de leurs mouvements, s’agencent, se répondent d’un livre à l’autre, se répètent, se développent mutuellement. Mais, attention, le réseau ainsi formé ne résulte pas de correspondances parfaites, ne s’enferme pas en des règles qui régiraient sa structure. C’est un réseau mouvant, aux réponses inattendues ; imprévisible dans ses moments de contradiction comme à ses heures de cohérence cristalline. Plus qu’à un ordre fermé, rigide, il ressemble à une formation végétale, en croissance, enchevêtrée et discontinue, un rhizome, pour reprendre le concept qui correspond parfaitement – tel que le décrivent Deleuze et Guattari dans Mille plateaux (1980) – à la forme d’écriture que mettent en acte Leopardi et Calvino. Employé pour la première fois en 1975 dans l’ouvrage de Deleuze Kafka. Pour une littérature mineure, le concept de rhizome a souvent été rapproché de celui d’hypertexte, tant dans l’acception littéraire de ce dernier, fondée par Genette, que dans son acception informatique. Nous verrons comment les principes de ce concept deleuzien – connexion ; hétérogénéité ; multiplicité ; rupture asignifiante ; cartographie et décalcomanie – correspondent à l’écriture léopardienne comme à l’écriture calvinienne, au point que l’on peut considérer qu’ils ont construit, chacun, une œuvre-rhizome."
fr_abstract_s
"Nous nous intéressons à la dynamique saturé du Laser à Electrons Libres Simple Passage, à travers une approche champ moyen. Nous proposons une méthode afin d'accroître la taille de la macro-particule. Celle-ci repose sur la reconstruction de tores invariants de la dynamique de particules-test. A cette fin, un terme de contrôle - qui agit comme une faible perturbation du système - est calculé. Les possibles conséquences de cette stratégie vis-à-vis de l'optimisation du laser sont discutées."
Une goutte de colorant que l'on agite dans un verre d'eau illustre la diffusion chaotique des particules dans tout le volume disponible. Ce phénomène est aussi observé dans les accélérateurs de particules ou dans les plasmas de tokamak. Cependant il est possible de contrôler ce chaos en construisant des barrières de transport qui agissent comme des murs immatériels empêchant ou canalisant la diffusion des particules. Ce contrôle est réalisé par une action extérieure associée à un faible coût énergétique.
"Les pertes d'énergie et de particules dues au transport anormal dans les appareils de confinement magnétique de type tokamak sont encore un sérieux obstacle pour la fusion thermonucléaire contrôlée. Au niveau d'ITER, des changements aussi modestes soient-ils dans les propriétés de confinement peuvent changer de manière drastique le facteur d'amplification d'énergie. Les états de meilleur confinement trouvés empiriquement et la possibilité de réduire et/ou supprimer le chaos avec les perturbations paramétriques (essentiellement développées pour les systèmes dissipatifs), suggèrent d'étudier la possibilité d'une stratégie de contrôle du transport chaotique anormal par des perturbations appropriées agissant au niveau microscopique des mouvements des particules chargées."
"Un siècle après le modèle d'univers de Friedmann-Lemaître, les observations le confortent avec une constante cosmologique Λ et une composante de matière sombre (noire) sans pression (poussière) et froide dominant celle baryonique, que l'on désigne par modèle ΛCDM ou encore modèle standard. L'accélération de l'expansion de l'Univers confirmée par le diagramme de Hubble des supernovae en 1998 impose une valeur strictement positive à la constante cosmologique. Mes travaux de thèse se focalisent sur l'estimation des valeurs de paramètres cosmologiques du modèle standard en utilisant la technique de corrélation nulle. Cette approche présente l'avantage d'être plus robuste que les techniques usuelles. En particulier, il n'est pas requis de préciser la fonction de luminosité, celle-ci est déduite par cette méthode. De plus, elle prend en compte le biais de Malmquist due à la limitation en magnitude apparent. Ce travail a consisté aussi à modéliser des échantillons de l'événement quasar ainsi que l'événement supernova, une extrapolation adaptée du premier. Ce qui a permis de générer des échantillons conformes aux hypothèses des modèles, afin de valider les approches statistiques. Nous avons exploité les données du Sloan Digital Sky Survey (SDSS) pour les quasars, et celles du SuperNova Legacy Survey (SNLS) et du SDSS-II pour les supernovae. Les inférences statistiques ont conduit à un univers spatialement fermé et une présence de matière noire plus faible. Dans le cadre d'une prochaine application de cette technique, elle sera utilisée pour contraindre les modèles d'énergie noire. De même, l'utilisation des amas de galaxies observées grâce à l'effet de Sunyaev Zel'dovich, servira d'échantillon cosmologique. Une telle étude pourra contribuer à apporter un élément de réponse à la validité du rôle supposé des neutrinos massifs dans la formation des amas dans l'ère primordiale de l'Univers."
"L'anneau Diff_{h}(n) des opérateurs différentiels h-déformés apparaît dans la théorie des algèbres de réduction.Dans cette thèse, nous construisons les anneaux des opérateurs différentiels généralisés sur les espaces vectoriels h-déformés de type gl. Contrairement aux espaces vectoriels q-déformés pour lequel l'anneau des opérateurs différentiels est unique à isomorphisme près, l'anneau généralisé des opérateurs différentiels h-déformés Diff_{h,σ}(n) est indexée par une fonction rationnelle σ en n variables, solution d'un système dégénéré d'équations aux différences finies. Nous obtenons la solution générale de ce système. Nous montrons que le centre de Diff_{h,σ}(n) est un anneau des polynômes en n variables. Nous construisons un isomorphisme entre des localisations de l'anneau Diff_{h,σ}(n) et de l’algèbre de Weyl Wn étendue par n indéterminés. Nous présentons des conditions irréductibilité des modules de dimension fini de Diff_{h,σ}(n). Finalement, nous discutons des difficultés a trouver les constructions analogues pour l'anneau Diff_{h}(n,N) correspondant à N copies de Diff_{h}(n)."
"La Chromodynamique Quantique (QCD) sur réseau permet d'étudier de façon ab-initio et non-perturbative les processus d'interaction forte. Ce formalisme, qui permet une régularisation covariante de la théorie de l'interaction forte, fournit aussi un cadre naturel pour le calcul et la simulation numérique de la Chromodynamique Quantique. Dans cette thèse, après un tour d'horizon des principales propriétés de la QCD et une présentation détaillée de notre discrétisation de cette théorie sur un réseau, nous étudions de façon approfondie deux problèmes de physique hadronique : le phénomène de diffusion résonante et la structure du nucléon. Les calculs sont réalisés avec les configurations de jauge de la Collaboration Budapest-Marseille-Wuppertal, générées avec une action de Wilson améliorée avec 2+1 saveurs de quarks dynamiques. Elles couvrent une large gamme de pas de réseau, de volumes et de masses des quarks différents, permettant ainsi une étude fine de la sensibilité de nos résultats à ces paramètres, et fournissant un bon contrôle sur l'extrapolation au continu. Notre étude de la diffusion de particules sur le réseau est menée grâce à une méthode proposée par M. Lüscher. Nous avons choisi le cas particulier de la diffusion pion-pion dans le canal résonant du méson rho, et analysé nos données avec une méthode variationnelle aux valeurs propres généralisées. Nous présentons les déphasages pion-pion ainsi que les paramètres de la résonance obtenus de façon détaillée, tout en garantissant un bon contrôle de nos erreurs systématiques. Nos résultats apportent une avancée importante dans le panorama des études de diffusion sur le réseau car ce sont les premiers réalisés à la masse physique du pion, pour laquelle la désintégration du rho en deux pions peut effectivement avoir lieu. Les valeurs obtenues pour les paramètres de la résonance du méson rho sont accord avec l'expérience, et confirment la faible dépendance du couplage entre le rho et les deux pions à la masse du pion. L'exploration de la structure du nucléon se fait à travers un calcul complet des facteurs de forme électrofaibles isovectoriels, avec une étude approfondie du rayon de charge électrique et de la charge axiale. Notre analyse présente aussi des données à la masse physique du pion, ce qui s'avère crucial pour maîtriser les extrapolations au point physique, étant données les variations violentes prédites par la perturbation chirale de ces quantités. Notre calcul utilise une projection sur les états du nucléon à la source et au puits, et une méthode de fit combinant les fonctions de corrélation à deux et trois points afin de réduire et de contrôler au maximum les contaminations pouvant venir des états excités. Bien que davantage de données seraient nécessaires pour déterminer très précisément le rayon et la charge axiale au point physique avec une évaluation pertinente des erreurs systématiques, les valeurs que nous obtenons sont en bon accord avec l'expérience, et suggèrent que les effets dus aux états excités sont faibles et sous contrôle. Notre analyse souligne aussi que l'utilisation de configurations de jauge avec des masses de pion proches de la valeur physique et avec des grands volumes semble indispensable à une étude précise de la structure du nucléon sur réseau."
"Courant Josephson dans une jonction ""S/2DQ/S"" : Nous caractérisons les effets d'Andreev croisés (CAR) dans une jonction composée de deux points quantiques couplés à deux contacts supraconducteurs. En présence d'un flux magnétique intérieur, la présence ou non de CAR influe directement sur la période d'oscillation du courant critique. Techniquement, on calcule l'énergie libre dans le formalisme d'intégrales de chemins et on utilise une approche de champ moyen pour traiter l'interaction coulombienne. On obtient ainsi le courant Josephson, le nombre d'occupation moyen et des diagrammes de phases pour la transition 0 - p. Courant dans une jonction ""S/QD/S+N"" : nous caractérisons la présence d'un contact normal dans une jonction de référence composée d'un point quantique couplé à deux contacts supraconducteurs. Un tel système pourrait servir à simuler la décohérence dans le transport. Nous montrons que la première harmonique du courant est déphasée en présence du contact normal. D'autres effets sur les réflexions multiples d'Andreev sont observés. Techniquement, nous utilisons le formalisme de Keldysh et une approche de champs moyens pour traiter l'interaction coulombienne. Bruit dans une fourche ""nanotube de carbone-métal"" : nous étudions le bruit en régime photo-assisté dans un système composé d'un nanotube de carbone de longueur finie couplé en son centre à un métal normal. Les corrélations de courant montrent les effets de l'interaction coulombienne dans le nanotube ainsi que les marches caractéristiques du régime photo-assisté."
"On présente un nouvel outil, formulé dans le langage de la géométrie différentielle, pour réduire les symétries de jauge: la méthode du champ d'habillage. Il est appliqué à des exemples variés, incluant le secteur électrofaible du modèle standard, la formulation tetrad de la relativité générale et la structure conforme du second ordre. Son extension au formalisme BRST est également proposée."
La première partie de la thèse concerne l'étude d'une classe particulière de systèmes dynamiques déterministes présentant deux problèmes: la présence de points fixes neutres et des points de discontinuité auxquels la dérivée n'est pas bornée. La seconde partie traite des systèmes dynamiques aléatoires: du problème de la récurrence dans ce type de système puis de leur application à la modélisation de petites perturbations stochastiques. On traite en particulier du problème de la stabilité stochastique.
"Cette étude présente la diffusion d'un champ électromagnétique par un batiment de bureaux modélisé suivant un réseau bipériodique, en particulier, l'objectif est de calculer le champ réfléchi et transmis. Ce champ est produit par une antenne extérieure au batiment, on suppose qu'elle émet une onde plane. On travaille dans un domaine de longueur d'onde qui est inférieur à la plus petite dimension de la structure, ce qui correspond à l'épaisseur des murs. Les calculs sont effectués suivant la méthode des ondes couplées (RCWA). Les résultats mettent en évidence la structure bipériodique du batiment, et l'on observe un effet significatif du flux transmis, la répartition des intensités calculées dans un plan d'observation extérieur au batiment montre une structure complexe."
"Cette thèse comprend deux parties. 1. Quantification conformément équivariante des fibrés supercotangents. Nous entendons par quantification du fibré supercotangent d'une variété M, un isomorphisme linéaire entre l'espace des superfonctions polynomiales en les fibres et l'espace des opérateurs différentiels spinoriels sur M. Nous montrons qu'il existe une unique quantification pour les fibrés supercotangents des variétés (M,g) conformément plates, qui soit équivariante sous l'action des transformations conformes de M. 2. Sur la géométrie projective du supercercle: une construction unifiée des super birapport et dérivée schwarzienne. Nous établissons, pour trois supergroupes agissant sur le supercercle, une correspondance entre le supergroupe, les invariants caractéristiques de son action et le 1-cocycle associé, définissant ainsi trois géométries sur le supercercle. L'invariant de la géométrie projective est le super birapport, son 1-cocycle associé étant la dérivée schwarzienne."
"Le premier objet de cette thése est de présenter une méthode de résolution pour l'équation de scission modulaire, équation qui permet de déterminer les symétries quantiques d'une théorie de champs conforme. On peut l'utiliser dans le cadre des théories associées aux graphes simplement lacés (les ADE de la famille SU2, ou leurs généralisations) et retrouver ainsi des résultats connus, en particulier la structure des groupoides quantiques associés. Le second objet de cette thése est d'appliquer cette technique dans le cadre plus général des graphes non simplement lacés afin de déterminer les algébres de symétries quantiques correspondantes, et d'explorer leurs propriétés. Plusieurs exemples de ce type sont analysés."
"Le sujet de cette thèse est l'établissement d'une nouvelle solution de l'équation de Yang-Baxter. Cette équation est présente dans de très nombreux domaines de la physique théorique (systèmes intégrables, mécanique statistique, QISM,...) ou des mathématiques (théorie des nœuds, groupes quantiques,...), mais l'étude de ses solutions est difficile (équations non-linéaires, variables non-commutatives, etc.). Une solution de l'équation de Yang-Baxter est aussi appelée tressage. Dans une première partie, nous présentons des résultats généraux sur le groupe des tresses et son algèbre de groupe. Nous nous intéressons ensuite aux analogues tressés que l'on peut considérer comme des analogues non-commutatifs de q-analogues. Nous présentons entre autres des analogues pour les coefficients binomiaux, les symboles de Pochhammer et les nombres de Fuß-Catalan, ainsi que pour le développent binomial et la convolution de Vandermonde. Ces deux premiers chapitres contiennent des résultats plus ou moins standards et forment l'assise des résultats qui suivent. La définition des nombres de Fuß-Catalan est toutefois originale. Dans une seconde partie, nous abordons les tressages d'espaces de tenseurs. Nous commençons par présenter les équations qui doivent être satisfaites par un tel objet et nous donnons une solution dont nous montrons l'unicité. Dans un dernier chapitre, nous plaçons ce tressage dans un contexte plus général et nous présentons les tressages dits « zébrés » qui prennent en compte une éventuelle cyclicité dans l'ordre des tenseurs sur lesquels ils se projettent. Le contenu de ces deux derniers chapitres est original. Nous fournissons ainsi une nouvelle solution de l'équation de Yang-Baxter et explorons ses propriétés."
"L'objet de cette thèse est l'étude du modèle plat, l'ingrédient principal du programme de quantification de la gravité par les mousses de spins, avec un accent particulier sur ses divergences. Outre une introduction personnelle au problème de la gravité quantique, le manuscrit se compose de deux parties. Dans la première, nous obtenons une formule exacte pour le comptage de puissances des divergences de bulles dans le modèle plat, notamment grâce à des outils de théorie de jauge discrète et de cohomologie tordue. Dans la seconde partie, nous considérons le problème de la limite continue des mousses de spins, tant du point de vue des théorie de jauge sur réseau que du point de vue de la ""group field theory"". Nous avançons en particulier une nouvelle preuve de la sommabilité de Borel du modèle de Boulatov-Freidel-Louapre, permettant un contrôle accru du comportement d'échelle dans la limite de grands spins. Nous concluons par une discussion prospective du programme de renormalisation pour les mousses de spins."
Etudes phénoménologiques des modèles et théories au-delà du Modèle Standard auprès de collisionneurs actuels et futurs. Etude de l'énergie noire et de l'extraction des paramètres cosmologiques à partir de différentes sondes cosmiques.
"L'intensité d'une onde électromagnétique en interaction auto-consistente avec un faisceau de particules chargées, comme dans un Laser Electron Libre par exemple, présente des oscillations importantes dues à un aggrégat de particules, appelé macro-particule. Dans cet article, nous proposons une stratégie pour stabiliser l'intensité en détruisant la macro-particule. Cette stratégie repose sur une analyse de la stabilité linéaire (à l'aide de la méthode des résidus) d'orbites périodiques spécifiques d'une modélisation champ moyen du système. La modulation d'un paramètre de contrôle fait apparaître dans le système des bifurcations qui provoquent des changements drastiques dans la dynamique auto-consistente, en particulier sur la macro-particule. Nous montrons comment il est ainsi possible de stabiliser l'intensité de l'onde grâce à l'introduction d'une onde-test, qui joue le rôle de paramètre de contrôle."
"Nous étudions la transition de phase survenant dans le gaz de Bose pour des systèmes sans invariance par translation. Bien qu'il soit prouvé depuis les années 60 que la condensation de Bose Einstein (CBE) est absente des systèmes invariants par translation en dimension 1 ou 2, on peut néanmoins déclencher cette transition de phase dans des gaz de Bose en faible dimension en ajoutant un potentiel externe approprié (et par conséquent, en perdant l'invariance par translation). Cependant, le condensat ainsi obtenu se trouve dans des états localisés, alors que la CBE est généralement comprise comme l'occupation macroscopique d'états cinétiques étendus. Il n'est pas à priori évident que cette transition de phase obtenue grace à la localisation est de la même nature que celle reliée au concept habituel de CBE. Dans cette thèse, nous considérons deux classes de systèmes localisés. La première est une famille de modèles aléatoires, pour lesquels le gaz de Bose est contenu dans un milieu désordonné, ce que nous modélisons par un potentiel externe aléatoire. La deuxième est constituée de modèles incluant un potentiel externe faible (d'échelle). Nous commençons par un rappel des conditions nécessaires sur ces potentiels pour obtenir une condensation dans les états localisés. Nous montrons sous certaines hypothèses très générales que dans ces modèles, la CBE au sens habituel est aussi présente, dans un sens généralisé. Cela signifie que les particules sont condensées dans des états cinétiques ayant une énergie arbitrairement faible. Pour le gaz de Bose sans interactions, nous pouvons en plus prouver que les densités des deux condensats sont en fait égales. Nous approfondissons ensuite notre étude de la CBE, en demandant si il est possible d'obtenir une condensation sur un seul état cinétique. Nous montrons qu'en dépit de l'existence à la fois d'une transition de phase et de la CBE généralisée, aucune condensation ne survient sur un seul état cinétique. En particulier, la fameuse condensation sur l'état fondamental est absente pour ces modèles localisés. Finalement, nous établissons une généralisation possible de l'approximation de nombres complexes de Bogoliubov pour prendre en compte les propriétés très particulières de la CBE en présence de localisation, et nous discutons la faon d'interpréter le resultat du problème variationnel correspondant."
"Le sujet de cette thèse est d'étudier la dynamique quantique d'une particule évoluant dans le plan sous l'influence de champs magnétique et électrique croisés. Dans le cas où ce système est actionné par un flux Aharonov-Bohm dépendant du temps, nous présenterons un théorème adiabatique basé sur une analyse spectrale fine en l'absence d'un potentiel électrique. Pour le cas sans champ extérieur et avec un petit potentiel électrique, nous présentons deux résultats. Premièrement, nous prouvons pour des potentiels arbitraires que la dynamique effective donne une approximation au premier ordre pour des temps longs. Ensuite, nous montrons que pour une classe de potentiels lisses et petits, nous pouvons construire une constante du mouvement non triviale. Pour cela, nous prouvons que l'hamiltonien est unitairement équivalent à un hamiltonien effectif commutant avec l'observable de l'énergie cinétique. Pour démontrer cela, nous utilisons un algorithme de diagonalisation partielle."
"Les technologies modernes permettent d'avoir des renseignements toujours plus précis sur les interactions entre individus. Dans ce contexte, la collaboration SocioPatterns a permis de développer une infrastructure mesurant, avec une très grande résolution temporelle, la proximité face-à-face d'individus volontaires, portant des badges de radio-identi cation. Cette infrastructure a été déployée dans divers contextes, tels que des conférences scienti ques, un musée, une école ou encore un service hospitalier. La simple analyse de ces données représente un enjeu majeur pour l'étude de la dynamique humaine et soulève des questions aussi fondamentales que la recherche d'outils et de techniques d'analyse adaptés. Cette thèse présente la caractérisation statistique de la dynamique de proximité physique, mise en relation avec le contexte et les autres métadonnées disponibles, telles que l'âge, le sexe des individus, ou bien la structure de leurs réseaux sociaux virtuels. Si la structure des contacts diff ère considérablement selon le contexte, les distributions empiriques des durées des interactions et entre interactions sont très similaires. Un modèle individu-centré, présenté dans cette thèse, propose des règles d'interactions microscopiques simples susceptibles de donner lieu à cette structure macroscopique complexe des temps d'interaction. Enfin, la caractérisation de la dynamique des contacts entre individus constitue une étape cruciale pour comprendre les mécanismes de propagation de maladies telles que la grippe dans une population. Les données de proximité humaine ont permis d'étudier la quantité d'informations nécessaires sur la dynamique des contacts pour la construction de modèles épidémiologiques de contagion. De tels modèles permettent de mieux estimer a priori l'impact de stratégies de santé publique telles que la fermeture de classes et les vaccinations ciblées."
"Les désintégrations rares $B^0_{(s)}\rightarrow \ell\bar{\ell}$ sont générées par des courants neutres avec changement de la saveur. Pour cette raison, ainsi qu'à cause de la suppression d'hélicité, leurs taux de désintégration sont très petits dans le Modèle Standard (MS), mais la présence de particules virtuelles de Nouvelle Physique peut radicalement modifier cette prédiction. Dans le MS la prédiction théorique des taux de désintégration est très précise, et la comparaison des valeurs mesurées avec ces prédictions théoriques peuvent donner des indications sur la structure de la Nouvelle Physique. Une partie du travail original présenté dans cette thèse est dédié à l'optimisation de l'algorithme d'Analyse Multi Varié (MVA) pour la recherche de la désintégration $B^0_{(s)}\rightarrow \mu^+\mu^-$ avec l'échantillon collecté par l'expérience LHCb pendant la première période de fonctionnement du LHC. Cet échantillon a été combiné avec celui collecté par l'expérience CMS et pour la première fois la désintégration $B^0_{(s)}\rightarrow \mu^+\mu^-$ a été observée. En vue d'améliorer la sensibilité au mode $B^0\rightarrow \mu^+\mu^-$, une nouvelle variable d'isolation, qui utilise un algorithme de reconstruction topologique inclusif, a été développée. De nouvelles études ont également été menées pour augmenter la performance des analyses multivariées. Une autre partie du travail original présenté dans cette thèse concerne la définition d'une chaine de sélection pour la recherche des désintégrations $B^0_{(s)}\rightarrow \tau^+\tau^-$, qui restent encore inexplorées. Dans cette thèse l'état final où les deux $\tau$ vont en trois $\pi$ chargées et un $\tau$ est étudié. La présence des deux $\nu$ dans l'état final de la désintégration rend difficile une reconstruction des impulsions des deux $\tau$. Cependant, la possibilité de mesurer les deux vertex de désintégration des $\tau$ ainsi que le vertex d'origine du candidat B, permet d'imposer des contraintes géométriques qui peuvent être utilisées dans la reconstruction des impulsions des deux $\tau$. En particulier, un nouvel algorithme pour la reconstruction complété, événement par événement, de ces impulsions et de leurs variables associées est présenté et discuté."
"Cette thèse présente une description nouvelle et les conséquences physiques de la seconde transition pour les gaz parfait de Bose dans des milieux fortement anisotropes. Nous développons ainsi dans le chapitre 1 une approche dite d'échelle qui permet de revisiter les différents concepts autour de la condensation de Bose-Einstein: la condensation généralisée (M.van den Berg, J.Lewis, J.Pulé, 1986), les cycles infinis (R.Feynman, 1953) et les corrélations à longue portée (O.Penrose, L.Onsager, 1956). Cette nouvelle approche nous permet, dans un premier temps, de montrer l'équivalence entre ces critères de condensation et entre les différentes classifications de condensats. Ensuite, dans les chapitres 2 et 3, nous caractérisons, les effets physiques (nouvelle température critique, modification des fractions condensées, la localisation énergétique et les longueurs de cohérence) pour les gaz de Bose dans des boîtes quasi-2D (Ch2) et des pièges harmoniques quasi-1D (Ch3) exponentiellement anisotropes. Dans le chapitre 4, nous discutons principalement l'analogie entre cycles et polymère à la P.-G de Gennes que fourni notre description des cycles via notre méthode d'échelle. Les modèles pièges et de boîtes tri-dimensionnelles exponentiellement anisotropes que nous présentons, sont, en limite thermodynamique, des cas intermédiaires entre les modèles tri-dimensionnels anisotropes conventionnels (anisotropie linéaire) et les modèles de basses dimensions, pour lesquels, pour le cas sans interactions entre particules, la notion de condensat sur le mode fondamental n'est pas évidente. Ceci donne une première approche pour le cas du gaz en faibles interactions qu'il conviendra de traiter par la suite."
"Avec la découverte étonnante que l'univers se trouve à présent dans une phase d'expansion accélérée, il y a dix ans, la cosmologie entra dans ce que l'on peut nommer l'ère de la cosmologie à haute précision. Les contraintes actuelles indiquent un modèle cosmologique à géométrie plate, où la plus grande partie du contenu en masse-énergie de l'univers est contribuée par une composante inconnue, souvent appelée l'`énergie noire', qui contribue environ soixante-dix pour cent à la densité totale de l'univers. Dans ce modèle, la matière baryonique ordinaire et la radiation ne contribuent qu'environ cinq pour cent, et la matière noire contribue vingt-cinq pour cent. Les propriétés mesurées de l'énergie noire étant consistant avec celles d'une Constante Cosmologique, $Lambda$, ce modèle standard cosmologique est connu sous le nom du modèle `$Lambda$-Cold-Dark-Matter' (`$Lambda$CDM'). Malgré son succes, ce modèle souffre de plusieurs problèmes. L'existence d'une Constante Cosmologique soulève des problèmes fondamentaux concernant sa nature physique, et beaucoup d'auteurs traitent le `problème de coîncidence'. Des essais de la décrire comme la contribution du vide quantique faillissent quantitativement. En conséquent, un grand nombre de modèles alternatifs a été développé, qui tentent décrire la composante d'énergie noire: des loi modifiés de la gravitation, de dimensions supplémentaires, les modèles de Quintessence. Aussi, des effets astrophysiques qui miment une expansion accélérée ont été considérés. Dans ce manuscrit, on expose les bases théoriques et observationneles du modèle $Lambda$CDM et les divers approches théoriques à expliquer l'énergie noire. Un autre problème du modèle standard provient de la dépéndance des résultats de l'analyse des données sur des hypothèses qui sont présentes dans les analyses pour l'extraction des paramètres. Il s'agit des hypothèses sur la physique, mais aussi des dépéndances des paramétrages notamment des propriétés de l'énergie noire. Aujourd'hui, des analyses combinées de divers sondes cosmologiques sont effectuées afin d'extraire les paramètres du modèle, dont le nombre peut s'élever jusqu'à vingt, dépendant des suppositions de modèle. De différentes hypothèses (géométrie plate, équation d'état de l'énergie noire constante, hypothèses sur la physique du CMB comme la vitesse du son ou le spectre de puissance initial,...) sont appliquées, qui peuvent dangereusement biaiser les résultat de l'analyse. La présence d'une mauvaise hypothèse, où une application d'un paramétrage non-approprié de la physique, pourra entraîner qu'on mesure à haute précision quelque chose qui n'est pas là. Nous montrons que, dû à la haute précision des mesures cosmologiques modernes, des approches purement cinématiques à la cosmologie ne permettent plus d'extraire des résultats fiables sur l'expansion de l'univers. Dans l'analyse des données cosmologiques on doit par conséquent se servir de la relation (exacte) intégrale pour les distances cosmologiques. Nous discutons le problème de dégénéréscence analytique entre les paramètres cosmologiques qu'introduit l'utilisation de cette relation. Puis, les résultats principaux de ce travail sont présentés. Ils concernent notamment la validité du paramétrage de l'équation d'état de l'énergie noire de Chevallier, Polarski, et Linder, et les éffets d'une évolution en redshift des magnitudes apparentes des Supernovae du type Ia."
"Cette thèse étudie la classification des théories conformes à 2d à l'aide de symétries quantiques de diagrammes. Les fonctions de partition d'un système conforme - l'invariante modulaire ou celles provenant de l'introduction de lignes de défauts - s'expriment en fonction d'un ensemble de coefficients qui forment des nimreps de certaines algèbres. Ces coefficients définissent les diverses structures d'une classe d'algèbres de Hopf, dites faibles, et peuvent être codés par un ensemble de graphes. Le chapitre 1 présente les connaissances actuelles sur ce sujet. Dans le chapitre 2 sont introduites l'algèbre de Hopf faible et ses structures, notamment l'algèbre des symétries quantiques d'Ocneanu, qui joue un rôle important dans l'étude des systèmes conformes à 2d. Nous analysons en détails ces structures pour le diagramme A3 du modèle affin su(2). Le chapitre 3 est dédié à la présentation d'une réalisation de l'algèbre des symétries quantiques d'Ocneanu, construite comme un quotient du carré tensoriel de l'algèbre d'un graphe G (de type ADE pour le modèle affin su(2)). Cette réalisation permet d'obtenir un algorithme simple permettant le calcul des fonctions de partition du modèle conforme associé. Notre construction se prête naturellement à une généralisation aux cas affins su(n), pour n > 2, pour lesquels peu de résultats étaient connus. Dans le chapitre 4, nous traitons explicitement tous les cas du type su(2) ainsi que trois exemples choisis du type su(3)."
"Il y a plus de trente ans, les physiciens ont montre un interet particulier pour les desintegrations semi-leptoniques du kaon, Kl4. Les mesures des observables liees a ces desintegrations fournissent des informations importantes sur les interactions faible et forte. Nous considerons ces desintegrations dans le cadre de la theorie desperturbations chirale basee sur un Lagrangien effectif dont les degres de liberte ne sont pas uniquement les mesons pseudo-scalaires mais aussi les leptons legers et le photon. Il existe de nombreuses experiences de basse energie et de haute precision, comme E865, Dafne, Na48/2 et KTeV, visant a mesurer ces modes avec une precision jamais atteinte auparavant, grace a une augmentation considerable de la statistique. L'exploitation de ces donnees de haute precision rend incontournable la necessite de controler d'une maniere quantitative certains effets indesirables dont les corrections radiatives. Nous avons evalue, analytiquement et numeriquement, ces effets a l'ordre d'une boucle et considere la brisure d'isospin due a la difference de masse des quarks legers."
"La convergence de la formule de Trotter en norme d'opérateur a été établie depuis 1990 avec différentes conditions pour des générateurs auto-adjoints dans un espace de Hilbert. Cette thèse étudie au contraire des semi-groupes holomorphes dont les générateurs ne sont pas auto-adjoints. Dans le premier ensemble de résultats, il s'agit d'estimations d'erreur en norme d'opérateur pour la formule de Trotter : je considère des perturbations accrétives dans un espace de Banach général, puis dans un espace de Hilbert. Dans la deuxième partie, j'étends certains résultats de convergence de la formule de Trotter au cas de générateurs m-sectoriels et pour la norme d'opérateur ou la norme de la trace. Enfin la dernière partie consiste en une généralisation de la théorie de Chernoff au cas de l'approximation des semi-groupes holomorphes en norme d'opérateur. Cette partie est fondée en particulier sur la notion nouvelle de contraction quasi-sectorielle, le résultat principal montre le lien entre la convergence généralisée (ou convergence au sens de la norme de la résolvante) des générateurs m-sectoriels et l'approximation en norme d'opérateur des semi-groupes holomorphes contractants par des puissances de contractions quasi-sectorielles."
"L'objectif de ce thèse est l'étude des systèmes dynamiques avec interaction à longue portée. La complexité de leur dynamique met en évidence des propriétés contre-intuitives et inattendues, comme l'existence d'états stationnaires hors-équilibre (QSS). Dans le QSS on peut observer des propriétés particulières: chaleur spécifique négative, inéquivalence des ensembles statistiques et phénomènes d'auto-organisation. Les théories des interactions LR ont été appliquées pour décrire la dynamique des systèmes auto-gravitants, de tourbillons bidimensionnels, de systèmes avec interactions onde-particule et des plasmas chargés. Mon travail s'est tout d'abord consacré à l'extension de la solution de Lynden-Bell pour le modèle HMF, en généralisant l'analyse à des conditions initiales de «water-bag"" à plusieurs niveaux, qui approchent des conditions initiales continues. En suite je me suis intéressé à la caractérisation formelle de la thermodynamique des QSS dans l'ensemble statistique canonique. En appliquant la théorie standard, il est possible de mesurer une chaleur spécifique ""cinétique'' négative. Cette propriété inattendue amène à la violation du second principe de la thermodynamique. Un tel résultat nous pousse à reconsidérer l'applicabilité de la théorie thermodynamique actuelle aux systèmes LR. En suite j'ai étudié, pour le modèle α-HMF, la persistance des caractéristiques typiques du régime LR, dans le limite dynamique à courte portée. Les résultats suggèrent une généralisation de la définition des systèmes LR. Le dernier chapitre est consacré à la caractérisation d'un nouveau modèle LR, extension naturelle du précédent α-HMF et d'intérêt potentiel applicatif."
"Cette thèse s'inscrit dans le domaine de la physique mésoscopique. Plus particulièrement, on s'est intéressé aux effets de taille finie et aux effets de l'écrantage causé par une pointe de STM dans un fil quantique, ceci à travers les comportements du courant, du bruit non-symétrisé et de la conductance. Ces études reposent sur, d'une part, la théorie des liquides de Luttinger qui permet de décrire les systèmes 1D d'électrons fortement corrélés et d'autre part, le formalisme Keldysh permettant de considérer des situations hors équilibre. Le bruit présente un comportement non poissonien, résultant des effets de taille finie. A travers le transport photo-assisté, il est également montré que ces effets masquent ceux des interactions coulombiennes. En considérant la proximité entre la pointe de STM, qui sert de sonde comme d'injecteur d'électrons, et un fil quantique, des effets d'écrantage apparaissent. Il est montré qu'ils jouent un rôle similaire à ceux des interactions coulombiennes."
"Cette étude propose une application innovante de deux concepts très étudiés par la communauté mathématique, le fibré des k-repères et la connexion de Cartan. D'une part, l'utilisation d'une connexion de Cartan particulière sur le fibré des 2-repères nous permet de proposer une généralisation de la notion de dérivée de Schwarz en dimension arbitraire, pour les difféomorphismes projectifs et conformes. D'autre part, nous avons pu élaborer une structure de BRS permettant de reproduire infinitésimalement l'action des difféomorphismes sur des champs de jauge à un terme de courbure près. Ainsi, la notion de connexion de Cartan sur le fibré des 2-repères a permis de résoudre un problème ouvert, originellement formulé par A.M. Polyakov en 1990 qui obtient formellement l'action de difféomorphismes (symétrie de l'espace-temps) à partir d'une transformation de jauge (symétrie interne). Les symétries d'espace-temps et les symétries internes peuvent ainsi être exprimées dans un formalisme similaire."
"Dans cette thèse nous avons étudié certaines questions mathématiques associées au calcul de l'action spectrale de Chamseddine--Connes sur des exemples fondamentaux de triplets spectraux non commutatifs, tels que le tore non commutatif et la 3-sphère quantique SUq(2). Nous avons montré en particulier qu'une condition diophantienne sur la matrice de déformation du tore est cruciale pour obtenir l'action spectrale en tenant compte de la structure réelle. Nous avons aussi étudié la question de l'existence de tadpoles (termes linéaires par rapport au potentiel de jauge de la fluctuation de la métrique dans l'action spectrale) dans le cas de géométries riemanniennes commutatives, et la construction d'un calcul pseudodifférentiel global permettant une généralisation du produit de Weyl--Moyal sur un espace de Schwartz de sections rapidement décroissantes sur un fibré cotangent d'une variété avec linéarisation."
"Le point central de cette thèse est la physique du bruit: la transformée de Fourier de la function de correlation temporelle courant-courant. Nous examinons des situations dans lesquelles le bruit généré par un circuit mésoscopique donné affecte le comportement d'un autre circuit mésoscopique. Dans une première partie, la source de bruit est inconnue, et le circuit mésoscopique qui lui est couplé de manière capacitive se comporte comme un détecteur de bruit à haute fréquence. Dans notre cas, le détecteur est constitué d'une jonction métal normal-supraconducteur, où le transport électronique est du au transfert de quasiparticules, ou, de manière plus intéressante, est du à la réflexion d'Andreev. La théorie du blocage de Coulomb dynamique est utilisée pour calculer le courant continu qui passe dans le circuit de détection, procurant ainsi une information sur le bruit à haute fréquence. Dans la deuxième partie de cette thèse, la source de bruit est connue : elle provient d'une barre de Hall avec un contact ponctuel, dont les caractéristiques de courant-tension et de bruit sont bien établies dans le régime de l'effet Hall quantique fractionnaire. Un point quantique connecté à des bornes source et drain, qui est placé au voisinage du contact ponctuel, acquière une largeur de raie finie lorsque le courant fluctue, et se comporte comme un détecteur de bruit de charge. Nous calculons le taux de déphasage du point quantique dans le régime de faible et de fort rétrodiffusion, tout en décrivant l'effet de l'écrantage faible ou fort de l'interaction Coulombienne entre la barre de Hall et le point quantique."
"La majeure partie de cette thèse concerne l'étude de la susceptibilité diamagnétique en champ magnétique nul d'un gaz d'électrons de Bloch à température et densité fixées dans la limite des faibles températures. Pour les électrons libres (i.e. en l'absence de potentiel périodique), la susceptibilité diamagnétique a été calculée par L. Landau en 1930; le résultat est connu sous le nom de formule de Landau. Quant au cas des électrons de Bloch, E.R. Peierls montra en 1933 que dans l'approximation des électrons fortement liés, la formule pour la susceptibilité diamagnétique reste la même en remplaçant la masse de l'électron par sa ''masse effective''; ce résultat est connu sous le nom de formule de Landau-Peierls. Depuis, de nombreuses tentatives pour clarifier les hypothèses de validité de la formule de Landau-Peierls ont vu le jour. Le résultat principal de cette thèse établit rigoureusement qu'à température nulle, lorsque la densité d'électrons tend vers zéro, la contribution dominante à la susceptibilité diamagnétique est donnée par la formule de Landau-Peierls avec la masse effective de la plus petite bande d'énergie de Bloch."
"Nous étudions les interfaces de quatre modèles de spins sur réseau: le modèle d'Ising à basse température, le modèle de Potts à la température critique, un modèle à symétrie continue et son approximation d'horloge. Pour chacun de ces modèles, nous imposons des conditions au bord spécifiques qui assurent l'existence d'une interface ; les mesures de Gibbs associées à de telles conditions au bord satisfont alors de puissantes inégalités de corrélation. Ces inégalités nous permettent de montrer que les interfaces considérées sont rigides, au sens où ce sont des hyperplans légèrement déformés par des aspérités locales. Cette méthode est une version restreinte de la méthode de positivité par réflexion, l'une des directions de réflexion étant prohibée par les conditions au bord choisies. Pour Ising et Potts, notre méthode simplifie considérablement les démonstrations historiques, puisque ni la théorie de Pirogov-Sinai, ni les développements en amas ne sont nécessaires à son application. Par ailleurs, la théorie-PS n'est directement envisageable ni pour le modèle continu ni son approximation car leurs états fondamentaux sont infiniment dégénérés; notre méthode est donc une réelle alternative à ces techniques."
"Nous montrons qu'un contrôle du chaos Hamiltonien est possible en utilisant des petites perturbations appropriées dont la forme peut être explicitement calculée. En particulier, il est possible de reduire la diffusion chaotique d'un système Hamiltonien avec 1.5 degrés de liberté qui modélise la diffusion de particules chargées dans un champ électrique turbulent à travers le champ magnétique de confinement dans les dispositifs de fusion thermonucléaire contrôlée. Bien qu'encore loin des applications expérimentales, ce résultat suggère qu'une stratégie pour contrôler le transport turbulent dans les plasmas magnétisés est envisageable. La robustesse du contrôle est étudiée en termes de changements par rapport à la valeur optimale du contrôle (existence d'une région significative d'efficacité autour du contrôle optimal)."
Nous présentons une application d'une méthode du contrôle du chaos pour les systèmes Hamiltoniens à un modèle d'advection chaotique en hydrodynamique. Cette méthode permet de créer des barrières à la diffusion chaotique des particules en ajoutant un terme de contrôle petit et simple à la fonction de courant du système.
"Nous nous intéressons dans un premier temps à l'étude des propriétés spectrale de localisation dynamique pour des opérateurs de Schrödinger ainsi qu'a leurs classifications. Nous introduirons trois classes de propriétés équivalentes en cherchant à établir le lien entre elles d'une façon optimale et illustrée par des contre-exemples. Certaines de ces propriétés s'avèrent jouer un rôle crucial dans l'étude mathématique de plusieurs phénomènes issus de la physique, notamment la quantifi cation de la conductance de Hall et l'apparition des plateaux dûs aux états localisés. Nous nous intéressons ainsi dans la seconde partie, aux conductances de Hall et de bord pour des modèles désordonnés continus et en présence d'un mur électrique aussi bien que magnétique. Nous expliquons comment les murs entrent en jeu pour pouvoir définir la conductance de bord, en tenant compte de la contribution des états localisés et la régularisation que les systèmes désordonnés requièrent. Nous établissons l'égalité de ces deux conductances directement et non par quantification séparée."
"Nous interrogeons la pertinence du concept de paradigme de Kuhn, ainsi que de son schéma des révolutions scientifiques, en les confrontant à l'histoire des fluides impondérables au XIXe siècle au sein de la thermodynamique et de l'électromagnétisme."
"Tout au long de la thèse, nous discuterons, améliorerons et fournirons un cadre conceptuel dans lequel des méthodes basées sur les propriétés de récurrence de dynamiques chaotiques peuvent être comprises. Nous fournirons également de nouvelles méthodes basées sur l'EVT pour calculer les quantités d'intérêt et présenteronsr de nouveaux indicateurs utiles associés à la dynamique. Nos résultats auront une rigueur mathématique totale, même si l'accent sera mis sur les applications physiques et les calculs numériques, car l'utilisation de telles méthodes se développe rapidement. Nous commencerons par un chapitre introductif à la théorie dynamique des événements extrêmes, dans lequel nous décrirons les principaux résultats de la théorie qui seront utilisés tout au long de la thèse. Après un petit chapitre dans lequel nous introduisons certains objets caractéristiques de la mesure invariante du système, à savoir les dimensions locales et les dimensions généralisées, nous consacrons les chapitres suivants à l'utilisation de EVT pour calculer de telles quantités dimensionnelles. L'une de ces méthodes définit naturellement un nouvel indicateur global sur les propriétés hyperboliques du système. Dans ces chapitres, nous présenterons plusieurs applications numériques des méthodes, à la fois dans des systèmes réels et idéalisés, et étudierons l'influence de différents types de bruit sur ces indicateurs. Nous examinerons ensuite une question d'importance physique liée à l'EVT : les statistiques de visites dans certains sous-ensembles cibles spécifiques de l'espace de phase, en particulier pour les systèmes partiellement aléatoires et bruyants. Les résultats présentés dans cette section sont principalement numériques et hypothétiques, mais révèlent un comportement universel des statistiques de visites. Le huitième chapitre établit la connexion entre plusieurs quantités locales associées à la dynamique et calculées à l'aide d'une quantité finie de données (dimensions locales, temps de frappe, temps de retour) et les dimensions généralisées du système, calculables par les méthodes EVT. Ces relations, énoncées dans le langage de la théorie des grandes déviations (que nous exposerons brièvement), ont de profondes implications physiques et constituent un cadre conceptuel dans lequel la distribution de ces quantités locales calculées peut être comprise. Nous tirons ensuite parti de ces connexions pour concevoir d'autres méthodes permettant de calculer les dimensions généralisées d'un système. Enfin, dans la dernière partie de la thèse, qui est plus expérimentale, nous étendons la théorie dynamique des événements extrêmes à des observables"
"On considère un système constitué de plusieurs atomes où les noyaux sont supposés fixes et ponctuels. Les particules interagissent via le potentiel de Coulomb et les électrons ont une énergie cinétique pseudo-relativiste donnée par (p2+m2)1/2-m. On démontre la loi de van der Waals-London qui dit que l'énergie d'interaction entre atomes neutres décroit comme |D|-6 où |D| est la distance entre atomes. Nous calculons rigoureusement tous les termes de l'énergie de liaison jusqu'à l'ordre |D|-9 avec un terme d'erreur en O(|D|-10). Comme étape intermédiaire, nous établissons la décroissance exponentielle des fonctions propres de l'opérateur de Schrödinger à plusieurs particules avec les symétries imposées par le principe de Pauli, et des estimations sur le terme d'erreur de localisation. Nous montrons de plus la loi de van der Waals-London pour l'opérateur de Dirac projeté connu sous le nom d'opérateur de Brown et Ravenhall. Dans ce dernier cas on obtient un terme d'erreur en O(|D|-7)."
"Dans cette thèse, nous étudions les propriétés spectrales des guides d'onde quantiques tridimensionnels (les tubes) perturbés. Nous considérons, principalement deux types différents de perturbation : Dans le premier type, il s’agit de la perturbation d’une déformation géométrique. Plus précisément, nous étudions l’opérateur de Laplace de Dirichlet défini dans un tube déformé à l’aide d’une torsion constante perturbée localement par une fonction de même signe (torsion répulsive).Le deuxième type de perturbation consiste à changer localement les conditions aux bords imposées sur la frontière du guide d’onde. En effet, il s’agit de l’étude du Laplacien avec des conditions aux bords mixtes.Nous imposons des conditions aux bords de Dirichlet par tout sur la frontière du guide d’onde, sauf sur une partie bornée où nous considérons des conditions aux bords de Neumann. D’une part, nous examinons les tubes droits (sans déformations géométriques) dans le but de comprendre l’effet de la perturbation des conditions aux bords. D’autre part, nous étudions les tubes torsadés afin d’établir une comparaison entre les effets opposés des deux perturbations (géométrique et des conditions aux bords)."
"On s'intéresse à l'intrication tri-partite pour des fermions. Tandis que les états GHZ ou W consistent en l'intrication des particules 3 par 3, on considère ici l'intrication 2 à 2 de 3 fermions de spin 1/2, du type ab+bc+ca. Avant interaction avec l'appareil de Stern-Gerlach, les 3 qu-bits sont discernables; à la sortie ce sont des particules indiscernables, dont la fonction d'onde anti-symétrique est de la forme det(b-a,c-a) (déterminant affine). Plus généralement, un système de 2S+2 fermions de spin S intriqués peut être representé par la fonction d'onde antisymétrique det(a1-a0,a2-a_0,..., ad-a0), avec d=2S+1. On étudie aussi les propriétés des déterminants de Slater affines, comme la valeur moyenne d'une observable, ou les matrices densité réduites."
"Notre connaissance actuelle de l'Univers repose sur l'existence de quatre interactions fondamentales, qui sont la gravitation, l'électromagnétisme, l'interaction forte et l'interaction faible. Elles forment la base conceptuelle de la physique moderne depuis un demi-siècle. Je m'intéresse dans ma thèse à l'aspect classique des théories physiques sous-jacentes, appelées « théories de jauge ». Ma démarche est celle d'un physicien mathématicien. Dans un premier temps, elle consiste à étudier les théories de jauge dans leur formulation mathématique, afin de mettre en lumière certaines structures géométriques et algébriques sous-jacentes. Dans un second temps, on propose de nouveaux cadres mathématiques possibles pour formuler des théories de jauge. On a exploré pour cela la géométrie conforme et les théories de jauge de la gravitation conforme associées, pour lesquelles le groupe de symétrie est élargi, passant du groupe de Lorentz au groupe conforme. Le tout est formulé dans le langage de la géométrie de Cartan. En appliquant la méthode de l'habillage, qui consiste à réduire la symétrie de jauge d'une théorie par un simple changement de variable, on retrouve les objets habituellement définis dans une telle géométrie, comme les Tractors et les Twistors, avec en prime une meilleure compréhension de leur nature géométrique. On présente également le cadre des algébroïdes de Lie transitifs, et différentes façons de formuler des théories de jauge en son sein. Premièrement, on développe une notion de tenseur sur les algébroïdes de Lie, le choix d'une base locale adaptée étant fondamentale dans la poursuite des calculs. On parvient, reprenant dans une formulation plus claire un travail de N. Boroojerdian, à décrire dans un unique lagrangien la relativité générale avec constante cosmologique ainsi que les théories de Yang-Mills pour les autres interactions. Le travail de C. Fournel est également présenté, dans lequel la notion de connexion généralisée sur des algébroïdes de Lie permet d'écrire un lagrangien contenant à la fois la théorie de Yang-Mills et un terme de Higgs plongé dans un potentiel quartique. Finalement, nous présentons un travail récent consistant à combiner géométrie de Cartan et algébroïdes de Lie transitifs. Pour cela, on écrit les suites d'Atiyah correspondant aux deux fibrés principaux sous-jacents à une géométrie de Cartan, puis nous donnons la définition d'une connexion de Cartan dans ce langage. Nous démontrons l'équivalence de cette définition avec la définition usuelle sur les fibrés principaux. Nous comparons également notre approche avec celle, récente également, de M. Crampin et D. Saunders."
"Jean-Pierre Luminet retrace l'histoire des relations entre cosmologie et topologie au XXe siècle. Elle commence dès 1900 avec Karl Schwarzschild, qui fait l'hypothèse d'un univers multiconnexe. Le modèle d'Einstein-de Sitter d'un espace euclidien simplement connexe dominera la communauté des cosmologistes jusqu'aux années 1990, et ce malgré les accomplissements du côté des topologistes dans la classification des espaces à dimension trois. A partir de 1995, on assiste à un retour de la topologie cosmique, assise sur des observations astronomiques nouvelles, et à la mise en place de méthodes expérimentales permettant de discriminer les différents modèles. On s'intéresse notamment aux résultats de la cristallographie cosmique et à l'observation du fond diffus cosmologique."
"L'objectif de cette thèse est d'étudier les déformations isospectrales du point de vue de la géométrie non commutative développée par Alain Connes. Cette classe d'espaces quantiques constitue une généralisation en espace courbe des plans de Moyal et des tores non commutatifs. Dans un premier temps, on s'intéresse à la construction de triplets spectraux sans unité, pour lesquels on propose une modification des axiomes. On vérifie ensuite que les plans de Moyal s'inscrivent dans ce cadre axiomatique, et on donne les points clefs de l'élaboration de triplets spectraux sans unité à partir des déformations isospectrales non compactes génériques. Pour se faire, de nombreux outils d'analyse sur les variétés Riemanniennes non compactes sont développés. Au moyen d'un calcul de traces de Dixmier, on montre que leurs dimensions spectrale et classique coïncident. Dans un deuxième temps, on étudie certains aspects de la théorie quantique des champs sur les déformations isospectrales courbes. Une attention particulière est portée aux phénomènes de mélange des divergences ultraviolettes et infrarouges. On montre son caractère intrinsèque sur tous ces espaces quantiques (compacts ou non, déformations périodiques ou non) et on étudie ses conséquences sur la renormalisabilité. En particulier, le comportement des fonctions de Green des secteurs planaire et non planaire est compris en termes de contributions du noyau de la chaleur hors et sur sa diagonale. On observe aussi de nouvelles ou plus fines manifestations du mélange UV/IR, en relation avec les propriétés géométriques de ces espaces quantiques et arithmétiques des paramètres de déformation."
"Nous nous intéressons à la convergence vers sa moyenne spatiale ergodique de la moyenne temporelle d'une observable d'un flow hamiltonien à un degré et demi de liberté avec espace des phases mixte. L'analyse est faite au travers de l'évolution de la distribution des moyennes en temps fini d'un ensemble de conditions initiales sur la même composante ergodique. Un exposant caractérisant la vitesse de convergence est défini. Les résultats indiquent que pour le système considéré la convergence évolue en $t^{\alpha}$, avec $\alpha=0.45$ pour alors qu'elle évolue en $t^{1/2}$ lorsque la dynamique est globalement chaotique dans l'espace des phases. De même une loi $\alpha=1-\beta/2$ reliant cet exposant $\alpha$ à l'exposant caractéristique du deuxième moment associé aux propriétés de transport $\beta$ est proposée et est vérifiée pour les cas considérés."
On donne des conditions suffisantes pour qu'une fonction sur l'espace des phases corresponde à un opérateur à trace dans le formalisme de la convolution gauche (correspondance de Weyl en Mécanique Quantique).
"Cette thèse est consacrée à la détermination de coefficients inconnus apparaissant dans l’équation linéaire de Boltzmann ou l’équation de la chaleur. Son objectif principal est d’établir des résultats d’unicité et de stabilité dans l’identification de ces coefficients à partir de données ad-hoc. La première partie de la thèse est consacrée à l’étude des problèmes inverses associés à l’équation linéaire de Boltzmann (équation de transport). Le problème inverse considéré concerne l’identification des coefficients de diffusion et d’absorption à partir de la mesure latérale du flux sortant de la solution. La deuxième partie de la thèse est consacrée à la résolution d’un problème inverse pour l’équation de la chaleur. Nous étudions la stabilité de la détermination du coefficient d’ordre zéro à partir de données de type Neumann complètes ou partielles, mesurées sur un intervalle de temps arbitraire."
"La description de la forme de notre espace physique à diverses échelles de grandeur (en taille ou en énergie) met en jeu une riche variété de modèles géométriques, chacun dépendant de la théorie physique sous-jacente. La description des distorsions spatio-temporelles engendrées par les champs gravitationnels et quantiques est l’un des grands défis de la physique fondamentale du XXIe siècle. Jean-Pierre Luminet parlera des représentations spatiales décrivant la forme de l’espace engendrée par les trous noirs, puis la forme globale de notre univers dans le cadre de la topologie cosmique, pour finir avec quelques indications sur la structure possible de l’espace-temps à l’échelle quantique. Jean-Pierre Luminet est Directeur de recherches au CNRS, Laboratoire d’astrophysique de Marseille (LAM) et Observatoire de Paris (LUTH). Cette manifestation est soutenue par la Mairie de Grenoble et la délégation Alpes du CNRS."
"Ce rapport synthétise le travail de recherche mis en oeuvre durant la cinquième Semaine d'Etude Maths-Entreprises à l'Ecole des Mines de Nancy. Le sujet a été proposé par le consortium GOCAD: comment reconstituer efficacement le sous-sol terrestre à partir de données discrètes éparses ? Un état de l'art est d'abord effectué sur les différentes méthodes existantes : cokrigeage statistique (Calcagno et al., 2008), discrétisation numérique (Caumon et al., 2013) et modélisation physique entre deux horizons géologiques (Hjelle and Petersen, 2011). Ensuite, nous avons tenté d'adapter l'approche (Hjelle and Petersen, 2011) à notre problématique. Il s'agit de représenter chaque couche géologique par les points d'annulation d'une fonction dont l'évolution est gérée par une loi qui contient les informations connues et permettra la reconstitution in fine du sous-sol. Finalement, on effectue la résolution numérique de l'équation de Hamilton-Jacobi associée à cette loi de propagation, s'aidant de (Osher and Fedkiw, 2003). Par souci de simplicité et surtout par manque de temps, le modèle sera résolu numériquement en 2-D et sans failles."
"Une nouvelle approche pour la théorie des représentations du groupe symétrique a été développée par Okounkov et Vershik ; elle fournit un éclairage différent sur le sujet par rapport aux approches ""traditionnelles"". Par ailleurs, cette méthode vise à établir un cadre reproductible pour étudier les représentations d'autres chaînes de groupes et d'algèbres, telles que les autres séries de groupes de Coxeter finis, ou les tours d'algèbres locales et stationnaires. Dans ce mémoire, nous rappelons la présentation traditionnelle du groupe symétrique, et résumons le fond de la nouvelle approche pour sa théorie des représentations. Ensuite, nous rappelons la définition de tour d'algèbres locale et stationnaire, de diagramme de Bratteli et de ce qui est appelée base de Gelfand-Tsetlin et algèbre de Gelfand-Tsetlin dans la nouvelle approche. Nous nous sommes intéressés à la possibilité de généraliser cette approche pour la chaîne des groupes alternés. Dans ce but, nous étudions les présentations usuelles du groupe alterné, et en donnons une nouvelle qui munit la chaîne des groupes alternés d'une structure de tour locale et stationnaire ; dans chaque cas, nous réalisons l'algorithme de Coxeter-Todd et donnons une forme normale pour les éléments du groupe. Nous entamons également la recherche d'analogues des éléments de Jucys-Murphy pour les groupes alternés."
"Nous avons étudié la classification des matrices R en dimension 2 en utilisant une relation d'équivalence provenant de la théorie des groupes quantiques, l'équivalence par un twist. Cette classification était déjà connue pour les matrices de type GL(2), et l'objet de ce travail a été de l'étendre aux matrices de type GL(1|1). Le cadre de travail est donc les groupes quantiques et les superalgèbres de Lie. Nous avons terminé cette classification au niveau numérique des matrices et obtenu également un twist universel au niveau des supergroupes quantiques."
"Une approche inductive est développée pour la théorie des représentations de la chaîne des algèbres de Hecke cyclotomiques de type G(m,1,n). Cette approche repose sur l'étude du spectre d'une famille commutative maximale, formée par les analogues des éléments de Jucys-Murphy. Les représentations irréductibles, paramétrées par les multi-partitions, sont construites avec l'aide d'une nouvelle algèbre associative, dont l'espace vectoriel sous-jacent est le produit tensoriel de l'algèbre de Hecke cyclotomique avec l'algèbre associative libre engendrée par les multi-tableaux standards. L'analogue de cette approche est présentée pour la limite classique, c'est-à-dire la chaîne des groupes de réflexions complexes de type G(m,1,n). Dans une seconde partie, une base des algèbres de Hecke cyclotomiques est donnée et la platitude de la déformation est montrée sans utiliser la théorie des représentations. Ces résultats sont généralisés aux algèbres de Hecke affines de type A. Ensuite, une procédure de fusion est présentée pour les groupes de réflexions complexes et les algèbres de Hecke cyclotomiques de type G(m,1,n). Dans les deux cas, un ensemble complet d'idempotents primitifs orthogonaux est obtenu par évaluation consécutive d'une fonction rationnelle. Dans une troisième partie, une nouvelle présentation est obtenue pour les sous-groupes alternés de tous les groupes de Coxeter. Les générateurs sont reliés aux arêtes orientées du graphe de Coxeter. Cette présentation est ensuite étendue, pour tous les types, aux extensions spinorielles des groupes alternés, aux algèbres de Hecke alternées et aux sous-groupes alternés des groupes de tresses."
"Dans cette thèse d'habilitation, nous considérons plusieurs aspects du transport à travers une molécule unique, connectée à des bornes de métal normal ou à des bornes supraconductrices. L'emphase à été mise sur la détection des signatures les plus marquantes du transport cohérent à travers les molécules, ainsi que sur la compréhension des problèmes de corrélations (problème à N corps) sur la dynamique de ces systèmes, provenant des degrés de liberté internes (vibrations, spin,...) du conducteur et affectant le passage du courant. En ce qui concerne le transport dans le régime normal à travers une molécule qui vibre (un nanotube de carbone suspendu par ses extrémités), nous avons procédé à une étude détaillée de la conductance différentielle négative (CDN) qui est observée dans ces dispositifs. En supposant des contacts tunnel, tel que les électrons qui s'échappent dans les bornes effectivement perdent leur cohérence de phase (c'est-à-dire à haute température), nous avons dérivé les équations cinétiques dans lesquelles la nature quantique de l'interaction électron-phonon au sein du point quantique moléculaire est prise en compte sans approximations (formation de polaron sur le point quantique moléculaire). Le fait que la conductance différentielle soit positive ou négative dépend de la position du niveau polaronique et de l'occupation des pics satellites associés au nombre d'occupation des phonons, qui sont compris entre la tension de source et de drain des électrodes. La CDN apparaît lorsque deux de ces pics satellites entrent en compétition dans le transport, et constitue une signature des effets hors équilibres associés au vibrations de la molécule. Nous avons clairement montré que pour des couplages tunnels asymétriques (situation qui correspond à la géométrie des expériences sur le domaine), on observe un CDN pour un vaste domaine de paramètres. Nous avons également exploré les effets de navette électronique, ou le déplacement de la molécule entre en compte dans l'Hamiltonien tunnel, qui peuvent être détectés en regardant l'asymétrie des courbes courant tension. Bien que le mécanisme de navette tend à renforcer la CDN, il n'est toutefois pas suffisant pour y donner lieu sans hypothèses sur la valeur relative des couplages tunnels. Nous avons également étudié le transport dans le régime normal à travers un point quantique moléculaire dans le cas d'un couplage fort aux contacts, mais loin du régime Kondo. En utilisant l'approche hors équilibre des fonctions de Green dans la représentation du polaron, nous sommes allés au delà du régime perturbatif pour calculer la caractéristique courant tension dans le régime de couplage électron-phonon intermédiaire. Nous avons montré qu'en accroissant le couplage tunnel au contacts, les corrélations associées au nuage de polaron deviennent très importantes à haute température, et donnent lieu à une réduction dramatique de l'élargissement des pics la densité d'états de la molécule. Nous proposons une détection de ces phénomènes par la mesure de la conductance différentielle, tout en variant la température locale de la molécule (nanotube de carbone). On note qu' en présence d'un environnement dissipatif les pic satellites dus aux phonons devraient acquérir un élargissement additionnel. L'inclusion des effets d'amortissement des modes phononiques constituerait une extension de ce travail. Dans cette thèse, nous avons également abordé le problème du transport cohérent en présence de phonons, dans un système moléculaire connecté à des contacts supraconducteurs. Nous avons calculé le courant DC (partie du courant stationnaire) pour toutes les valeurs de la tension à l'aide de l'approche des fonctions de Green Keldysh, pour une fréquence de vibration arbitraire, mais dans le régime du couplage faible électron-phonon. Nos principaux résultats sont les suivants : i) dans le régime sous le gap $eV<\Delta$, les processus de réflexions multiples d'Andreev (MAR) sont accompagnés de processus d'émission/absorption de phonons et donne lieu à une structure très riche près des valeurs de tension ou le nombre de réflexions d'Andreev changent d'une unité (ces tensions sont appelées les « MAR onsets »). On observe alors un effet pair impair ou le courant est augmenté/diminué suivant la transition de « MAR onset » (entre pair/impair et vice versa). Ces phénomènes trouvent un interprétation physique en comparant avec la théorie de la diffusion de Buttiker-Landauer, adaptée au contacts supraconducteurs, une théorie connue sous le nom d' « échelle de MAR ». A l'équilibre $V=0$, nous avons obtenu des résultats analytiques pour le courant Josephson dans la limite adiabatique ou la fréquence de vibration est faible comparée au gap supraconducteur, qui est interprétée en terme des états liés d'Andreev avec une transparence aux contacts renormalisée par les phonons. Pour le futur, une extension de cette théorie au calcul du bruit (fonction de corrélation courant-courant) est envisagée. Le bruit peut en effet procurer une information supplémentaire sur la charge transmise à travers la jonction, et il serait intéressant d'étudier l'effet des phonons dans ce cadre. Nous avons également considéré le cas des contacts supraconducteurs, mais cette fois pour les interactions fortes, et uniquement à l'équilibre ou le courant Josephson dépend de la différence de phase entre les deux supraconducteurs. Cette fois on s'intéresse à un diagnostique sur l'état des phonons sur le point quantique moléculaire. Nous trouvons que pour le régime de faible couplage tunnel, des états non-classiques de type « chat de Schrodinger » (une superposition d'états cohérents opposés) sont associés aux états du courant et donc aux liés d'Andreev dans la jonction. Ces états non classiques peuvent être explicités en procédant à une mesure projective du courant. Pour des contacts transparents, nous avons montré que l'effet Josephson génère des fluctuations de phonon cohérentes, et induit des états quantiques « comprimés » de phonon, analogues aux états « comprimés » de photons en optique quantique : l'impulsion canoniquement conjuguée à la distorsion de la molécule possède des fluctuations inférieures a la valeur minimale habituelle de fluctuations du point zéro. La compression d'états de phonons s'observe pour une grande plage de paramètres : elle est contrôlée par la différence de phase et devient maximale près de la transition de polaron. La détection expérimentale de tels états comprimés pourrait être effectuée en nanoélectronique à l'aide de nanotubes suspendus. Il faudrait recourir à un diagnostique optique tel que l'effet Raman résonant, pour démonter l'existence de ces états de phonons non-classiques. Une autre manière d'explorer les phénomènes cohérents dans le cadre du transport Josephson est d'étudier les situations ou les degrés de liberté de spin des électrons du point quantique moléculaire et des électrodes sont importants. Nous avons donc calculé le courant Josephson à travers un point quantique moléculaire doté d'un grand spin, qui possède une interaction d'échange avec l'électron du point quantique. Ce couplage d'échange peut donner lieu à une transition à l'état pi de la jonction (relation courant phase opposée par rapport a une jonction normale, de phase 0). La contribution relative du courant provenant des états liés d'Andreev et du continuum détermine si la jonction est dans l'état 0 ou l'état pi. Un débouché possible de cette étude est d'étudier les effets de décohérence, et de rétroaction du supercourant sur la dynamique du spin moléculaire. Dans un autre contexte, les effets de spin associé au courant Josephson ont été étudiés pour un point quantique possédant plusieurs nivaux, et sujet à l'interaction spin orbite Rashba et Dresselhaus. Pour un point quantique ne possédant qu'un seul niveau les effet du couplage spin orbite sont inexistants en l'absence d'un champ magnétique externe. En présence de ce dernier, le courant de ce point quantique possède des oscillations de type Datta Das en fonction du paramètre de couplage spin orbite multiplié par la longueur du point quantique. Ces oscillations ont une amplitude de quelques dixièmes du courant nominal Josephson, et pourraient donc être observées expérimentalement. Le cas d'un point quantique possédant plusieurs niveaux est plus intéressant. Pour un point quantique à deux niveaux en particulier, Le courant possède une dépendance sur le couplage spin orbite même en l'absence de champ magnétique. Le supercourant possède des maxima et des minima marqués pour certaines valeurs de ce couplage. Leur observation constituerait une première évidence du fonctionnement d'un transistor à effet spin orbite dont les bornes sont supraconductrices. Dans le futur il serait intéressant d'inclure les interactions Coulombiennes sur le dot. Nous avons développé en parallèle une théorie pour modéliser un bit quantique basé sur les états liés d'Andreev : un dispositif constitué d'un SQUID (dispositif d'interférométrie supraconducteur) et d'un contact ponctuel supraconducteur, combinant donc un circuit macroscopique et microscopique. Le contact ponctuel – qui implique une transparence elevée entre les contacts, peut être vu comme un point quantique qui contient deux états fermioniques localisés, à leur tout couplés à la dynamique de la phase supraconductrice (un mode bosonique local). Nous avons étudié la décohérence de ce bit quantique d'Andreev, associée à un couplage des électrons des contacts avec des modes de phonons acoustiques. La nature fermionique des nivaux d'Andreev n'affecte pas le pilotage du bit quantique, mais elle joue un rôle important en ce qui concerne sa décohérence : la relaxation et le déphasage induit suivent une loi de puissance dans le temps plutôt qu'une exponentielle. De plus, nous avons trouvé que le taux de transition entre les nivaux du bit quantique, induit par les transition phononiques est réduit de manière considérable comparé au taux de transition électron-phonon dans les contacts : l'étalement de la fonction d'onde de ses niveaux dans les contacts réduit l'espace de phase disponible pour ces transitions assistées par les phonons. Dans une étude séparée, nous nous sommes intéressé à la mesure du bruit à haute fréquence ainsi qu'a celle des moments supérieurs du courant, à l'aide d'un circuit résonant en présence de dissipation. Le circuit résonant est couplé à un circuit mésoscopique placé dans le régime cohérent. L'information sur les moments supérieurs du courant est codée dans les histogrammes de la charge du condensateur du circuit résonant. La dissipation est prise en compte par le modèle de Caldeira Leggett, et il est essentielle de l'inclure pour obtenir des fluctuations de charge (donc un bruit mesuré) finies. Nous identifions également quelle Combinaison des corrélateurs de courant entrent dans l'expression du troisième moment mesuré. Ce dernier fait appel à la même susceptibilité généralisée que pour le bruit mesuré, mais elle ne diverge pas dans la limite d'un circuit non dissipatif. Les prédictions sur la mesure de ces quantités sont testées pour le cas du bruit émanant d'un contact ponctuel."
"Dans cet article, nous donnons une nouvelle preuve de l’universalité du polynôme de Tutte pour les matroïdes. Cette preuve utilise des caractères appropriés de l’algèbre de Hopf des matroïdes introduite par Schmitt (1994). Nous montrons que ces caractères algèbre de Hopf sont des solutions d'équations différentielles du même type que les équations différentielles utilisées pour décrire le flux du groupe de renormalisation en théorie quantique de champs. Cette approche nous permet aussi de démontrer, d’une manière différente, une formule de convolution du polynôme de Tutte des matroïdes, formule publiée par Kook, Reiner et Stanton (1999). Cette contribution FPSAC est un résumé étendu."
"Ce livre est le fruit de plusieurs années d'expériences d'enseignements à l'université d'Aix Marseille en première année de licence des cycles scientifiques (physique, chimie, mathématiques) et du cycle préparatoire aux écoles d'ingénieurs POLYTECH. Il s'adresse à la fois aux étudiants voulant réaliser des études en sciences fondamentales et à ceux désirant s'orienter vers les métiers de l'ingénieur. La transition secondaire-supérieur pose des problèmes de plus en plus difficiles que ce livre tente d'amoindrir. Il a été rédigé dans le cadre de la mise en place d'une nouvelle méthode pédagogique reposant sur les concepts d' ""apprentissage par problèmes"" et d' ""apprentissage par les pairs"". En résumé, cet ouvrage expose les aspects les plus fondamentaux d'un cours de mécanique et insiste sur les techniques de résolution des problèmes de physique. Afin d'initier les nouveaux étudiants à ces différentes méthodes une grande partie du cours est rédigée sous la forme ""d'exercices de cours"" où la résolution des questions posées est grandement détaillée. L'objectif principal de ce manuel est de remplacer le cours magistral réalisé traditionnellement par les enseignants, qui avec le temps est devenu un des meilleurs anesthésiants pour étudiants et une des sources de la dépression des professeurs... Ce livre veut développer la plus grande autonomie possible des étudiants dans l'analyse et la résolution des problèmes de physique."
Liste de publications et bref exposé des travaux et activités scientifiques de l'auteur.
"Ces dernières années, la puissance croissante des ordinateurs a permis a` la fois de rassembler une quantité sans précédent de données décrivant la société moderne et d’envisager des outils numériques capables de s’attaquer a` l’analyse et la modélisation les processus dynamiques qui se déroulent dans cette réalité complexe. Dans cette perspective, l’approche quantitative de la physique est un des catalyseurs de la croissance de nouveaux domaines interdisciplinaires visant a` la compréhension des systèmes complexes techno-sociaux. Dans cette thèse, nous présentons dans cette thèse un cadre théorique et numérique pour simuler des épidémies de maladies infectieuses émergentes dans des contextes réalistes. Dans ce but, nous utilisons le rôle crucial de la mobilité des agents dans la diffusion des maladies infectieuses et nous nous appuyons sur l’ étude des réseaux complexes pour gérer les ensembles de données à grande échelle décrivant les interconnexions de la population mondiale. En particulier, nous abordons deux différents probl`emes de sant ́e publique. Tout d’abord, nous consid ́erons la propagation d’une ́epid ́emie au niveau mondial, et pr ́esentons un mod`ele de mobilit ́e (GLEAM) conc ̧u pour simuler la propagation d’une maladie de type grippal a` l’ ́echelle globale, en int ́egrant des donn ́ees r ́eelles de mobilit ́e dans le monde entier. La derni`ere pand ́emie de grippe H1N1 2009 a d ́emontr ́e la n ́ecessit ́e de mod`eles math ́ematiques pour fournir des pr ́evisions ́epid ́emiques et ́evaluer l’efficacit ́e des politiques d’interventions. Dans cette perspective, nous pr ́esentons les r ́esultats obtenus en temps r ́eel pendant le d ́eroulement de l’ ́epid ́emie, ainsi qu’une analyse a posteriori portant sur les strat ́egies de lutte et sur la validation du mod`ele. Le deuxi`eme probl`eme que nous abordons est li ́e a` la propagation de l’ ́epid ́emie sur des syst`emes en r ́eseau d ́ependant du temps. En particulier, nous analysons des donn ́ees d ́ecrivant les mouvements du b ́etail en Italie afin de caract ́eriser les corr ́elations temporelles et les propri ́et ́es statistiques qui r ́egissent ce syst`eme. Nous étudions ensuite la propagation d’une maladie infectieuse, en vue de caractériser la vulnérabilité du syst`eme et de concevoir des strat ́egies de controˆle. Ce travail est une approche interdisciplinaire qui combine les techniques de la physique statistique et de l’analyse des syst`emes complexes dans le contexte de la mobilit ́e des agents et de l’ ́epid ́emiologie num ́erique."
"Dans ce travail, nous étudions les propriétés statistiques de certains systèmes dynamiques déterministes et stochastiques. Nous nous intéressons particulièrement aux valeurs extrêmes et à la récurrence. Nous montrons l’existence de Lois pour les Valeurs Extrêmes(LVE) et pour les Statistiques des Temps d’Entrée (STE) et des Temps de Retour (STR) pour des systèmes avec décroissance des corrélations rapide. Nous étudions aussi la convergence du Processus Ponctuel d’Evènements Rares (PPER).Dans la première partie, nous nous intéressons aux systèmes dynamiques déterministes, et nous caractérisons complètement les propriétés précédentes dans le cas des systèmes dilatants. Nous montrons l’existence d’un Indice Extrême (IE) strictement plus petit que 1 autour des points périodiques, et qui vaut 1 dans le cas non-périodique, mettant ainsi en évidence une dichotomie dans la dynamique caractérisée par l’indice extrême. Dans un contexte plus général, nous montrons que le PPER converge soit vers une distribution de Poisson pour des points non-périodiques, soit vers une distribution de Poisson mélangée avec une distribution multiple de type géométrique pour des points périodiques. De plus, nous déterminons explicitement la limite des PPER autour des points de discontinuité et nous obtenons des distributions de Poisson mélangées avec des distributions multiples différentes de la distribution géométrique habituelle. Dans la deuxième partie, nous considérons des systèmes dynamiques stochastiques obtenus en perturbant de manière aléatoire un système déterministe donné. Nous élaborons deux méthodes nous permettant d’obtenir des lois pour les Valeurs Extrêmes et les statistiques de la récurrence en présence de bruits aléatoires. La première approche est de nature probabiliste tandis que la seconde nécessite des outils d’analyse spectrale. Indépendamment du point choisi, nous montrons que l’IE est constamment égal à 1 et que le PPER converge vers la distribution de Poisson standard."
"La gravité quantique à boucles est une théorie candidate à la description unifiée de la relativité générale et de la mécanique quantique à l'échelle de Planck. Cette théorie peut être formulée de deux manières. L'approche canonique, d'une part, cherche à résoudre l'équation de Wheeler--DeWitt et à définir les états physiques. L'approche par les écumes de spins, d'autre part, a pour but de calculer les amplitudes de transition de la gravité quantique via une intégrale de chemin covariante. Ces deux approches s'appuient sur a même structure d'espace de Hilbert, mais la question de leur correspondance exacte reste un important problème ouvert à ce jour. Dans ce travail de thèse, nous présentons quatre résultats en rapport avec ces deux approches. Après un premier chapitre introductif, le second chapitre concerne l'étude de la théorie classique. Historiquement, l'introduction des variables d'Ashtekar complexes (self-duales) dans la formulation hamiltonienne de la relativité générale fut motivée par l'obtention d'une contrainte scalaire polynomiale. Cette simplification drastique est à la base du programme de la gravité quantique à boucles. Pour un certain nombre de raisons techniques, ces variables complexes furent ensuite abandonnées au profit des variables d'Ashtekar-Barbero, pour lesquelles le groupe de jauge est SU(2). Avec ce choix de variables réelles, la contrainte hamiltonienne n'est malheureusement plus polynomiale. La formulation en terme des variables SU(2) réelles peut être obtenue à partir de l'action de Holst, qui contient le paramètre dit de Barbero-Immirzi comme constante de couplage additionnelle. Dans un premier temps, nous allons utiliser les variables d'Ashtekar complexes pour effectuer l'analyse canonique de l'action de Holst avec un paramètre de Barbero-Immirzi réel. Les contraintes qui découlent de cette analyse canonique dépendent de ce paramètre libre, et ont l'avantage d'être polynomiales. Afin de garantir que la métrique soit une quantité réelle, un ensemble de contraintes de réalité doivent être imposées. Il s'avère que ces conditions de réalité correspondent aux contraintes de simplicité linéaires utilisées pour la construction des modèles d'écumes de spins. Ces contraintes sont préservées par l'évolution hamiltonienne si et seulement si la connexion est sans torsion. Cette condition sur l'absence de torsion est en fait une contrainte secondaire de l'analyse canonique. La second chapitre concerne également la théorie classique, mais s'intéresse à sa discrétisation en terme des variables de premier ordre dites holonomie-flux. L'espace des phases qui résulte de cette construction possède une structure non-linéaire. Le formalisme des twisteurs permet d'accommoder cette non-linéarité en travaillant sur un espace des phases linéaire paramétré par les coordonnées canoniques de Darboux. Ce formalisme fut introduit par Freidel et Speziale, mais uniquement dans le cas des variables SU(2) d'Ashtekar-Barbero. Nous généralisons ce résultat au cas du groupe de Lorentz. Nous étudions ensuite la dynamique en terme d'écumes de spins obtenue à partir de ces variables, et développons une nouvelle formulation hamiltonienne de la gravité discrétisée. Ce nouveau formalisme est obtenu en écrivant l'action de la théorie continue sur une discrétisation simpliciale de l'espace-temps fixée. L'action discrète ainsi obtenue est la somme de l'analogue en terme de spineurs d'une action topologique de type BF et des contraintes de réalité qui garantissent l'existence d'une métrique réelle. Cette action est polynomiale en terme des spineurs, ce qui permet de procéder à sa quantification canonique de manière relativement aisée. Le dernier chapitre s'intéresse à la théorie quantique obtenue suivant cette procédure. Les amplitudes de transition reproduisent celles du modèle d'écume de spins EPRL (Engle Pereira Rovelli Livine). Ce résultat est intéressant car il démontre que la formulation de la gravité quantique en termes d'écumes de spins peut être obtenue à partir d'une action classique écrite en terme de spineurs."
"Le sujet principal de la thèse est le transport dans les systèmes mésoscopiques. Dans une première partie de lathèse, on étudie le cas d’un branchement adiabatique d’un biais de potentiel sur un système unidiensionnel sansrépartition initiale. On démontre que le courant complet est uniformément borné par rapport à la vitesse debranchement adiabatique, lorsque celle-ci tend vers zéro. On démontre l’existence de la partie linéaire de l’étatet du courant. La seconde partie de la thèse a donné lieu à a publication d’un article et elle consiste en l’étuded’un modèle discret, sans répartition initiale. On démontre que, dans ce système et après une perturbationélectrochimique, il existe un état stationnaire hors équilibre, et on retrouve la formule de Landauer-Büttikerpour ce modèle. La dernière partie de la thèse, qui a également donné lieu à un article, porte sur l’étude del’approximation des guides d’onde quantiques par des graphes quantiques. On s’intéresse à un guide d’ondelocalement torsadé. On étudie moins le Laplacien sur ce guide d’onde torsadé. Lorsque e diamètre du guidetend vers zéro et, simultanément, lorqsue le support de la courbure tend vers zéro, on démontre que le graphelimite est la ligne droite, et que l’opérateur limite est moins le Laplacien sur L2 (R) plus une condition deDirichlet à l’origine. Cette condition de Dirichlet est la conséquence des rétrécissements faits. En Annexe, ondonne des démonstrations et explications plus détaillées et utiles pour la compréhension de points clés de lathèse."
"Cette thèse non publiée et datant de plus de quarante ans -- en français -- traite d'""Un modèle de particule à spin de masse nulle dans le champ de gravitation"". Ce modèle est construit dans le cadre de celui de Souriau pour des particules à spin en relativité générale. D'abord les équations les plus générales pour le mouvement de particules à spin de masse nulle sont obtenues. Ensuite, une solution analytique est trouvée dans le cas de l'univers de de Sitter en tant qu'univers homogène: elle n'apporte aucun effet nouveau. Dans le cadre de la solution de Schwarzschild des équations d'Einstein, de nouveaux phénomènes sont prédits par l'intermédiaire d'une résolution numérique des équations déduites de ce modèle. Parmi ceux-ci, un minuscule effet analogue à la biréfringence est prédit entre les photons polarisés à droite et à gauche, pris comme exemple de particules sans masse. Mais il était loin d'être mesurable dans les situations qui y étaient alors étudiées. Toutefois, un nouvel intérêt pour ce type de modèle est apparu récemment."
"Les représentations de l'algèbre de Hecke H_n(q) pour q générique sont indexées par les diagrammes de Young. Le polynôme caractéristique de l'élément de battage Sha_(1,n−1) est connu dans H_n(q). En revanche, il reste à déterminer la contribution de chaque représentation irréductible. Dans ce rapport, les valeurs propres et leur multiplicité sont calculées pour les représentations associées aux diagrammes de type « crochet »."
"Cette thèse montre en quoi la quantification de Berezin–Toeplitz peut être incorporée dans le cadre de la géométrie non commutative. Tout d’abord, nous présentons les principales notions abordées : les opérateurs de Toeplitz (classiques et généralisés), les quantifications géométrique et par déformation, ainsi que quelques outils de la géométrie non commutative. La première étape de ces travaux a été de construire des triplets spectraux (A, H, D) utilisant des algèbres d’opérateurs de Toeplitz sur les espaces de Hardy et Bergman pondérés relatifs à des ouverts Ω de Cn à bord régulier et strictement pseudoconvexes, ainsi que sur l’espace de Fock sur Cn. Nous montrons que les espaces non commutatifs induits sont réguliers et possèdent la même dimension que le domaine complexe sous-jacent. Différents opérateurs D sont aussi présentés. Le premier est l’opérateur de Dirac usuel sur L2(Rn) ramené sur le domaine par transport unitaire, d’autres sont formés à partir de l’opérateur d’extension harmonique de Poisson ou de la dérivée normale complexe sur le bord ∂Ω. Dans un deuxième temps, nous présentons un triplet spectral naturel de dimension n+1 construit à partir du produit star de la quantification de Berezin–Toeplitz. Les éléments de l’algèbre correspondent à des suites d’opérateurs de Toeplitz dont chacun des termes agit sur un espace de Bergman pondéré. Plus généralement, nous posons des conditions pour lesquelles une somme infinie de triplets spectraux forme de nouveau un triplet spectral, et nous en donnons un exemple."
"Les nanostructures ferromagnétiques supportent des texturesàtexturesà la topologie non triviale, comme des vortex ou skyrmions. Soumises à un courant polarisé en spin l'aimantationévolueaimantationévolue par effet du spin-transfert torque. On étudie les mécanismes de dissipationàdissipationà l'origine des changements de topologie observés, et on propose une généralisation de l'équation de Landau-Lifshitz, pour tenir compte de la source de topologie."
"Dans cette thèse, nous étudions l’influence des propriétés diverses des réseaux dynamiques et statiques sur la propagation des épidémies. Nous utilisons des données récentes de contacts en face à face de haute résolution. Pour les simulations de la propagation des épidémies, les nœuds du réseau sont divisés en compartiments de nœuds susceptibles (S), infectés (I), exposés (E) et guéris (R). Outre le modèle SEIR, des modèles avec moins de compartiments, comme SIR ou SI, seront utilisés aussi."
"On considère un analogue électronique de l'interféromètre de Hong-Ou-Mandel (HOM), dans lequel deux électrons uniques propagent selon des états de bords chiraux opposés et rentrent en collision au niveau d'un point contact quantique. En étudiant le bruit de courant, on montre qu'en raison des interactions entre les canaux co-propageant, le degré d'indistinctibilité entre deux paquets d'ondes électroniques est dramatiquement réduit, ce qui résulte en un contraste réduit pour le signal HOM. Ce phénomène de décohérence dépend fortement de la résolution en énergie des paquets. Étant donné que les interactions provoquent la fractionalisation de la charge, on montre que le mode de charge et le mode neutre interfèrent l'un avec l'autre, ce qui crée des creux ou des pics satellites dans le bruit de courant. Nos calculs expliquent de récents résultats expérimentaux qui révèlent un signal électronique HOM avec un contraste réduit."
"Dans cette thèse, nous nous intéressons aux propriétés statistiques des systèmes dynamiques aléatoires et non-autonomes. Dans le premier chapitre, consacré aux systèmes aléatoires, nous établissons un cadre fonctionnel abstrait, couvrant une large classe de systèmes dilatants en dimension 1 et supérieure, permettant de démontrer de nombreux théorèmes limites annealed. Nous donnons aussi une condition nécessaire et suffisante pour que la version quenched du théorème de la limite centrale soit valide en dimension 1. Dans le chapitre deux, après avoir introduit la notion de système non-autonome, nous étudions un système composé d'applications en dimension 1 ayant un point fixe neutre commun, et nous montrons que celui-ci admet une vitesse de perte de mémoire polynomiale. Le chapitre trois est consacré aux inégalités de concentration. Nous établissons de telles inégalités pour des systèmes dynamiques aléatoires et non-autonomes, et nous étudions diverses applications. Dans le chapitre quatre, nous nous intéressons aux lemmes dynamiques de Borel-Cantelli pour l'induction de Rauzy-Veech-Zorich, et présentons quelques résultats liés aux statistiques de récurrence pour cette application."
"Cette Thèse est consacrée à l'étude des systèmes de spins à symétrie continue sur un réseau 2-D. Pour le modèle XY, on considère les transitions de phase de seconde espèce [Berezinskii, Kosterlitz et Thouless], en liaison avec la vorticité des états de Gibbs ou des paramètres d'ordre (minimiseurs de l'énergie libre $\cal F$). Les vortex présentent une analogie avec les interfaces dans le modèle d'Ising ; la symétrie continue du système a toutefois un effet régularisant sur les transitions de phase, excluant en 2-D toute aimantation spontanée, même à basse température, ce qui se traduit par une décroissance des fonctions de corrélation. Pour le modèle d'Heisenberg avec potentiel de Kac, les vortex sont remplacés par des instantons. Dans le Chapître 1, on rappelle quelques propriétés de l'interaction entre plus proches voisins, pour le rotateur, ou sa version simplifiée appelée modèle de Villain. On introduit aussi le modèle du champ moyen. Le modèle de Kac, qui partage certains aspects de ces deux modèles, est étudié au Chapître 2. Par un procédé d'homogénéisation, on ramène essentiellement l'étude de la mesure de Gibbs en volume fini à celle de la fonctionnelle énergie libre $\cal F$, généralisant des techniques utilisées dans le modèle d'Ising. Les propriétés de vorticité du modèle de Kac sont analysées dans le Chapître 3, où l'on détermine les extrema de $\cal F$, avec conditions limite. On met ainsi en évidence des configurations très similaires à celles des solutions des équations de Ginzburg-Landau. Dans le Chapître 4 on passe au cas quantique, en introduisant la notion de ""matrice de vorticité"" à température inverse $\beta$, dont on calcule le ""degré non-commutatif"". Il apparaît ainsi, pour le modèle XY de spin 1/2, des configurations de vorticité analogues à celles rencontrées dans le cas classique."
"Le spectre d'absorption optique des nanotubes de carbone semiconducteurs est analysé par une approche mathématique rigoureuse. Un modèle quantique décrivant le nanotube est suggéré et est étudié à l'aide de la théorie de perturbation. En utilisant la petitesse du rayon du tube, le problème est d'abord réduit à une dimension. La théorie de la réponse linéaire permet ensuite d'exprimer le spectre d'absorption en fonction des états propres de l'Hamiltonien unidimensionel associé au tube. Plusieurs arguments physiques ainsi que l'utilisation intensive de la théorie de perturbation amènent l'étude de l'Hamiltonien unidimensionel, à grand nombre de particules, à être réduite à celle de l'Hamiltonien de l'exciton, système composé de deux particules de charges opposées. Des expressions quasi-analytiques pour les états propres de ce système, dépendantes du rayon du tube, sont obtenues perturbativement. Les pics d'absorption de lumière correspondant à des énergies dans la lacune entre bande de valence et bande de conduction du nanotube semiconducteur sont alors reliés à la présence d'excitons et la localisation des pics est donnée en fonction du rayon du tube par une expression approchée avec un terme d'erreur contrôlé."
"Dans notre thèse nous nous sommes intéressés à l'étude des fluctuations de courant, de l'admittance quantique ainsi que la densité d'états pour un nano système en interaction. Notre travail se divise en deux parties. Dans la première partie, nous avons étudié les fluctuations de courant et l'admittance pour un conducteur unidimensionnel, en décrivant le système par un liquide de Tomonaga-Luttinger. Nous avons utilisé les techniques de bosonisation et de refermionisation afin d'aboutir à des résultats exacts pour tous les régimes de température, toutes les valeurs de la tension appliquée et toute la gamme des fréquences. Les résultats obtenus sont appliqués à un conducteur cohérent couplé à un quantum de résistance, et aux états de bord dans le régime de l'effet Hall quantique fractionnaire. Dans le cas d'un conducteur cohérent, le bruit non symétrisé à fréquence finie exhibe un profil différent de celui de la théorie de la diffusion, et la conductance à fréquence finie est directement liée au courant. Dans le cas du régime de l'effet Hall quantique fractionnaire, nous avons pu établir que dans certaines limites, il existe une relation entre les corrélations de courant à l'admittance quantique. En particulier, les singularités qui apparaissent dans les corrélations de courant sont celles de l'admittance. Dans la deuxième partie, nous avons étudié un fil quantique connecté à deux réservoirs qui sont représentés par deux impuretés. Le système est décrit par un liquide de Tomonaga-Luttinger. Nous avons établi et résolu l'équation de Dyson pour la fonction de Green retardée. Ce qui permet de calculer la densité d'états pour un fil quantique homogène puis inhomogène. Dans le cas d'un paramètre d'interaction homogène, l'effet des impuretés modifie le profil de la densité d'états. Dans le cas d'un paramètre d'interaction inhomogène, le calcul de la densité d'états est plus difficile et une approche numérique est indispensable."
"Les effets de rétroaction sont inévitables dans les plasmas de fusion: les équations de Maxwell, décrivant l'évolution des champs électromagnétiques, impliquent la charge et les densités de courant des particules. À leur tour, les trajectoires des particules sont modifiées par les champs à travers les équations du mouvement. Ensuite, l'effet cumulatif de cette boucle de rétroaction peut conduire au confinement du plasma. Dans ce travail, nous abordons le problème de l'amélioration du confinement du plasma par le contrôle de la turbulence et du transport, en particulier, nous explorons la possibilité de formation de barrières internes. Les variations auto-coherentes des champs électromagnétiques et la densité des particules sont à l'origine d'instabilités du plasma et des phénomènes de transport turbulents. Afin de comprendre les mécanismes sous-jacents, nous étudions la dynamique d'un plasma non-collisionnel en appliquant des methodes hamiltoniennnes. D'un point de vue général, la dynamique du plasma peuvent être étudiés à différents niveaux: en particulier, cinétique et fluide. Tous deux admettent une formulation hamiltoniennne. Dans le premier cas, par exemple, la structure hamiltonienne canonique apparaît lors de la construction du modèle de centre-guide des particules: un mouvement dans un espace des phases de 6 dimensions. Un tel modèle nous permet d'étudier la dynamique des particules dans un champ électromagnétique externe et ne prend pas en compte la rétroaction des particules sur le champ. La deuxième approche, suite à la précédente, étudie l'évolution de la distribution des particules fonction sur l'espace des phases à 6 dimensions. Une formulation hamiltonien non-canonique est possible: c'est le modèle de Maxwell-Vlasov. Enfin des structures hamiltoniennes sont construites pour des modèles fluide, pour decrire l'évolution d'une fonction de distribution en trois dimensions d'espace de configuration. L'utilisation de l'approche hamiltoniennne implique que les sources de dissipations (viscosité et autres...) ne sont pas prise en compte: par exemple dans le modèle de Charney-Hasegawa-Mima, modèle à deux fluides, etc."
"Connus des mécaniciens de la géométrie de Poisson, les algébroïdes de Lie transitifs sont ici étudiés du point de vue de leurs sections afin de développer un formalisme algébrique plus proche de celui développé par les théories de jauge. Ici, les algébroïdes de Lie transitifs s'apparentent à une généralisation des champs de vecteurs sur la variété de base. Ce mémoire de thèse a pour objet l'étude des connexions généralisées sur les algébroïdes de Lie transitifs et la construction de théories de jauge. Les connexions ordinaires sur les algébroïdes de Lie transitifs sont définies par des 1-formes de connexion de l'algébroïde de Lie à valeurs dans son noyau et vérifiant une contrainte de normalisation sur ce noyau. En relâchant cette contrainte, on construit l'espace des 1-formes de connexions généralisées qui se décomposent, à l'aide d'une connexion ordinaire de fond, comme la somme d'une connexion ordinaire et d'un paramètre purement algébrique définit sur le noyau. Dans l'esprit des théories Yang-Mills, une action invariante de jauge est définie comme la “norme” de la courbure associée à une connexion généralisée. De cette action, il découle un lagrangien composé des termes des théories de jauge de type Yang-Mills-Higgs : le terme cinétique associé aux champs de jauge et le terme de couplage minimal pour un champ tensoriel scalaire plongé dans un potentiel quartique. La réduction du groupe de symétrie de la théorie s'effectue par une redistribution des degrés de liberté dans l'espace fonctionnel des champs de la théorie. Il résulte de ces manipulations la définition d'une théorie de type Yang-Mills dont les bosons vecteurs sont des champs massifs."
"La gyrocinétique est un modèle clef pour la microturbulence en physique des plasmas, couramment utilisé pour les plasmas de fusion ou pour la turbulence plasma de petite échelle en astrophysique, par exemple. Ce modèle présente encore plusieurs difficultés, qui pourraient amener à reconsidérer ses équations. Le présent rapport de thèse clarifie trois d'entre elles. Tout d'abord, l'une de des coordonnées était source d'interrogations, tant d'un point de vue physique que d'un point de vue mathématique ; une coordonnée adéquate a été introduite, qui dissipe les difficultés et explique les structures intrinsèques sous-jacentes. Ensuite, le changement de coordonnées perturbatif de la gyrocinétique n'était implémenté qu'aux ordres les plus bas ; des relations de récurrence explicites ont été obtenues pour aller à tous les ordres dans le développement. Enfin, le couplage entre le plasma et le champ électromagnétique n'était pas introduit de manière complètement satisfaisante ; en utilisant la structure hamiltonienne de la dynamique, il a été implémenté d'un façon plus adaptée, avec d'importantes conséquences sur les équations gyrocinétiques, en particulier concernant leur structure hamiltonienne. Pour aborder ces trois principaux points, plusieurs autres résultats sont obtenus, par exemple sur l'origine de l'invariant adiabatique centre-guide, sur une transformation centre-guide minimale très efficace, ou sur un modèle hamiltonien intermédiaire entre Vlasov-Maxwell et la gyrocinétique, dont les caractéristiques de la densité de Vlasov contiennent à la fois la dynamique lente centre-guide et la dynamique rapide du gyro-angle. D'autre part, diverses méthodes de réduction sont utilisées, développées ou introduites, par exemple une transformée des équations du mouvement, une méthode de relèvement pour transférer les réductions de la dynamique des particules à la dynamique des champs correspondante, ou une méthode de troncature reliée à la fois à la théorie des contraintes de Dirac et à une projection sur une sous-algèbre. Outre la gyrocinétique, cela permet de clarifier d'autres réductions hamiltoniennes en physique des plasmas, par exemple pour une dynamique incompressible ou électrostatique, pour la magnétohydrodynamique, ou pour des fermetures fluides incluant des moments de la densité de Vlasov d'ordre deux."
"Nous étudions des systèmes dynamiques quantiques discrets décrits par un opérateur unitaire U agissant sur l'espace des fonctions à carré sommables défini sur les sommets d'un graphe infini. Les orbites du système sont définies par les itérations U^n x, où x est la condition initiale et n parcoure les entiers. Nous considérons certaines classes d'opérateurs U dépendants de paramètres du système. Nous sommes en particulier intéressés par des propriétés spectrales qui sont topologiques, donc caractérisé par un entier qui dépend continuellement des paramètres du système.Les questions auxquelles nous répondons sont basées sur des observations des applications récentes dans les sciences physiques et informatiques ; nous fournissons des définitions et des résultats mathématiques précises et pertinentes pour certaines de ces applications. Dans cette thèse nous avons obtenu quatre résultats : en une dimension, nous avons démontré l'existence de valeurs propres, stable par des petites ou compactes perturbations, pour une classe de marche quantique. En dimension deux, nous avons obtenu trois résultats concernant la stabilité du spectre absolument continu de U couvrant tout le cercle unité.Nous avons utilisé plusieurs outils mathématiques La théorie des opérateurs fibrés du quelle nous nous sommes servis pour donner un aperçu des propriétés spectrales de U, dans le cas où ses paramètres sont invariants par translation. Pour les considérations topologiques, nous avons utilisé la théorie de l'ensemble des opérateurs de Fredholm, en particulier la caractérisation complète de ses composantes connexes par l'indice. Pour le cas des valeurs propres stables nous avons démontré une borne inférieure non triviale et explicite sur leur nombre en faisant appel au théorème d'indice pour les opérateurs de Toeplitz. Pour nos trois résultats qui concernent la stabilité du spectre absolument continu nous avons exploité l'indice relatif d'une paire de projecteurs orthogonales. Pour chaque cas et pour une paire définie par U nous avons pu démontrer sa non-trivialité. Ceci nous a permis d'utiliser des résultats récents concernant des implications spectrales pour les opérateurs U."
"La thèse couvre certains aspects de la théorie des représentations de l’algèbre de Brauer murée et son analogue quantique. L'algèbre de Brauer murée est une algèbre unitaire associative. Il s’agit d’une algèbre de diagramme engendré par des diagrammes «murés» particuliers. Cette algèbre peut être définie par des générateurs et des rélations. Dans le premier chapitre de la thèse, nous construisons la forme normale de l'algèbre de Brauer murée - un ensemble de monômes de base (mots) dans les générateurs. Pour construire l'ensemble, nous introduisons une modification «ordonnée» du fameux lemme du diamant de Bergman, à savoir, nous présentons un ensemble de règles qui, étant appliquées dans un certain ordre, permet de réduire tout monôme dans les générateurs à un élément de la forme normale. Nous appliquons ensuite la forme normale pour calculer la fonction génératrice du nombre de mots avec une longueur minimale donnée. Une procédure de fusion donne une construction de la famille maximale d'idempotents orthogonaux minimaux par paire dans l'algèbre, et par conséquent, fournit un moyen de comprendre les bases dans les représentations irréductibles. Nous construisons la procédure de fusion pour l'algèbre de Brauer murée et montrons que tous les idempotents primitifs est trouvés par les évaluations consécutives une fonction rationnelle en plusieurs variables. Dans le deuxième chapitre, nous étudions le produit tensoriel mixte des représentations fondamentales tridimensionnelles de l'algèbre de Hopf $U_q sl(2|1)$. L'un des principaux résultats consiste à établir les formules explicites de décomposition des produits tenseurs de tout $U_q sl(2|1)$-module simple ou projectif avec les modules générateurs. Un autre résultat important consiste à décomposer le produit tensoriel mixte en un bimodule."
"Cette thèse est divisée en deux grandes parties : la première est consacrée aux propriétés statistiques des systèmes dynamiques et l’autre porte sur des propriétés des systèmes dynamiques topologiques.Dans la première partie, d’abord nous rappelons quelques notions de base des systèmes dynamiques aléatoire et de la théorie des valeurs extrêmes. Ensuite, par l’application de cette théorie aux réseaux des applications couplées nous montrons que la probabilité de l’apparition de la synchronisation dans ces réseaux est liée à la distribution du maximum d’une certaine observable évaluée le long de presque toutes les orbites. De plus, nous montrons qu’une telle distribution appartient à la famille des lois des valeurs extrêmes, où le paramètre de cette distribution (indice extrémal) nous permet d’obtenir une description détaillée de la probabilité de synchronisation. Enfin, nous illustrerons les résultats théoriques par des calculs numériques robustes qui nous permettent d’aller au-delà du cadre théorique fourni.Dans la seconde partie, nous commençons par introduire quelques notions de base des systèmes dynamiques topologiques. Ensuite, nous étudions la relation entre les ensembles des points récurrents et périodiques d’une application continue de dendrites locales dans lui-même dans lequel l’ensemble des points d’extrémité est dénombrable."
"Récemment, une attention particulière a été accordée à l’étude des systèmes dynamiques défini sur des espaces unidimensionnels, par exemple les arcs, les cercles, les graphes, les dendrites et les dendrites locales. L’intérêt d’étudier la dynamique des fonctions définies sur ces espaces est dû à: Premièrement, les dendrites sont des exemples de l’ensemble de Julia dans la dynamique complexe. Deuxièmement, les dendrites (locales) sont des Péaniens continus (espace compact, connexe, localement connexe) dans la théorie du continus. L’objectif de cette thèse est de trouver une caractérisation du système dynamique équicontinu défini sur un espace métrique compact. Comme application de ce résultat, on a démontré des conditions nécessaires et suffisantes à l’équicontinuité pour un système dynamique défini sur une dendrite locale particulière et donc c’est une généralisation du théorème 2.8."
"Cette thèse conclut et résume une partie de mes travaux au Centre de Physique Théorique, effectués sous la supervision de Serge Lazzarini. Deux thématiques sont abordées ici, toutes deux essayant de combler les lacunes de théories existantes, en incorporant les effets de spin, ou de polarisation, de particules élémentaires, qui sont souvent négligés. En premier lieu, nous verrons une étude de l’équation de Lévy-Leblond–Newton (LLN) basée sur les travaux [1, 2]. Cette équation décrit l’évolution d’un système quantique consistant d’une particule élémentaire avec spin soumise à son propre potentiel gravitationnel. Après avoir revu les symétries (accidentelles) en mécanique quantique non relativiste, et comment les géométriser grâce aux structures de Bargmann, nous reverrons ce qu’est l’équation de Lévy-Leblond. Elle est à l’équation de Schrödinger ce que l’équation de Dirac est à l’équation de Klein–Gordon. Ensuite, nous reverrons quelques résultats à propos de l’équation de Schrödinger–Newton (SN), notamment ses symétries et les quantités conservées. Cette étude de l’équation de LLN a pour but de l’écrire d’une manière tout à fait covariante, ce qui est accompli en l’écrivant sur une structure de Bargmann. Cette formulation covariante a l’avantage de faciliter l’étude des symétries dynamiques de l’équation, et de ses quantités conservées. Le groupe de symétrie de cette équation se trouve être le groupe de Schrödinger–Newton, qui a été trouvé comme étant le groupe de symétrie de l’équation de SN [3]. Les quantités conservées de l’équation de LLN seront aussi déduites de cette analyse, et nous les comparerons aux quantités conservées de l’équation de SN. La deuxième partie du manuscrit traite de la trajectoire des particules élémentaires en relativité générale lorsqu’on ne néglige pas leur spin. Tout d’abord nous reverrons la littérature existante sur ce sujet, notamment en soulignant la méthode géométrique de Souriau pour obtenir les équations de Mathisson–Papapetrou–Dixon (MPD). Ces équations n’étant pas fermées, nous discuterons aussi des différentes conditions supplémentaires sur le spin présentes dans la littérature qui permettent de les compléter. Ensuite, nous rappellerons comment obtenir les équations de Souriau–Saturnini à partir des équations de MPD, et en supposant l’équation supplémentaire de Tulczyjew pour décrire la trajectoire d’un photon avec son spin dans un espace-temps courbe. Après avoir rappelé quelques applications des équations de Souriau–Saturnini, nous présenterons deux résultats issues de [4, 5], où nous avons appliqué ces équations dans, respectivement, un espace-temps de Schwarzshild, puis dans un espace temps déformé par une onde gravitationnelle. La première étude [4] traite de la biréfringence gravitationnelle, c’est-à-dire que lorsqu’on prend la polarisation du photon en considération, sa trajectoire sort du plan géodésique usuel. Nous trouvons que le signe de l’angle que fait la trajectoire avec le plan dépend de l’hélicité du photon, et l’amplitude dépend de sa longueur d’onde et de la masse de l’étoile. La deuxième étude [5] essaye de déterminer si une onde gravitationnelle peut perturber la trajectoire d’un photon suffisamment pour être observable lors des expériences d’interférométries gravitationnelles. Bien que nous trouvions un effet, son ordre de magnitude est largement en dessous de ce que nous pouvons détecter avec la technologie actuelle. Nous commenterons aussi sur l’utilité de la constante cosmologique dans ces calculs."
"Cette thèse traite de différents aspects de la théorie spectrale d’opérateurs utilisés pour modéliser le graphène. Elle est constituée de deux parties. La première traite du cas périodique. Je commence par présenter la théorie générale des systèmes périodiques. J’introduis ensuite les différents modèles de graphène en les comparant. Enfin, je m’intéresse à différentes façons de rendre le graphène semi-conducteur. Je fais en particulier une étude de nanorubans de divers types et présente un résultat d’ouverture d’une lacune spectrale pour un opérateur pseudo-différentiel. La deuxième partie traite du cas désordonné. Je commence par présenter la théorie générale des opérateurs aléatoires. J’explique ensuite succinctement l’analyse multi-échelles qui est la méthode permettant de montrer le résultat essentiel de cette théorie, appelé localisation d’Anderson. Enfin, je donne la preuve de cette localisation pour un modèle de graphène ainsi qu’un résultat sur la densité d’états intégrée."
"Les supraconducteurs sont des sources naturelles d'états intriqués de par la possibilité d'y apparier des électrons au sein de paires de Cooper et dans les dispositifs nanoscopiques contenant de tels matériaux, le transport électronique s'avère être un outil important de caractérisation. Les excitations collectives mises en jeu dans les systèmes mésoscopiques apportent une physique riche et les problématiques d'information quantique ont trouvé dans le monde de la matière condensée un domaine d'application stimulant, comme en témoigne la proposition de calcul quantique utilisant des qubits de Majorana protégés de la décohérence et qui peuvent être synthétisés dans les supraconducteurs topologiques. Alors que de plus en plus d'observations compatibles avec la présence de modes de Majorana s'accumulent, l'étude des propriétés de transport dans des systèmes abritant de telles excitations peut apporter des signatures complémentaires. Dans une jonction Josephson entre deux supraconducteurs séparés par deux points quantiques (dots), les paires de Cooper, qui sont les entités élémentaires du courant Josephson provoqué par une différence de phase supraconductrice, peuvent être éventuellement scindées (processus de réflection d'Andreev croisée CAR) et les électrons constituants peuvent être séparés spatialement pour transiter sur les deux dots. Lorsqu'un champ magnétique est appliqué entre ces deux dots, la dépendance du courant critique (maximum du courant Josephson en fonction de la phase supraconductrice) en fonction de la phase Aharonov-Bohm renseigne sur ce processus de transport non local. Ce dispositif permet une mesure à l'équilibre et constitue une alternative aux propositions hors-équilibre de jonctions en fourche avec un supraconducteur injecteur et des bornes métalliques collectrices, polarisées en tension. Le calcul basé sur une formulation en termes d'intégrale de chemin de la fonction de partition du système est non-perturbatif en le couplage entre supraconducteurs et dots, une telle approche s'avérant incontournable à la vue des avancées expérimentales. Une efficacité de séparation des paires de Cooper a été définie et permet d'indiquer quels jeux de paramètres (énergies et répulsions coulombiennes des dots) pourraient permettre à l'expérimentateur d'optimiser cette délocalisation spatiale d'électrons. Dans une jonction Josephson entre deux supraconducteurs polarisée, un courant alternatif apparaît avec une fréquence Josephson proportionnelle à la tension appliquée. Dans une jonction entre trois supraconducteurs, plusieurs différences de tension et donc plusieurs fréquences Josephson cohabitent. Néanmoins, lorque la borne centrale est mise à la terre et les deux bornes latérales sont polarisées avec des tensions commensurables, un courant Josephson DC apparaît traduisant le transport effectif de paires de Cooper. Le cas le plus simple correspond à des tensions opposées et au processus d'intrication de deux paires de Cooper scindées par double CAR, connu sous le nom de quartet. Une technique d'intégrale de chemin dans le formalisme de Keldysh a été adoptée pour calculer courant et bruit hors équilibre de manière non-perturbative et une représentation fréquentielle est permise par la commensurabilité des voltages. Des résultats numériques pour la résonance quartet sont présentés. Un calcul unifié basé sur les fonctions d'onde solutions de l'équation de Bogoliubov - de Gennes a permis de calculer courant et bruit dans une jonction entre deux supraconducteurs qu'ils soient conventionnels ou topologiques (la différence étant faite par l'équation de raccordement à la position du contact). Dans une jonction entre deux supraconducteurs topologiques, des quasiparticules d'énergie nulle aux propriétés de Majorana peuvent exister au niveau du contact et l'étude des statistiques du courant permet de dégager de nouvelles signatures pour la présence de ces excitations."
"Dans le cadre algébrique et microlocal élaboré par Helffer et Sjöstrand, on propose une ré-écriture de la règle de quantification de Bohr-Sommerfeld pour un opérateur auto-adjoint h-Pseudo-différentiel 1-D; elle s'exprime par la non-inversibilité de la matrice de Gram d'un couple de solutions WKB dans une base convenable, pour le produit scalaire associé à la "" norme de flux "" ."
"Nous présentons une étude théorique du transport hors équilibre à travers une boîte quantique à basses températures en présence d’un champ magnétique Zeeman et d’un courant injecté dans l'une des électrodes. En utilisant une approche d'équations de mouvement renormalisée self-consistente, nous montrons que l'injection d'un courant polarisé en spin conduit à une modulation du splitting de Zeeman du pic de conductance Kondo. Nous montrons qu'une accumulation de spin donnée peut restaurer le pic Kondo en compensant le splitting introduit par le champ magnétique. Dans le cas où le courant injecté est non polarisé, nous montrons que les deux pics Kondo sont également décalés par l'injection d'un courant n'affectant pas la valeur du splitting. Nos résultats expliquent quantitativement les résultats expérimentaux obtenus dans KOBAYASHI T. et al., Phys. Rev. Lett. 104, 036804 (2010). Ces résultats pourraient être exploités pour le contrôle et la manipulation du spin dans les dispositifs de nanoélectronique et de spintronique."
"Les interactions à longue portée (c'est-à-dire en 1/r^a, avec a < d la dimension du système) sont présentes dans de nombreux domaines de la physique, de l'interaction ondes-particules (physique des plasmas, lasers à électrons libres, etc) `a l'astrophysique et aux condensats de Bose-Einstein. Or leur dynamique présente une caractéristique très particulière, celle de se retrouver piégée dans des régimes hors d'équilibre sur des temps très longs (divergents avec le nombre de particules). Ces dynamiques métastables sont appelés “états quasi-stationnaires”. Nous nous intéressons à ces états à travers le modèle paradigmatique Hamiltonian Mean Field. Ce système de N rotateurs couplés est caractérisé macroscopiquement par sa magnétisation M=,j=1:N, qui quantifie le degré d'agrégation des N corps. Les états quasi-stationnaires peuvent alors être décomposés en deux grandes familles, les états “magnétisés” et les états “non-magnétisés”. On peut alors montrer que lorsque le nombre de degrés de liberté du système (c'est-à-dire le nombre de parti- cules) augmente, les orbites régulières apparaissent et se multiplient, associées à des tores invariants de la dynamique d'une particule-test. L'observation de ces tores représente une interprétation dynamique de l'émergence des états quasi-stationnaires, parallèlement à l'explication statistique de ce phénomène (réalisé grâce à un mécanisme de maximisation d'entropie). La transition de phases hors d'équilibre de ce système (d'un régime magnétisé à non-magnétisé) peut alors être réinterprétée comme une bifurcation dynamique des structures de l'espace des phases. Une phénoménologie similaire est observée dans un modèle de laser à électrons libres."
"On étudie les résonances semi-excitées pour un Opérateur h-Pseudo-différentiel (h-PDO)H(x, hDx) sur L2(M) induites par une orbite périodique de type hyperbolique à l’énergie E = 0. Par exemple M = Rn et H(x, hDx; h) est l’opérateur de Schrödinger avec effet Stark, ouH(x, hDx; h) est le flot géodesique sur une variété axi-symétrique M, généralisant l’exemplede Poincaré de systèmes Lagrangiens à 2 degrés de liberté. On étend le formalisme de Gérard and Sjöstrand, au sens où on autorise des valeurs propres hyperboliques et elliptiques del’application de Poincaré, et où l’on considère des résonances dont la partie imaginaire est del’ordre de hs, pour 0 < s < 1.On établit une règle de quantification de type Bohr-Sommerfeld au premier ordre en fonction des nombres quantiques longitudinaux (réels) et transverses (complexes), incluantl’intégrale d’action le long de l’orbite, la 1-forme sous-principale, et l’indice de Conley-Zehnder."
Dans cette thèse nous étudions les théorèmes limites dans l’analyse statistique dessystèmes dynamiques. Le premier chapitre est consacré aux notions des bases des systèmesdynamiques ainsi que la théorie ergodique. Dans le deuxième chapitre nous introduisonsun cadre fonctionnel abstrait pour lequel la version quenched du théorème de la limitecentrale (TLC) en dimension 1 pour les systèmes dynamiques uniformément dilatantsest satisfaite sous une condition de validité nécessaire et suffisante. Le troisième chapitreest consacré au principe d’invariance presque sûr (PIPS) pour les application aléatoiresdilatantes par morceaux. Nous présentons certaines hypothèses sous lesquelles le (PIPS)est vérifié en utilisant la méthode d’approximation des martingales de Cuny et Merlèvede.Nous étudions aussi le théorème de Sprindzuk et ses conséquences. Nous établissons dansle chapitre quatre la décroissance des corrélations pour les systèmes dynamiques aléatoiresuniformément dilatants par la méthode de couplage en dimension 1. Nous terminons cetravail par une présentation des concepts de base de la théorie des mesures et probabilitéset une présentation de l’espace des fonctions à variation bornée.
"On étudie l'ampltude de survie associéè a un opérateur de Schrödinger matriciel représentant la prédissociation d'une molécule arbitraire dans l'approximation de Born-Oppenheimer. On montre que celle-ci est donnée par l'habituelle contribution exponentielle, modulo un reste exponentiellement petit (par rapport au paramètre semi-classique) dont le taux est arbitrairement grand. Ce résultat s'applique en dimension quelconque, et il est possible d'y prendre en compte un nombre de résonances tendant vers l'infini lorsque le paramètre semiclassique tend vers zéro. Pour citer cet article : P. Briet, A. Martinez, C. R. Acad. Sci. Paris, Ser. I 340 (2005). Abstract Estimates on the molecular dynamics for the predissociation process. We study the survival amplitude associated with a semiclassical matrix Schrödinger operator that models the predissociation of a general molecule in the Born-Oppenheimer approximation. We show that it is given by its usual time-dependent exponential contribution, up to a reminder term that is exponentially small (in the semiclassical parameter) with arbitrarily large rate of decay. The result applies in any dimension, and in presence of a number of resonances that may tend to infinity as the semiclassical parameter tends to 0. To cite this article: P. Briet, A. Martinez, C. R. Acad. Sci. Paris, Ser. I 340 (2005)."
"Dans ces notes nous présentons et démontrons un théorème de Kerckhoff, Masur et Smillie sur l'unique ergodicité du flot directionnel sur une surface de translation dans presque toutes les directions. La preuve suit essentiellement celle présentée dans un survol de Masur et Tabachnikov. Nous donnons une preuve complète et élémentaire du théorème."
"Dans cette thèse, nous étudions la dynamique du flot géodésique de Teichmüller. L'origine de cet intérêt provient de l'étude d'une classe très importante de systèmes dynamiques : celle des échanges d'intervalles. Dans des travaux classiques, Masur et Veech montrent en 1982 que la dynamique de ces échanges d'intervalles est reliée avec la dynamique du flot géodésique de Teichmüller sur l'espace des modules des courbes complexes. L'espace des phases de ce flot peut être vu comme l'espace des modules des différentielles quadratiques sur une surface. Ces espaces sont naturellement stratifiés par le type des singularités des formes. De plus ces strates sont préservées par l'action de ce flot. Des résultats classiques affirment que ces strates sont des orbifolds complexes et sont non-vides et non-connexes en « général ». La motivation du travail expliqué dans cette thèse est donnée par le résultat fondamental, démontré indépendamment par Masur et par Veech (1982), qui affirme que le flot géodésique de Teichmüller agit de façon ergodique sur chaque composante connexe de chaque strate (normalisée), par rapport à une mesure invariante de masse finie. Kontsevich et Zorich ont classifié les composantes connexes des strates de l'espace des modules Hg des différentielles abéliennes. Dans cette thèse, nous donnons une description précise des composantes des strates dans le cas complémentaire de celui de Kontsevich- Zorich, c'est-à-dire de l'espace des modules Qg des différentielles quadratiques qui ne sont pas globalement le carré de différentielles abéliennes. Par ailleurs, nous donnons une formule explicite pour le calcul de la structure spin d'une différentielle quadratique de Qg en termes uniquement des singularités de la strate. Ceci contredit une conjecture de Kontsevich-Zorich sur la classification des composantes connexes non-hyperelliptiques de Qg par cette structure spin. En utilisant cette formule, nous donnons une application dans le contexte des billards dans un polygone rationnel."
"Les systèmes complexes résultent de l’interaction d’un grand nombre de constituants élémentaires, et présentent une architecture non triviale et adaptative. La compréhension de ces sytèmes s’est accrue considérablement grâce au récent développement de la science des réseaux, qui s’appuie sur la théorie des graphes. Cette approche permet de mettre en lumière une relation fonction-structure complexe, et simplifie l’étude théorique de nombreux processus dynamiques dont ils peuvent être le siège. On s’intéresse depuis peu à la dynamique instantanée de ces réseaux. Dans le cas particulier des réseaux sociaux, ces dynamiques non-markoviennes, c'est-à-dire présentant des effets de mémoire. Cette thèse a pour but d’approfondir notre compréhension des réseaux sociaux dynamiques, et en particulier de déterminer l'impact d’une topologie à dynamique non-markovienne sur les processus collectifs, de diffusion ou de propagation susceptibles d’apparaître au sein de tels systèmes. Nous appuyant sur un modèle stochastique existant de dynamique sociale à base d'agents actifs, nous généralisons à l'étude d'une distribution quelconque du temps d'attente entre deux interacions d'un agent. Supposée exponentielle dans le modèle original, cette distribution, mesurable dans les réseaux sociaux réels, suit en réalité une loi de puissance, révélant le caractère non-markovien de la dynamique sociale chez les êtres humains. Incluant cette propriété dans notre modèle, nous donnons une étude théorique détaillée des propriétés topologiques du réseau agrégé, union de tous les liens apparus au moins une fois dans une fenêtre temporelle donnée."
"Les interactions sociales sont une composante importante de la condition physique des animaux vivants en groupe.L'analyse des réseaux sciaux, et en particulier des réseaux temporels, fournit des outils puissants pour décrire ces interactions sociales et leur évolution. L'étude des réseaux dans leurs aspects temporelles nécessite la disponibilité de grands volumes de données à haute résolution temporelle.L'utilisation du formalisme et des outils du réseau temporel est cependant encore limitée pour les animaux, car les données sur les interactions animales sont encore largement obtenues à partir de méthodes manuelles traditionnelles.Dans cette thèse, nous avons produit des données à haute fréquence à long terme concernant les interactions sociales animales et nous avons conçu un nouveau model de réseau temporel. Ce travail pourrait aider à l'avenir à comprendre les processus écologiques et évolutifs sous-jacents à la formation et à l'organisation des réseaux sociaux, et à étudier comment les changements dans l'environnement, la composition du groupe ou les relations clés uniques influencent l'ensemble de la structure du réseau."
"Les contacts face-à-face entre individus permettent de caractériser les réseaux sociaux et jouent un rôle prépondérant dans la compréhension des mécanismes de propagation des épidémies dans une population. De récentes avancées technologiques ont rendu possible l'acquisition de données précises sur les interactions humaines. Cette thèse présente, dans un premier temps, l'analyse de données de contacts collectées trois années de suite (2011, 2012 et 2013) dans un lycée français entre des étudiants de classes préparatoires. L'analyse a montré que la plupart des contacts se produisent entre étudiants de même classe et que les structures des contacts sont très similaires d'un jour sur l'autre. Dans un second temps, on compare différentes méthodes de collecte de données qui permettent d'obtenir des informations de nature différente (par exemple existence d'un contact face-à-face vs existence d'une amitié).L'utilisation de données rapportant les amitiés entre les étudiants ne permet pas d'obtenir une bonne estimation du réseau de contact (i.e., les amitiés ne correspondent pas forcément à des contacts face-à-face et vice versa) résultant en une sous-estimation du risque épidémique dans cette population.Dans la dernière partie, nous essayons de reproduire les biais provenant du réseau d'amitié en échantillonnant le réseau de contact. Ceci pourrait nous donner des indications sur comment compenser ces biais et comment utiliser des données incomplètes pour obtenir des prédictions fiables sur le risque épidémique."
"Le plan de ce manuscrit est le suivant : — Dans le chapitre 1, je présente les notions et concepts de base de la science des réseaux qui sont les plus pertinents pour mon travail. Je décris ensuite comment ces concepts doivent être repensés et adaptés pour convenir à l’analyse des réseaux temporels, en accordant une attention particulière aux définitions qui sont les plus importantes pour mon travail original. — Le chapitre 2 représente un résumé des principaux résultats de l’utilisation de la science des réseaux pour étudier les systèmes en réseau dans le cerveau. Je présente ici les propriétés des réseaux cérébraux structurels et fonctionnels, et je souligne comment mon travail va dans le sens de la découverte du lien manquant entre ces deux dimensions de la connectivité, à travers la dynamique. Je présente également et passe brièvement en revue deux systèmes cérébraux spécifiques qui font l’objet de mes recherches : l’hippocampe, à l’échelle micro, et le système du langage, à l’échelle macro. Je décris aussi brièvement une condition pathologique spécifique, l’épilepsie, car une partie de mon travail vise à décrire les changements majeurs qui se produisent dans la connectivité des réseaux cérébraux épileptiques. — Dans le chapitre 3, je présente mon premier projet de recherche. Ici, je montre comment les séquences d’activation d’assemblées cellulaires transitoires à l’échelle micro, dans la formation hippocampique de rats anesthésiés, peuvent être tra- duites dans un formalisme de réseau temporel. Ici, j’extrais la CF résolue dans le temps d’un réseau de neurones en mesurant la quantité d’informations partagées entre leurs rythmes de décharge. Je décris ensuite les différents états méta-stables (états du réseau) de ces neurones partageant l’information en termes de recrutement variable dans le temps dans les différentes couches d’une organisation globale persistante cœur-périphérie du réseau, et en termes de taux 3 de changement, la liquidité, de leurs connexions fonctionnelles. Je caractérise ensuite les comportements typiques des nœuds de ces réseaux fonctionnels au sein des états du réseau au moyen de mesures de la théorie des réseaux, montrant ainsi comment, dans le cadre de cette vision théorique de l’information et des réseaux des assemblées de cellules, une variété de comportements neuronaux de type hub apparaît. En outre, je décris comment la proéminence d’un nœud par rapport aux autres dans un réseau change, également de manière drastique, d’un état du réseau à l’autre, et n’est pas liée de manière significative à la localisation anatomique ou aux propriétés physiologiques des cellules. — Dans le chapitre 4, je décris une deuxième étude dans laquelle je m’attaque à l’analyse des réseaux cérébraux à macro-échelle variant dans le temps. Je présente ici une nouvelle approche pour étudier l’apparition, et peut-être les mécanismes sous-jacents, de l’aphasie comme symptôme post-crise chez les patients épileptiques humains. J’extrais un CF résolu dans le temps comme un réseau multicouche variant dans le temps dont les liens dans les différentes couches représentent l’activité cohérente en phase dans différentes bandes de fréquence des signaux enregistrés par des électrodes intracrâniennes implantées chez des patients épileptiques pharmaco-résistants. J’étudie le recrutement ou le rejet dynamique de nœuds dans ou hors de différentes communautés au sein de chaque couche en extrayant un flux de matrices ""module allegiance"" à fenêtre temporelle. En récupérant des états méta-stables discrets d’allégeance aux modules, je montre comment ils se rapportent aux différentes périodes d’un enregistrement, en identifiant notamment deux états qui se rapportent le plus aux symptômes de troubles du langage évalués par les médecins pendant la récupération des patients après une crise. Je décris également comment les relations entre la dynamique de la communauté des nœuds dans différentes couches, c’est-à-dire les bandes de fréquence (de delta, 1-4Hz,àgamma,20−40Hz), sont également affectées par la crise et spécule sur les implications sur le système du langage. Je discute donc de la façon dont cette recherche en cours pourrait éventuellement évaluer l’implication du système du langage dans une réorganisation multifréquence de la dynamique de la CF. Je décris en fait comment cela pourrait conduire à une communication désordonnée induite par la crise entre les zones cérébrales liées au langage, donnant ainsi lieu à l’aphasie comme un effet de déconnexion fonctionnelle transitoire. Cette étude permet d’introduire une nouvelle façon de comparer non seulement les réseaux instantanés, mais aussi les propriétés du flux de réseau à différents moments, ce qui peut avoir une application générale dans les études de réseau temporel au-delà du problème spécifique. — Dans le chapitre 5, j’étudie la relation entre la simultanéité des connexions dans un réseau temporel général et l’existence de structures pertinentes. En particulier, je me concentre sur la question fondamentale de savoir comment discerner si les nœuds importants, prominents, d’un réseau temporel interagissent entre 4 eux, donnant lieu à des structures cohésives temporelles. À cet égard, je présente une nouvelle définition du rich club temporel comme une structure impliquant les nœuds les plus connectés dans un réseau variant dans le temps, qui reste plus stable sur un certain laps de temps que ce qui est attendu si les connexions étaient établies au hasard. J’étudie la temporalité pure d’une telle structure, en soulignant comment les clubs riches temporels peuvent apparaître de manière transitoire même dans des réseaux où un rich club statique ne peut être retrouvé, et comment différents rich clubs temporels peuvent être liés à différents moments d’intérêt de l’évolution du réseau dynamique. En outre, j’étudie le rôle de cette structure dans les processus de propagation sur les réseaux temporels, en dévoilant comment un groupe temporellement cohésif de nœuds importants peut favoriser la propagation d’un pathogène dans un réseau de contacts sociaux. Je conclus ce manuscrit par le chapitre 6, où je discute de la signification de mes résultats, en précisant où se situe mon travail dans le cadre plus large de la neuroscience des réseaux et dans le domaine encore plus large des réseaux temporels."
"Dans ce travail, nous étudions le spectre de l’opérateur Laplacien dans un guide d’ondes quantique 2D cisaillé, avec une condition au bord de Dirichlet. Dans un premier temps, nous établissons des inégalités de type Hardy pour l’opérateur Laplacien. Ensuite, nous localisons le spectre essentiel de l’opérateur sous une certaine hypothèse sur la géométrie du guide d’ondes. Nous montrons la stabilité du spectre essentiel vis-à-vis d’une perturbation dans le cas où la déformation de cisaillement admet une limite (éventuellement infinie) à l’infini. Enfin, nous établissons des conditions suffisantes qui garantissent l’existence des valeurs propres discrètes au-dessous du seuil du spectre essentiel."
Les simulations numériques de propagations de maladies infectieuses deviennent de plus en plus détaillées. Des progrès théoriques montrent comment des données même incomplètes peuvent être utilisées dans les modèles.
"La formulation Hamiltonienne de la mécanique classique révèle une structure algébrique de Lie sous-jacente qui est un élément clé pour développer une théorie de perturbation efficace. Mais on trouve des structures de Lie dans une classe plus grande de systèmes dynamiques, appelé systèmes de Poisson; certains exemples sont, entre autres, la dynamique des fluides, l'électrodynamique, la théorie cinétique. Dans la première partie de cette thèse, on propose une approche purement algébrique à la théorie classique des perturbations, qui s'applique donc à tout les système de Poisson. Dans cette méthode, introduite dans [Vittot, 2004], une transformation (de Lie) permet de diviser la perturbation en un terme préservant le flot non perturbé, et une correction plus petite, quadratique par rapport à la perturbation originale. Dans la deuxième partie de la thèse on considere la dynamique d'une Toupie Pulsante (un Corps Rigide non autonome). S'agissant probablement de l'exemple le plus basique de système de Poisson, la Toupie était un choix naturel pour une application de notre théorie. Nous considérons d'abord une toupie symétrique àvec des moments d'inertie qui oscille periodiquement. En introduisant des coordonnées appropriées, le système est réduit à un systeme presque classique: en effet, on montre que notre théorème s'applique et reproduit le théorème de KAM de la mécanique classique. Puis on considere une Toupie non symétrique avec moments d'inertie qui presentent des fluctuations quelconques: dans ce cas, on etudie sous quelles conditions les trajectoires du systeme sont proches de celle du système statique. Dans la troisième partie de ce mémoire, on étudie la dynamique d'une particule charg\'ee confinée magnétiquement. Dans ce cas le flot non perturbé est la dynamique dans un champ électromagnétique donné arbitraire. Alors par la théorie des perturbations on peut réduire la dimensionnalité de la dynamique, ou étudier la rétroaction de la particule sur le champ. Cependant, fournir une description du flot non perturbé est une tâche redoutable, liée à la question de longue date de la théorie du centre-guide en physique des plasmas. Récemment une version relativiste et non perturbative de la théorie des centres guides a été proposée [Di Troia, 2018]. Nous dérivons les équations du mouvement et leur structure de Poisson dans cette description."
fr_abstract_s
"Cet article a pour but de présenter les grandes lignes du fonctionnement de la circulation marine et atmosphérique de la Méditerranée, afin de mieux comprendre les phénomènes physiques qui affectent les îles d'Hyères, d'optimiser la gestion de leur environnement et le cas échéant des risques potentiels. Mots-clés : Méditerranée, îles d'Hyères, Port-Cros, circulation des masses d'eau, courant, vent, état de mer, circulation atmosphérique, moyenne échelle. Abstract. The Hyères Archipelago in the marine and atmospheric circulation systems. The scope of this paper is the general features of the marine and atmospheric circulations in the Mediterranean. It aims at providing keys to managers and other scientists to better understand the physical phenomena that impact the Hyères archipelago, in order to improve the environmental management if needed, and face potential risks."
"Grace au soutien du Parc National de Port-Cros, un radio-phare, en fait un petit émetteur d'ondes radioélectriques décimétriques dite ""HF"", fonctionne depuis le mois de mai 202 sur la côte sud de l'ile de Porquerolles."
Reponse de la nappe d'eau sous forçage infragravitaire sur la plage du Truc Vert
"Ecologie des sars Diplodus cervinus cervinus (Lowe, 1838) et Diplodus puntazzo (Cetti, 1777), de la badèche Epinephelus costae (Steindachner, 1875) et du corb Sciaena umbra (Linnaeus, 1758) dans le golfe d'Annaba (Est, Algérie)."
"Le projet BEEST est un projet de Recherche et Développement reposant sur une démarche exploratoire. Celle-ci a pour ambition le développement d’une méthode de mise en contexte (spatiale et historique) des évolutions de la qualité des eaux et de la biodiversité côtières, en rapport avec les aménagements relatifs à l’assainissement et aux traitements des eaux urbaines, mais également en considérant l’évolution socio-économique du territoire. Le projet repose sur une base bibliographique : mesures existantes issues de l'autosurveillance, connaissances du gestionnaire assainissement, travaux universitaires des partenaires biologistes ou géographes, mais également sur des résultats des études disponibles et d‘approches historiques. Ces données consolidées, sont mises en perspective historique à l’échelle de la baie maritime et terrestre de Marseille. La méthode a pour but d'organiser les données afin de les exploiter sous un nouvel angle, notamment en mettant en perspective l'anthropoécosystème de Marseille et son histoire environnementale. En mettant en scène les évolutions du couple pressions-états, l’objectif est de mieux comprendre les co-évolutions homme-nature et d’apporter des éléments complémentaires à ceux existant déjà pour orienter les politiques publiques de gestion du milieu et du territoire."
"Coastal and open ocean regions of the Western Tropical South Pacific ocean have been identified as a hotspot of N2 fixation. However, the environmental factors driving the temporal variability of abundance, composition, and activity of diazotrophs are still poorly understood, especially during the winter season. To address this, we quantified N2 fixation rates and the abundance of seven diazotroph phylotypes (UCYN-A1 symbiosis, UCYN-B, UCYN-C, Trichodesmium, Het-1, Het-2, and Het-3) on a monthly basis during two full years (2012 to 2014) at four stations along a coast to open ocean transect in the New Caledonian lagoon. The total nifH gene concentration (sum of all nifH gene copies) clearly decreased from the barrier reef to the shore. Apart from UCYN-B, which peaked at very high abundances (10 6–10 8 nifH gene copies L -1) at two occasions at the coastal station, the UCYN-A1 symbiosis was the most abundant group at all stations, accounting for 79% of the total nifH gene copy counts along the transect (average abundance 4.2 +/- 10.3 x 10 4 nifH gene copies L-1). The next most abundant groups were in order Trichodesmium (accounting for 14% of the total nifH gene copies), Het-groups (6% of the total) and UCYN-C (1% of the total). Statistical analyses reveal that the UCYN-A1 symbiosis and Het groups were associated with cold (25°C) waters, low NOx and PO4 3- concentrations, strong and (mostly) easterly winds. Average N2 fixation rates over the survey were 6.5 +/- 6.7 nmol N L-1 d-1 and did not differ significantly among seasons. The year to year variability was more pronounced with average integrated rates significantly higher in the second year of the survey (162 +/- 122 mmol N m-2 d-1) than the first year (66 +/-91 mmol N m-2 d-1). This dataset suggests that seasonality is less pronounced than previously thought, and that relatively high N2 fixation rates are maintained in the New Caledonian lagoon all year long, despite seasonal changes in the diazotroph community composition."
"Ce rapport du programme Dynamine « Dynamique des métaux de la mine au lagon » constitue le volume 2 (sur 4) du programme intégré « Dispersion et exposition humaine aux métaux en Nouvelle-Calédonie » composé de 3 projets (DMML, Dynamine, Métexpo) étudiant les métaux et leur toxicité sur des sites pilotes similaires. - Le programme Dynamine a proposé de quantifier et qualifier les flux des métaux potentiellement toxiques (Ni, Cr, Co et Mn) dans l’hydrosphère, le long du continuum massif minier– rivière–écosystèmes littoraux, pour mieux évaluer la contribution relative des phénomènes naturels et anthropiques (activité minière) sur les flux et les cycles biogéochimiques de ces éléments à l’interface terre-mer en Nouvelle-Calédonie."
"L’action B1 de l’Observatoire des Sédiments du Rhône (OSR5) vise à pérenniser le réseau de suivi des flux de matières en suspension (MES) et de contaminants associés. Ce rapport dresse le bilan des avancées réalisées sur l’ensemble du réseau en 2020 en termes de stations actives, de prélèvements de MES et d’analyses physico-chimiques, ainsi que sur les calculs de flux de contaminants dans la Base de Données des Observatoires en Hydrologie (BDOH). L’année 2020 a été principalement orientée sur la continuité du fonctionnement des 12 stations actives du réseau de mesure (11 station équipées de mesures de turbidité et de prélèvements de MES pour l’analyse des contaminants + 1 station équipée seulement de mesures de turbidité), et la production de données à ces stations. En plus de cela, le réseau de mesure a été mis davantage en avant par différents moyens, incluant la production de fiches techniques sur les stations, la mise à jour de protocoles dans un format tout public, et leur partage sur le site internet de l’OSR (lui aussi mis à jour)."
"An underwater volcanic eruption off the Vava'u island group in Tonga on 7 August 2019 resulted in the creation of floating pumice on the ocean's surface extending over an area of 150 km2. The pumice's far-reaching effects from its origin in the Tonga region to Fiji and the methods of automatic detection using satellite imagery are described, making it possible to track the westward drift of the pumice raft over 43 days. Level 2 Moderate Resolution Imaging Spectroradiometer (MODIS), Visible Infrared Imaging Radiometer Suite (VIIRS), Sentinel-3 Ocean and Land Color Instrument (OLCI), and Sentinel-3 Sea and Land Surface Temperature Radiometer (SLSTR) imagery of sea surface temperature, chlorophyll-a concentration, quasi-surface (i.e., Rayleigh-corrected) reflectance,and remote sensing reflectance were used to distinguish consolidated and fragmented rafts as well as discolored and mesotrophic waters. The rafts were detected by a 1 to 3.5°C enhancement in the MODIS-derived ""sea surface temperature"" due to the emissivity difference of the raft material. Large plumes of discolored waters, characterized by higher satellite reflectance/backscattering of particles in the blue than surrounding waters (and corresponding to either submersed pumice or associated white minerals), were associated with the rafts. The discolored waters had relatively lower chlorophyll-a concentration, but this was artificial, resulting from the higher blue/red reflectance ratio caused by the reflective pumice particles. Mesotrophic waters were scarce in the region of the pumice rafts, presumably due to the absence of phytoplanktonic response to a silicium-rich pumice environment in these tropical oligotrophic environments. As beach accumulations around Pacific islands surrounded by coral shoals are a recurrent phenomenon that finds its origin far east in the ocean along the Tongan trench, monitoring the events from space, as demonstrated for the 7 August 2019 eruption, might help mitigate their potential economic impacts."
"Vers une reconstitution des populations de grande nacre Pinna nobilis en Méditerranée française ? À la suite d'une mortalité massive des populations de grande nacre Pinna nobilis à travers toute la Méditerranée depuis l'automne 2016, et une estimation de leur mortalité dans les eaux de l'aire marine protégée de l'archipel de Port-Cros (Parc national de Port-Cros, Provence orientale, France) proche de 100 %, des grandes nacres vivantes ont été trouvées en août 2020 sur la face sud de l'île. Les premiers suivis ont permis d'identifier 8 jeunes individus vivants en novembre 2020. Ces individus étaient toujours vivants en mars 2021."
"L’étang de Berre est un écosystème lagunaire historiquement perturbé par de forts rejets industriels et par des apports importants en eau douce naturels et anthropiques (centrale hydroélectrique EDF). Alors que les politiques de réhabilitation de l'étang de Berre, initiées depuis 1994, commencent à enregistrer leurs premiers succès, l'introduction et la prolifération de Mnemiopsis leidyi en 2005 pourraient limiter leur efficacité. L’originalité de cette étude est d’associer océanographie et sociologie afin d’estimer l’impact de M. leidyi sur le fonctionnement de ce socio-écosystème. Nous avons pu montrer que la population de M. leidyi est capable de se maintenir sur une large gamme de températures (3°C-28°C) et de salinités (10-30), pour une quantité de carbone disponible > 3 µgC L-1. Nous avons démontré en laboratoire que la ponte est observée chez les adultes dès 8°C, et à partir de 10°C chez les larves transitoires. La température apparait ainsi être un facteur déterminant dans la dynamique de la population de ce cténaire. Les larves voient leur développement en adulte bloqué pour des températures inférieures à 14°C. En été la population est dominée par des adultes dont les taux de reproduction sont élevés (2 221 ± 2 496 œufs ind-1 j-1) soutenant de larges blooms (jusqu’à 96 ind m-3). L’ingestion de Mnemiopsis sur la communauté zooplanctonique (principalement copépodes et larves méroplanctoniques) est très variable de 4 ± 2 proies ind-1 j-1 à 1 370 ± 97 proies ind-1 j-1, mais montre une capacité à contrôler jusqu’à 80 % du stock de zooplancton réduisant le control top-down sur les communautés phytoplanctoniques. De plus, Mnemiopsis contribue faiblement via son excrétion N-NH4 (jusqu’à 3,8 %) au pool d’ammonium et à la production régénérée, favorisant aussi la croissance du phytoplancton. De ce fait, la population agit favorablement sur le maintien de l’eutrophisation de l’étang de Berre par effet « top-down » et « bottom-up ». L’absence de ce cténaire au cours d’évènements froids et sa réapparition plusieurs mois après laissent penser soit à l’existence d’une source externe à l’étang ou à la présence de zone refuge. L’utilisation de la modélisation lagrangienne des transports de particules couplée aux mesures des variables environnementales a non seulement permis de comprendre la distribution de Mnemiopsis dans l’étang mais aussi d’identifier une zone refuge (l’étang de Vaine) favorable au maintien de M. leidyi. Les fortes proliférations de M. leidyi dans l’étang de Berre affectent principalement la pêche professionnelle. Le fort colmatage des filets, la mutilation des prises, la dégradation accélérée du matériel et l’augmentation de la pénibilité induisent une perte économique annuelle estimée à 50 % par les pêcheurs. La baignade est peu impactée sauf en cas de fortes proliférations durant lesquelles il peut y avoir désertion des plages de la lagune ; de même, le nautisme n’indique une gêne qu’en cas de fortes proliférations lorsque les cténaires colmatent les circuits de refroidissement des moteurs. Dans le cadre interdisciplinaire, la compréhension des interactions biologiques et anthropiques a permis de montrer que Mnemiopsis présentait une entrave à la mise en œuvre des efforts de réhabilitation. De plus, la récente installation de l’hydroméduse invasive urticante Gonionemus vertens pourrait également être une menace en raison de l’élargissement de son aire de répartition liée à la transplantation des herbiers à zostère. Par conséquent, dans un contexte d’invasions biologiques, les perspectives de réhabilitation de l’étang de Berre restent toutefois soumises au caractère imprévisible de la nature."
"Ce travail de thèse présente une étude sur la circulation océanique de surface s'écoulantle long des talus continentaux (aux abords de l’isobathe 200 m) de la mer Méditerranée centrale. L'objectif étaitde mieux renseigner le schéma de circulation de surface dans la région et les régimes de variabilité associés ens’appuyant essentiellement sur les observations altimétriques conventionnelles (AVISO/X-TRACK) le longdes traces satellite, disponibles sur la période 1993-2015. Cette étude est également basée sur une utilisationcombinée des données spatiales de SST, de mesures in-situ hydrographiques et de courantométrieconjointement avec l'altimétrie. L'analyse a d'abord permis de caractériser la variabilité saisonnière et deproposer un nouveau schéma de circulation de surface, y compris pour les saisons intermédiaires, incluant unenouvelle branche de courant Atlantique sur le plateau continental Tunisien. Le second point fort de ce travailréside aussi dans la caractérisation du transport de surface et des modes de variations inter-annuelles de l'eauAtlantique à partir de l'altimétrie côtière sur le long terme. L‘observabilité de la méso-échelle océanique de lazone d‘étude est enfin abordée à partir des mesures altimétriques haute résolution (AltiKa). Cela a permis dedétecter des structures physiques à plus fine échelle et d'affiner le détection des variations des courants les pluscôtiers. Au delà des aspects scientifiques, cette thèse a également permis des avancées méthodologiquesimportantes concernant l’utilisation combinée de différents jeux de données spatiaux et in-situ, aisémentapplicables pour les pays du Sud."
"In natural world, no organism exists in absolute isolation, and thus every organism must interact with the environment and other organisms. Next-generation sequencing technologies are increasingly revealing that most of the cells in the environment resist cultivation in the laboratory and several prokaryotic divisions have no known cultivated representatives. Based on this, we hypothesize that species that live together in the same ecosystem are more or less dependent upon each other and are very large in diversity and number, outnumbering those that can be isolated in single-strain laboratory culture. In natural environments, bacteria and archaea interact with other organisms (viruses, protists, fungi, animals, plants, and human) in complex ecological networks, resulting in positive, negative, or no effect on one or another of the interacting partners. These interactions are sources of ecological forces such as competitive exclusion, niche partitioning, ecological adaptation, or horizontal gene transfers, which shape the biological evolution. In this chapter, we review the biological interactions involving prokaryotes in natural ecosystems, including plant, animal, and human microbiota, and give an overview of the insights into the evolution of living beings. We conclude that studies of biological interactions, including multipartite interactions, are sources of novel knowledge related to the biodiversity of living things, the functioning of ecosystems, the evolution of the cellular world, and the ecosystem services to the living beings."
"L'objectif général de cette thèse est d'étudier les mécanismes de transport des déchets marins flottants ainsi queleur accumulation potentielle en Méditerranée, en s'appuyant sur la modélisation numérique de leur dérive ainsique sur des observations in-situ de leur distribution. La dynamique du transport des déchets marins y estnotamment analysée en terme d'échelles caractéristiques du bassin, en partant de la grande échelle jusqu'auxéchelles côtière et littorale.Dans un premier temps, l'examen d'un ensemble d'expériences Lagrangiennes numériques a permis d'identifierdans le bassin des zones probables d'accumulation non permanentes à grande échelle (Mansui et al., 2015a).L'impact à l'échelle côtière d'un courant de bord (Courant Nord) et des forçages atmosphériques sur ladistribution locale et l'échouage des déchets marins flottants a ensuite pu être estimé en utilisant notammentdes données originales recueillies en mer et à terre (Mansui et al., 2015b, en révision)."
Certains microorganismes associés aux gisements pétroliers seront bientot des alliés précieux pour mieux exploiter cette ressource
"Le travail de recherche doctorale développé ici vise à améliorer les connaissances et la représentation desprocessus hydro-sédimentaires dans la Rade de Toulon, afin de mettre en place un modèle de dispersion desradionucléides comme outil de gestion post-accidentelle. L’intérêt de cette étude pluridisciplinaire repose surune stratégie qui unit la modélisation et les observations in situ et aborde la problématique complexe descontaminants en milieu marin.Les résultats obtenus permettent de rendre compte de la circulation hydrodynamique dans la Rade de Toulon etde sa forte dépendance au forçage par le vent. Ainsi, les temps d’échange des masses d’eau de la Petite Radesont estimés entre deux et six jours selon les conditions météorologiques. Les mesures en continu réaliséesdans les fleuves Las et Eygoutier ont permis de décrire et de quantifier les apports liquides et solides desbassins versants de la Rade, très peu étudiés jusqu’ici. La Petite Rade représente une zone de sédimentationprivilégiée vis à vis du millier de tonnes par an de matières particulaires apportées par le Las, et apparaît à cetitre comme une zone de piégeage des radionucléides en cas de rejet accidentel."
"Une campagne de mesures menée au Bassin de Génie Océanique First en 2008, dans le cadre du GIS HYDRO, nous a permis d'observer une forte amplification de l'amplitude de la houle non prévue par l'équation ""mild-slope"". Nous avons par ailleurs constaté l'existence d'un cisaillement vertical dans le champ de courant observé. Ce cisaillement semble avoir un impact important sur la dynamique de la houle. Du point du vue théorique, ce type cisaillement n'est pas pris en compte par l' équation ""mild-slope"", habituellement connue pour bien décrire l'interaction entre la houle et le courant. Forts de ce constat, nous étudions la sensibilité de la houle au cisaillement vertical du courant, en nous basant sur la résolution de la relation de dispersion linéaire. Nous établissons ensuite, en nous inspirant des approches variationelles de Kirby \cite{Kirby1984} et de Simmen \cite{Simmen1984}, une équation linéaire gouvernant la propagation de la houle au dessus d'une bathymétrie lentement variable et en présence d'un courant cisaillé verticalement. Cette équation aura pour but de quantifier l'influence du cisaillement sur la focalisation géométrique de la houle."
"Les communautés du coralligène dominées par des gorgonaires ont été sévèrement affectées par des évènements de mortalités massives liés au réchauffement de la Méditerranée. Pour évaluer la contribution des bactéries associées à l’holobionte Paramuricea clavata à son fonctionnement et sa santé, il est apparu primordial de caractériser le compartiment microbien naturel de ce gorgonaire tempéré.Dans ce contexte, l’objectif général de cette thèse était de décrire les interactions existant entre la gorgone rouge P. clavata et ses bactéries associées en Méditerranée nord-occidentale. Les analyses entreprises par des techniques culture-indépendantes basées sur l’analyse des ADN ribosomiques 16S bactériens ont inclus (i) la caractérisation de la variation spatio-temporelle des communautés bactériennes, (ii) la localisation des bactéries dans les tissus de l’hôte, (iii) l’évaluation de la stabilité des associations gorgones-bactéries en conditions de stress et (iv) la détermination de la spécificité d’hôte des bactéries dominantes entre différentes espèces de gorgonaires sympatriques (Eunicella singularis, Eunicella cavolini et Corallium rubrum). Les résultats obtenus ont établi que P. clavata et son microbiote forment un holobionte au sein duquel hôte et bactéries vivent en étroite association, stable dans le temps et l’espace ou en conditions de stress. Les communautés bactériennes associées sont principalement endosymbiotiques et dominées par un ribotype bactérien appartenant à un genre nouveau de la famille des Hahellaceae qui semble présenter une forte spécificité d’hôte. Ces résultats suggèrent un rôle particulier de ce genre bactérien chez les holobiontes gorgonaires. Ces travaux ont permis d’initier la connaissance du compartiment bactérien des quatre espèces de gorgonaires les plus abondantes dans les habitats côtiers. Les éléments acquis et les différents outils mis au point pourront être intégrés à de nouvelles recherches sur le rôle des associations symbiotiques dans la santé et le devenir des populations de gorgonaires face aux changements environnementaux en cours."
"Les processus méso et sub-méso échelles (""(sub-)méso-échelle"" par la suite) ont été reconnus comme jouant un rôle primordial au niveau global (sur les transferts d'énergie, la productivité primaire, le cycle du carbone etc.), mais leurs impacts en zone côtère font encore l'objet de débats. En effet, compte tenu de la difficulté de mesurer les propriétés physiques de ces structures hautement variables en temps et en espace, peu d'observations permettaient une bonne caractérisation de la (sub-)méso-échelle côtière. Cependant, des progrès récents réalisés en altimétrie satellite ont ouvert des perspectives nouvelles. Des développements précurseurs effectués au LEGOS (Toulouse), à CLS (Toulouse) et à l'IMEDEA (Majorques) ont permis de caractériser des processus physiques jusque-là inaccessibles au moyen d'algorithmes classiques, ouvrant ainsi un nouveau champ d'application pour l'altimétrie satellite: l'étude de la dynamique aux échelles régionales et côtières. L'exploitation scientifique de données altimétriques expérimentales - de type X-TRACK, PISTACH etc. - utilisées conjointement avec d'autres mesures complémentaires (thermographie satellite, gliders, bouées dérivantes etc.) a déjà permis d'étudier et de mieux comprendre la phénoménologie associée à la dynamique océanique régionale: méandres et tourbillons, courants côtiers de densité et intrusions sur le plateau, formations d'eaux profondes. De par leur répétitivité, leur couverture spatiale et leur résolution accrue, ces données devraient également contribuer, sur le long terme, à l'analyse des transferts d'énergie impliquant la (sub-)méso-échelle dans les zones côtieres et de plateaux. Au cours de la prochaine décennie, de nouveaux instruments spatiaux donneront accès a une mesure de hauteur de mer à haute résolution (cf. mission SWOT d'une résolution de l'ordre du km), ce qui devrait permettre une meilleure compréhension de la (sub- )méso-échelle océanique. En vue de préparer l'avènement de nouvelles missions aux technologies prometteuses, l'exploitation scientifique de l'altimétrie satellite côtière a vocation à se poursuivre. Cet exposé se focalisera particulièrement sur les progrès significatifs accomplis au cours des 5 dernières années et présentera, brièvement, les perspectives de recherches ouvertes à plus long terme."
"Les branchies de la plupart des espèces de poissons représentent un biotope particulier pour de nombreux ectoparasites. Que ces organismes aient ou non des affinités taxinomiques, les peuplements parasitaires ainsi constitués doivent se partager les différentes ressources, et notamment l'espace. L'étude de leur répartition branchiale respective, et de leur déterminisme, est un champ important de l'écologie parasitaire depuis une trentaine d'années. Cependant, les méthodes d'approche (définition de l'espace colonisé, localisation des parasites et modes de représentation ...) sont nombreuses, le plus souvent imprécises et non reproductibles. Elles ne permettent généralement pas une approche comparée de guildes ou de peuplements homologues. La formalisation de la méthode d'échantillonnage la plus couramment utilisée est proposée."
"Cette thèse s'est intéressée aux processus hydrodynamiques dans une baie semi-fermée telle que la Rade d Toulon et leurs importances pour la dispersion de contaminants dissous. Pour cette étude, une configuration à très haute résolution (100 m de résolution spatiale) nommée TBAY100, basée sur le modèle de circulation océanique MITgcm a été mise en place. Un emboîtement multi-modèle a été effectué pour arriver à une telle résolution, en partant d'une configuration NEMO-GLAZUR64 à 1.3 km de la Méditerranée Nord-Occidentale puis NEMO-NIDOR à 400 m du littoral Varois pour forcer correctement les frontières de TBAY100. Dans un premier temps, une analyse mathématique a permis de quantifier les échanges d'énergie pour un système simplifié pour ensuite étendre cette réflexion à la Rade de Toulon et mieux comprendre les échanges aux frontières ouvertes du domaine. Cette configuration a été ensuite validée avec diverses observations dont des données d'ADCP et des trajectoires de flotteurs qéolocalisables dérivants. Des schémas de circulation typiques dépendant des conditions hydrodynamiques et météorologiques ont été dégagés. Dans un deuxième temps, nos recherches se sont portées sur les processus de distribution de polluants en s'appuyant sur des prélèvements chimiques, principalement le cuivre relargué par les peintures-antifouling. Les hypothèses de contamination (sources et taux) ont fait l'objet d'un travail collaboratif avec une équipe de chimie. Le transport des contaminants a été analysé à l'aide de traceurs passifs implémentés dans TBAY100 et a abouti à 4 schémas de dispersion de polluants indiquant un export soit vers le Courant Nord, soit vers le Parc National Marin de Port-Cros, ainsi qu'une rétention des polluants dans la Grande Rade de Toulon. Cette maquette pourra avoir d'autres applications sociétales importantes puisqu'elle peut servir d'outil de prédiction de courants et de dispersion de contaminant dans la Rade de Toulon."
Les mesures des activités du cycle de l'azote dans le sédiment ont montré que celui ci est capable d'éliminer une partie de sa charge en azote sous forme de gaz (N2) principalement via le processus de dénitrification. Les valeurs de taux de dénitrification se situent dans la gamme la plus élevée de la littérature. Le processus de dénitrification est couplé au processus de nitrification permettant une élimination additionnelle d'ammonium du sédiment sous forme de N2.
"Les branchies des poissons constituent un biotope hétérogène pour de nombreuses communautés d'ectoparasites. Leurs modifications au cours de la croissance des poissons sont en partie à l'origine de cette hétérogénéité. Le nombre de filaments branchiaux et la surface branchiale colonisée par les ectoparasites augmentent généralement avec l'âge du poisson. Pour Hemichromis fasciatus, un Cichlidae africain (Teleostei), l'augmentation du nombre de filaments peut être décrit par un modèle exponentiel, et la croissance de la surface branchiale colonisable par une fonction polynomiale. Ces modèles sont comparés avec ceux, connus, de deux téléostéens des milieux tempérés."
"Les écoulements en milieux poreux non-saturés sont modélisés par l'équation de Richards qui est une équation non-linéaire parabolique dégénérée. Ses limites et les défis que soulèvent sa résolution numérique sont présentés. L'obtention de résultats robustes, précis et efficaces est difficile en particulier à cause des fronts de saturation raides et dynamiques induits par les propriétés hydrauliques non-linéaires. L'équation de Richards est discrétisée par une méthode de Galerkine discontinue en espace et des formules de différentiation rétrograde en temps. Le schéma numérique résultant est conservatif, d'ordre élevé et très flexible. Ainsi, des conditions aux limites complexes sont facilement intégrées comme la condition de suintement ou un forçage dynamique. De plus, une stratégie adaptative est proposée. Un pas de temps adaptatif rend la convergence non-linéaire robuste et un raffinement de maillage adaptatif basée sur des blocs est utilisée pour atteindre la précision requise efficacement. Un indicateur d'erreur a posteriori approprié aide le maillage à capturer les fronts de saturation raides qui sont également mieux approximés par une discontinuité introduite dans la solution grâce à une méthode de Galerkine discontinue pondérée. L'approche est validée par divers cas-tests et un benchmark 2D. Les simulations numériques sont comparées à des expériences de laboratoire de recharge/drainage de nappe et une expérience à grande échelle d'humidification, suite à la mise en eau du barrage multi-matériaux de La Verne. Ce cas exigeant montre les potentialités de la stratégie développée dans cette thèse. Enfin, des applications sont menées pour simuler les écoulements souterrains sous la zone de jet de rive de plages sableuses en comparaison avec des observations expérimentales."
"L’objectif général de cette thèse est de contribuer à l’avancement de la connaissance de la variabilité du Courant Nord Méditerranéen (CN) et de ses interactions avec la dynamique côtière, en s’appuyant principalement sur une configuration numérique réaliste à haute résolution de la façade méditerranéenne française, basée sur le modèle de circulation océanique NEMO et nommée GLAZUR64.La validation de cette configuration avec toutes les observations disponibles sur la période d'étude (CTD, gliders, ARGO, radar HF, ADCP, altimétrie et SST satellite) a permis d'évaluer le réalisme des simulations et leur paramétrisation, et de montrer l'apport de la haute-résolution par rapport aux configurations de bassin au 1/12° utilisées aux frontières de GLAZUR64.Enfin, l'utilisation d'un forçage océanique opérationnel a permis d'utiliser une simulation en complément des données d'une campagne en mer, pour l'étude ciblée d'un tourbillon anticyclonique associé à un méandre du CN au large de Toulon, en avril 2011 [Guihou et al., 2013]."
"Les Muges (Téléostéens) sont parasités par des Monogènes congénériques du genre Ligophorus. Ces ectoparasites branchiaux ont une spécificité stricte ; plusieurs espèces peuvent néanmoins coexister sur une même espèce hôte, constituant ainsi de véritables guildes. Les hôtes circulent entre mer et lagunes dans des environnements changeants ; ces guildes sont donc soumises à des contraintes environnementales fluctuantes. La variabilité phénotypique d'un de ces Monogènes, L. imitans, parasite de Liza ramada, est analysée à l'échelle d'une population géographiquement isolée dans le delta du Rhône (Camargue, France). Une étude biométrique des pièces sclérifiées du hapteur et de l'appareil génital (14 variables), s'appuyant sur des techniques d'analyse d'images, est menée. Une analyse en composantes principales permet d'identifier une variabilité morpho-anatomique assez importante dans cette population, mais au sein d'un continuum. Les différentes parties de ces organismes ne répondent pas à une croissance allométrique simple et généralisée. L'analyse des corrélations entre variables montre notamment que le système génital est morphologiquement indépendant du système de fixation."
"Ligophorus imitans est un ectoparasite branchial de Liza ramada (Teleostei). Chaque individu-hôte abrite une infrapopulation parasitaire qu'il soumet aux contraintes résultant de sa propre biologie ou écologie. L'influence de huit variables environnementales ou structurelles sur la plasticité morphologique des pièces sclérifiées de l'opisthohapteur et de l'appareil génital de ce monogène est étudiée. La taille des pièces sclérifiées du hapteur augmente avec la taille de l'hôte et donc l'hétérogénéité des branchies. Une relation de même nature existe entre taille de l'hôte et taille du parasite, sans que le stade de maturité sexuel et donc l'âge de ce dernier soit impliqué. La densité et la composition locales du peuplement parasitaire n'ont pas d'effet décelable sur la morphométrie du hapteur. La variabilité de l'appareil génital est indépendante des variables environnementales considérées. Les implications écologiques et évolutives de ces résultats sont discutées."
"Les Plathelminthes, ou vers plats, sont représentés par des milliers d'espèces, principalement parasites. Ce sous-embranchement comprend plusieurs classes, dont celle des Monogènes. Majoritairement ectoparasites de vertébrés aquatiques, ces derniers sont connus depuis le 19ème siècle, surtout sous l'angle taxinomique. L'étude de la dynamique de leurs populations reste marginale, malgré l'intérêt écologique et épidémiologique que ce groupe zoologique présente. Les approches prédictives de leur démographie, via la modélisation, sont rares ; elles ne concernent que quelques Monopisthocotylea, ectoparasites de poissons téléostéens. Les principales tendances biomathématiques des travaux déjà réalisés sont résumés ; leur intérêt est souligné, mais aussi certains de leurs défauts. Avec le modèle Bar (Dicentrarchus labrax)-Diplectanum que nous avons tout particulièrement étudié, nous illustrerons comment une recherche interdisciplinaire, associant biologie des populations, mathématique et informatique, peut faire évoluer notre connaissance dans un tel domaine."
"L'objectif de ce travail était d'améliorer la compréhension et la modélisation de la propagation de la houle au-dessus d'une topographie lentement variable et en présence d'un courant inhomogène. Nous nous sommes en particulier intéressés à l'influence d'un courant cisaillé linéairement et verticalement sur la dynamique de la houle. Dans ce but un modèle linéaire de propagation de la houle a été développé et une campagne expérimentale a été menée en bassin de génie océanique. Au cours de cette campagne expérimentale les paramètres de la houle et du courant ont été mesurés avec une haute résolution spatio-temporelle. Nous avons pu décrire l'interaction complexe entre la houle et le courant, en particulier les effets de l'évolution spatiale des gradients verticaux et horizontaux de ce dernier sur la propagation de la houle."
"L’évolution des techniques altimétriques de la bande Ku Nadir vers la bande Ka et l’interféro-métrie large fauchée proche Nadir dans le contexte de la mission SWOT (« Surface Water Ocean Topography », CNES/NASA) soulève de nouvelles questions scientiﬁques quant à la validité des modèles de rétrodiffusion des surfaces d’eau dans cette bande de fréquence et les erreurs sur les estimations de hauteurs d’eau dues aux mouvements de ces surfaces au cours du temps. Un modèle de rétrodiffusion (GO4) adapté à la conﬁguration SWOT est présenté. Il conserve la précision du modèle de référence de l’Optique Physique tout en gardant la simplicité du modèle plus couramment employé de l’Optique Géométrique. En plus du paramètre classique de pente, il introduit un paramètre supplémentaire, dit de « courbure effective » (msc). Le modèle permet l’inversion des paramètres de pente et de courbure de la surface sous certaines conditions déve-loppées dans ce manuscrit. La validité des modèles conjoints de rétrodiffusion en bande Ka et de surface d’eau a été vériﬁée sur des mesures radar effectuées en soufﬂerie dans un environnement contrôlé. Dans une dernière partie, les propriétés temporelles du signal rétrodiffusé ont été étudiées, en particulier le temps de corrélation et le décalage Doppler induit par le mouvement des vagues. Nous étudions l’inﬂuence de ces quantités sur les performances de la synthèse SAR non focalisée du système SWOT."
"La dynamique des particules dans les systèmes aquatiques est régit par l’interaction entre la resuspension et la sédimentation. Cette étude est consacrée à la détermination des processus responsables de la resuspension des particules dans le lac de Guiers situé au nord du Sénégal. Des échantillons ont été collectés au cours d’un cycle annuel au niveau de trois stations prises dans la partie centrale du lac. Dans ce site, la resuspension des particules est liée à l’action des vents mais aussi à la crue du fleuve Sénégal. La resuspension sous l’effet des vents se produit pendant l’harmattan, alors qu’en période d’alizés maritimes, c’est la crue du fleuve Sénégal qui est responsable de la remise en suspension des particules. Le taux de particule en suspension est plus important en période d’harmattan qui est marquée par des vitesses du vent plus élevées (moyenne de 3,6 m.s-1 fluctuant entre 1 et 7 m.s-1), mais surtout par un fetch fort (24 km). Cette évolution saisonnière de la resuspension affecte la turbidité de l’eau, les taux de matière en suspension, la biomasse phytoplanctonique, le coefficient d’atténuation lumineuse, le flux des particules et la structure de la colonne d’eau. La resuspension sous l’effet des vagues créées par le vent associé au fetch élevé constitue dans ce système peu profond le principal processus contrôlant la dynamique des particules."
"L’incidence rasante est un problème spécifique, qui apparaît notamment lorsqu'une antenne est placée sur un mât (télécommunications, défense…) ou sur la côte (surveillance environnementale ou militaire de l'espace maritime). Elle rend la modélisation du problème de diffraction difficile, de par le faible niveau de rétrodiffusion et l’importance de phénomènes complexes comme la diffusion multiple. La question reste importante même si l’écho est très faible, puisqu’il est potentiellement suffisant pour perturber le bon fonctionnement de systèmes antennaires microondes sur un navire. Il porte de plus des informations intéressantes sur l’état de la mer, comme cela a été démontré aux bandes HF et VHF. Un modèle rigoureux de diffraction tridimensionnelle précédemment développé est étendu au calcul des quatre polarisations fondamentales (polarisations Horizontale et Verticale des ondes incidente et réfléchie). Il permet désormais de prendre en compte la conductivité finie de la surface, point crucial dans le cas d'une polarisation incidente Verticale. L’opérateur hyper-singulier impliqué dans l’équation intégrale discrétisée par la méthode des moments est étudié pour évaluer la précision des calculs numériques.Les méthodes approchées de diffraction permettent des calculs numériques beaucoup plus rapides, et sont donc en pratique incontournables. Le modèle rigoureux est donc utilisé, en conjonction avec des données expérimentales, pour servir de référence permettant d’étudier la précision, en incidence rasante et dans le cas de la surface de la mer, de ces méthodes approchées. Nous étudions en particulier la méthode à deux échelles GOSSA, et proposons une correction à son comportement aux angles rasants.Le mouvement de la surface de la mer crée un décalage de fréquence radar dans l’onde rétrodiffusée (effet Doppler), décalage mesuré expérimentalement et que l’algorithme de méthode des moments permet de simuler. Nous étudions par des simulations bidimensionnelles l’évolution du décalage Doppler micro onde avec l’incidence, et l’influence des nonlinéarités de la surface de la mer. Le comportement limite en incidence rasante est précisé, et les contributions respectives des phénomènes électromagnétiques et hydrodynamiques discutées."
"Le genre Mullus (Teleostei) est représenté par deux espèces en mer Méditerranée : Mullus barbatus et M. surmuletus. Ces téléostéens abritent une communauté parasitaire composée principalement de trématodes, au nombre des plus riches et des plus diversifiées de cette aire géographique. Parmi les 18 espèces de digènes signalées chez les Rougets, neuf ont été retrouvées dans les populations étudiées de Méditerranée nord-occidentale. Trois dessins originaux, concernant les espèces Aponurus laguncula et Proctoeces maculatus, sont présentés. Une synthèse des problèmes de synonymie et du spectre d'hôtes de ces neuf espèces est réalisée. Des cartes de leur répartition géographique dans le bassin méditerranéen sont établies. La plupart des espèces étudiées ont une répartition géographique restreinte à la mer Méditerranée occidentale et à la mer Adriatique, exceptés Proctoeces maculatus signalé jusqu'à présent uniquement chez les Rougets de la mer Méditerranée orientale, et Opecoeloides furcatus dont la distribution s'étend à l'ensemble de la mer Méditerranée. L'existence d'autres vers mésoparasites (Cestodes, Nématodes, Acanthocéphales), connus ou retrouvés, est également rapportée."
Transport sédimentaire et morphodynamique en zone littorale
"La connaissance de la source et du transport atmosphérique de l’aérosol marin est primordiale pour mieux comprendre un grand nombre de processus physico-chimiques régissant les propriétés de la basse troposphère. C’est dans ce cadre que s’inscrit cette thèse dont l’objectif principal concerne la modélisation de la génération et du transport des aérosols marins en zone côtière Méditerranéenne. Cette étude a permis notamment l’amélioration du modèle de transport MACMod à partir d’un travail sur les conditions aux surfaces frontières. Une formulation de la fonction source spécifique à la région méditerranéenne nord-occidentale a été déterminée et certains effets des transformations subies par les particules issues de la production primaire au cours de leur transport atmosphérique ont été analysés. De plus, dans le but de mieux tenir compte du cas des zones anthropisées pour les conditions aux limites du modèle, un travail sur l’implémentation du code CHIMERE sur la région méditerranéenne a été initialisé. Enfin, une dernière partie a été consacrée à l’adaptation du modèle MACMod à la prévision de la qualité de l’air et le suivi des concentrations de PM2.5 et PM10."
no abstract
"La première partie de ce document présente un modèle basé sur la résolution des équations Lagrangiennes du mouvement nommé “Choppy Wave Model”. Il permet de tenir compte des nonlinéarités hydrodynamiques de la surface et d’établir une description complète des grandeurs statistiques. Les résultats obtenus soulignent leur caractère non Gaussien et l’importance du spectre déshabillé. Des échantillons de surfaces nonlinéaires illustrent la modulation des petites vagues par les grandes. L’étude menée en seconde partie quantifie l’impact des nonlinéarités hydrodynamiques sur le phénomène de diffraction et permet de s’affranchir des erreurs dues à l’hypothèse Gaussienne dans l’estimation de paramètres météo-océaniques. Une nouvelle méthode de calcul de l’intégrale de Kirchhoff basée sur des convolutions radiales rapides est aussi introduite. Enfin, l’impact de l’écume en micro-ondes est détaillé et les principaux résultats montrent son importance à forts vents et notamment en polarisation HH."
"L'objet de ce travail était d'étudier l'effet de la surface spécifique sur la dissipation d'énergie à travers un milieu poreux ""modèle"", constitué d'un réseau dense de cylindres verticaux émergeants. Les expériences ont été menées dans un canal hydraulique à surface libre de longueur utile 10 ètres. Trois maquettes ont été réalisées à l'aide de cylindre de diamètres variables. Chacune de ces maquettes, d'une longueur maximale de 4.80m, est constituée d'un réseau régulier de cylindres de diamètre constant. Ce dispositif nous permet d'étudier, à porosité constante égale à 0.7, l'effet des variations de surface spécifique sur l'écoulement. Une première série de mesure a été réalisée pour des courants stationnaires, dans des conditions de vitesse et de hauteurs d'eau variables, une deuxième série sur la propagation de houles régulières. Les résultats en présence d'un courant stationnaire ont montré une influence très significative de la surface spécifique sur la perte de charge à travers le poreux. Cette influence se retrouve sur l'atténuation de l'onde et sur le phénomène d'interférence observé, d'autant plus faible que la structure est longue par rapport à la longueur d'onde. La longueur d'onde dans le milieu poreux est bien décrite par la porosité et le choix adéquat d'une masse ajoutée."
"Le Bar Dicentrarchus labrax est parasité par deux espèces de Monogènes (Monopisthocotylea) congénériques : Diplectanum aequans et Diplectanum laubieri. Les étapes de leur cycle ont été étudiées afin d'estimer leur vitesse de développement et d'identifier les critères de reconnaissance à tout âge. Leur croissance est quasiment synchrone ; les modèles de croissance sont proposés. Aux température voisines de 20°C, un mois est nécessaire, à compter de leur fixation sur les branchies de l'hôte, pour que les deux espèces atteignent le stade adulte ; ils sont alors capables d'engendrer une descendance. Les deux seuls critères permettant une identification certaine de chacun des Diplectanum sont le squamodisque chez les jeunes, le squamodisque et le pénis chez les adultes ; en leur absence, chez les stades précoces, les deux espèces ne peuvent pas être distinguées sur des bases morpho-anatomiques"
"Le cycle de l'azote joue un rôle important dans le fonctionnement des écosystèmes en contrôlant le degré d'eutrophisation. La compréhension des flux biogéochimiques nécessite l'indentification et la mesure de l'intensité des processus sous-jacent. Les méthodes isotopiques basées sur l'enrichissement en 15N permettent de suivre le devenir d'un substrat et de discriminer par exemples le N2 produit par denitrification ou par Anammox ; les relations entre la nitrification (production de nitrate/nitrite) et la dénitrification (consommation de nitrate/nitrite) ; ainsi que la balance entre élimination de nitrate (denitrification ) et son recyclage en ammonium (RDNA, Dissimilative Reduction du nitrate en ammonium). Cette approche sera illustrée par l'étude du cycle de l'azote dans des sédiments marins méditerranéens (golfe de Fos, Etang de Berre) et de mangrove (Goa, Inde) impactés différemment par des activités anthropiques telles que la conchyliculture, des variations de salinité ou des rejets de mines. En parallèle, les communautés procaryotiques, acteurs de ses processus peuvent être appréhendées grâce aux suivis de leurs gènes fonctionnels caractéristiques. Cette approche sera illustrée par une étude de l'impact des variations de la salinité sur les processus du cycle de l'azote, à partir de microcosmes contenant du sédiment de l'étang de Berre. Face aux perturbations, la dénitrification et la nitrate réduction ont présenté une résistance plus importante que la nitrification pour laquelle des shifts de communautés ont été identifiés. Néanmoins, la réponse des écosystèmes aux variations de salinité semble dépendante de stress antérieurs à long terme, précédemment supporté par les communautés naturelles sédimentaires"
"Les travaux présentés ici abordent différentes thématiques de recherche relatives à l'océanographie, en particulier la description de la dynamique océanique à différentes échelles. L'angle choisi pour présenter ces travaux porte sur les atouts et limitations de la modélisation numérique océanique, un outil incontournable de l'océanographie moderne pour simuler la circulation océanique. Ces aspects sont illustrés en se focalisant principalement sur les travaux de recherche menés au cours de ma carrière. Une première partie est centrée sur l'assimilation de données (AD) dans les modèles océaniques physiques et couplés physique-biogéochimiques. Un résultat original de mise en œuvre de filtre de Kalman (SEEK) via une procédure IAU (Incremental Analysis Update) est montré puis un exemple d'assimilation de données physiques et biogéochimiques pour un modèle couplé de l'Atlantique Nord est présenté. Les résultats montrés mettent en avant d'une part l'aspect incontournable de l'utilisation de l'AD en océanographie pour l'amélioration de la compréhension des processus, et d'autre part la complexité de manipulation de ces configurations « modèle – système d'AD » de par leur très grand nombre de degré de liberté et de possibilité de paramétrisation. Une seconde partie est focalisée sur les points forts et les limites de la modélisation océanique à haute résolution (HR). Certains atouts de ce type de configuration à HR sont montrés tels que la capacité à mieux représenter les instabilités dynamiques à méso-échelle, le guidage bathymétrique, les échanges côte-large et à être exploitable pour de nombreuses applications sociétales (échouage de méduse, suivi de polluant, etc...). Puis quelques limitations connues sont évoquées telles que les problèmes d'adéquation avec les forçages (océaniques et atmosphériques) et les limites de validité de certaines hypothèses physiques habituellement codées dans ces modèles classiques (hypothèse hydrostatique, paramétrisation des flux air-mer). Une dernière partie vise à discuter les perspectives de travail dans les domaines évoqués telles que les méthodes innovantes d'AD, les modèles à très HR et leur couplage avec des modèles d'atmosphère ou de prévision de vagues ainsi que l'intérêt croissant de ce type de modèle pour les applications sociétales."
"Deux campagnes ont prospecté dix lagons d'atoll des Tuamotu (Polynésie française, océan Pacifique). Sur des échantillons d'eau, le spectre UV (250-400 nm) in vitro est décrit par l'absorption à 254 nm, A25,i, et la pente, S* (régression de ln:\ vs Î.). Ces deux descripteurs sont négativement corrélés. Les points s'organisent le long d'une hyperbole entre un pôle océanique (fort S*, faible A254) et un pôle confiné (faible S*, fort A254). Le carbone organique dissous (COD) présente des concentrations ([Cl. déterminées par HTCO) peu variables (0, 7-1,0 mg C.L 1), contrastant avec la large diversité des propriétés optiques. [C] et A254 ont une corrélation positive, avec une ordonnée à l'origine significative {0,5 mg c.L-') correspondant à une fraction« non chromophorique ,, du COD. L'absorption spécifique ramenée au carbone, 8254, croît (de 0,4 à 1,3 m2.g-1) pour des [Cl croissantes, surtout (d'après la littérature) en relation avec une augmen-tation du poids moléculaire moyen, qui abaisse les valeurs de S*. Nos mesures optiques décrivent ainsi un gradient de confinement (ou de temps de séjour) qui correspond à un continuum dans la nature du COD, dont son poids moléculaire et donc sa susceptibilité à une action biologique. Cette méthode rapide confirme son efficacité pour la description de la distribution de la MOD."
"Comparées à leurs composantes horizontales (jusqu'à plusieurs dizaines de cm/s), les composantes verticales des courants océaniques sont généralement très faibles (quelques mm/s) dans toutes les régions du monde. En raison de leur rôle majeur dans la distribution verticale des propriétés physiques et biogéochimiques de l'eau de mer, leur connaissance est devenue une sorte de « graal » pour les océanographes. Cependant, leur mesure in-situ représente un véritable défi technique, même en utilisant des instruments sophistiqués tels que les ADCP. Dans ce contexte, nous développons un instrument alternatif, appelé le VVP (Vertical Velocity Profiler). Il s'inspire de plusieurs travaux qui exploitent la différence entre la vitesse verticale réelle Wr d'un planeur sousmarin (~dP/dt, du capteur de pression embarqué) et sa vitesse verticale théorique Wth extraite d'un modèle de vol (e.g. [1]). La vitesse verticale océanique Woc s'exprime ainsi par la simple différence Woc = Wr - Wth en tout point de la colonne d'eau. L’instrument utilise un propulseur qui l’entraîne jusqu'à une profondeur de consigne prédéfinie (jusqu’à 400m actuellement). Une fois la profondeur atteinte, le propulseur est stoppé et le profileur remonte alors lentement (~0,1 m/s) vers la surface sous le seul effet de sa flottabilité. Dans une eau au repos, l'équilibre mécanique entre la flottabilité et la traînée hydrodynamique se traduit par une vitesse verticale de remontée qui ne dépend que de la flottabilité du profileur, de sa géométrie et de la densité de l'eau de mer. Tout écart par rapport à cette vitesse théorique est alors interprété comme un signal de vitesse verticale océanique. Des essais en bassin d'essai, en soufflerie et en mer sont mis en œuvre depuis 2019 afin d'affiner la conception du prototype et d'établir définitivement les paramètres de son modèle de vol."
"Cette thèse s'inscrit dans le domaine de l'imagerie non-destructive en électromagnétisme. L'originalité du travail réside, tout d'abord, dans sa forte connotation expérimentale. Celle-ci a abouti à la construction d'un prototype RADAR capable d'acquérir des données multisources-multistatiques dans la gamme de fréquence [2-4] GHz. De plus, ce système implémente la formation de voies au moyen d'un réseau d'atténuateurs/déphaseurs commandé numériquement.Les expériences menées relèvent, d'une part, de l'imagerie qualitative. Le Retournement Temporel, ainsi que les méthodes DORT et TR-MUSIC, ont été appliqués afin de détecter et localiser des cibles diffractantes. Le cas des milieux réverbérants a notamment été abordé.D'autre part, le prototype a été utilisé dans le cadre de la diffraction inverse quantitative sur des données très limitées en ouverture. Un algorithme itératif non-linéarisé prenant en compte l'aspect multi-fréquentiel des données a été adapté à la configuration expérimentale notamment grâce à une procédure de calibration performante.Enfin, la possibilité de greffer les avantages du Retournement Temporel sur ces techniques quantitatives a été étudiée. L'objectif est l'amélioration des résultats dans des milieux aléatoires proches de ceux rencontrés notamment en imagerie médicale (détection de tumeurs) ou en sondage du sous-sol (détection de mines, de nappes de pétrole)."
"Le lido de Villeneuve-lès-Maguelone (Languedoc-Roussillon) est un archétype de littoral microtidal, dominé par la houle, en érosion, et largement affecté par la submersion marine lors des tempêtes. Les observations réalisées dans le cadre du Système d'Observation Littoral - Trait de Côte (SO LTC) concernent l'hydrodynamique et le transport sédimentaire dans l'avant-côte et la lagune. La tache d'observation MAGOBS (Observatoire Aresquiers-Maguelone-Palavas) de cet observatoire regroupe un dispositif instrumental automatisé composé de 30 points d'acquisition en temps réel entre 6 mètres de profondeur et le haut de plage (à terre) ainsi que dans la lagune. Le dispositif est opérationnel depuis mai 2011. Son déploiement a permis de déverrouiller les difficultés techniques liées à la mesure temps réel en zone littorale : choix du type des appareils de mesure, de la technologie embarquée, des cadences d'acquisition, résolution des questions du transfert de la donnée. Le dispositif produit 2 Tera-octets de données par an. Les données sont stockées sur un ensemble de serveurs dédiés. Les données sont mises en ligne sur le site du SO-LTC (www.soltc.org), immédiatement après premiers traitements et qualification. Elles commencent à être utilisées pour des calculs de niveau d'eau à la côte, et dansdes outils de prédiction des vagues et des niveaux à la côte associant mesure et modélisation."
"L'objectif général de cette thèse est de contribuer à l'avancement de la connaissance de l'impact de l'activité àméso-échelle du Courant Nord (CN) au large du Var sur sa circulation en aval et des interactions de ce courantde bord avec la dynamique côtière, particulièrement dans une baie semi-fermée peu profonde : la baie deHyères. Ces travaux se sont appuyés sur deux configurations numériques réalistes à haute résolution basées surle code NEMO et emboîtées avec AGRIF : une première de la façade méditerranéenne française à unerésolution spatiale de 1,2 km et une seconde le long des côtes varoises à 400 m de résolution.La comparaison des simulations obtenues avec des observations (radar HF, ADCP, glider, SST satellite) apermis de confirmer le réalisme des configurations, et de montrer l'apport d'une résolution de l'ordre de 400 msur la dynamique dans une baie mais également sur le CN et son écoulement en aval.Enfin, une étude de paramétrisation de l'advection horizontale et du mélange vertical a permis d'améliorerl'impact d'un downscaling dans la région d'étude, et particulièrement concernant la représentation de lacirculation au sein de la baie semi-fermée."
"INFORMATIQUE-ROBOTIQUE-ELECTRONIQUE (61 ème section CNU) RESUME-Depuis les années 80 et l'avènement des caméras numériques, une multitude de techniques de capture ou de quantification du mouvement par traitement d'images vidéo ont émergées. Toutes ces méthodes, très différentes les unes des autres, ont pourtant toute un point commun : la dimension temporelle est directement déduite de la fréquence d'acquisition des images vidéo qui, par hypothèse, est toujours supposée connue et stable, tout au long de la séquence d'images analysée. Or, il s'avère que cette condition, fondamentale pour une bonne quantification du mouvement, n'est pas toujours scrupuleusement respectée par les systèmes d'acquisition. Dans cet article, nous proposons un dispositif permettant de montrer que pour certaines caméras numériques, les instants d'acquisition des images ne sont pas aussi réguliers qu'on aurait pu l'espérer. Nous verrons ensuite comment dater précisément chaque image de la séquence pour compenser ce défaut lors de la détermination du modèle cinématique du mouvement observé. Mots-clés-Traitement d'images, capture de mouvement."
"Un nombre croissant de données satellitaires et aéroportées acquises dans le domaine micro-ondes sur la surface demer est aujourd’hui disponible. L’interprétation correcte de ces observations dépend d’une part de la précision desmodèles de diffusion électromagnétiques, et d’une autre part de la maîtrise des propriétés hydrodynamiques etstatistiques de la surface. Ces dernières années ont connu une amélioration considérable des modèlesélectromagnétiques et spectraux. Cependant, certains phénomènes sont encore mal compris et non pris en comptepar ces modèles. En particulier, la variation angulaire de la surface de mer est à ce jour non totalement caractériséeet modélisée. Ce travail de thèse concerne l’étude de cette variation azimutale et des asymétries directionnelles de lasurface de mer. Une première étape consiste à effectuer une analyse expérimentale en se basant sur les données dela littérature mais également sur d’autres jeux de données acquises par l’ONERA et le DSTO. Cette analysepermettra de caractériser les asymétries directionnelles en fonction de la géométrie d’observation, l’état de mer et lafréquence électromagnétique. Une seconde étape consiste à proposer des mécanismes physiques pouvant être àl’origine des asymétries directionnelles. L’asymétrie upwind-crosswind étant essentiellement liée à la fonctiond’étalement du spectre directionnel, notre étude théorique a principalement porté sur la caractérisation del’asymétrie upwind-downwind. Nous étudions l’influence de la prise en compte des formes déferlantes, initialementà travers des formes simples de vagues fortement asymétriques, et ensuite à travers une distribution de pentesexpérimentale prenant en compte ces formes de vagues. Les asymétries obtenues par un modèle deux-échellesprenant en compte ces formes de vagues sont en accord qualitatif avec les asymétries observées pour les bandes defréquences X et L. Une étape supplémentaire consiste ensuite à calculer les asymétries obtenues par un code dediffusion rigoureux sur des profils numérisés d’une expérience en soufflerie et permet la validation des résultatsobtenus avec un modèle deux-échelles."
"Ce travail de thèse s’inscrit dans le contexte général de l’étude de la dynamique de la frangelittorale. Le but de ce travail de thèse était l’étude des couplages entre les ondes de surface (vagues, ondeslongues dont les ondes infra-gravitaires), les variations du niveau marin et les courants ainsi que les variationsà haute-fréquence du niveau d’eau sur la plage et les déplacements de la nappe phréatique au sein de la plagesableuse. L’approche retenue est essentiellement expérimentale in-situ.La campagne de mesure ROUSTY201412 a permis de récolter un jeu de données hydro- morphodynamiquesconsidérable, qui viendra alimenter l’étude des plages sableuses dans de nombreux domaines. Elle permeten particulier de décrire l’ensemble d’une saison hivernale sur un même site soumis à une grande variété declimats de houles et de vents, ce qui en fait un jeu de données unique dans le contexte méditerranéen.Dans cette thèse nous nous intéressons à trois domaines d’étude : la dynamique de la surface libre avecl’analyse des relations existantes entre les différentes échelles (vagues, variations du niveau marin, ondeslongues, wave-setup, ...), la structure et l’évolution du courant ainsi que le lien avec la dynamique de surface,et la circulation souterraine dans la nappe de plage et en particulier sous la zone de swash."
"Cette thèse de doctorat traite d'un aspect particulier des intéractions houle-courant-bathymétrie, à savoir l'influence de la vorticité sur un phénomène de résonance de Bragg. Pour l'aspect modélisation, une analyse numérique est présentée, basée sur la comparaison de deux modèles hyperboliques - Pente Douce et Pente Douce Modifiée - étendus pour intégrer des courants cisaillés. L'influence de deux paramètres caractéristiques du courant, le cisaillement ( considéré comme constant sur la colonne d'eau) et la célérité en surface, est évaluée pour établir une première tendance vis-à-vis du comportement de la houle. Pour aller plus loin, une campagne expérimentale en canal à houle a fourni un jeu de données originales sur la propagation d'une houle régulière de fréquence variable, réfléchie par une bathymétrie ondulée et un courant cisaillé évolutif. Une analyse poussée a confronté les résultats numériques - basés sur différentes représentations du courant - aux données expérimentales, et plusieurs conclusions fortes en sont ressorties. Sous l'influence d'un courant cisaillé, le pic de Bragg se trouve réduit, en amplitude comme en fréquence, mais un pic de réflexion inattendu survient pour des houle à hautes fréquences ( entre 1.25 et 1.45Hz). Ensuite, si des modélisations précises du comportement du courant nous ont mené à une très bonne approche de l'intensité de la réflexion de Bragg, la fréquence pic associée a systématiquement été surestimée par les modèles. Grâce à une profonde remise en question de la théorie et une collaboration des membres de l'équipe, une nouvelle modélisation a vu le jour, avec une décomposition originale du potentiel des vitesses pour tenir compte de la dissymétrie des nombres d'onde incident et réfléchis (dissymétrie causée par le courant et sa vorticité). Les premiers résultats montrent qu'avec cette nouvelle théorie, la détermination de la fréquence pic n'est plus un problème. Cette nouvelle approche de la réflexion, par fortes variations bathymétriques et en présence de courant à structure variable, promet de belles découvertes sur l'influence de la vorticité."
"La mer Méditerranée est souvent considérée comme un océan laboratoire pour comprendre les changements globaux liés à l’augmentation de CO2 atmosphérique. Ce travail, basé sur l’étude de données recueilles dans trois régions méditerranéennes, étudie les variations du CO2 océanique dans ce bassin. À l’échelle de la saison, outre les changements de température, le contenu en alcalinité influe sur le contenu en CO2 en Méditerranée orientale, tandis que les changements en carbone total sont responsables des variations dans le bassin occidental. En zone côtière urbanisée, l’émission de CO2 anthropique conditionne les échanges air-mer de CO2. Cette étude montre que l’augmentation de carbone et l’acidification à l’échelle de plusieurs années ne sont pas seulement dues à l’augmentation du CO2 atmosphérique : le contenu en alcalinité module ces tendances dans le bassin oriental, tandis que, dans le bassin occidental, ces tendances sont vraisemblablement influencées par la dynamique des courants."
"D'initiative française, le projet international HyMeX a pour objectif d'améliorer la compréhension du cycle de l'eau en Méditerranée, de sa variabilité, de l'échelle de l'événement météorologique aux échelles saisonnières et interannuelles, et de ses caractéristiques sur une décennie, dans un contexte de changement global. Le projet est motivé par le rôle déterminant des processus de mésoéchelle, couplés entre l'atmosphère, la mer et la terre, sur la variabilité du système climatique et sur le déclenchement d'événements hydrométéorologiques extrêmes (précipitations et inondations, vents forts et convection océanique, canicules et sécheresses). Le projet vise enfin à évaluer les conséquences de ces événements extrêmes sur la vulnérabilité sociale et économique de cette région et sa capacité d'adaptation."
"Les risques hydrométéorologiques sont au coeur du programme de recherche HyMeX sur le cycle de l'eau en Méditerranée. HyMeX a pour ambition d'améliorer leur prévision, de mieux connaître leur variabilité à l'échelle pluriannuelle et de renseigner sur leur évolution, dans le contexte du changement climatique. À cette fin, deux campagnes de mesures dédiées à ces événements extrêmes (SOP1 : pluies intenses et crues rapides ; SOP2 : vents forts et formation d'eaux denses) sont organisées en Méditerranée nord-occidentale, entre septembre 2012 et mars 2013."
"Réserves sous réserves... Pour y voir plus clair. La charte graphique de "" Mer Vivante "" des espaces marins protégés par des limitations ou interdictions de pêche. Mer Vivante 2013."
Ecologie du dinoflagellé benthique toxique Ostreopsis cf. ovata en Méditerranée Nord-Occidentale.
"Le dinoflagellé Ostreopsis : petite cause grandes conséquences. GisPosidonie : Plus de 30 ans au service de la protection et de la gestion du milieu marin. Le Diréach L. et Boudouresque C.F. eds., Gis Posidonie publ., Marseille : 151-154."
not available
Not available
not available
not available
"En Méditerranée, la macroalgue du nord-est de l’Atlantique Saccorhiza polyschides (Lightfoot) Batters n’est pas commune. Les seules populations permanentes sont celles de la mer d’Alboran, près du détroit de Gibraltar et du détroit de Messine (Italie). Depuis le début du xixe siècle, plusieurs observations, sur des coques de bateaux ou dans des ports, traduisent la dispersion de propagules en Méditerranée; l’espèce n’y a pas dépassé le stade d’adventice. L’observation de Saccorhiza polyschides près du port de Jijel (Algérie), la première nouvelle observation en Méditerranée depuis plus d’un siècle, montre que la dispersion de propagules en Méditerranée se poursuit de nos jours. De plus, grâce à sa taille et à la facilité de son observation, elle contribue à illustrer les voies d’expansion d’une espèce à la limite de son aire actuelle de distribution."
not available
not available
"Cystoseira granulata C. Agardh var. turneri Montagne a été décrit par Montagne (1838) d’Algérie (Sud de la Méditerranée). Par la suite, Agardh (1842) a renommé ce taxon C. montagnei J. Agardh, sur la base de spécimens de France et du Nord de l’Adriatique (Nord de la Méditerranée) qu’il croyait identiques au taxon de Montagne. Enfin, Sauvageau (1912) a décrit C. spinosa Sauvageau et C. adriatica Sauvageau comme des nomina nova, y incluant en partie le concept de C. montagnei de J.G. Agardh et de divers auteurs ultérieurs. Ces traitements taxonomiques ont créé la confusion quant à la délimitation de ces taxons et des doutes sont apparus sur la valeur taxonomique du taxon de Montagne, aujourd’hui souvent placé parmi les taxa inquirenda dans les inventaires et les flores. Depuis 2014, nous avons récolté près d’Alger (Algérie) une espèce de Cystoseira qui constitue des forêts clairsemées entre 10 et 25 m de profondeur. Nos spécimens correspondent tout à fait à la description originale ainsi qu’au syntype du taxon de Montagne. Ils sont bien caractérisés et se distinguent de C. montagnei J. Agardh et de tous les autres taxons de Cystoseira décrits à ce jour par : la présence d’un tronc portant des tophules épineux quand ils sont jeunes, devenant lissestuberculés quand ils sont plus vieux ; les rameaux rimaires qui sont soit légèrement comprimés avec une nervure peu marquée et des ramifications irrégulièrement alternes dans un plan, soit cylindriques et ramifiés dans toutes les directions, avec des épines courtes et espacées ; des réceptacles à la fois basaux-intercalaires, juste au-dessus du tophule, et terminaux sur des ramules. Ici, nous proposons le nom de Cystoseira michaelae Verlaque et al. nom. et stat. nov. pour le taxon de Montagne (Cystoseira granulata C. Agardh var. turneri Montagne) et la lectotypification de l’espèce sur la base du protologue et d’un spécimen appartenant au syntype de Montagne, conservé au Muséum National d’Histoire Naturelle de Paris (PC). Cystoseira michaelae semble être une espèce endémique de l’Algérie et du Nord de la Tunisie. Par ailleurs, nous proposons la lectotypification de C. montagnei J. Agardh, sur la base d’un spécimen original de J.G. Agardh, appartenant au syntype conservé au Botanical Museum de Lund University (LD). L’étude des lectotypes de Cystoseira spinosa Sauvageau et de son synonyme C. adriatica Sauvageau confirme que ce sont des synonymes de C. montagnei J. Agardh."
not available
Projet OSCREEN : Screening sur le littoral français méditerranéen du stock d'Ostreopsis sp. macroalgal.
Cartographie à grande échelle des écosystèmes au niveau de la mer. Mer Vivante 2013.
Projet GIREL - Pilote CYSTORE : Valorisation écologique lagunaire- Etat initial du site de transplantation et de prélèvement.
Note Naturaliste. Evaluation écologique du littoral rocheux de l'Aire Marine Protégée de Karaburini-Sazani-Méthode Carlit- Mission du 27 mai au 01 juin 2013. Initiative pour les petites îles de Méditerranée.
Trente années d'évolution des interdictions de prélèvements dans les AMP littorales des côtes françaises de la Méditerranée.
"Distribution de Cystoseira amentacea var. stricta et des encorbellements de Lithophyllum byssoides - Site Natura 2000 FR9301602 "" Calanques - Iles Marseillaises Cap Canaille - Massif du Grand Caunet ""."
CYSTORE. Valorisation écologique des ouvrages maritimes par la transplantation des algues du genre Cystoseira.
Côtes méditerranéennes françaises : inventaire et impact des aménagements gagnés sur la mer.
"MEDAM.org : inventaire et impact des aménagements gagnés sur le domaine marin - côtes méditerranéennes françaises. Laboratoire Ecomers, Université Nice Sophia Antipolis. Publication électronique : www.medam.org."
Projet GIREL - Pilote CYSTORE : Valorisation écologique des digues - Etat initial.
"Le radar HF est actuellement le seul instrument courantométrique permettant d'obtenir une description synoptique à haute résolution spatiale et temporelle de la circulation côtière de surface. Un système radar déployé depuis 2010 en Méditerranée sur les côtes varoises offre pour la première fois une description exhaustive de la circulation, encore peu documentée dans cette zone.La cartographie des courants se fait classiquement en combinant les mesures d'au moins deux radars. Cependant des résultats significatifs ont été obtenus avec un seul radar concernant : l'identification de tourbillons méso-échelle ; la signature de phénomènes périodiques affectant la circulation superficielle dans les bandes diurne, inertielle et semi-diurne ; et les caractéristiques et les instabilités du Courant Nord Méditerranéen (CN).L'assimilation des mesures radar au moyen d'un lisseur de Kalman d'ensemble dans un modèle régional de la Méditerranée Nord Occidentale a été réalisée pour la première fois dans la zone d’étude. Cette méthode, qui contraint les courants de surface en optimisant le vent et les forçages aux frontières ouvertes, améliore la description de la veine du CN en vitesse et position"
"La compétition est un élément central du processus évolutif, et la silicification ne fait pas exception: entre les organismes biominéralisés et non biominéralisés, entre les organismes biominéraliseurs siliceux et non siliceux, et entre différents groupes silicifiants. Nous discutons ici de la compétition évolutive à différentes échelles et de la manière dont cela a affecté les cycles biogéochimiques du silicium, du carbone et d'autres nutriments. Au cours des temps géologiques, nous examinons comment les fossiles, les sédiments et la géochimie isotopique peuvent fournir des preuves de l'émergence et de l'expansion de la biominéralisation de la silice dans l'océan et de la compétition pour l'acide silicique entre les organismes silicifiants . Les données métagénomiques provenant d'environnements marins peuvent être utilisées pour illustrer la compétition évolutive entre des groupes d'organismes marins silicifiants et non silicifiants. Les écosystèmes modernes fournissent également des exemples de courses aux armements entre les silicifiants en tant que prédateurs et proies, et comment la silicification peut être utilisée pour fournir un avantage concurrentiel pour l'obtention de ressources. En étudiant la biologie moléculaire des espèces silicifiantes et non silicifiantes, nous pouvons établir comment elles ont réagi aux interactions compétitives observées et comment les solutions ont évolué grâce à une dynamique évolutive convergente."
Nous étudions la propagation d’un paquet d'ondes à fréquence modulée dans une eau de profondeur finie en présence d’un courant. On a montré que la distance et l’amplitude de focalisation dépendent essentiellement du sens et de l’intensité de l’écoulement ainsi que de la fréquence d’excitation des ondes. Les résultats obtenus sont en bon accord avec ceux des simulations numériques.
"We investigate the performance of a phase-resolved algorithm for the prediction of non-linear ocean wave fields based on spatio-temporal remote optical measurements of free surface elevations. To quantify the prediction accuracy, error estimates are calculated for the case of a unidirectional wave field by ensemble averaging a large number of synthetic data sets. For several characteristic wave steepnesses, we compare results based on linear and weakly nonlinear approaches. It is shown that our nonlinear wave model, based on a Lagrangian description of the free-surface dynamics, allows to catch relevant nonlinear effects that play an increasingly large role as the waves get steep, i.e., wave shape asymmetry and phase shift. Experimental data are also used to illustrate the performance of the method applied on real field measurements."
"Global environmental changes are challenging the structure and functioning of ecosystems. However, a mechanistic understanding of how global environmental changes will affect ecosystems is still lacking. The complex and interacting biological and physical processes spanning vast temporal and spatial scales that constitute an ecosystem make this a formidable problem. A unifying framework based on ecological theory, that considers fundamental and realized niches, combined with metabolic, evolutionary, and climate change studies, is needed to provide the mechanistic understanding required to evaluate and forecast the future of marine communities, ecosystems, and their services."
"De nombreux modèles empiriques de distribution statistique du fouillis de mer existent mais ne sont pas directement paramétrable par l’état de mer, et ainsi il n’est pas possible d’y pratiquer une méthode d’inversion. Pour modéliser la distribution statistique de l’intensité, nous utilisons un modèle deux-échelles (TSM) paramétré directement par l’état de mer via la mss (mean square slope). Ce modèle permet de retrouver de manière cohérente la NRCS mais ne parvient pas à décrire de manière la distribution du fouillis de mer simultanément dans les deux canaux de polarisations directs dû à une surestimation du rapport de polarisation (RP) de Bragg.Pour corriger ce problème, nous avons développé une formulation originale du RP qui inclue un unique paramètre effectuant une transition dynamique entre le régime de Bragg et de Kirchhoff. Cette formulation intégrée au modèle TSM permet de corriger le RP et d’obtenir une modélisation cohérente de la statistique du fouillis de mer, simultanément dans les deux polarisations. A l’aide d’une méthode d’inversion, le modèle permet d'estimer l’état de mer d’un radar imageur. La pertinence de ce modèle est établie dans différentes configurations, mais aussi pour des données présentant des incertitudes de calibration."
"Les polymères à empreintes ioniques (IIPs) sont des matériaux poreux hautement réticulés présentant des cavités de reconnaissance spécifiques d'un ion cible, leur permettant ainsi d'avoir une sélectivité élevée. Le travail de thèse présenté se concentre sur la préparation de polymères à empreintes ioniques fluorescents du plomb (Il), c'est-à-dire capables de transformer la reconnaissance de cet ion en un signal de fluorescence, en vue de leur application pour la détection de ce contaminant en milieu marin. Ainsi, dans une première étape, la stratégie adoptée a été de sélectionner un ligand fluorescent original, sélectif du plomb, de l'étudier et de l'utiliser pour l'élaboration des polymères. A partir de ce ligand, un monomère fluorescent de type styrénique a donc été synthétisé (ANQ-ST) dont le signal de fluorescence est exalté lors de l'ajout de plomb (11). La deuxième étape a été consacrée à l'élaboration d'IIPs du plomb (11) à base d'ANQ-ST. Différents paramètres ont été testés: le solvant de polymérisation, la nature de l'agent de réticulation, le diméthacrylate d'éthylène glycol (EGDMA) ou le divinylbenzène (DVB), ainsi que le ratio de monomère fonctionnel (ANQ-ST) par rapport à l'agent de réticulation (2 % et 5 % molaires). Un panel diversifü de techniques de caractérisation a permis d'étudier les propriétés structurales des différents polymères synthétisés ainsi que de valider l'intégration du monomère fonctionnel ANQ-ST dans la matrice polymère. La dernière étape a consisté à évaluer les performances des IIPs pour la détection du plomb (11) par fluorescence Les polymères préparés avec l'EGDMA et 5 % de monomère fonctionnel présentent les meilleurs résultats. Pour ces polymères, l'intensité de fluorescence des IIPs augmente fortement en présence de plomb (11). De plus, la réponse est très peu impactée par l'ajout d'une espèce ionique interférente, contrairement à leurs analogues non-imprimés, soulignant l'efficacité de l'effet d'empreinte. Des droites de calibration ont été établies en milieu aqueux à différents pH et dans différentes matrices, paramètres qui n'ont pas eu d'influence majeure. Les limites de détection obtenues, de 2, 1 et 2,4 µg.L-1, sont inférieures à la recommandation de 1 O µg.L-1 de l'OMS. Ces résultats ont permis de valider avec succès l'utilisation des IIPs fluorescents synthétisés dans le cadre de ce travail de thèse pour la détection du plomb (Il) dans des échantillons naturels en particulier marins."
"Le travail présenté dans cet article propose une analyse des spikes radar créés par une surface de mer, illuminée en incidence rasante, à partir des données radar acquises pendant la campagne MARLENE (pour MediterraneAn RFC and cLutter ENvironmental Experiment). De nombreux résultats à faibles angles de rasance peuvent être établis grâce aux données radar disponibles à plusieurs fréquences, conditions de mer et angles d’incidence. La haute résolution spatiale des systèmes radar utilisés a permis une étude précise des spectres Doppler. Ceux-ci contiennent de très fortes vitesses Doppler moyennes révélant la présence de diffuseurs rapides. Le calcul de cartes de vitesses Doppler est alors intéressant pour l’étude des spikes de mer. Cet article introduit des résultats de l’analyse des distributions croisées entre NRCS (Normalized Radar Cross Section) et vitesses Doppler."
"Le Bar ou Loup Dicentrarchus labrax est un poisson téléostéen faisant l'objet d'un élevage intensif. Il est notamment l'hôte d'ectoparasites branchiaux de la classe des Monogènes, et à cycle biologique direct. Quand le confinement et les densités de l'hôte augmentent, les populations de ces Plathelminthes peuvent s'accroître et induire alors des mortalités importantes. C'est en particulier le cas de Diplectanum aequans, un Monopisthocotylea contre lequel aucun traitement efficace n'est connu. Les effets d'un anthelminthique, le nitroxinil© (ou nitroxynil©), ont été testés in vitro sur ce Monogène. Les parasites ont été soumis à 4 heures de traitement et à des concentrations variant de 50 à 2000 mg.l-1 . Les Diplectanum réagissent à cette molécule par trois principaux types de comportements : perte de mobilité spontanée, perte de l'excitabilité et/ou décrochage. Cette variabilité comportementale et l'efficacité de la molécule ont été analysées en fonction des différentes concentrations et du temps."
Le but du projet de thèse est de tester l'hypothèse théorique : Plus un individu est soumis à un environnement fluctuant plus sa plasticité est grande
"Nous présentons ici les problèmes liés à l'exploitation et à la surexploitation des ressources marines naturelles, comme la mise sur le marché de poissons issus de la pêche de qualité sanitaire inconnue ou en deçà des critères légaux et l'appauvrissement voire l'épuisement des stocks naturels de poissons. Nous décrivons ce que représente l'aquaculture mondiale aujourd'hui en termes de quantité et de qualité pour l'alimentation humaine et discutons des conditions à mettre en place pour que celle­ci puisse devenir une solution permettant de proposer à l'humanité les produits de la mer indispensables à son alimentation."
"Le matériel particulaire du néphéloïde benthique, situé au large de l'embouchure du Rhône, a été prélevé à partir d'un SYstème de Mesures et de Prélèvements HYdrologiques (SYMPHY) permettant d'échantillonner le gradient de concentration des suspensions à proximité du fond. Les mesures courantométriques, hydrologiques, néphélométriques et l'analyse du particulaire mettent en évidence différentes étapes de dispersion matériel rhodanien sur le plateau continental et permettent de détailler les mécanismes qui contrôlent l'évolution des suspensions au cours de leur transfert dans une mer sans marée importante. Le néphéloïde benthique, défini par un gradient vertical de concentration, résulte de l'équilibre entre les composantes horizontales (apports rhodaniens) et verticales (panache de surface) du flux de particules. variabilité de cet équilibre, contrôlée par les fluctuations dynamiques, induit des temps de résidence plus ou moins longs, lamodification de la nature du matériel particulaire par des processus biogéochimiques."
Culture et detection d'une Archaea méthanogène.
Bactéries de l'extrême.
"L’environnement maritime du littoral implique de nombreux processus complexes, mais le manque de données en haute résolution couvrant une large zone sur une longue période est souvent l’obstacle principal à des recherches plus approfondies. Le radar haute-fréquence (HF) est un moyen de faire de la télédétection afin d’obtenir pratiquement en temps réel de l’information sur la surface de la mer et sur une large zone. Ainsi l’étude de l’inversion des paramètres marins à partir de données issues de radars HF est réellement porteuse de sens. Cette thèse fait l’usage d’un jeu de données collectées durant 13 mois par deux réseaux de radar HF à commande de phase pour étudier les caractéristiques de signaux d’échos de la mer, étudier les données à traiter et les méthodes d’inversion, calculer les paramètres de la surface de la mer et évaluer la précision de l’inversion radar des paramètres de la houle.Cette thèse se réfère à l’onde de sol radar HF, dont les ondes radio interagissent avec l’océan du fait de la diffraction de résonance de Bragg. Nous passons en revue l’historique et les applications du radar HF. Nous rappelons les bases de la théorie des ondes électromagnétiques. Nous décrivons les principes d’inversion des courants de surface de la mer, direction du vent et paramètres de houles. La faisabilité de l’inversion de paramètres de houle est examinée. A partir de l’analyse théorique et des études statistiques de nombreux échantillons de données, cette thèse propose une série de méthodes sur le traitement du signal brut et le contrôle de qualité, ce qui inclut la détermination du niveau de bruit, le moyennage des données dans l’espace et le temps, l’identification correcte des pics spectraux, le seuil de largeur de pic, etc. Respectant les caractéristiques de différents processus physiques, les inversions de courant et de vent utilisent des spectres collectés toutes les 20 minutes ; l’inversion des paramètres de houle utilise des spectres moyennés sur 1 heure. Les statistiques des spectres utilisés pour le calcul des paramètres de houle sont présentées pour chacune des stations. Un ensemble de programme efficaces de calculs automatiques avec une complexité algorithme réduite sont développés pour réaliser le traitement et en tirer les paramètres marins.Les vitesses de courants radiales sont obtenues à partir d’une unique station radar. Les champs de vecteurs de courants sont obtenus en combinant chaque station. On montre une année de débit moyen dans la mer d’Iroise, ainsi que le calcul de la vorticité et de la divergence. On étudie un ensemble de données d’un mois du radar SeaSonde de Qingdao. Les schémas de débit moyen, ainsi que la vorticité et la divergence sur un mois sont présentés.La direction relative du vent par rapport à la direction de visée du radar est mesurée à travers le ratio des amplitudes des pics de Bragg. Différents modèles empiriques sont employés pour obtenir la vitesse relative du vent par inversion radar. Les résultats présentés sont en accord avec les estimations prédites par le modèle. Différents modèles de distribution directionnelle sont utilisés pour mesurer le facteur de diffusion pour la mer d’Iroise.Cette thèse se concentre sur l’étude des paramètres de houle. Les résultats sont validés à l’aide de bouées et de données du modèle de vagues (Wavewatch III). L’estimation montre que la précision de la fréquence de houle est très bonne, la précision sur la hauteur significative de houle est très raisonnable et la précision sur la direction absolue de la houle est faible. La cohérence des mesures par chacune des stations radars est vérifiée par comparaison entre les deux. L’utilisation conjointe des échantillons est également prise en charge pour réaliser l’inversion. L’utilisation de deux radars n’améliore pas seulement la précision, mais résout aussi l’ambiguïté de direction relative de houle à partir d’une unique station et donne la direction absolue de vague avec une certaine précision."
"De plus en plus de données satellitales ou aéroportées acquises au dessus de la surface de la mer sont disponibles notamment dans la gamme micro-ondes. Pour interpréter correctement ces données, il est nécessaire de disposer d'une part d'un modèle de diffusion qui soit capable de prendre en compte l'aspect multi-échelles de la surface de mer et d'autre part une bonne représentation spectrale de la surface de mer. Ces dernières années, plusieurs modèles de diffusion électromagnétiques unifiés (capables de prendre en compte la diffusion électromagnétique pour les petites et grandes vagues) ont été développés sous statistiques gaussiennes de la surface de mer. Cependant, ces modèles sont insuffisants pour interpréter les observations lorsque différents jeux de données (multi-bande et multi-incidence) sont confrontés. Le plus de cette thèse est de progresser dans une modélisation cohérente de ces données radar.La première étape est d'incorporer les aspects non-gaussiens de la surface de mer, connus pour influer significativement sur la section efficace de rétrodiffusion (SER). Cela est réalisé dans le cadre du modèle électromagnétique ""Weighted Curvature Approximation » (WCA) en introduisant le kurtosis des pentes et en se limitant à la SER omnidirectionnelle et à la polarisation verticale.Ces corrections permettent une meilleure modélisation de la section efficace radar mais ne sont pas suffisantes pour obtenir un accord avec les données dans toutes les configurations (bande, incidence, vent). Cela suggère une amélioration nécessaire du spectre des vagues courtes, qui fait l'objet de la deuxième partie de ces travaux de recherche.Un nouveau spectre omnidirectionnel est calculé afin d'obtenir une meilleure modélisation de la SER omnidirectionnelle en polarisation verticale tout en respectant des contraintes a priori sur les pentes mesurées par des techniques optiques. Ce spectre s'avère assez semblable au spectre unifié d'Elfouhaily, avec quelques différences notables cependant dans la gamme des échelles décimétriques."
"La mission spatiale internationale Mars96 devait emporter vers la planète rouge un aérostat développé en France par le CNES. Le guiderope métallique de cet aérostat devait contenir un radar de sondage de la subsurface jusqu'au kilomètre de profondeur. La mission du ballon sur Mars96 a été reportée puis annulée. Ce mémoire présente et discute l'essentiel des développements de ce nouvel instrument radar, certaines parties spécifiques et originales sont détaillées. L'évaluation de performance, l'étude de la dynamique des signaux avec adaptation de l 'instrument, et l'évaluation de la quantité d'information disponible terminent ce mémoire."
"Biofutur, "" Vivre dans les environnements extrêmes "", N° 336,"
"Les enquêtes itinérantes auprès des pêcheurs aux lignes fournissent des indications sur des sessions de pêche incomplètes. Dans ce cas, il est recommandé d'estimer le rendement de pêche en effectuant la moyenne des ratios (captures divisées par temps de pêche), calculés pour chaque session de pêche après avoir éliminé les sessions les plus courtes. La durée de pêche minimale pour retenir une session varie suivant la valeur du paramètre à estimer et la précision requise (fixée arbitrairement au niveau d'un coefficient de variation de 30 %). Les captures par unité d'effort de truite fario Salmo trutta, sur six parcours à salmonidés répartis à travers la France, sont en moyenne de 1,6 poisson/h. La durée minimale préconisée pour cette espèce est de 15 min. Le même paramètre estimé pour le sandre Stizostedion lucioperca et le brochet Esox lucius, sur huit parcours répartis à travers la France, prend respectivement la valeur de 0,040 poisson/h et 0,027 poisson/h. La durée minimale préconisée pour ces deux espèces est de 1 h. Mots-clés : enquête itinérante, pêcheurs aux lignes, estimation du rendement de pêche, durée de pêche minimale. MINIMUM FISHING SESSION DURATION FOR CATCH RATE ESTIMATION IN ROVING CREEL ANGLER SURVEYS."
"Dans cet article de synthèse, les observations sur la structure des communautés de plancton sont analysées à la fois pour des expériences de fertilisation artificielle en fer et pour des expériences consacrées à l'étude de systèmes à fertilisation naturelle en fer dans les secteurs Atlantique, Indien et Pacifique de l'océan Austral dans la POOZ (Zone océanique ouverte de façon permanente) et la PFZ (Zone du front polaire). Les observations effectuées dans des systèmes naturels sont combinées à celles de systèmes artificiellement perturbés, afin d'évaluer l'évolution saisonnière des communautés pélagiques, en tenant compte des facteurs de contrôle liés aux cycles de vie et à l'écophysiologie des organismes dominants. L'analyse considère plusieurs types de communautés planctoniques, y compris les autotrophes et les hétérotrophes. Ces communautés sont spatialement séparées en raison de stratégies de vie différentes. Un schéma général conceptuel est proposé pour prendre en compte ces observations et leur variabilité, quel que soit le type d'expérience. Les diatomées peuvent être séparées en 2 groupes: le groupe 1 comprend des cellules à croissance rapide légèrement silicifiées, réparties de manière homogène dans la couche mélangée de surface, et le groupe 2, des cellules à croissance lente fortement silicifiées au sein de couches discrètes. Au cours de la saison de croissance, les diatomées du groupe 1 présentent une succession saisonnière typique d’espèces dominantes, dans des créneaux de développement conditionnés par des facteurs physiques (lumière et température) ainsi que par des rythmes endogènes spécifiques (horloge interne) et la biomasse est contrôlée par la disponibilité des nutriments. Les diatomées du groupe 1 ne sont pas directement broutées par le mésozooplancton, qui est alimenté par le protozooplancton, ce qui relie le réseau trophique microbien aux niveaux trophiques supérieurs. Au lieu de cela, les espèces dominantes successives du groupe 1 sont dégradées par l'activité bactérienne à la fin de leur saison de croissance. Des fragments de détritus organiques nourrissent le protozooplancton et le mésozooplancton. L'efficacité de la pompe de silicium entraîne la disparition progressive de l'acide silicique dans les eaux de surface. En revanche, le groupe 2 résiste au broutage en raison de sa forte silicification et sa biomasse s'accumule de manière continue mais relativement lente tout au long de la période de production. Les diatomées du groupe 2 se concentrent au niveau de la pycnocline saisonnière ou à proximité de celle-ci et bénéficient donc des flux d'éléments nutritifs ascendants par mélange diapycnal. La diminution de la lumière et le mélange convectif profond en automne entraînent à la fois une limitation de la lumière et des éléments nutritifs, ce qui entraîne une exportation massive de carbone des diatomées du groupe 2, événement annuel majeur de la pompe biologique. Ce schéma décrit l'évolution saisonnière des communautés de plancton dans les eaux de surface de l'océan Austral. Le système peut sans doute être étendu aux écosystèmes caractérisés par une prolifération saisonnière sous l'influence du fer ou d'autres nutriments."
"Les régions polaires sont particulièrement impactées par les changements climatiques en cours qui ont de profondes conséquences sur le développement du phytoplancton et le fonctionnement de la pompe biologique (transfert vertical de carbone organique particulaire depuis l’océan de surface vers l’océan profond). La structure des communautés planctoniques et les propriétés spécifiques aux espèces (taille, forme, contenu cellulaire en Si et C, état physiologique, stade de vie, etc.) influencent les cycles biogéochimiques et l'export de la matière organique (e.g. C, N) et biominérale (e.g. Si). Aux hautes latitudes, les diatomées contribuent de manière importante à la production primaire et présentent une grande diversité d'espèces. Cette thèse a pour but (1) de déterminer les facteurs contrôlant la structure et le développement des communautés de diatomées dans les régions polaires, (2) d'évaluer l'influence de la diversité des espèces sur la composition et la stœchiométrie de la matière particulaire, et (3) de comprendre les modes de mortalité et d'export des diatomées dans l'océan profond. Ces travaux reposent sur l'étude de deux régions subpolaires situées aux antipodes : (1) la baie de Baffin, une zone de glace saisonnière située en Arctique (programme Green Edge), et (2) la région des Kerguelen située dans la zone ouverte en permanence de l'océan Austral (programme MOBYDICK), où nous avons comparé deux zones contrastées, l'une dite HNLC (High Nutrient Low chlorophyll), l'autre influencée par le plateau naturellement fertilisé par le fer. Dans la baie de Baffin, nous avons mis en évidence une succession des communautés de diatomées en lien avec la fonte de la banquise et le gradient des masses d'eau. Une communauté peu productive constituée de diatomées pennées est présente sous la banquise (condition pré-bloom), dans les eaux dépourvues de glace mais influencées par le Pacifique, ou encore à l'extrémité́ est de la baie (condition post-bloom). À l'inverse, un bloom de diatomées centriques (e.g. Chaetoceros, Thalassiosira) et pennées, activement en train de silicifier, se développe dans la zone marginale des glaces. Ces résultats montrent l'importance de la fonte de la banquise pour le développement du bloom printanier mais également d'autres paramètres tels que : le gradient des masses d'eau (Atlantique vs. Pacifique), la bathymétrie, ou encore la fonte de la couverture neigeuse. Dans la région des Kerguelen, nous avons confirmé le rôle du fer avec l'identification de deux communautés distinctes situées de part et d'autre du plateau. Dans la zone HNLC, les diatomées sont davantage silicifiées — ce qui s'explique par la composition taxonomique de l'assemblage et par une réponse physiologique des diatomées résultant d'une limitation par le fer — ce qui conduit à une augmentation des rapports BSi:POC des diatomées et de la matière particulaire. Une communauté inactive se trouve systématiquement dans le gradient de la pycnocline, issue probablement de la sédimentation rapide de certaines espèces/stades de vie plus denses et davantage résistants au broutage et/ou aux processus de reminéralisation. Dans la zone HNLC, cette communauté est constituée principalement de frustules vides de Fragilariopsis kerguelensis et de Chaetoceros atlanticus, qui transportent efficacement le Si sous la couche de mélange. Nos résultats révèlent une forte pression exercée par le micro- et le mésozooplancton sur F. kerguelensis, une espèce habituellement considérée comme résistante au broutage. Au dessus du plateau des Kerguelen, l'accumulation de spores de Chaetoceros encore intactes permet le transport du C et du Si en profondeur, ce qui souligne l'importance des stades de vie dans l'export de la matière particulaire. Grâce à l'utilisation du Bottle-net, un nouvel instrument permettant d'échantillonner les couches profondes de l'océan, nous avons montré que près de 93 % des particules >20 μm collectées dans les couches intermédiaires (125-500 m) et profondes (>500 m) sont des cellules individuelles de diatomées — les pelotes fécales et les agrégats ne représentant que 2 % des particules collectées — ce qui montre l'importance des petites particules dans les flux d'export. De manière générale, cette thèse met en exergue la nécessité d'étudier la composition spécifique des communautés de diatomées afin de de comprendre de manière plus fine le fonctionnement de la pompe biologique de carbone et de prédire son rôle futur dans le contexte des changements climatiques."
"Notre projet concerne le développement d’un jeu de dix drones de surface motorisés et coordonnés pour mesurer les variations spatiales de la température de surface de la mer, et fournir pour la première fois des champs de température superficielle de la mer à dix mètres de résolution sur une zone de 50 m x 50 m avec une précision de 0.002°C, et à 10 cm de profondeur, ce qui est sans précédent, au mieux de notre connaissance. On espère ainsi visualiser des variations spatiales de sub-submesoéchelle correspondant soit à des circulations de Langmuir sous l’effet de convergence-divergence du courant de surface généré par le vent, soit à des tourbillons en température, soit à des schémas spatiaux de température de surface correspondant à des risées, à des plumes ou thermiques (wind gustiness), ou à des phénomènes de circulation océanique de surface. Le projet est initié, un prototype a déjà été réalisé, un second prototype est en cours de réalisation, et la demande que nous soumettons ici à LEFE-IMAGO porte sur l’extension du projet à une flotte de dix drones. Référence : Puigserver Cécile, Denis Bourras, Luneau Christopher, Jean-Luc Fuda, Hubert Branger, et al.. Observation des variations spatiales de la SST à sub meso-échelle : quelle stratégie de mesure adopter ? AEI 2019, Atelier d’Expérimentation et d’Instrumentation de l’INSU, Jul 2019, Lille, France. ⟨hal-02373301⟩"
"Les unités de dinanderies de la ville de Fès produisent des effluents très fortement contaminés par des éléments métalliques tels que le nickel, l’argent, le plomb, le cuivre ou encore le zinc. Ces effluents, déversés directement dans le réseau collectif, ont un impact négatif sur la station d’épuration proche. Il s’avère ainsi nécessaire de décontaminer ces effluents en amont de leur traitement. Dans ce contexte, les objectifs de ce travail étaient la valorisation de substances minérales naturelles, bentonite et diatomite, minerais abondants dans le sol national et peu exploités au Maroc, pour leur utilisation en tant que phase d’adsorption du nickel et de l’argent. L’objectif de ce travail a été de modifier les propriétés physico-chimiques de ces matériaux naturels pour maximiser leur capacité d'adsorption. Dans une première étape, la modification de la diatomite a été réalisée par traitement thermique à différentes températures (de 550 à 950 °C). La diatomite calcinée à 550 °C présente le meilleur pouvoir adsorbant vis-à-vis du nickel et de l’argent. La deuxième étape a été consacrée à la modification de la bentonite par traitement thermique (550 °C et 750 °C), par activation chimique par le carbonate de sodium et par activations combinées chimique et thermique à 450 °C. Les bentonites modifiées par activation chimique par le carbonate de sodium et par activations combinées chimique et thermique à 450 °C présentent les meilleurs résultats. En outre, des polymères à empreintes ioniques (IIPs) du nickel ont été considérés comme des matériaux alternatifs pour remplacer les matériaux naturels afin d'améliorer la sélectivité. La dernière partie a consisté à comparer les performances de ces matériaux, afin de sélectionner le meilleur matériau pour une future application dans la dépollution des effluents contaminés avant leur traitement par la station d'épuration de la ville de Fès. Les concentrations en nickel et argent résiduelles deviennent négligeables dans les échantillons naturels après l'adsorption par les matériaux étudiés dans ce travail, ce qui répond parfaitement aux Normes de Qualité Environnementale. Ces résultats ont permis de valider avec succès l’utilisation des matériaux naturels et de synthèse pour l’extraction du nickel et de l’argent issus d’effluents contaminés."
"La rade de Toulon est un écosystème côtier fortement anthropisé qui présente un gradient multiple de contamination en éléments traces métalliques, qui en fait un site atelier remarquable. Le couplage des campagnes de terrain et des expérimentations en laboratoire a permis d'étudier l'impact de la contamination métallique sur les communautés microbiennes planctoniques et en biofilm. L'utilisation de la chimie analytique, de la cytométrie en flux et du métabarcoding a permis d'étudier plusieurs aspects des communautés, comme l'abondance et la diversité taxonomique en réponse à la contamination métallique dans la rade de Toulon. Ainsi, les communautés d'ultraphytoplancton et de bactérioplancton ont montré une structuration spatiale forte le long des gradients métalliques. Les expérimentations en laboratoire ont montré que les ETMs jouaient un rôle important sur l'abondance et la diversité des communautés ultraplanctoniques, par des effets directs (toxicité) et indirects (couplage phytoplancton-bactérioplancton). La communauté microbienne de biofilm a été beaucoup moins impactée par les gradients métalliques dans la rade de Toulon que la communauté ultraplanctonique. En revanche, la communauté de biofilm a semblé influencée par sa proximité avec le compartiment sédimentaire, qui ont pu fournir des microorganismes colonisateurs lors d'épisodes de remise en suspension. En conclusion, les ETMs ont semblé avoir un impact sur l'ensemble des communautés microbiennes de la rade de Toulon avec toutefois, des variations d'influence selon le compartiment."
"Les activités anthropiques ont apporté des changements majeurs à notre système global. Par ailleurs, la matière organique dissoute(MOD) du littoral a une grande influence sur le cycle global du carbone et donc sur le changement climatique. L'apport côtier enMOD représente la matière organique terrestre. Les rivières urbanisées sont fortement impactées par la MOD anthropiqueprovenant des usines de traitement des eaux usées. La MOD chromophorique est un sous-groupe de la MOD qui peut absorber lalumière. La MOD fluorescente est à son tour un sous-groupe de la MOD chromophore. Le signal de fluorescence de la MODanthropogénique dans la zone côtière n'est pas bien caractérisé et évalué dans la littérature. Les dégradations induites parphotochimie et les changements au niveau moléculaire sont peuvent de plus influencer la MOD. Dans la présente étude, plusieursexpériences d'irradiation solaire ont été menées avec plusieurs modes de filtration de mélange d’eau de rivière, d’eau de mer et d'uneffluent de station de traitement des eaux usées dans le but de trouver un signal spécifique de fluorescence comme un traceur de laMOD anthropique en utilisant les matrices d'émission d'excitation de la spectroscopie de fluorescence (EEMs) couplées à latechnique statistique chimiométrique de l'analyse factorielle parallèle CP/PARAFAC. Un modèle de régression multilinéaire a étédéveloppé entre la contribution des composantes CP/PARAFAC et la composition du mélange. La cinétique des paramètres derégression multilinéaire a également été étudiée. Des suivis géographiques de l'évolution du signal de fluorescence dans la rivièreGapeau jusqu'à la mer ont été menées ainsi qu’une étude temporelle du signal de fluorescence. Le modèle de régressionmultilinéaire développé a été appliqué pour modéliser les résultats des expériences de champs géographiques et temporelles. Lesrésultats ont montré que le modèle de régression multilinéaire est excellent. Par contre la recherche d'un signal ou d'une signaturede fluorescence spécifique pour l'eau de rivière, les stations d'épuration des eaux usées ou l'eau de mer n'a pas pu être réalisée dansce travail. Dans la zone côtière affectée par l'homme, les matières organiques fluorescentes résiduelles proviennent principalementsinon uniquement de l'usine de traitement des eaux usées, et aucun signal spécifique provenant de l'eau de mer n'a pu être détectéprès de la côte."
"Ces travaux proposent une analyse de l’écho radar de mer grâce au traitement d’une base de données issue de campagnes de mesures en zone côtière. L'analyse de l'amplitude de l’onde rétrodiffusée montre qu’elle respecte un modèle à deux échelles, combinaison de deux composantes nommées texture et speckle. La texture restitue les variations de la puissance rétrodiffusée associées aux grandes vagues, tandis que le speckle caractérise les variations rapides de la phase. Un traitement Doppler met alors en évidence la relation entre « spikes » de mer (événements ponctuels à forte rétrodiffusion) et vitesses Doppler élevées (diffuseurs rapides). La composante basse-fréquence présente sur les spectres 2D des cartes temps-distance de puissance rétrodiffusée, appelée « group line », est ensuite modélisée. Il apparaît que, contrairement à une hypothèse répandue, le déferlement n’est pas la cause principale de la présence de la « group line ». Une proposition de modélisation de l’écho de mer basée sur les observations expérimentales est finalement proposée, permettant de restituer des cartes spatio-temporelles dont les caractéristiques sont proches des mesures."
"The purpose of this book is to show the essential and indispensable role of prokaryotes in the evolution of aliving world. The evolutionary success of prokaryotes is explained together with their role in the evolution of the geosphere, the biosphere and its functioning, as well as their ability to colonize all biotopes, including the most extreme ones. We consider that all past and present living beings emerged from prokaryotes and have interacted with them. Forces and mechanisms presented in the various theories of evolution apply to prokaryotes. The major stages of their evolution and biodiversity are also described. Finally, it is emphasized that prokaryotes are living organisms that provide indisputable evidence of evolutionary processes. Many examples of ongoing evolution in prokaryotes, observable at the human scale, are provided."
"L’espèce Indo-Pacifique Avrainvillea amadelpha (Montagne) A. Gepp & E.S. Gepp, est signalée pour la première fois en Méditerranée (Iles Kerkennah, Tunisie). L’espèce est considérée comme introduite et invasive dans l’archipel Hawaiien. Les spécimens méditerranéens sont étudiés et les organes reproducteurs sont décrits et illustrés pour la première fois. Les origines et les vecteurs possibles de cette introduction sont discutés, ainsi que les risques de propagation de l’espèce en Méditerranée."
"Sur la base d’une analyse bibliographique, morphologique et génétique (SSU rDNA), le rétablissement dans son rang d’espèce de Chaetomorpha stricta Schiffner, une espèce endémique méditerranéenne peu connue décrite au début du XXe siècle est proposé. Précédemment réduite au rang de synonyme hétérotypique de C. linum (O.F. Müller) Kützing, C. stricta diffère nettement de cette dernière par ses filaments non-fixés et enchevêtrés en masses denses, constitués de cellules 0,5 à 1,6 (rarement 2) fois plus longues que larges à parois cellulaires lamelleuses et très épaisses (jusqu’à 75-90 µm), et par ses caractéristiques génétiques. Chaetomorpha stricta a été redécouverte, en mai 2011, dans les eaux claires et oligotrophes d’un étang de pêche de la Valle Cavallino (Bassin nord de la Lagune de Venise). Un inventaire critique des taxons de Chaetomorpha signalés en Méditerranée et une clé de détermination des espèces méditerranéennes actuellement acceptées sont présentées."
"L'océan est en grande partie profond, la majorité de son volume (> 80 %) se situant à une profondeur supérieure à 1000 m, et est peu échantillonné (< 0,01 %). Il est caractérisé par une pression hydrostatique élevée, une température basse, des nutriments inorganiques élevés et de faibles concentrations de carbone organique. L'apport de matière et énergie est principalement issu de la production primaire en surface sous forme de matière organique particulaire et dissoute puis exporté vers le fond des océans par différentes voies d’export et de transfert. Les fractions particulaire et dissoute biodisponibles sont principalement consommées dans les eaux de surface par les procaryotes, et ne laissant qu’une fraction dite réfractaire, plus complexe à dégrader comme seule source de carbone pour les procaryotes de l’océan profond (zone bathypélagique). La plupart des mesures d’activités métaboliques et de diversité des procaryotes bathypélagiques sont généralement sous-estimées en raison des limitations technologiques pour récupérer les échantillons et les maintenir dans des conditions environnementales in situ (pression hydrostatique élevée, température, etc.). Pour étudier les procaryotes dans les conditions in situ, nous disposons d'une technologie hyperbare pour collecter, transférer et incuber des échantillons dans des conditions in situ (pression hydrostatique et température élevées) qui est maintenant commercialisée et disponible pour la communauté scientifique. En utilisant la versatilité de nos équipements, nous pouvons également simuler l'augmentation de la pression hydrostatique le long de la colonne d'eau. Les résultats mettent en lumière qu’en maintenant les conditions de pression, il est possible d’échantillonner des microorganismes piezophiles (avec une affinité pour la vie sous pression) représentatif du milieu profond. Ces résultats ont été confirmés lors d’une expérimentation sur la dégradation de matière organique dissoute de haut poids moléculaire par une communauté adaptée au milieu profond. Enfin, nous avons également montré l’importance des conditions environnementales pour étudier le devenir de la matière organique dans la colonne d’eau."
"Les études menées sur le Rhône soulignent que dans le contexte actuel de changement climatique les situations de faible débit et d’étiage seront probablement de plus en plus fréquentes. Pour ces périodes extrêmes, il est attendu que la proportion de matière organique (MO) contenue dans les particules en suspension augmente. Or, la fixation de certains contaminants est fortement liée à la MO car sa forme favorise la complexation/adsorption de ceux-ci. C’est ce que confirment les résultats du projet CANADER conduit sur le Rhône et ses principaux affluents (Saône, Ardèche et Durance) où les concentrations dans les matières en suspension de certains contaminants (métaux anthropiques, mercure et radionucléides) ont tendance à augmenter en période d’étiage. Pour certains métaux anthropiques (Cr, Ni, Zn), les concentrations dépassent parfois le seuil d’effet probable (PEC). Par ailleurs, la diminution des métaux naturels (issus de la croûte terrestre – Co Cs Rb et V) couplée à l’augmentation du méthylmercure (suggérant une bioaccumulation des concentrations dans le phytoplancton) montre qu’une proportion non négligeable de la MO qui transite à l’étiage est autochtone. Les premiers résultats de mesure des stérols suggèrent que l’origine de cette MO diffère selon le Rhône et ses affluents, et des travaux additionnels doivent être réalisés pour caractériser les sources de ces particules."
Ce document décrit les interventions et les mesures réalisées sur le suivi du réseau de stations de mesure des flux en continu de l’Observatoire des Sédiments du Rhône pour l’année 2015 (OSR 4). Il présente également l’avancement de la bancarisation des données issues de ce suivi dans la base BDOH/OSR (flux). L’année 2015 a été marquée par la mise en place d’une nouvelle station de suivi sur la Durance et l’avancée des prospections pour la mise en place d’une station sur l’Ardèche. L’achat et la mise en service d’une nouvelle centrifugeuse mobile permettra d’affiner le suivi en crue du Rhône et de ses affluents.
"Ce document décrit les interventions et les mesures réalisées sur le suivi du réseau de stations de mesure des flux en continu de l’Observatoire des Sédiments du Rhône pour les années 2013 et 2014 (OSR3). Pour le Haut-Rhône, l’ensemble des prélèvements a été réalisé par Irstea de façon conforme aux prévisions, de même que la plupart des analyses chimiques. Pour le Bas-Rhône, l'ensemble des prélèvements a été réalisé par le MIO et le CEREGE comme prévu par le contrat. Les analyses métaux de 2013 sont complètes pour Arles ; pour Jons 9 mois sur 12 sont analysés et les autres seront réalisées prochainement. Il présente également l’avancement de la bancarisation dans BDOH/OSR (flux) des données issues de ce suivi dans et détaille la stratégie adoptée pour transmettre ces données à l’Agence de l’eau RMC dans le format souhaité."
Présentation du réseau de l'observatoire des sédiments du Rhône pour traiter les données de métaux traces particulaires.
"L’action III.1 du programme OSR 4 vise à estimer les flux de MES et de contaminants associés sur le Rhône et ses principaux affluents. Pour répondre à cet objectif, un réseau de mesure en continu des concentrations en MES et de prélèvements de MES a été mis en place. Ce document décrit les interventions et les mesures réalisées sur le réseau d’observation des flux particulaires de l’OSR pour l’année 2016 (OSR 4). Il présente également l’avancement de la bancarisation des données issues de ce suivi dans la base BDOH/OSR (flux). L’année 2016 a été marquée par la mise en place d’une nouvelle station sur l’Ardèche et par le suivi de l’opération d’abaissement partiel du barrage de Verbois (APAVER) sur le Rhône amont. L’outil BDOH, qui permettait jusqu’alors de gérer uniquement des données quantifiées, dispose désormais d’une nouvelle fonctionnalité pour la bancarisation de données reconstituées et inférieures aux limites de quantification."
"Cette action vise à mieux connaître les caractéristiques des particules transportées dans le Rhône pour mieux comprendre les processus de transfert et leur lien avec les polluants. - Caractérisation d’une signature géochimique de certains affluents (cévenols, Durance, Arve, Saône) à partir d’éléments chimiques, de radio-­‐isotopes et/ou de minéraux. - Détermination de la granulométrie moyenne des matières en suspension. Inter comparaison des techniques granulométriques des laboratoires associés - Traitement de l’ensemble des données radio-­isotopes de la base de l’IRSN. - Premières approches de caractérisation de la matière organique particulaire du Rhône."
"Microscopiques, les parasites sont la plupart du temps invisibles, mais présents partout. Ils infectent tous les organismes du monde vivant. La dernière décennie a révélé une incroyable diversité chez les parasites viraux, bactériens et eucaryotes. Ceux infectant le phytoplancton pourraient avoir une importance capitale dans la dynamique des populations algales et dans le fonctionnement des écosystèmes aquatiques, mais leur rôle est encore très largement méconnu à ce jour (Brussaard, 2004). Sur ces questions, la recherche ne fait que commencer."
"Situées dans les vallées encaissées ou dans de vastes plaines, les zones alluviales résultent de l'accumulation de sédiments véhiculés puis déposés par le fleuve. Elles associent des écosystèmes terrestres, forestiers et/ou prairiaux, et des écosystèmes aquatiques (les bras du fleuve déconnectés ou non), en interaction par l'intermédiaire de l'élément eau. Toute restauration doit donc prendre en compte l'ensemble de ces écosystèmes. Leur valeur biologique et patrimoniale est à l'heure actuelle largement reconnue. Toutefois en raison des multiples utilisations se développant dans les vallées alluviales et des impératifs de lutte contre les crues, les zones alluviales présentent des conflits d'intérêts qu'il convient de gérer dans l'optique d'un développement durable ..."
"Conceived as a baseline for the management and conservation of the marine protected area of the French Southern Territories (réserve naturelle nationale des Terres australes françaises), the checklist of marine macroalgae of the Kerguelen Islands was updated based on an extensive review of the literature and scientific databases, from the first report of the Ross expedition, in 1840, to the most recent works. This work was also conceived as a starting point for forthcoming investigations using molecular systematics tools and for monitoring the effects of global change on sub-Antarctic marine ecosystems. After a brief history of scientific campaigns, a list of 166 species was established (103 Rhodophyta, 35 Chlorophyta and 28 Ochrophyta [Phaeophyceae]). Molecular systematics studiess have shown the existence of recurrent discrepancies between the established, morphology-based taxonomy and molecular species delimitation, calling for a revision of systematics. Nevertheless, a first analysis of biogeographical affinities of the marine flora of the Kerguelen Islands is carried out and preliminary results are partially congruent with the main regions currently recognized in the Southern Ocean suggesting the importance of long-distance dispersal to explain the observed distribution patterns."
"Phytoplankton plays a key role in aquatic ecosytems as an oxygen producer, a CO2 trap, a primary source of food in trophic chains and as an indicator of changes in the environment. However, despite this positive importance, it can also develop into harmful algal blooms. With the aim of increasing knowledge about this group of microorganisms in Mexican aquatic ecosystems, a list of the phytoplankton species of the Sontecomapan Lagoon was made indicating those that potentially can provoke red tides. Besides, the distribution and abundance of these species was studied in two seasons, the rainy one (June, 2015) and the dry one (February, 2016) on eight sampling stations. Phytoplankton samples were collected with a Van Dorn bottle to measure environmental factors (transparency, salinity, temperature, pH and dissolved oxygen). A list with 357 species with a clear dominance of diatoms (67.8%) and dinoflagellates (20.16%) was obtained from literature review and materials derived from this study. Among them, 19.88% can potentially from red tides, and some of them are toxic. From the sample collected, 102 species of phytoplankton were recorded; 42 of them during the rainy season, 65 during the dry one and 7 présents in both. Among these species, 17 can potentially form red tides and from these, only two can be toxic for humans : Dinophysis caudata and Lyngbya majuscula. The cluster analysis of the environmental factors showed the formation of four groups in the rainy season and three in the dry season, associated to teh salinity gradients."
"L’objectif de ces travaux de thèse visait une meilleure compréhension et représentation de lapropagation de la houle à travers les milieux poreux afin de proposer une nouvelle caractérisation desouvrages de défense du littoral. L’influence du paramètre de surface spécifique des milieux poreux (surfacede contact fluide-solide), à porosité constante, est mise en évidence sur des écoulements permanents etoscillants forcés par la houle à l’aide de séries d’expériences réalisées en canal et bassin d’essai. Les donnéesexpérimentales obtenues sont comparées à des modèles théoriques basés sur la théorie potentielle des ondeset résolus à travers des méthodes intégrales de raccordement des potentiels aux frontières entre domaines.Les processus de réflexion, transmission et dissipation sont étudiés dans le cas bidimensionnel, les processusde réflexion, réfraction-diffraction et dissipation de la houle sont étudiés dans le cas tridimensionnel. Desphénomènes d’interférence des ondes sont mis en évidence en observant le caractère oscillant du coefficient deréflexion en fonction de la fréquence de la houle dans le cas 2D. A ce processus d’interférences dans la directionde propagation de la houle dans le cas 2D s’ajoute, dans le cas 3D, un phénomène d’interférences dans ladirection transversale à la direction de propagation de la houle. Un comportement linéaire ou quadratique dutaux de dissipation de l’onde à l’intérieur du milieu poreux est observé. Le rôle de la surface spécifique dans ladissipation de l’énergie de l’onde à travers le milieu poreux est mis en évidence. Les régimes d’écoulement etles effets d’échelle sont également discutés."
"Multispectral and hyperspectral sensor data of the bio-optical parameters with a high spatial resolution are important for monitoring and mapping of the coastal ecosystems and estuarine areas, such as the Kneiss Islands in the Gulf of Gabes. Sentinel 2 S2A and Hyperion Earth observing-1 (EO1) imaging sensors reflectance data have been used for water quality determination and mapping of turbidity TU and chlorophyll Chl-a in shallow waters. First, we applied a tidal swing area mask based on uncorrelated pixel via 2D scatter plot between 665 nm and 865 nm to eliminate the overestimation of the concentration of water quality parameters due to the effect of the bottom reflection. The processing for mapping and validating Chl-a, Turbidity S2A, and EO1 were performed using a relation between reflectance bands and in situ measurements. Therefore, we were able to validate the performance of the case 2 regional coast colour processor (C2RCC) as well as our region-adapted empirical optical remote sensing algorithms. Turbidity was mapped based on the reflectance of 550 nm band for EO1 (R² = 0.63) and 665 nm band for S2A (R² = 0.70). Chlorophyll was mapped based on (457/528 nm) reflectance ratio (R² = 0.57) for EO1 and (705/665 nm) reflectance ratio (R² = 0.72) for the S2A."
"Tropical coastal lagoon environments provide a number of ecosystem services, but are threatened by the pressure lmposed by human activities and climatic change; these systems are particularly vulnerable because of a high demographic growth. Therefore, the understanding of their ecological behavior and the characterization of lagoon health indicators have attained importance. Under thls perspective Mexican (UAM-Xl and French (UMRs MIO and MARBEC) researchers have collaborated from 2011 to 2014 as part of one action of the international exchange program ECOS/ANUIES, and chose the Sontecomapan lagoon (at the Mexican state of Veracruz) as a case study. This book provides information of the ecological behavior, water quality indicators, and details of microorganisms and plankton, which due to their short life cycles and their high reactivity to environmental conditions are good indicators of ecological changes. The equilibria of benthic and pelagic compartments and their interaction are other important elements in this shallow environment. The nature, magnitude and effects of human-related activities also have been considered in order to understand the possible evolution of the ecosystem's behavior to evaluate its capacity for resilience and gives some hints on rehabilitation actions."
"Tropical coastal lagoon environments provide a number of ecosystem services, but are threatened by the pressure lmposed by human activities and climatic change; these systems are particularly vulnerable because of a high demographic growth. Therefore, the understanding of their ecological behavior and the characterization of lagoon health indicators have attained importance. Under thls perspective Mexican (UAM-Xl and French (UMRs MIO and MARBEC) researchers have collaborated from 2011 to 2014 as part of one action of the international exchange program ECOS/ANUIES, and chose the Sontecomapan lagoon (at the Mexican state of Veracruz) as a case study. This book provides information of the ecological behavior, water quality indicators, and details of microorganisms and plankton, which due to their short life cycles and their high reactivity to environmental conditions are good indicators of ecological changes. The equilibria of benthic and pelagic compartments and their interaction are other important elements in this shallow environment. The nature, magnitude and effects of human-related activities also have been considered in order to understand the possible evolution of the ecosystem's behavior to evaluate its capacity for resilience and gives some hints on rehabilitation actions."
"The various schemes proposed to classify microorganisms in the living world have long been subject of heated debates. The classical dichotomic distinction between Prokaryotae (cells without nucleus) and Eukaryotae (cells with nucleus) functional and phenotypic categories was deeply changed by rRNA gene-based analysis that divided the living world into three phylogenetic domains: the Bacteria, the Archaea (originally Archaebacteria), and the Eukarya. In this chapter, we review the terms of this debate between the prokaryotic/eukaryotic functional and phenotypic dichotomy and the 16S/18S phylogenetic dichotomy that separates prokaryotes into two distinct domains. The specific characteristics that emphasize the organizational and functional complexity of prokaryotes and justify maintaining this terminology are discussed. We conclude that the organizational and functional concept of a prokaryotes/eukaryotes dichotomy can be easily supplemented by the phylogenetic concept Bacteria/Archaea/Eukarya. The two concepts are not irreconcilable but complementary, resulting in a consensual proposal that integrates both phenotypic and genotypic criteria"
"En février 2013, le Conseil Scientifique de l’OSR a construit collectivement la structuration du 3ème programme scientifique. La structuration et le nombre d’axes de recherche ont évolué afin de s’adapter aux nouveaux besoins des opérationnels et offrir la possibilité de réunir les équipes scientifiques pour développer des démarches plus interdisciplinaires. Les quatre premiers axes structurent des champs thématiques spécifiques. Ils sont alimentés par un système d’observation et de mesures, et reposent sur un axe commun et transversal qui vient techniquement soutenir toutes les actions de recherche via le développement d’outils de modélisation, la gestion collective des données ou encore la valorisation des résultats (synthèse de l’ensemble des connaissances produites). L’OSR3 a été conçu comme une phase de transition dans le cadre de la relance du nouveau Plan Rhône. Cette programmation a permis à la fois de consolider les acquis des quatre premières années de recherche, d’assurer la continuité des mesures et de préparer les actions ensuite mises en place dans la programmation 2015/2017."
"The 'species-by-species', or 'stock-by-stock' approach in the case of fisheries, characterized the 20th century ecology. The ecosystem-based approach, which in the case of fisheries emerged at the end of the 20th century, represents the 'new frontier', the 21st century revolution in ecology. According to the Marine Strategy Framework Directive, the ecosystem-based approach (EA) should enable us to understand and assess the functioning of marine and coastal ecosystems and their dependent services. As an integrated management process, this strategy promotes both conservation and sustainable use and provides benefits in a more equitable way via the social-ecosystem concept. First used for fisheries management, the EA has become a valuable tool for ecosystem services assessment and for marine protected areas (MPA) monitoring and governance, especially in the Mediterranean Sea but it could also be considered in the spatial planning and the management of other coastal areas. The perception that humans belong to ecosystems (socio-ecosystems) is a key feature of the EA and the catastrophic events due to anthropic pressures, in the global change context, remind us the price to pay. The aim of organizing a Workshop on Ecosystembased Management in Marseille was to create the opportunity to gather managers, stakeholders and scientists to discuss this crucial topic within the framework of the Integrated LIFE Marha project, headed by the French Office of Biodiversity and funded by the European Union. Aix Marseille University, Pythéas and the Mediterranean Institute of Oceanography, the Marseille's city Tourism Office and Toulon Provence Metropole have also supported the organization of the event. The upcoming challenge, in the current Global change, is to move forward to a suitable and carbon free management and use of our natural ecosystems. Only a concern at the Ecosystem scale will make it possible."
Suivi in situ et à haute fréquence de la dynamique des assemblages phytoplanctoniques dans l'étang de Berre
"La mer Méditerranée (MS) est une mer semi-fermée divisée en deux bassins: le bassin occidental et le bassin oriental. Les deux sites méditerranéens étudiés dans cette thèse sont au nord-ouest de MS et sud-est de France pour la rade de Toulon et à l'est de MS, Liban pour la baie de St-Georges. Les deux sites sont soumis à une forte densité de population le long de la côte et, sont exposés à de nombreuses activités anthropiques (telles que le tourisme, les transports maritimes, l’aquaculture, les rejets d’eaux usées et des activités industrielles) contaminant l’environnement (colonne d’eau, sédiments, biote, etc.) avec différents contaminants tels que les éléments traces métalliques et métalloïdes (ETMM). Chaque site est caractérisé par la présence d'un fleuve urbanisé: le Las (France) et le fleuve Beyrouth (Liban). Les sédiments sont considérés comme une source secondaire de contamination en raison des processus biogéochimiques influençant la mobilité des ETMM dans les sédiments. Dans ce contexte, les objectifs de cette thèse sont (1) d’étudier la contamination des ETMM le long des rivières et dans les deux baies; (2) d'élucider l'influence de la diagenèse précoce sur la mobilité des ETMM; et (3) de caractériser la matière organique sédimentaire dans les deux sites. Pour ces raisons, des sédiments superficiels, des carottes sédimentaires, des eaux superficielles et interstitielles ont été collectés le long des deux rivières et dans les deux sites durant la période 2016-2018. Les échantillons de sédiments et d'eaux ont été analysés pour déterminer leurs principaux paramètres physiques, nutriments, carbone organique (dissous et particulaire) et les éléments majeurs/traces. Les résultats ont montré que les processus de la diagenèse précoce contrôlent fortement la mobilité de la ETMM dans les sédiments. De plus, les résultats ont confirmé que la rade de Toulon est fortement contaminée suite à la deuxième guerre mondiale mais aussi aux activités contemporaines, et que les apports du Las ne contribuent et n’affectent pas de manière significative les sédiments de la baie de Toulon. Quant à la baie de St-Georges, il a été constaté qu’elle est impactée par plusieurs activités (effluents industriels rejetés sans traitement, ruissellement de la zone agricole) transportées par le fleuve Beyrouth et/ou par des apports directs (décharge côtière)."
not available
not available
"Situées à l'interface entre continent et océan, les zones côtières revêtent une importance cruciale dans le fonctionnement des écosystèmes marins. Leur production primaire locale est importante et elles subissent les apports et les influences de diverses sources de matière organique allochtone. Comprendre le fonctionnement trophique des écosystèmes côtiers s'avère donc complexe, particulièrement dans la baie de marseille, deuxième agglomération française, où les influences des activités anthropiques s'ajoutent aux influences naturelles. Dans le cadre du programme ""RECIFS PRADO"", 400 récifs artificiels ont été implantés dans la baie de marseille. Ce programme, le plus important en méditerranée, représente une immersion de près de 27 000 m 3 de modules artificiels pour favoriser la petite pêche côtière. Il représente une occasion unique de comprendre le fonctionnement trophique d'un tel système, en mettant l'accent sur les comportements alimentaires des téléostéens. Deux approches méthodologiques complémentaires ont été utilisées pour aborder ce problème : l'analyse des isotopes stables du carbone (δ 13 C) et de l'azote (δ 15 n), et l'analyse des contenus stomacaux. Les isotopes stables sont un indicateur de l'alimentation moyenne des téléostéens et permettent de déterminer leur niveau trophique et les sources de carbone principalement utilisées. L'analyse des contenus stomacaux permet de connaître leur alimentation récente et de mieux expliquer les valeurs des ratios isotopiques. Cette étude a porté sur 32 espèces de téléostéens, échantillonnées en été et en hiver sur deux récifs artificiels. Elle a montré : (1) que les téléostéens présents dans la zone d'implantation des récifs artificiels occupent au moins trois niveaux trophiques et utilisent une large gamme de proies (euarthropodes pélagiques et benthiques, producteurs primaires, téléostéens, mollusques). Une grande partie de ces organismes a été retrouvée dans les inventaires réalisés sur les récifs, ce qui confirme le rôle trophique important que peuvent jouer les structures artificielles en milieu marin; (2) que les signatures isotopiques des téléostéens présentent une faible variabilité spatiale et temporelle. Seules quatre espèces (Boops boops, Scorpaena scrofa, Symphodus tinca et Trigloporus lastoviza) ont des signatures isotopiques différentes entre les deux récifs. Une différence de signatures isotopiques entre l'été et l'hiver n'a été observée que chez sept espèces. Elle peut être attribuée à une modification des signatures isotopiques des proies mais aussi à une différence d'alimentation associée à une variation des facteurs de discrimination isotopique."
"Le poisson flûte est considéré comme le champion des invasions lessepsiennes. Dans les années 2000, après une croissance exponentielle de ses populations en Méditerranée orientale, il est devenu très abondant dans les débarquements des pêcheries de cette région. Dans cette étude est signalée la première capture de deux individus de poisson flûte dans la baie de Marseille (France, Bouches du Rhône) en octobre 2016, constituant la capture avérée la plus nord-ouest de la Méditerranée. Cette espèce est un prédateur piscivore très actif. L'individu échantillonné, une femelle de 99 cm LT, avait consommé 6 poissons appartenant à 3 genres : Spicara, Symphodus et Atherina."
"Les éléments traces métalliques arrivant dans le milieu marin s’accumulent naturellement dans les sédiments. En zone portuaire, ces sédiments peuvent être remis en suspension dans la colonne d’eau par des phénomènes naturels (houles, tempêtes…) et/ou anthropiques (trafic maritime, activité de dragage). En particulier, la Méditerranée Nord-occidentale représente un lieu d’échanges maritimes conséquents, engendrant des évènements de remise en suspension d’origine anthropique fréquents, tout en étant peu soumise à l’influence des phénomènes naturels liés aux marées. Dans ce contexte, les objectifs de cette thèse consistaient à étudier le potentiel de remobilisation des éléments traces lors d’évènements de remise en suspension afin d’apporter des connaissances contribuant à mieux prédire et ainsi mieux gérer le risque associé. Les cinétiques de transferts des ETMMs, étudiés lors d’expérimentations en laboratoire, ont été précisées à la lumière de la variabilité des contextes portuaires. Le transfert des ETMMs durant les 5 premiers jours de remise en suspension est apparu déterminé principalement par des processus abiotiques. La compréhension de ces processus a permis d’expliquer partiellement les observations de terrain réalisées lors d’une étude d’impact d’une opération de dragage."
"Les brise-lames détachés sont une solution largement utilisée pour l’aménagement et la protection des plages. Disposés régulièrement le long de la côte, ils constituent des barrières à la propagation de la houle, et atténuent son impact à la côte par réflexion et diffraction entre deux ouvrages successifs, grâce à un choix adéquat de la taille des ouvrages et de leur espacement. La zone abritée par le brise-lame est souvent comblée avec du sable, formant un tombolo. Les ouvrages poreux sont de plus en plus considérés, présentant d’un point de vue environnemental les avantages de favoriser le développement de la biodiversité et de maintenir une circulation des masses d’eaux pour une meilleure qualité de l’eau. Le travail présenté ici consiste en l’étude d’un brise-lame poreux, constitué d’un réseau dense de cylindres verticaux émergeants. Des expériences sur l’hydrodynamique ont été menées en 3D dans le bassin d’essai de SeaTech, elles sont comparées à des modèles numériques basés sur des formulations intégrales pour les potentiels des vitesses. La symétrie du système expérimental permet de considérer le cas d’étude comme celui d’un ensemble de brise-lames régulièrement espacés le long de la côte. Du fait de la porosité du brise-lame, une partie de la houle continue à se propager avec dissipation à travers la structure, modifiant significativement les conditions d’agitation à l’abri de la structure. L’énergie de la houle est plus homogène le long de la côte, la diffraction, moins importante, étant compensée par un flux à travers la structure. Pour certaines conditions de houle, des interférences peuvent apparaitre non seulement en fonction de la distance entre deux brise-lames successifs, comme pour les ouvrages imperméables classiques, mais aussi au niveau de la structure poreuse, augmentant la dissipation de l’énergie des vagues."
"Nous nous concentrons sur la caractérisation des fronts thermohalins en Méditerranée occidentale, avec un accent particulier sur le Front Nord-Baléares (NBF), séparant les eaux atlantiques qui se répandent dans le bassin algérien des eaux plus salées et plus froides de la zone Liguro-Provençal. Nous utilisons une méthode simple de détection des fronts basée sur le gradient, appliquée à une réanalyse de 20 ans (de juin 1993 à juin 2013) de la Méditerranée. Les statistiques des indices frontaux quotidiens sont utilisées pour identifier les zones de fronts récurrents, c'est-à-dire les zones frontales. Des comparaisons avec des données issues de transects de planeurs et des données de température de surface de la mer et d'altimétrie télédétectées ont été utilisées pour valider notre approche, de l'échelle journalière à l'échelle saisonnière et interannuelle. Notre méthode a permis d'identifier deux zones frontales coexistantes dans la région du NBF. L'une avec une zone frontale de surface haline quasi permanente s'étendant vers le sud-est des Baléares à la Sardaigne, représentant la limite nord des eaux atlantiques plus fraîches qui se répandent via les instabilités du courant algérien et des tourbillons algériens associés. D'une année sur l'autre, sa position se déplace d'environ 1° de latitude, probablement en raison de processus associés à la fois à la formation d'eau profonde dans le bassin provençal et à la propagation des tourbillons algériens du sud. La deuxième zone frontale est saisonnière et thermique, s'étendant du nord-est de Minorque au nord-ouest de la Corse. Le Front des Pyrénées, un front thermique net au large du Cap de Creus qui marque la limite entre les eaux chaudes de surface de la mer des Baléares et les eaux plus froides du Golfe du Lion à la fin de l'été, semble faciliter la formation du front thermique saisonnier susmentionné par l'advection de ses eaux vers le nord-est du courant de Corse occidentale. Les extensions divergentes vers l'est des deux zones frontales, et leurs différences de nature, de structure et de variabilité spatio-temporelle, appellent à une révision de l'appellation NBF et à une clarification de sa dynamique."
"Suite à l’acquisition de nouvelles informations sur les habitats marins méditerranéens, une mise à jour du référentiel national était nécessaire pour qu’il soit le reflet des connaissances actuelles. Ces nouvelles informations proviennent principalement de deux campagnes d’exploration des roches profondes et des canyons méditerranéens appelées MEDSEACAN et CORSEACAN, de CARTHAM (Cartographie des Habitats Marins) – deux programmes pilotés par l’Agence des Aires Marines Protégées – ainsi que de propositions émanant directement de la communauté scientifique. En collaboration avec les experts scientifiques benthologues, ces nouvelles observations ont été confrontées à la typologie (Michez et al., 2011) et des modifications et compléments ont été décidés. Ce rapport présente la version actualisée de la partie « Méditerranée » du référentiel français des habitats marins benthiques. Il explique en détail les choix opérés ainsi que les points à approfondir."
"Cette synthèse constitue le volume 4 (sur 4) du programme intégré « Dispersion et exposition humaine aux métaux en Nouvelle-Calédonie » composé de 3 programmes complémentaires étudiant les métaux et leur toxicité : DMML « Dispersion des métaux de la mine au lagon », Dynamine « Dynamique des métaux de la mine au lagon », Métexpo « Niveaux d’imprégnation et déterminants de l’exposition humaine aux métaux ». - Elle regroupe les synthèses de chacun des trois programmes et propose une conclusion commune ouvrant sur les perspectives en termes de recherche."
"Nous présentons une analyse des flux de masses d'eau spécifiques dans la mer Méditerranée occidentale issus d'une réanalyse sur vingt ans (1992-2013) (MEDRYS1V2). Les masses d'eau sont identifiées sur la base des propriétés de salinité et de densité potentielle et calculées ; les fractions de chaque masse d'eau impliquées dans le flux total sont calculées sous les hypothèses des schémas de lignes de mélange. Cette méthode a été initialement conçue pour éviter les troncatures grossières entre les masses d'eau sur le diagramme T-S lors de l'utilisation de seuils fixes de propriétés thermo-halines. La méthode n'utilise pas le marqueur de température en raison de sa grande variabilité saisonnière dans les eaux proches de la surface (0-200 m) et nous considérons que la densité potentielle est un meilleur marqueur pour discriminer les masses d'eau profondes et intermédiaires. L'algorithme discrimine successivement cinq masses d'eau différentes : les eaux atlantiques (AW) provenant du détroit de Gibraltar (salinité entre 36,1 et 38,45 PSU), les eaux intermédiaires du Levant (LIW) provenant du détroit de Tunisie-Sicile (salinité entre 38,45 et 39. 1 PSU), les eaux atlantiques modifiées (MAW) définies comme des eaux proches de la surface (densité potentielle inférieure à 28,9 kg m-3) qui ne sont ni AW ni LIW, tandis que les eaux intermédiaires occidentales (WIW) sont celles qui restent jusqu'à ce que le seuil sigma(theta) = 29,10 kg m-3 pour les eaux profondes de la Méditerranée occidentale (WMDW) soit atteint. Ces fractions calculées de chaque masse d'eau, dont la somme est contrainte à l'unité, sont ensuite utilisées pour calculer les transports de leurs masses d'eau tout au long des vingt années de la réanalyse. Les transports sont évalués à travers les calculs sur des transects clés délimitant des entités de sous-bassins connues (mer Ligure, golfe du Lion, mer Baléares...), avec des transports totaux montrant un bilan de masse équilibré. Les transports totaux ainsi calculés révèlent des différences marquées dans leur variabilité saisonnière et inter-annuelle, tandis que l'analyse des transports de masse d'eau permet d'identifier ceux qui ont principalement induit cette variabilité. Les résultats montrent d'abord une faible variabilité saisonnière et une variabilité inter-annuelle non significative à la sortie de la mer d'Alboran qui résulte de l'équilibre entre le flux sortant AW/MAW vers l'est et les flux entrants WIW et WMDW vers l'ouest. Le détroit de Corse, la ligne de la mer Ligure et les détroits de Tunisie et de Sardaigne présentent une variabilité saisonnière marquée (0,37-0,39 Sv) principalement due à l'AW/MAW. En revanche, une forte variabilité inter-annuelle domine la variabilité saisonnière (-2 à 1 Sv) entre le bassin algérien et le bassin nord, corrélée à la formation de l'WMDW. L'analyse de chaque transport spécifique de masses d'eau a montré que cette variabilité marquée est d'abord déterminée par les transports de masses d'eau intermédiaires et profondes. De même, la variabilité inter-annuelle des transports AW et MAW dans la partie centrale de la Méditerranée occidentale suggère un certain couplage entre les masses d'eau profondes, intermédiaires et de surface, même à travers la mer des Baléares, moins profonde."
"ILICO, une infrastructure de recherche (IR) française pour l'observation de l'océan côtier et du littoral, est un exemple remarquable des efforts nationaux et pan-institutionnels visant à étendre la connaissance des processus complexes à l'œuvre dans la zone côtière critique, conformément à la perspective du système européen d'observation de l'océan. Offrir à sa communauté un forum pour travailler ensemble sur les questions prioritaires est un défi, et la structure organisationnelle et la gouvernance d'ILICO sont conçues en conséquence. Les défis à venir pour ce IR incluent la question de savoir si le modèle original de la France, qui combine à la fois la terre et le littoral dans son étude du domaine côtier, est transférable au contexte paneuropéen et jusqu'où nous pouvons aller dans l'intégration des questions ultramarines."
"L'action III.1 du programme OSR 4 vise à estimer les flux de matières en suspension (MES) et de contaminants associés sur le Rhône et ses principaux affluents. Pour répondre à cet objectif, un réseau de mesure en continu des concentrations en MES et de prélèvements de MES a été mis en place. Ce document décrit les interventions et les mesures réalisées sur le réseau d'observation des flux particulaires de l'OSR pour l'année 2017 (OSR 4). Il présente également l'avancement de la bancarisation des données issues de ce suivi dans la base de données BDOH/OSR (flux). L'année 2017 a été marquée par la mise en place d'une nouvelle station sédimentaire sur le Gardon à Remoulins. Dans BDOH, les chroniques de concentrations en contaminants comportant des lacunes ont été reconstituées à l'aide de forfaits déterminés par gamme de débits."
"Mer intercontinentale presque entièrement fermée, la Méditerranée s'étend sur environ 2,5millions de km2. Elle s'ouvre sur l'océan Atlantique via le détroit de Gibraltar et est reliée à lamer Noire par le Bosphore et, depuis 1869, à la mer Rouge par le canal de Suez. Située à la limitedes plaques africaine et eurasienne, elle est le siège d'une intense activité sismique etvolcanique.D'un point de vue physique, elle se divise principalement en deux bassins bien individualisés(Méditerranée occidentale et orientale), eux-mêmes subdivisés en nombreuses entitéscontrastées. Sous l'influence des grands fleuves qui s'y jettent (Pô, Rhône, Nil, Ebre…) et desdifférents courants dont elle est le siège, elle se caractérise par des transferts horizontaux etverticaux rapides, fortement dépendants des conditions météo-climatiques et soumis à unegrande variabilité à toutes les échelles temporelles.D'un point de vue biologique, la mer Méditerranée est identifiée comme l'un des ' hot spots ' dela biodiversité marine, abritant de très nombreuses espèces, dont un fort pourcentaged'endémiques. Lagunes littorales, marais maritimes, estuaires, deltas, côtes rocheuses etsableuses, herbiers, coralligènes, fonds meubles et rocheux, canyons, plateaux, montagnes sousmarines, sont autant d'habitats remarquables qui favorisent la diversité des organismes enMéditerranée.D'un point de vue géopolitique, la mer Méditerranée représentait dans l'Antiquité un carrefourd'échanges commerciaux et culturels entre les peuples de la région. Elle constitue aujourd'hui unespace partagé par 23 pays riverains, traversé et exploité par ces pays et des tierces parties.C'est un espace disputé, avec des tensions entre les usagers et des compétitions sur lesressources. Néanmoins, sur l'ensemble des problématiques liées à la mer, les pays riverains sontliés par une ' communauté de destin '. D'où la nécessité de mieux coopérer, à l'échelle del'ensemble du bassin, en termes d'analyse et d'évaluation des ressources ; d'observation, de suiviet de contrôle ; et de gestion des ressources et des usages qui en découlent.D'un point de vue humain, les pays riverains rassemblaient en 2011 une population de 475millions d'habitants, dont un tiers sur le littoral et une forte proportion dans des ' mégacités 'côtières ou proches de la mer (Le Caire, Istanbul, Athènes, Rome, Barcelone, Marseille, Alger…).Depuis le littoral jusqu'au large, la mer Méditerranée est le siège de nombreuses activités :espace de navigation, exploitation de ses ressources vivantes ou minérales, mais aussi de sesparticularités paysagères, culturelles et climatiques qui attirent chaque année sur le littoralméditerranéen plus de touristes que celui-ci ne compte d'habitants ! Elle procure également unegrande diversité de services écosystémiques non marchands (nurseries pour les espècesexploitées, recyclage des éléments nutritifs, absorption et séquestration de CO2 atmosphérique,filtration des substances toxiques, protection contre l'érosion des côtes…), dont dépendent laqualité de vie des populations riveraines et certaines de leurs activités économiques, comme lapêche, l'aquaculture, le tourisme…"
"Les bioindicateurs évaluent l'état écologique des mangroves en fonction des types de pressions mais ils diffèrent selon les spécificités de l'écosystème. Nous avons étudié la diversité et la structure de la méiofaune benthique dans des mangroves à faible impact humain en Guyane française (Amérique du Sud) en réponse à des variables sédimentaires et à différentes distances de la ville principale. Les concentrations de contaminants différaient selon les stations, mais elles restaient inférieures aux recommandations de toxicité. La structure de la méiofaune (Foraminifera, Kinorhyncha, Nematoda) a cependant varié en conséquence. L'identification des nématodes a apporté des détails sur la qualité des sédiments. Le genre opportuniste Paraethmolaimus (Jensen, 1994) était fortement corrélé aux concentrations plus élevées de Hg, Pb. Les sédiments anoxiques étaient marqués par un enrichissement organique en pesticides, PCB, et produits de la litière de mangrove et par la dominance de deux genres tolérants, Terschellingia (de Man, 1888) et Spirinia (Gerlach, 1963). Dans chacune de ces deux stations, nous avons trouvé de nombreux individus Desmodora (de Man, 1889) avec la présence d'épibiontes mettant en évidence la diminution de la fitness et des capacités de défense des nématodes. Les sédiments oxiques sans contaminants se distinguaient par la présence de genres sensibles Pseudocella (Filipjev, 1927) et une plus grande diversité de groupes trophiques. Nos résultats suggèrent une sensibilité des nématodes à de faibles concentrations de contaminants. Des études supplémentaires à différentes échelles spatio-temporelles et à différents niveaux de détérioration seraient nécessaires pour utiliser ce groupe comme bioindicateur de l'état écologique des mangroves."
"L’action B1 de l’Observatoire des Sédiments du Rhône (OSR5) vise à pérenniser le réseau de suivi des flux de matières en suspension (MES) et de contaminants associés. Ce rapport dresse le bilan des avancées réalisées sur l’ensemble du réseau en 2019 en termes de stations actives, de prélèvements de MES et d’analyses physico-chimiques, ainsi que sur les calculs de flux de contaminants dans la Base de Données des Observatoires en Hydrologie (BDOH). L’année 2019 a été marquée par la première année d’exploitation de la station de mesure sur le Rhône à Saint-Vallier, avec l’acquisition de données de débit et de turbidité par la CNR. Par ailleurs, un piège à particules a été installé en amont, au niveau du village d’Andancette, depuis fin 2018. Aussi, en 2019, une nouvelle méthode de bancarisation et de stockage des échantillons a été mise en place, avec l’acquisition d’une enceinte -80°C et l’utilisation de la plateforme Banquise."
"Alexandrium catenella est un dinoflagellé marin invasif impliqué dans les phénomènes d'efflorescences massives toxiques (Harmful Algal Blooms ou HABs). Les HABs sont notamment observés dans des écosystèmes marins côtiers perturbés par les activités anthropiques, avec des fréquences, des distributions et des intensités croissantes. A. catenella produit des phycotoxines (saxitoxine et dérivés) responsables de graves intoxications, pouvant conduire jusqu'à la mort, chez l'être humain ayant consommé des fruits de mer contaminés (Paralytic Shellfish Poisoning ou PSP). Or, des efflorescences de ce dinoflagellé ont lieu dans des écosystèmes soumis à des contaminations par divers éléments traces métalliques (ETMs) (Étang de Thau : Eric Abadie, communication personnelle ; Rade de Toulon : Jean et al., 2006), ce qui suggère une certaine capacité de résistance et/ou d'adaptation d'A catenella aux stress métalliques, laquelle a peu été étudiée (Jean et al., 2017 ; Jean et al., en préparation). Dans cette étude, la croissance, la morphométrie, les profils toxiniques et les réponses protéomiques de cellules d'A. catenella exposées à un cocktail polymétallique (Cu, Pb, Zn, Cd) ont été analysés. L'approche protéomique permet de comprendre les mécanismes de résistance/d'adaptation développés par des organismes soumis à des stress environnementaux, grâce à des protéines de stress dont ils modifient l'expression au sein de voies métaboliques spécifiques. Cette étude vise donc à (i) identifier les protéines de stress dont l'expression est modifiée par A.catenella en réponse à une contamination polymétallique, (ii) analyser la contribution de ces protéines à des mécanismes cellulaires spécifiques pouvant expliquer le développement/maintien de ce dinoflagellé dans des écosystèmes contaminés par les ETMs, (iii) mettre en évidence des biomarqueurs protéomiques de résistance/d'adaptation d'A. catenella aux conditions de stress métalliques. Les résultats obtenus peuvent contribuer à comprendre la capacité de ce dinoflagellé à coloniser et à se maintenir dans des écosystèmes marins côtiers fortement anthropisés. Mots clés Alexandrium catenella, biomarqueurs, phycotoxines, protéomique, stress métallique"
"How can the evolutionary success of prokaryotes be explained ? How did they manage to survive conditions that have fluctuated, with drastic events over 3.5 billion years ? Which significant metabolisms and mechanisms have appeared over the course of evolution that have permitted them to survive the most inhospitable conditions from the physicochemical point of view ? In a 'Red Queen Race', prokaryotes have always run sufficiently fast to adapt to constraints imposed by the environment and the other living species with which they have established interactions. If the criterion retained to define the level of evolution of an organism is its capacity to survive and to yield the largest number of offsprings, prokaryotes must be considered highly evolved organisms."
"Du 18 mars au 02 avril 2017, s'est déroulée la mission PUFFAlis à bord du navire océanographique Alis de l'IRD. Cette campagne s'inscrit dans la continuité d'expéditions océanographiques centrées sur les zones d'alimentation des oiseaux marins, effectuées précédemment en mer (MOMAlis) comme à terre sur la colonie de puffins de Gouaro Deva. Elle est aussi la prolongation des campagnes NECTAlis destinées à étudier la chaîne trophique océanique qui conduit aux thons. Six scientifiques ont embarqué pour échantillonner les différents niveaux de la chaîne trophique océanique du bassin de Nouvelle-Calédonie au large de Pindaï. En milieu de mission, une journée a été consacrée à la communication avec les lycéens de Pouembout et les services de l'environnement et des pêches de la Province nord. Le journal de bord de la mission est précédé du communiqué de presse diffusé à cette occasion."
Ce chapitre répertorie les différents facteurs extrêmes et les mécanismes misent en jeu lors de la croissance des microorganismes extremophiles
"L’action B1 de l’Observatoire des Sédiments du Rhône vise à pérenniser le réseau de suivi des flux de matières en suspension (MES) et de contaminants associés. Ce rapport dresse le bilan des avancées réalisées sur l’ensemble du réseau en 2018 en termes de stations actives, de prélèvements de MES et d’analyses physico-chimiques, ainsi que sur les calculs de flux de contaminants dans la Base de Données des Observatoires en Hydrologie (BDOH). L’année 2018 a été marquée par des crues échantillonnées en janvier sur plusieurs stations (Rhône à Jons et à Arles, Saône et Fier) et la mise en place d’une nouvelle station sur la Drôme. Ce rapport présente également une synthèse des différents essais qui ont été menés pour tester et valider les protocoles de mesure et les stratégies d’observation des flux. Sur les deux stations principales d’observation du Rhône, le suivi des concentrations en contaminants particulaires s’effectue à travers l’analyse de MES prélevées par centrifugation en continu à Jons (en complément du prélèvement par piège à particules) et à Arles. Les deux systèmes de centrifugation utilisés successivement à Jons, suite à la mise en place de cette station en 2011, ont été comparés. Les résultats obtenus démontrent que les MES prélevées par centrifugation sont représentatives de celles qui transitent dans le Rhône au moment du prélèvement. Le changement de centrifugeuse, survenu en juillet 2013, n’a pas eu d’influence sur la continuité des séries temporelles de concentrations en contaminants particulaires. Les prélèvements de MES requièrent un temps de préparation du matériel conséquent, ainsi qu’une importante étape de préparation des échantillons avant analyse. La fréquence de prélèvement sur la station de Jons a été réévaluée en étudiant les tendances temporelles des concentrations en contaminants particulaires dans des jeux de données bimensuels et mensuels. La fréquence de prélèvement en régime hydrologique de base pour le suivi des concentrations en contaminants à Jons, jusqu’alors bimensuelle, est fixée à une fois par mois depuis le 13 mars 2018. Entre les deux stations principales sur le Rhône à Jons et à Arles, la traversée de la ville de Lyon et de la vallée de la chimie est susceptible de contribuer à l’augmentation des concentrations en contaminants particulaires. La contamination des MES en amont et en aval de la Métropole de Lyon a donc été comparée en régime hydrologique de base. Dans les conditions testées, il n’a pas été observé de différences sur la contamination particulaire des 5 métaux étudiés. En revanche, pour les 11 composés organiques étudiés, il est difficile de conclure car ils sont retrouvés en très faibles concentrations."
"La matière organique particulaire (MOP) constitue une phase solide qui peut avoir un rôle important pour la fixation de contaminants organiques ou minéraux, et donc sur leur transfert dans l’environnement. Ces liens connus pour les sols contaminés le sont beaucoup moins dans les milieux aquatiques, où la diversité de l’origine de la matière organique et des sources de contaminants induit une hétérogénéité encore à contraindre. Ce livrable présente la synthèse des études réalisées dans le cadre de l’OSR 5 sur ces liens qui pourraient exister entre matière organique particulaire et teneurs en contaminants métalliques ou radionucléides dans les matières en suspension du Rhône (MES). Il se base sur l’ensemble des données disponibles de l’OSR aux stations d’Arles et de Jons, et sur des résultats du programme de recherche CANADER (porteur H. Lepage) financé dans le cadre de l’OHM-VR en 2019-2020. Les teneurs en carbone organique particulaire (COP) sont utilisées comme traceur de la MOP. Leur variabilité est élevée et représente entre 1 à 10% de la masse des MES sur Arles. Elles diminuent en proportion dans les MES avec le débit et la charge : plus le débit est fort, plus la part de fraction organique est faible. Certains évènements de crue se dissocient de cette tendance dans le cas où ils constituent des apports de matière organique fossile géologique issues des sols érodés (Durance et Isère notamment). Les résultats montrent cependant que la mesure du seul COP ne suffit pas à différencier les sources de MOP, et il faut passer par d’autres traceurs (chlorophylle, stérols…) qui révèlent trois origines pour la MOP actuelle : une production algale autochtone, un apport des végétaux supérieurs des bassins versants, des apports de stations d’épuration. Les proportions de ces apports ne peuvent pas être précisées avec la base des données disponibles, mais de manière relative ils varient dans l’espace et le temps. La production algale est notamment marquée par des blooms printaniers ou estivaux sur de courtes périodes qui restent difficile à capter lors des échantillonnages. Aucune relation claire et/ou généralisable n’a pu être mise en évidence entre le COP et les teneurs en éléments traces métalliques (ETM), le 14C ou le tritium. On peut toutefois retenir les points suivants : a) il existe une possibilité d’intégration du 14C rejeté par les industries nucléaires au sein de la matière organique autochtone (photosynthèse), constituant une voie de transfert privilégiée vers la chaîne trophique. Cette incorporation serait notamment favorisée lors des faibles teneurs en MES. b) les données actuelles ne montrent pas de lien entre la variabilité de la MOP naturelle et celle des teneurs en tritium organiquement lié soulignant la prédominance des formes technogéniques du tritium dans le fleuve (tritium « horloger ») c) les relations entre ETM et COP en Arles ou Jons ne sont pas généralisables. On observe sur Jons une relation positive entre COP et Cu, Zn, Pb et Cd, mais elle se caractérise par un comportement inversé entre les chasses-crues et débits normaux. La relation positive lors des débits normaux confirme cependant le lien fort entre ces ETM et MOP. Sur Arles, cette relation existe encore mais n’est plus significative statistiquement. Les MES sur cette dernière station résultent d’une homogénéisation des apports de tout le bassin, et la diversité d’origine du COP et des contaminants lisse et annule les relations qui pourraient peut-être exister au sein de chaque affluent (comme celle observée sur Jons)."
"Comment, depuis plusieurs décennies, la recherche scientifique contribue-t-elle au développement des pays du Sud ? À travers plus de 100 succès emblématiques de la recherche en partenariat, cet ouvrage nous plonge au coeur des grandes questions de développement : oeuvrer pour des sociétés plus justes, lutter contre les maladies, faire face aux risques naturels, mettre en place une agriculture durable garantissant la sécurité alimentaire, préserver la biodiversité, partager les savoirs... Il montre ainsi comment la recherche contribue à l'amélioration des conditions de vie et à la préservation de l'environnement dans les pays en développement, en soulignant le rôle de la science pour répondre aux défis du monde actuel et à venir. Composé de textes courts, didactiques et richement illustrés, il s'adresse à tous les publics."
"Ce rapport présente les évaluations de sensibilité des habitats benthiques de Méditerranée à certaines pressions physiques d’origine anthropique réalisées à partir des meilleures connaissances actuelles en collaboration avec des experts scientifiques. Il présente un rappel du cadre d’évaluation et des limites du travail ainsi que les matrices d’évaluation de chaque habitat. Chaque matrice comporte, pour chaque pression physique, un score de résistance, un score de résilience et un score de sensibilité auxquels sont associés des indices de confiance, ainsi qu’une description des critères justifiant les scores attribués. Les évaluations de sensibilité génériques issues de ce projet ont pour vocation à servir d’outil d’aide au suivi et à la gestion du milieu marin, notamment à travers les évaluations de vulnérabilité/risque d’impact des habitats benthiques."
fr_abstract_s
no abstract
"Ce travail de thèse propose une méthode générale de reconstruction de défauts. Cette méthode donne un aperçu sur le problème d’observabilité des entrées inconnues. Par la suite, une méthodologie de détection et d’isolation de défauts capteurs et actionneurs est proposée. Le schéma de FDI est basé sur une banque d’observateurs. L’implémentation de cette méthode pour un modèle ASM1 réduit conduit à une table de signature fortement localisante.La deuxième partie porte sur la problématique de « l’observation des systèmes non linéaires ». Le filtre de Kalman étendu (FKE) est l’un des observateurs les plus largement utilisé à cette fin. Cependant, la convergence de cet observateur n’est pas prouvée. Lorsque le FKE est appliqué à un système mis sous une forme canonique d’observabilité, il acquiert, des propriétés de convergence exponentielle globales. Cependant, ce dernier entraine une amplification de bruit. Afin de combiner l’efficacité d’un FKE en termes de lissage de bruit, et la réactivité d’un OKE grand gain face aux larges variations, [Boizot et al., 2010] ont proposé un observateur adaptatif. Ainsi, cet observateur est appliqué au système non-linéaire MIMO d’une station d’épuration biologique. Une étude comparative entre ces trois observateurs est menée afin de mettre en évidence la pertinence de l’observateur adaptatif."
"Les savoirs professionnels spécialisés liés à la notion de métier s'expriment dans des formes langagières spécifiques qui ne sont pas toujours celles employées dans les corpus. Ces différences de langues d'usages au sein d'un même domaine sont liées à la coexistence de diverses communautés de pratiques. Nous analyserons, à travers une étude empirique menée dans l'ingénierie nucléaire, comment se forment cultures, langues et savoirs spécialisés de métier, et comment la richesse de ces variétés terminologiques au sein d'un même domaine peut être prise en compte dans un système utilisant une conceptualisation commune."
"L'utilisation des technologies du web dans les métiers de la formation permet d'envisager de nouvelles approches d'apprentissage. Toutefois la qualité de ces approches dépend de leur capacité à fournir aux apprenants, des contenus et des parcours pédagogiques adaptés à leurs besoins. La solution présentée propose de mettre à la disposition des apprenants un ensemble de services pédagogiques. Un service fournit un fragment de processus pour répondre à un certain objectif pédagogique. C'est en composant des services de manière dynamique que l'on peut construire des parcours adaptés. L'approche POPS (Process-Oriented Pedagogic Service) est un cadre conceptuel qui définit un modèle pour décrire les services pédagogiques et un processus de composition de services pour construire des parcours personnalisés. Des ontologies du domaine enseigné et de la pédagogie sont proposées pour, d'une part, décrire les services et d'autre part, en faciliter la recherche et la composition. L'approche a été appliquée pour l'enseignement du langage UML dans le cadre d'un projet Campus Numérique."
"Ce mémoire de thèse présente une étude fondamentale enrichie par des contributions qui sont articulées autour de la modélisation de processus ainsi qu'un diagnostic de défauts en utilisant l'analyse en composantes principales (ACP). Dans l'objectif d'un choix optimal du modèle ACP, une étude comparative de quelques critères connus dans la littérature nous a permis de conclure que le problème rencontré est souvent lié à une ignorance des variables indépendantes et quasi-indépendantes. Dans ce cadre, nous avons réalisé deux démonstrations mettant en évidence les limitations de deux critères en particulier la variance non reconstruite (VNR). En s'appuyant sur le principe d'une telle variance, nous avons proposé trois nouveaux critères. Parmi eux, deux ont été considérés comme étant empiriques car seule l'expérience permettra de prouver leur efficacité. Le troisième critère noté VNRVI représente un remède à la limitation du critère VNR. Une étude de sa consistance théorique a permis d'établir les conditions garantissant l'optimalité de son choix. Les résultats de simulation ont validé une telle théorie en prouvant ainsi que le critère VNRVI étant plus efficace que ceux étudiés dans cette thèse. Dans le cadre d'un diagnostic de défauts par ACP, l'approche de reconstruction des indices de détection ainsi que celle des contributions ont été utilisées. A travers une étude de généralisation, nous avons étendu le concept d'isolabilité de défauts par reconstruction à tout indice quadratique. Une telle généralisation nous a permis d'élaborer une analyse théorique d'isolabilité de défauts par reconstruction de la distance combinée versus celles des indices SPE et T2 de Hotelling en mettant en avant l'avantage de l'utilisation d'une telle distance. D'autre part, nous avons proposé une nouvelle méthode de contribution par décomposition partielle de l'indice SPE. Cette approche garantit un diagnostic correct de défauts simples ayant de grandes amplitudes. Nous avons également étendu une méthode de contribution classiquement connue par la RBC au cas multidimensionnel. Ainsi, la nouvelle forme garantit un diagnostic correct de défauts multiples de grandes amplitudes. En considérant la complexité de défauts, nous avons exploité la nouvelle approche de contribution RBC afin de proposer une nouvelle qui s'appelle RBCr. Cette dernière s'appuie sur un seuil de tolérance pour l'isolation de défauts. Une analyse de diagnosticabilité basée sur la RBCr montre que celle-ci garantit l'identification des défauts détectables. Ces derniers sont garantis isolables si leurs amplitudes satisfont les mêmes conditions d'isolabilité établies pour l'approche de reconstruction des indices."
"Les récents progrès des microtechnologies permettent le développement de drones d'envergure inférieure à 15 cm, susceptibles de réaliser des missions d'observation ou d'intervention en milieu risqué. La possibilité de vol en présence d'obstacles ou en espace fermé nécessite une grande agilité à basse vitesse et des capacités de déplacement silencieux, pour lesquelles un concept à ailes battantes – inspiré du vol des insectes et du colibri – semble être le plus prometteur. Dans le cadre de cette thèse, nous avons donc développé un modèle de simulation de type mécanique du vol d'un tel engin, à partir de résultats antérieurs concernant l'aérodynamique du vol des insectes. Nous avons ensuite déterminé les cinématiques de battement optimales à l'aide d'algorithmes heuristiques, avant de chercher à commander en boucle fermée ce système non linéaire naturellement instable. Une technique inspirée du backstepping a permis d'obtenir de très bonnes performances, en statique comme en dynamique."
"Malgré leur volume important et leur accessibilité, de nombreuses données numériques ne peuvent être correctement exploitées car elles sont contenues dans des textes sous des formes peu ou pas structurées. L'extraction de relations est un processus qui rassemble des techniques pour extraire des entités et des relations à partir de textes, nous donnant la possibilité d'enrichir des bases de connaissances de façon automatique. Cependant le langage naturel est de façon intrinsèque porteur d'ambiguïté, ce qui constitue un premier niveau d'incertitude auquel on peut rajouter l'imprécision due aux formulations telles que ""je crois que"", ""il semble que"", etc. La base de connaissances doit donc tenir compte de cette incertitude par exemple en associant à chaque nouvelle connaissance extraite un score de confiance dépendant du degré de certitude. Cet article est une communication de synthèse qui détaille les différentes problématiques liées à l'incertitude et à l'imprécision au cours de la chaîne de traitement allant de l'extraction d'informations dans les textes à l'inférence de connaissances. Il y sera notamment"
"3 Université d'Aix-Marseille-LSIS 4 Kware {vincent.bouvier, patrice.bellot}@lsis.org RÉSUMÉ. La tâche de désambiguïsation des entités nommées consiste à lier une mention ambiguë d'une entité dans un document à l'entité correspondante dans une base de connaissances. Dans ce travail, nous nous plaçons dans un cadre applicatif ""inverse"" et nous ajoutons une contrainte temporelle : nous souhaitons surveiller un flux de nouveaux documents Web et déterminer quels sont ceux mentionnant une entité donnée tout en mesurant l'importance de l'information conte-nue. Une telle approche peut servir à recommander des documents à des contributeurs si une information mérite d'être ajoutée dans la base de connaissances cible. Notre approche repose sur l'utilisation de deux classifieurs prenant en compte, pour déterminer l'intérêt d'un document du flux, des indices comme la fréquence de mentions de l'entité dans le temps ou dans le document, sa position ou encore la présence d'entités liées connues. Notre approche et l'impact des paramètres utilisés ont été évalués via une participation à la tâche ""Knowledge Base Acce-leration"" de TREC 2012 et a positionné notre équipe au rang 3 sur 11 (Bonnefoy et al., 2012). ABSTRACT. Name entity disambiguation is the task of linking an ambiguous name in a document to the unique real-world entity in a kwnoledge base (KB) its represents. We took the opposite problem and add a time constraint : we monitor a data stream to detect in real-time documents about an entity from a KB and determine to what extent the information in those documents matter. It could be used to reduce time lag between the moment a new important information about an entity shows up and the moment it is added to the knowledge base. We used Random Forests combined with time-related features (eg. count of mentions in time) and document and related entities centric features to tackle this problem. The effectiveness and impact of the features used have been evaluated through our participation to the ""Knowledge Base Acceleration"" task at TREC 2012 and positionned our team rank 3 on 11 (Bonnefoy et al., 2012). MOTS-CLÉS : entité nommée, base de connaissances, kba, trec, flux"
no abstract
no abstract
no abstract
no abstract
"L'identification et l'exploitation de structures cachées dans un problème est reconnue comme étant un moyen fondamental pour contrecarrer l'explosion combinatoire de sa résolution. Récemment, une structure particulière appelée (strong) backdoor a été identifiée pour le problème de satisfaisabilité de formule CNF (SAT). Certaines connexions entre les ensembles strong backdoor et la difficulté intrinsèque des problèmes SAT ont été mises en évidence, permettant une meilleure approximation de la borne de complexité en temps dans le pire des cas. On peut calculer des ensembles strong backdoor pour chaque classe polynomiale. Dans [Parisetals06], une méthode d'approximation d'ensembles strong backdoor pour la classe des formules de Horn a été proposée. Cette approximation est réalisée en deux étapes. Dans un premier temps, on calcule le meilleur Horn renommage du point de vue du nombre de clauses de Horn de la CNF de départ. Ensuite on extrait un ensemble Horn strong backdoor de la partie non Horn de la formule renommée. Dans cet article, nous proposons de calculer des ensembles Horn strong backdoor en utilisant le même procédé mais en minimisant le nombre de littéraux positifs dans la partie non Horn de la formule renommée au lieu du nombre de clauses. Puis nous étendons cette méthode à la classe des formules ordonnée [benoist99] qui est une extension de la classe des formules de Horn. Cette méthode nous garantit l'obtention d'ensembles Ordonné strong backdoor de taille plus petite ou égale à ceux des ensembles Horn strong backdoor (jamais plus grande). Les résultats expérimentaux montrent que ces nouvelles méthodes permettent de réduire la taille des ensembles strong backdoor sur certaines instances et que leur exploitation permet également d'améliorer les performances des solveurs SAT."
"Real-time adaptive production control in the flexible manufacturing cell (FMC) is a complex issue that needs to be addressed to realize good performance and high productivity. In this paper, we have considered a support vector machine (SVM)-based simulation approach to resolve a production control problem in an FMC that operates in a dynamic environment. A SVM-based simulation approach chooses the most relevant scheduling rule out of several predefined ones on the basis of the current states of the system. This paper examines and compares the performance of the SVM-based simulation approach with the competent scheduling rules under two different operational environments which are characterized by the uncertainty of demand. We have also developed a Visual Basic-based simulation approach for scheduling of component parts in the context of FMC under different situations. The SVM methodology to control the production offers better performance than the single-rule-based production control system."
"Dans cette contribution, nous proposons une nouvelle évaluation de la complexité de la procédure Forward Checking appliquée à la résolution de CSP n-aires à domaines finis. Généralement, cette évaluation prend en compte la taille des domaines associés aux variables. Ici, nous l'exprimons relativement à la taille des relations de compatibilité associées aux contraintes. Cette nouvelle approche permet ainsi, parfois, de fournir une meilleure borne de complexité théorique. Nous montrons l'intérêt essentiel de cette démarche en revenant sur les résultats proposés dans [10] et qui concernent la hiérarchie des décompositions de CSP (comparaisons des méthodes de décompositions structurelles), et qui constituent un résultat fondamental du domaine. Nous invalidons une partie de ces résultats en démontrant notamment que le concept de décomposition en hyperarbres, situé au sommet de cette hiérarchie, n'est finalement pas meilleur que celui de décomposition arborescente. Ce résultat, essentiellement théorique, s'avère toutefois en adéquation avec les observations expérimentales obtenues ces dernières années au sein de la communauté."
"Tester la consistance de CSP est en théorie un problème NP-Complet. Il existe deux familles de méthodes pour le test de consistance. La première famille regroupe les méthodes complètes qui font un parcours exhaustif de l'espace de recherche de solutions. Ces méthodes ont l'avantage de prouver l'inconsistance de CSP, cependant leur complexité croît exponentiellement avec la taille du problème. La seconde famille inclut les méthodes incomplètes qui font de la recherche locale sur l'espace de recherche de solutions. Ces méthodes ont été utilisées efficacement pour trouver des solutions pour des problèmes de grande taille que les méthodes complètes ne peuvent résoudre. Le principal inconvénient des méthodes incomplètes reste tout de même leur incapacité de prouver l'inconsistance d'une instance CSP. L'un des challenges mis en avant par la communauté CP (Selman et al. 1997) est de proposer des méthodes incomplètes efficaces pour la preuve d'inconsistance de CSP. Le travail que nous présentons ici est une contribution à ce difficile challenge. Nous introduisons une nouvelle méthode incomplète pour le test d'inconsistance qui se base sur la notion de dominance entre CSPs et la coloration de la micro-structure du CSP. Nous avons expérimenté la méthode sur des instances CSP générées aléatoirement et les résultats obtenus sont très encourageant."
"Les duels sportifs tels que l'escrime, la boxe et le tennis de table comme certains face-à-face individuels en sport collectif, donnent lieu à des interactions visuo-motrices extrêmement rapides. Ces situations, caractérisées par une forte incertitude spatio-temporelle, exigent des protagonistes un très haut niveau d'attention visuelle. Des enquêtes réalisées dans ces sports révèlent, au plus haut niveau des classements mondiaux, une fréquence exceptionnelle des gauchers manuels, la plupart d'entre eux ayant une dominance oculaire droite. Les droitiers ayant l'oeil gauche dominant s'y trouvent aussi à un taux relativement élevé. Des tâches expérimentales avec incertitude spatio-temporelle ont déjà permis de confirmer un avantage visuo-moteur, en temps de réponse, chez des sujets ayant une latéralité oeil–main croisée. Objectifs. – Le but de l'étude est d'identifier le processus central permettant d'expliquer un tel avantage. Il s'agit pour cela, d'une part, d'actualiser les données sur la dominance oculaire, d'autre part, de faire un bilan des travaux capables d'éclairer la problématique révélée par les duels sportifs. Données actuelles. – Il apparaît que la fonction de l'oeil dominant emprunte la voie géniculo-striée, c'est-à-dire une voie ipsilatérale entre l'oeil et le cortex hémisphérique. Par ailleurs, il se confirme que les effets spécifiques de la dominance oculaire sur la coordination oeil–main n'apparaissent qu'à partir d'un certain niveau d'incertitude spatiale. Conclusion. – Dans les conditions de tâche qui caractérisent les duels sportifs, l'oeil dominant est sollicité et mis en rapport fonctionnel, via le noyau géniculé-latéral, avec l'hémisphère ipsilatéral. Pour la réponse manuelle, au contraire, la main de réponse est en rapport, via son aire motrice, avec l'hémisphère contralatéral. En conséquence, la liaison fonctionnelle entre l'entrée visuelle et la sortie motrice n'implique qu'un hémisphère chez les sujets dont l'oeil dominant et la main de réponse sont eux-mêmes contralatéraux. Ces sujets font ainsi l'économie d'un transfert interhémisphérique, relativement coûteux en temps. Il en résulte pour eux un avantage, en temps de réponse, par rapport aux sujets dont l'oeil dominant et la main de réponse sont ipsilatéraux."
no abstract
no abstract
no abstract
no abstract
no abstract
"Integrated process planning and scheduling (IPPS) is a manufacturing strategy that considers process planning and scheduling as an integrated function rather than two separated functions performed sequentially. In this paper, we propose a new heuristic to IPPS problem for reconfigurable manufacturing systems (RMS). An RMS consists mainly of reconfigurable machine tools (RMTs), each with multiple configurations, and can perform different operations with different capacities. The proposed heuristic takes into account the multi-configuration nature of machines to integrate both process planning and scheduling. To illustrate the applicability and the efficiency of the proposed heuristic, a numerical example is presented where the heuristic is compared to a classical sequential process planning and scheduling strategy using a discrete-event simulation framework. The results show an advantage of the proposed heuristic over the sequential process planning and scheduling strategy."
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"Le problème de placement orthogonal (OPP) consiste à déterminer si un ensemble d'objets peut etre placé dans un conteneur de taille connue. Ce problème est NP-complet. Une modélisation de ce problème à base de graphes d'intervalles a été proposée par S. P. Fekete et al. Cette modélisation permet de représenter des classes de placements équivalents, diminuant d'autant l'espace de recherche. Dans cet article nous proposons de représenter par des formules de la logique propositionnelle la modélisation de S. P. Fekete et al. Nous avons implémenté cette approche en utilisant le solveur MiniSat, et nous l'avons comparée d'une part avec les résultats de S. P. Fekete et al. sur des problèmes classiques, et d'autre part avec l'approche de T. Soh et al. basée aussi sur un codage SAT sur des problèmes de Strip Packing."
no abstract
"Les travaux présentés ici sont centrés sur l’entreprise et proposent une méthode d’intégration du territoire et de ses parties prenantes dans les processus internes de l’entreprise. L’intégration de ces ressources latentes à travers les réseaux de création de valeur doit permettre aux entreprises d’améliorer leur performance globale par l’innovation organisationnelle. La prise en compte de nouveaux actifs matériels et immatériels dans l’analyse stratégique peut également mener les entreprises à repenser leur modèle d’affaire. Toutes les activités de l’entreprise sont impactées par cette approche globale : ressources humaines, contrôle de gestion, opérations… La mobilisation de ressources territoriales couplée à une vision partenariale de la création de valeur doit permettre de créer de la valeur partagée entre l’entreprise, ses parties prenantes et son territoire. L’objectif de ces travaux étant d’accompagner les entreprises dans leur démarche de responsabilisation à moyen terme. Les entreprises visées par ces travaux sont, dans un premier temps, les PME/PMI productives, inscrites sur un seul territoire. Après une première phase de développement, des expérimentations seront menées dans les entreprises partenaires du projet ANR Convergence."
"La présente invention concerne le domaine de l’usinage par enlèvement de matière. L’invention a ainsi pour objet une machine d’usinage et, plus particulièrement, une machine de perçage ou de fraisage. La machine à la particularité d’intégrer deux moteurs électriques destinés à contrôler les mouvements d’avance et de rotation d’un outil, tout en permettant une action d’assistance vibratoire, particulièrement utile pour le perçage de matériaux difficiles à usiner."
no abstract
no abstract
no abstract
no abstract
no abstract
"Les récentes avancées en matière de systèmes d'acquisition et de modélisation ont permis la mise à disposition d'une très grande quantité de données numériques (e.g. images, vidéos, modèles 3D) dans différents domaines d'application. En particulier, la création d'Environnements Virtuels (EVs) nécessite l'exploitation de données nu-mériques pour permettre des simulations et des effets proches de la réalité. Malgré ces avancées, la conception d'EVs dédiés à certaines applications requiert encore de nombreuses et parfois laborieuses étapes de modélisation et de traitement qui impliquent plusieurs experts (e.g. experts du domaine de l'application, experts en modélisation 3D et programmeur d'environnements virtuels, designers et experts communication/marketing). En fonction de l'application visée, le nombre et le profil des experts impliqués peuvent varier. Les limitations et difficultés d'au-jourd'hui sont principalement dues au fait qu'il n'existe aucune relation forte entre les experts du domaine qui ont des besoins, les experts du numérique ainsi que les outils et les modèles qui prennent part au processus de déve-loppement de l'EV. En fait, les outils existants focalisent sur des définitions souvent très détaillées des formes et ne sont pas capables de supporter les processus de créativité et d'innovation pourtant garants du succès d'un pro-duit ou d'une application. De plus, la grande quantité de données numériques aujourd'hui accessible n'est pas réellement exploitée. Clairement, les idées innovantes viennent souvent de la combinaison d'éléments et les don-nées numériques disponibles pourraient être mieux utilisées. Aussi, l'existence de nouveaux outils permettant la réutilisation et la combinaison de ces données serait d'une grande aide lors de la phase de conception conceptuelle de formes et d'EVs. Pour répondre à ces besoins, cette thèse propose une nouvelle approche et un nouvel outil pour la conception conceptuelle d'EVs exploitant au maximum des ressources existantes, en les intégrant et en les combinant tout en conservant leurs propriétés sémantiques. C'est ainsi que le Modèle de Description Générique de Formes (MDGF) est introduit. Ce modèle permet la combinaison de données multimodales (e.g. images et maillages 3D) selon trois niveaux : Conceptuel, Intermédiaire et Données. Le niveau Conceptuel exprime quelles sont les différentes parties de la forme ainsi que la façon dont elles sont combinées. Chaque partie est définie par un Elément qui peut être soit un Composant soit un Groupe de Composants lorsque ceux-ci possèdent des carac-téristiques communes (e.g. comportement, sens). Les Eléments sont liés par des Relations définies au niveau Con-ceptuel là où les experts du domaine interagissent. Chaque Composant est ensuite décrit au niveau Données par sa Géométrie, sa Structure et ses informations Sémantiques potentiellement attachées. Dans l'approche proposée, un Composant est une partie d'image ou une partie d'un maillage triangulaire 3D. Quatre Relations sont proposées (fusion, assemblage, shaping et localisation) et décomposées en un ensemble de Contraintes qui contrôlent la po-sition relative, l'orientation et le facteur d'échelle des Composants au sein de la scène graphique. Les Contraintes sont stockées au niveau Intermédiaire et agissent sur des Entités Clés (e.g. points, des lignes) attachées à la Géo-métrie ou à la Structure des Composants. Toutes ces contraintes sont résolues en minimisant une fonction énergie basée sur des grandeurs physiques. Les concepts du MDGF ont été implémentés et intégrés au sein d'un outil de design conceptuel développé par l'auteur. Différents exemples illustrent le potentiel de l'approche appliquée à différents domaines d'application."
no abstract
no abstract
no abstract
"Les ondes acoustiques subissent peu de dispersion dans le milieu marin, comparé au milieu aérien. Certaines espèces de cétacés communiquent ainsi à grande distance, d'autres utilisent leurs émissions sonores pour s'orienter. La bioacoustique consiste à étudier ces espèces à partir de l'analyse de leurs sons, c'est-à-dire à les détecter, classer, localiser. Cela peut se faire via un réseau d'hydrophones au déploiement fastidieux. Afin de contribuer au passage à l'échelle de la bioacoustique, cette thèse propose des modèles originaux mono-hydrophone pour l'analyse de ces signaux stationnaires ou transitoires. Premièrement, nous dérivons un nouveau modèle d'estimation de la distance entre une source impulsive (ex. biosonar) et un hydrophone. Notre modèle théorique, l'Intra Spectral ATténuation(ISAT), dérive des lois acoustiques de déformation spectrale du signal transitoire induite par l'atténuation durant sa propagation. Ce modèle relie les rapports énergétiques des bandes de fréquences pondérés par un modèle de perte par atténuation fréquentielle (Thorp ou Leroy) à la distance de propagation. Nous approximons aussi ISAT par un modèle neuromimétique. Ces deux modèles sont validés sur le sonar du cachalot (Physeter macrocephalus) enregistré avec notre bouée acoustique autonome BOMBYX et notre système d'acquisition DECAV en collaboration avec le Parc National de Port-Cros et le sanctuaire Pelagos pour la protection des mammifères marins en Méditerranée. Les mesures d'erreur (RMSE) d'environ 500 mètres sur nos références du centre d'essai OTAN aux Bahamas présentent un intérêt opérationnel. Deuxièmement, nous proposons une analyse originale de l'évolution des voisements de cétacé par codage parcimonieux. Notre encodage des cepstres par apprentissage non supervisé d'un dictionnaire met en évidence l'évolution temporelle des bigrammes des chants que les baleines à bosse mâles émettent durant la période de reproduction. Nous validons ce modèle sur nos enregistrements du canal de Sainte-Marie à Madagascar entre 2008 et 2014, via notre réseau d'hydrophones BAOBAB qui constitue une première dans l'Océan Indien. Nos modèles s'inscrivent dans le projet Scaled Bioacoustics (SABIOD, MI CNRS) et ouvrent de nouvelles perspectives pour les passages à l'échelle temporelle et spatiale de la bioacoustique."
"Un CSP ou problème de satisfaction de contraintes, consiste à donner des valeurs à des variables en respectant les contraintes qui les lient. Mais le problème de décision associé à un CSP est NP-complet. Dans [Jégou, 1993], Jégou propose une méthode, basée sur la décomposition de la micro-structure du CSP et dans [Chmeiss, 2003], une généralisation de cette méthode est présentée en utilisant une généralisation des graphes triangulés : les graphes CSGk. Nous proposons une amélioration de la décomposition de CSP basée sur les graphes CSG2. Cela passe par un calcul de 2-triangulation plus efficace tant au niveau de la complexité qu'à celui de la qualité de la décomposition, mais aussi un algorithme de recherche de cliques maximales dans les graphes 2-triangulés, apportant de bons résultats en pratique."
"Cette thèse porte sur l’apprentissage statistique et l’analyse de données multi-dimensionnelles. Elle se focalise particulièrement sur l’apprentissage non supervisé de modèles génératifs pour la classiﬁcation automatique. Nous étudions les modèles de mélanges Gaussians, aussi bien dans le contexte d’estimation par maximum de vraisemblance via l’algorithme EM, que dans le contexte Bayésien d’estimation par Maximum A Posteriori via des techniques d’échantillonnage par Monte Carlo. Nous considérons principalement les modèles de mélange parcimonieux qui reposent sur une décomposition spectrale de la matrice de covariance et qui oﬀre un cadre ﬂexible notamment pour les problèmes de classiﬁcation en grande dimension. Ensuite, nous investiguons les mélanges Bayésiens non-paramétriques qui se basent sur des processus généraux ﬂexibles comme le processus de Dirichlet et le Processus du Restaurant Chinois. Cette formulation non-paramétrique des modèles est pertinente aussi bien pour l’apprentissage du modèle, que pour la question diﬃcile du choix de modèle. Nous proposons de nouveaux modèles de mélanges Bayésiens non-paramétriques parcimonieux et dérivons une technique d’échantillonnage par Monte Carlo dans laquelle le modèle de mélange et son nombre de composantes sont appris simultanément à partir des données. La sélection de la structure du modèle est eﬀectuée en utilisant le facteur de Bayes. Ces modèles, par leur formulation non-paramétrique et parcimonieuse, sont utiles pour les problèmes d’analyse de masses de données lorsque le nombre de classe est indéterminé et augmente avec les données, et lorsque la dimension est grande. Les modèles proposés validés sur des données simulées et des jeux de données réelles standard. Ensuite, ils sont appliqués sur un problème réel diﬃcile de structuration automatique de données bioacoustiques complexes issues de signaux de chant de baleine. Enﬁn, nous ouvrons des perspectives Markoviennes via les processus de Dirichlet hiérarchiques pour les modèles Markov cachés."
"Cette thèse est une contribution au problème du pronostic des systèmes complexes. Plus précisément, elle concerne l'approche basée modèles et est composée de trois contributions principales. Tout d'abord, dans une première contribution une définition du concept de pronostic est proposée et est positionnée par rapport aux concepts de diagnostic et de diagnostic prédictif. Pour cela, une notion de contrainte temporelle a été introduite afin de donner toute pertinence à la prédiction réalisée. Il a également été montré comment le pronostic est lié à la notion d'accessibilité en temps fini. La deuxième contribution est dédiée à l'utilisation des observateurs à convergence en temps fini pour la problématique du pronostic. Une méthodologie de pronostic est présentée pour les systèmes non linéaires à échelle de temps multiple. Puis, une troisième contribution est introduite par l'utilisation des observateurs par intervalle pour le pronostic. Une méthodologie de pronostic est proposée pour les systèmes non linéaires incertains à échelle de temps multiple. Pour illustrer les différents résultats théoriques, des simulations ont été conduites sur un modèle de comportement d'un oscillateur électromécanique."
"Le suivi des zones côtières nécessite à la fois une bonne résolution spatiale, une bonne résolution spectraleassociée à un bon rapport signal sur bruit et enfin une bonne résolution temporelle pour visualiser deschangements rapides de couleur de l’eau.Les capteurs disponibles actuellement, et même ceux prévus prochainement, n’apportent pas à la fois unebonne résolution spatiale, spectrale ET temporelle. Dans cette étude, nous nous intéressons à la fusion de 2futurs capteurs qui s’inscrivent tous deux dans le programme Copernicus de l’agence spatiale européenne:MSI sur Sentinel-2 et OLCI sur Sentinel-3.Comme les capteurs MSI et OLCI ne fournissent pas encore d’images, il a fallu les simuler. Pour cela nousavons eu recours aux images hyperspectrales du capteur HICO. Nous avons alors proposé 3 méthodes : uneadaptation de la méthode ARSIS à la fusion d’images multispectrales (ARSIS), une méthode de fusion baséesur la factorisation de tenseurs non-négatifs (Tenseur) et une méthode de fusion basée sur l’inversion dematrices (Inversion)Ces 3 méthodes ont tout d’abord été évaluées à l’aide de paramètres statistiques entre les images obtenuespar fusion et l’image « parfaite » ainsi que sur les résultats d’estimation de paramètres biophysiques obtenuspar minimisation du modèle de transfert radiatif dans l’eau."
Les outils sont des moyens techniques et cognitifs mais aussi un processus social et culturel révélateur d'une action située et de connaissances partagées et distribuées. Les outils et dispositifs sont des constructions humaines et leur développement dépend étroitement du mode opératoire spécifique et de la réflexion des utilisateurs sur les finalités. Le processus artefactuel permet de modéliser ces aspects.
"Le concept français d'intelligence économique est un concept riche et complexe, construit progressivement et récemment institutionnalisé à travers la mise en place d'une "" politique publique d'intelligence économique "" française. Lié à l'histoire et la culture françaises d'une place importante donnée à l'Etat, influencé par la conception systémique et constructiviste qui parcourt de nombreuses recherches en sciences sociales en France, le concept revêt des caractéristiques différentes de ce que l'on peut trouver dans d'autres pays. L'article présente l'histoire, la définition et les tendances actuelles du concept, qui a semble-t-il trouvé sa légitimité en se rapprochant de la politique territoriale française à travers l'intelligence économique territoriale."
"Le CEA développe et utilise des logiciels de calcul, également appelés codes de calcul, dans diﬀérentes disciplines physiques pour optimiser les coûts de ses installations et de ses expérimentations. Lors d'une étude, plusieurs phénomènes physiques interagissent. Un couplage et des échanges de données entre plusieurs codes sont nécessaires. Chaque code réalise ses calculs sur une géométrie, généralement représentée sous forme d'un maillage contenant des milliers voire des millions de mailles. Cette thèse se focalise sur le transfert de déformations géométriques entre les maillages spéciﬁques de chacun des codes de calcul couplés. Pour cela, elle présente une méthode de couplage de plusieurs codes, dont le calcul des déformations est réalisé par l'un d'entre eux. Elle traite également de la mise en place d'un modèle commun aux diﬀérents codes de l'étude regroupant l'ensemble des données partagées. Enﬁn, elle porte sur les transferts de déformations entre des maillages représentant une même géométrie ou des géométries adjacentes. Les modiﬁcations géométriques sont de nature discrète car elles s'appuient sur un maillage. Aﬁn de les rendre accessible à l'ensemble des codes de l'étude et pour permettre leur transfert, une représentation continue est calculée. Pour cela, deux fonctions sont développées : l'une à support global, l'autre à support local. Toutes deux combinent une méthode de simpliﬁcation et un réseau de fonctions de base radiale. Un cas d'application complet est traité dans le cadre du réacteur Jules Horowitz. L'eﬀet des dilatations diﬀérentielles sur le refroidissement d'un dispositif expérimental est étudié."
"Dans un contexte d’amélioration constante de la productivité et de la flexibilité des machines de production, les vibrations mécaniques demeurent un phénomène limitant les performances dynamiques. Le problème de réduction des vibrations induites par les perturbations extérieures (environnement dynamique, procédés de coupe) est généralement abordé par le biais de la mise en oeuvre de systèmes de dissipation passifs ou actifs. Quant aux vibrations induites par le mouvement des axes de la machine, elles peuvent être efficacement prises en compte lors de la synthèse de la trajectoire du système. Ce papier dresse un état de l’art des méthodes de réduction de vibration s’appuyant sur une adaptation de la trajectoire du système. Une analyse unifiée des méthodes applicables au domaine de la machine de production est réalisée. Des essais menés sur un robot industriel 6 axes viennent illustrer cette analyse."
"Cet article décrit comment l'évolution dynamique du trafic routier peut être prédite au niveau mésoscopique par Réseaux de Petri Lots Triangulaires (RdPLots Triangulaires). Dans ce contexte, nous comparons la modélisation et l'analyse du trafic routier par RdPLots Triangulaires avec le Modèle Cellulaire de Transmission (CTM). Les résultats de simulation de deux formalismes sur un exemple d'autoroute montrent l'efficacité, la facilité et la précision du formalisme événementiel RdPLots Triangulaire par rapport au CTM au regard de la réduction du nombre de pas de simulation et de la variation de la longueur de congestion."
no abstract
"Les hélicoptères sont le siège de comportements dynamiques difficiles à maîtriser et récurrents en phase de conception. Ces comportements sont essentiellement liés à des couplages mal maîtrisés entre certains sous-systèmes. Un état de l'art sur les pratiques de modélisation existantes met en évidence un manque de prise en compte des interactions énergétiques entre sous-systèmes, rendant difficile l'analyse et la maîtrise de ces phénomènes et conduisant à des solutions ponctuelles, sans capitalisation possible des méthodes mises en œuvre. Ces travaux offrent une introduction à une approche de représentation multiphysique et multiniveau, complémentaire aux approches existantes, offrant une vision énergétique et structurelle pour la maîtrise de la dynamique des systèmes tels que les hélicoptères. Une réflexion sur les outils de représentation existants a conduit au choix du bond graph (BG), du multibond graph (MBG) et de la Représentation Energétique Macroscopique (REM) en tant qu'outils complémentaires pour la modélisation et la commande des systèmes multiphysiques multicorps. Une analyse énergétique d'hélicoptère a conduit à la proposition d'une description macroscopique basée sur le MBG à mots, complétée par deux autres niveaux détaillant le modèle MBG du sous-système rotor-fuselage. Les hypothèses de modélisation sont choisies de manière à reproduire les conditions d'apparition du phénomène de résonance air, phénomène de couplage connu sur les hélicoptères. Cette étude met en évidence le potentiel des représentations énergétiques en application aux hélicoptères et ouvre de nombreuses perspectives, tant pour l'analyse des appareils existants que pour la conception d'aéronefs innovants."
no abstract
no abstract
"Ce texte envisage une modélisation du processus métier comme un système. Ce système met en relation des individus, des outils, des connaissances, une situation et un contexte. Pour cela, nous proposons de regarder les gestes professionnels comme des réponses liées à un contexte d'action. Nous distinguons les notions de processus et procédures et cette perspective nous autorise à regarder le processus métier comme un système cognitif indexé. C'est un système lié au contexte socio-informationnel. Il est organisé comme une connaissance communément partagée pour servir de système commun informationnel. Avec notre approche, le processus métier a une trois fonction. La première est organisante. Elle organise les connaissances situées. La deuxième est informationnelle. Elle est la source des médiations entre la base des connaissances et le geste professionnel. La troisième est régulatrice des deux premières. Notre point de vue se fonde sur l'étude du processus métier en tant que phénomène social et culturel révélateur d'une action située et d'une connaissance partagée et distribuée. C'est une approche ethnographique."
no abstract
"En tant qu'enseignants, nous cherchons à transmettre des savoirs et à faire de nos étudiants des personnes qualifiées, dotées de compétences professionnelles spécifiques à leur futur métier. La transmission du savoir s'est traditionnellement faite par une longue période d'apprentissage au contact des pairs, mais les situations actuelles de formation et de travail changent, se dirigeant vers des environnements toujours plus médiatés. Dans cet article nous exposons les notions clés touchant à la formation à une profession et à son exercice. Cela nous porte à nous interroger sur la manière dont s'intègre aujourd'hui la dimension expérientielle dans l'acquisition de la compétence. Pour répondre à cette question nous analysons deux études de cas relevant de situations d'enseignement médiaté par les Technologies de l'Information et de la Communication, l'une dans le milieu universitaire, l'autre dans le milieu industriel, plus spécifiquement dans l'ingénierie nucléaire d'EDF. Elles démontrent que le travail intellectualisé fait aujourd'hui appel à des capacités subjectives et cognitives qui trouvent leur accomplissement dans des pratiques sociales de réseaux."
pas de résumé en français
"La gestion à moyen terme de l'accroissement des risques territoriaux requiert des moyens de représentation et de simulation des dynamiques spatiales. Pour le cas du risque d'incendie de forêt, des modèles de dynamiques spatiales de zones combustibles et des zones vulnérables, à différentes échelles, existent. Pour assurer l'intégration des simulateurs de ces dynamiques de différentes thématiques et opérant à des niveaux d'échelle différents, nous proposons une infrastructure logicielle d'intégration à base d'agents spatiaux dans un environnement de système d'information géographique nommée Pyroxène. / Medium term management of territorial risk increase requires means to represent and simulate spatial dynamics. For forest fire risk, models of spatial dynamics of fuel zones and vulnerable zones exist. In order to integrate simulators of spatial dynamics of different thematic at different scale levels, we propose a spatial agent based and GIS based software infrastructure called Pyroxene."
"Les attributs structuraux des arbres sont des éléments forestiers cruciaux. Ils se trouvent au coeur de multiples applications telles que l'inventaire forestier, la gestion forestière et les modèles allométriques. Récemment, les Scanners Lasers Terrestres (SLT) ont été présentés comme une alternative fiable à la mesure manuelle des paramètres structuraux. Dans cet article nous présentons une estimation innovante du diamètre à hauteur de poitrine et du défilement des troncs à partir de données acquises par SLT. Notre méthodologie utilise un schéma multirésolution dans lequel intervient une combinaison de transformée de Hough et de contours actifs croissants ouverts. Bien que des résultats chiffrés ne soient pas encore disponibles, les tests effectués sur des données simulées et réelles (acquises en environnement forestier naturel) montrent des résultats encourageants."
"Le patrimoine documentaire des entreprises s'est souvent accumulé sans que ces dernières puissent s'adapter au rythme des évolutions des TIC. La mémoire collective qui ne cesse d'être produite voit sa masse croître, est devenue éparse et hétérogène et nombre d'entreprises aujourd'hui confrontées à des problématiques transverses ont du mal à mobiliser leurs connaissances de façon opérationnelle. Nous présentons ici le cas de la Division Ingénierie Nucléaire (DIN) d'EDF et la nécessité de valoriser son patrimoine informationnel. Nous exposons pourquoi un travail amont de contextualisation est essentiel dans des cas comme celui-ci, afin de situer le mode de fonctionnement du système d'information dans une problématique structurelle, les aspects techniques devant être rapidement dépassés pour prendre en compte l'organisation dans sa globalité. Dans ce contexte où problématiques micro et macro se confondent, les métiers cœurs de l'entreprise s'imposent comme la base de toute réflexion. La documentation qu'ils produisent et utilisent véhicule les connaissances techniques de l'entreprise, qui y sont exprimées par des concepts propres aux métiers. Leur terminologie est la clé permettant de valoriser les connaissances et de mieux gérer le patrimoine documentaire par lequel elles transitent. A travers l'exemple de la DIN, nous présentons notre approche résolument empirique et qualitative, pour faire évoluer le système existant vers une base de connaissances centrée sur le "" sens métier "" de l'organisation. Mots-clés : patrimoine documentaire, document technique, gestion des connaissances, base de connaissances, ontologie, terminologie."
"Le champ dans lequel se situe cette HDR est celui de la médiation des savoirs. Ce qui nous intéresse plus particulièrement dans cette médiation, ce sont : - Le rôle que jouent les Technologies de l'Information et de la communication ; - Le rôle que joue l'organisation que se donnent les individus en situation - Le rôle que jouent les relations qui se construisent entre les individus, les outils et les savoirs. Ces trois points pouvant se regrouper sous l'appellation d'artefact Bien que je ne souhaite pas développer une approche communicationnelle de la complexité, j'essaie d'envisager la communication comme un processus plus large que les champs disciplinaires qui la questionnent pour avoir une vision plus large, plus anthropologique dans laquelle la communication est considérée comme une construction humaine, comme un outil de la culture commune mais utiliser avec des règles établies dans un contrat situé. A travers cette approche je souhaite clarifier le rôle de la contextualisation de la communication sur la médiation des savoirs. C'est-à-dire que je souhaite questionner la nature intentionnelle, communicationnelle, éducative des outils à travers les modes d'utilisation au sein d'une communication socialement élaborée et collectivement validée. Ce questionnement devrait permettre de dépasser les approches disciplinaires qui envisagent les TIC comme des ressources pour l'action. Elle devrait permettre également d'envisager les Tic et leurs usages comme un système qui met en relation un utilisateur, un outil, une connaissance, une situation et un contexte. Nous pourrions appeler ce système relationnel : « système artefactuel ». Le système artefactuel serait donc un système relationnel et le terme d'artefact ne désignerait plus l'objet ou l'outil, mais le système représentatif, un système de pensée, qui se construit dans la relation. En SIC, nous pourrions envisager un artefact communicationnel comme un système de pensée qui autorise la compréhension d‘une situation de communication dans un contexte qui donne une intentionnalité communicative aux outils. Le programme de recherche qui pourrait se construire à partir de ce positionnement s'articulerait autour de deux axes : l'un théorique et l'autre pratique. Les questions théoriques que nous souhaitons éclaircir sont significatives d'une phase intermédiaire dans la construction de notre champ de recherche en SIC. Cette phase est nécessairement ouverte à plusieurs possibles, c'est-à-dire à un questionnement communicationnel aux frontières des différentes disciplines sollicitées. Plusieurs pistes seront donc revisitées : - Nous pensons particulièrement aux situations de communication instrumentée ; aux phénomènes de régulation intra et interindividuel ; au contrat communicationnel qui s'établit dans les espaces communs de communication... - La question de l'identification des processus interindividuels ou collectifs d'appropriation des instruments de communication qui renvoie aux pratiques induites par l'usage des TIC ou aux modes de contractualisation qui se construisent entre acteurs individuels ou collectifs et dans le cadre de la communication des connaissances. - La question des situations de communication sous-tendues par une intention communicationnelle, éducative, sociale qui renvoie aux rapports qu'entretiennent la communication et le champ d'investigation. Sur le terrain des TIC, trois questions que nous avons déjà étudiées avec des approches disciplinaires retiennent notre attention. La question des interfaces. Traditionnellement traitée par la psychologie cognitive et l'informatique relève pourtant aujourd'hui d'un dialogue « homme-machine-homme », c'est-à-dire d'un problème de prélèvement et de traitement d'information mais surtout d'une mise en relation instrumentée, médiatisée par un média informatique. La question des connaissances. Traditionnellement traités par la psychologie cognitive et les sciences de l'éducation lorsqu'elles isolent le sujet de la situation de communication dans laquelle se déroule l'apprentissage et du contexte situé des usages des TIC. Que cela soit sur les questions théoriques ou de terrain, j'ai déjà des résultats de recherche que j'ai exposés dans plusieurs articles et communication depuis 1994. Pour exemple : - La recherche que nous menons actuellement (avec l'INRP et des profs de collège) sur l'usage des Tic dans l'enseignement des mathématiques aux collèges, montre que ce sont les contraintes à la fois didactique et communicationnelle imposées par le dispositif qui favorisent la modification des gestes professionnels de l'enseignant et particulièrement sa communication orale qu'il appelle « son cours ». - Dans l'ouvrage « Comment penser la communication des connaissances » (1999) j'ai discuté comment un contrat de communication implicite permet de structurer l'activité exploratoire d'élève qui travaille en réseau. - Dans l'ouvrage « Rencontre avec Watzlawick » (1998) j'ai proposé une approche du multimédia comme système de communication. - Dans ma thèse (1994) j'ai montré comment les stratégies d'exploration adoptées par les sujets sont influencées par le mode de présentation des informations et le type de communication utilisateur-ordinateur. Bien que ces résultats soient intéressants, ils n'en demeurent pas moins, très réducteurs car dans chacune de ces recherches je ne me suis pas intéressé à l'environnement pris dans sa totalité, mais à ce qui dans cet environnement était pertinent au regard des disciplines mises en jeu. Aujourd'hui, à ces recherches, je souhaite introduire une dimension locale et contextuelle car la communication des savoirs via les TIC ne peut pas être pensée indépendamment des interactions humaines organisées par un contrat de communication qui s'énonce moins comme une suite de règles faisant loi que comme une série d'énonciations a priori, impliquant des possibles autant que des impossibles, des incertitudes et des confusions autant que de certitudes et d'axiomatisations dans un contexte révélateur d'un « provisoire décrit ». Dès lors, les artefacts révélés de fait par leur existence empirique comme un système de pensée qui amplifie les processus communicationnels de la médiation des savoirs deviennent étudiables comme une instance concrète des Technologies de l'Information et de la Communication observable dans la dynamique des usages comme l'apparente maîtrise par et sur l'individu utilisateur. Après avoir largement utilisé la méthode expérimentale pour les résultats dont je viens de parler, il est devenu clair pour moi que la posture vers laquelle j'allais ne pouvait plus uniquement utiliser cette méthode. Ma posture ayant évolué d'une étude de « Qu'est-ce que permettent de faire les TIC ? » à « Que fait-on avec les TIC ? » Et particulièrement en m'intéressant aux interactions collectives qui construisent un savoir partagé dans un espace commun de communication, il me semble intéressant pour moi d'utiliser une approche ethnologique qui semble autoriser plus facilement une étude des modes de contextualisation et contractualisation de la communication. Nous sommes passés d'une approche qui envisage les NTIC comme des outils pour agir et particulièrement pour « transmettre des savoirs » à une approche plus anthropologique qui autorise l'étude d'artefact communicationnel à travers une autre réalité qui relève de la « médiation des savoirs »."
"L'analyse et le contrôle du comportement des barrages en vue de maîtriser leur fiabilité et leur sécurité nécessitent la mise en ½uvre de modèles prenant en compte le passé (diagnostic), le présent (évaluation) et le futur de l'ouvrage (prédiction et proposition d'actions correctives). Nous avons montré au cours de travaux antérieurs qu'il était possible de modéliser le comportement de l'ouvrage par une approche multi-modèle qui rend possible la prise en compte de ces trois dimensions temporelles. Nous nous proposons dans cet article de montrer l'intérêt de cette modélisation pour l'évaluation des barrages en remblai et le diagnostic de leur comportement. / The analysis and control of dams' behaviour, with the aim of controlling reliability and safety, require the development of models that can take into account the present life of the dam (reliability and safety assessment), its past (diagnosis) and its future (prognosis and corrective actions). Previous works we carried out showed that it is possible to model the dam behaviour using a multi-model approach that takes into account the three temporal dimensions. In this paper, we propose to show the interest of such a modelling for the assessment and diagnosis of embankment dams."
no abstract
no abstract
no abstract
"Ce document est un document de travail servant de base à une publication à paraître dans Hermès, 56. L'article se propose de montrer que le langage en traduction joue un rôle performatif parce que les termes qu'il emploie retraduisent des normes sociales et des positions institutionnelles construites dans une autre culture, avec d'autres conditions historiques. Dans le cas de la naissance des sciences de la communication en Chine, des concepts américains sur la communication sont venus par leur traduction à la rencontre des normes et positions chinoises sur le journalisme et la propagande, engendrant une reconfiguration mutuelle des actants, qu'ils soient sociaux ou langagiers. Nous étudions comment le système socio-culturel chinois a recréé un ordre local propre sous la forme d'une discipline spécifique au pays, xinwen chuanboxue, les « sciences de la communication journalistique »."
"Cet article examine les différentes méthodes de recherche envisageables en communication organisationnelle en Chine, à partir de la littérature et d'un retour d'expérience, en trois parties. Dans la première, nous proposons un court aperçu de l'organisation académique des chercheurs en communication en Chine, en mettant en avant l'aspect dispersé de la recherche et le manque de recherches sur la communication en entreprise. Nous examinons dans un deuxième temps les méthodes de recherche généralement utilisées pour étudier la communication organisationnelle en Chine et soulignons les besoins en méthodes qualitatives. Nous résumons dans une dernière partie notre expérience en Chine, en mettant en avant les difficultés que nous avons rencontrées sur le terrain."
pas de résumé en français
Pas de résumé en français
Résumé de la situation des sciences de l'information et de la communication en Chine
"L'amélioration des capacités des scanners laser, de leur précision et de leurs temps d'acquisition a rendu ces instruments très populaires dans des domaines comme l'urbanisme, l'archéologie ou encore la foresterie. Ces instruments permettent d'échantillonner rapidement et avec précision les surfaces scannées sous la forme de nuages de points tridimensionnels contenant plusieurs dizaines de millions de points. Avec l'utilisation croissante de ces capteurs LiDAR (Light Detection And Ranging), le traitement de nuages de points est devenu un domaine d'étude important. En effet, l'utilisation efficace de cette technologie implique le développement de traitements rapides et automatiques en vue d'expliciter les informations contenues par les données brutes. Dans cet article, nous présen-tons une méthode de segmentation de nuages de points tridimensionnels pour en extraire les surfaces sans arêtes vives. Nous abordons cette problématique avec une approche de type split-and-merge basée sur la planéité locale des surfaces ainsi que sur la cohérence de la direction de leurs normales. Des contraintes additionnelles ont été mise en place pour améliorer la qualité des résultats. Un ensemble de solides de base et de combinaison de ces derniers a été utilisé pour valider cette méthodologie. Nous exposons aussi plusieurs applications de ces travaux à des données réelles et simulées. Dans de tels cas, le traitement présenté permet de segmenter efficacement les toits, murs et routes des scènes urbaines, ainsi que le sol et les branches principales des arbres présents sur des données forestières."
"Le but de cette communication est de montrer l'importance de la dimension culturelle dans les pratiques de veille à l'intérieur des organisations, en s'appuyant sur l'exemple des entreprises chinoises. La communication se déroulera en trois parties. Après avoir défini le concept de pratiques communicationnelles de veille, et avoir explicité les dimensions culturelles, nous montrerons comment les pratiques communicationnelles de veille, envisagées comme un ensemble particulier de pratiques informationnelles, sont culturellement fondées. Cela nous permettra ensuite, en étudiant la spécificité du contexte chinois à partir de la littérature et d'une enquête de terrain effectuée sur plusieurs mois de 2005 à 2007 de définir les spécificités du système culturel informationnel chinois qui contraignent la mise en place de systèmes de veille. Nous examinerons en dernier lieu les pratiques chinoises de veille dans le cadre de la « culture informationnelle » proposé par Davenport et Prusak (1997), pour nous interroger sur l'adéquation des systèmes de veille proposés sur le marché chinois."
no abstract
"Cet article décrit les premières étapes du développement d'un processus d'agrégation supervisée visant à produire des profils de vols à partir des enregistrements de l'ensemble des paramètres de plusieurs vols, afin de contribuer à l'amélioration de la sécurité aérienne d'aéronefs. La première étape, de décomposition, consiste à transformer les traces des vols étudiés en un ensemble de paramètres et d'indicateurs significatifs de l'activité représentée. A l'étape suivante, la maïeutique, on cherche alors à qualifier (à définir différents types) et mettre en commun ces manoeuvres élémentaires. Nous présentons ici la mise au point du processus permettant de trouver la règle la plus pertinente au sein d'une base de règles candidates pour l'identification de la croissance d'un paramètre."
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"Une instance CSP binaire qui satisfait la propriété des triangles cassés (BTP) peut etre résolue en temps polynomial. Malheureusement, en pratique, peu d'ins-tances satisfont cette propriété. Nous montrons qu'une version locale de BTP permet de fusionner des valeurs dans les domaines d'instances binaires quelconques. Des expérimentations démontrent la diminution significative de la taille de l'instance pour certaines classes de pro-bì emes. Ensuite, nous proposons une généralisation de cette fusion a des contraintes d'arité quelconque. En-fin, une version orientée nous permet d'´ etendre la classe polynomiale BTP. Ce papier est un résumé de l'article M. C. Cooper, A. El Mouelhi, C. Terrioux et B. Zanuttini. On Broken Triangles In Proceedings of CP,LNCS 8656, 9–24, 2014."
"L'étude des classes polynomiales, pour les problèmes de satisfaction de contraintes (CSP), constitue depuis longtemps un domaine de recherche important qui s'avère aujourd'hui très actif. Cependant, les travaux réalisés jusqu'à présent se sont révélés pour l'essentiel théoriques. En effet, ils se cantonnent en général à la définition de classes d'instances pour lesquelles des algorithmes polynomiaux ad hoc, à la fois pour la reconnaissance et pour la résolution, sont proposes. Ces algorithmes ne peuvent être, en fait, utilisés que pour le traitement d'une classe d'instances donnée. Ils s'avèrent ainsi difficilement exploitables en pratique, et ne sont donc pas exploités au sein de solveurs généraux. L'intérêt pratique des classes polynomiales est ainsi très limitée. Dans cet article, nous abordons la question des classes polynomiales CSP d'un point de vue différent de l'approche classique, en nous intéressant aux algorithmes que l'on peut retrouver dans les systèmes de résolution opérationnels. Pour cela, nous _étudions d'abord la complexité d'algorithmes génériques de résolution de CSP tels que le Forward-Checking par exemple. Cette étude s'appuie sur l'exploitation d'un paramètre issu de la théorie des graphes, et qui permet de proposer de nouvelles bornes de complexité. La mise en relation de ces nouvelles bornes avec certains résultats issus de la théorie des graphes nous permet d'exhiber de nouvelles classes polynomiales. De cette façon, nous montrons comment des algorithmes classiques de résolution de CSP peuvent traiter efficacement en pratique ainsi qu'en théorie, des instances de CSP, sans devoir reconnaître au préalable leur appartenance à d'éventuelles classes polynomiales."
Ch.11: Approches de la révision et de la fusion d’informations - 11.1 Introduction - 11.2 Révision de croyances en logique - 11.3 Révision itérée - 11.4 Approche logique de la fusion d’informations - 11.5 Approches - valuées de la révision et de la fusion - 11.6 Conclusion
Sélection des meilleurs articles de la Conférence BDA'08
no abstract
"Pour répondre aux enjeux du développement durable, une transition systémique est nécessaire. Notre proposition est d’appliquer, au niveau de l’entreprise, des principes issus d’une approche hybride, entre forecasting (tendances) et principled backcasting. Ces principes proposent des lignes directrices pour chacune des cinq dimensions du développement durable (politique, territoire, Homme, environnement et économie). La méthode développée s’appuie sur le processus stratégique (analyse, choix et déploiement) pour intégrer des dimensions complémentaires dans la conception de produit à travers le réseau interne de création de valeur de l’entreprise étendue. Deux trajectoires ont été explorées : l’intégration de l’environnement (cas d’études) et du territoire (théorie) dans la conception de produit grâce à l’innovation organisationnelle portée par le renouveau de la gouvernance stratégique et opérationnelle. Cette recherche interdisciplinaire pose les fondations pour le développement d’une méthode permettant de supporter la transition de l’entreprise industrielle vers le développement durable à 5 dimensions, par l’intégration dans le processus de conception de produit de dimensions jusqu’alors peu ou pas exploitées."
"Un système dynamique Booléen (SDB) représente l'évolution au cours du temps des interactions dans un ré-seau fini d'entités. La représentation des réseaux de ré-gulation de gènes, c'est à dire des interactions entre les gènes/protéines d'une cellule, en est un exemple. Dans ce domaine des théorèmes fondateurs ont porté sur les cycles d'interactions et l'étude des cycles, circuits et point fixes est fondamentale. Des travaux ont été fait sur la représentation de certains réseaux biologiques en utilisant des formalismes non-monotones, en particulier la logique de défauts ou les ASP. Mais une représentation des SDB par la plupart des formalismes non-monotones n'est pas satisfaisante, car elle ne permet pas de représenter les cycles stables et instables. Ici on représente les cycles en utilisant la logique des hypothèses qui généralise la logique des défauts. Pour cette logique on a toujours des extensions mais certaines d'entre elles, les extensions fantômes, vont jouer un rôle particulier pour la représentation et la gestion des cycles. L'article donne une représentation des SDB en logique des hypothèses. Le but est de permettre de discriminer les états stables, les cycles stables et instables."
"Dans le but d'exploiter les opinions dans les tweets, cet article présente une classification à partir du sentiment contenu au sein des tweets. Nous présentons une méthode d'identifi-cation de nouveaux mots-germes. Ils sont utilisés pour la prédiction de l'intensité de sentiments des mots en co-occurrence avec ces mots-germes. Ensuite, le calcul de similarités entre sentiments est appliqué en utilisant: la mesure de la similarité entre deux mots et l'utilisation de plongement de mots (e.g. word2vec, GloVE) couplé à la mesure cosinus. Les résultats montrent l'importance de l'utilisation de mots-germes adaptés aux tweets, ainsi que la taille et le prétrai-tement de corpus. Pour conclure, nous avons obtenu les meilleurs résultats grâce à l'application de la méthode utilisant le plongement de mots couplée à la mesure cosinus. ABSTRACT. For the purpose of opinion exploring in tweets, this article presents a sentiment classification of tweets content. First, we present a method to identify new sentiment similarity seed words. These seed words are used for predicting sentiment intensity of other words and short phrases in co-occurrence. Then, for testing sentiment similarity, we use: Similarity Measures methods between words and cosine similarity measure between the word embedding representations (e.g. word2vec, GloVE). The experiments results highlight the importance of adapted for tweets seed words. In addition of the corpora size and its pre-treatement. As a conclusion, best results were achieved using cosine similarity measure between the word embedding representations. MOTS-CLÉS : Mots-germes, Twitter, Mesure de la Similarité, Plongement de mot, Word2vec, GloVe."
"Une plus grande circulation de l'information géographique numérique sur le marché ainsi que la difficulté pour les usagers non experts d'en apprécier la qualité risquent de résulter en de mauvaises utilisations ou interprétations et de provoquer une hausse du contentieux entre les parties impliquées. Compte tenu de la complexité de l'information géographique et des multiples incertitudes juridiques reliées au droit des nouvelles technologies de l'information, la transmission d'un manuel d'instructions devient un moyen privilégié de prévention au sein d'une stratégie prudente de gestion du risque juridique. En fait, de la transmission d'informations relatives à la qualité interne de l'information dans un contexte d'usages non contrôlés, l'analyse démontre la pertinence de glisser vers la transmission d'informations relatives à la qualité externe de l'information dans un contexte d'usages contrôlés."
"Cette thèse se propose de montrer comment les interactions vont influer sur le système complexe de communication d'une formation à distance, et en dévoiler les limites. L'objectif général est de montrer que, dans ce système, il est possible d'expliquer l'abandon par les pertes de prises qui se dessinent dans la dynamique entre les représentations et les communications des acteurs, et que certaines limites du système favorisent ces pertes de prise. La finalité de cette thèse est de cerner ces limites pour définir une catégorie d'abandon communicationnel. Cette recherche qualitative s'appuie sur la théorie de la communication pragmatique de Palo Alto, pour analyser les interactions entre les acteurs, et sur la théorie des représentations sociales, pour traduire en images l'évolution des représentations sur une année. Le concept de "" prise "" relie la situation communicationnelle à l'abandon. Il permet de confronter les repères communs aux acteurs et leurs perceptions personnelles des situations vécues, pour rendre compte de leur moyen d'agir et de s'engager."
"Une des préoccupations majeures du monde industriel est d'avoir une exploitation performante permettant de garantir au mieux la qualité des missions réalisées, le respect des délais demandés et la minimisation des coûts d'exploitation. Toutefois, aujourd'hui la concurrence accrue et la complexité des systèmes conduit les industriels à développer des approches qui permettent la maîtrise de la disponibilité et en particulier à prendre en compte ce paramètre pour l'élaboration de la politique de maintenance. L'objectif de ce manuscrit est de proposer des éléments méthodologiques permettant de caractériser le concept de disponibilité pour en assurer la maîtrise. Nous proposons une méthode de modélisation d'une exploitation en vue d'évaluer les performances en termes de disponibilité. Les résultats de cette évaluation permettent entre autres d'identifier les leviers d'amélioration pouvant agir sur les performances de l'exploitation. Ensuite, nous nous sommes focalisés, dans cette thèse, sur un de ces leviers, à savoir le réordonnancement de la maintenance programmée permettant de répondre au besoin du partenaire industriel. Dans cette optique, nous avons proposé une méthode permettant d'améliorer le réordonnancement d'un programme de maintenance en vue d'optimiser la disponibilité d'un système voire d'un ensemble de systèmes. Dans le cadre de cette méthode, nous avons proposé des heuristiques permettant de réordonnancer les opérations de maintenance en fonction des opportunités d'un exploitant de système. Nous chercherons dans ce manuscrit à apporter des éléments méthodologiques expérimentées chez la société Eurocopter, reposant sur des principes opérationnels et basés sur des approches par scénario."
"Les travaux de cette thèse portent sur : − la proposition d'algorithmes de simulation distribuée conservative de modèles DEVS / G-DEVS, − la définition et la réalisation d'un environnement de modélisation & simulation (M&S) G-DEVS com-patible HLA implémentant les algorithmes proposés, − l'application de l'environnement à la M&S de Workflow. Dans un premier temps, nous avons introduit un composant coordinateur racine G-DEVS distribué, incluant un algorithme de communication avec le RTI HLA basé sur le mécanisme de synchronisation conservative et utilisant un Lookahead positif. Nous avons ensuite proposé deux algorithmes originaux pour le calcul d'un Loo-kahead relatif à l'état courant d'un modèle G-DEVS. Ces algorithmes, basés sur l'analyse du domaine de varia-tion de la fonction « durée de vie » du modèle, augmentent les performances de la simulation distribuée comme l'illustrent les expériences menées. Basé sur ces approches, nous avons développé un environnement de M&S distribué G-DEVS / HLA. Cet environnement a été intégré à une application de Workflow. Les possibilités offertes par l'environnement ont été illustrées par l'étude de cas réels d'entreprises."
"Notre communication se propose de relier les perspectives micro et macro opposées par la littérature en sciences sociales. Pour cela, nous envisageons la culture comme élément médiateur de l'organisation sociale pour dépasser la théorie traditionnelle des réseaux sociaux. Notre but dans cette communication théorique est donc de montrer comment la culture, en tant que réseau dynamique de règles génératrices de pratiques possibles, permet l'émergence de l'organisation sociale. Nous défendons particulièrement l'idée que la théorie des réseaux sociaux ne suffit pas à expliquer l'organisation sociale. En effet, elle ne hiérarchise pas les relations et ne prend pas en compte l'aspect proprement humain du social. Nous présentons dans une première partie comment le concept d'émergence issu des sciences « dures » de la complexité peut relier le micro et la macro et peut s'appliquer aux sciences sociales à travers la théorie dynamique des réseaux. Dans la seconde partie nous expliquons les caractéristiques structurelles et dynamiques des réseaux sociaux ; puis, dans la troisième et dernière partie, nous proposons une relecture de la théorie de la structuration de Giddens (1984) en analysant l'émergence des réseaux sociaux organisés à travers le concept d'émergence réticulaire de règles culturelles lié à une approche probabiliste de l'action."
"Ce texte, a pour objectif de montrer comment le quotidien nous informe sur les significations construites dans les processus de changement organisationnels. Il cherche particulièrement à exposer comment l'étude du contexte et du sens commun peut être révélatrice d'un changement organisationnel au quotidien, révélateur des significations particulières que toute organisation se donne sur son fonctionnement avec pour principe la recherche d'un équilibre entre des modèles « disciplinaires » tels de ceux de l'économie ou de la gestion ou encore de la communication... et ses règles particulières. Toutefois, il ne s'agit pas ici d'opposer la construction du sens commun aux modèles scientifiques introduisant ainsi un dualisme conceptuel ou épistémologique. Il s'agit d'envisager « une histoire » du processus de changement dans laquelle, ce que font les acteurs à partir des divers modèles disciplinaires sous-jacents prend des significations différentes. Une forme de biographie du sens commun qui serait en même temps, une trace de ces changements."
Ma thèse s'est inscrite dans une recherche interdisciplinaire qui a fait l'objet d'un contrat de recherche financé par le « Programme Cognisciences » du CNRS sur le thème « Modélisation des processus d'apprentissage ». Elle s'est particulièrement attachée à rendre compte des processus de régulation et d'interaction à l'œuvre dans l'acquisition de connaissances dans des situations de Formation Assistée par Ordinateur (FAO.). Les résultats mettent en évidence le rôle que peuvent jouer les « modes de présentation des connaissances » et les « modalités de communication ». Ils montrent également que le croisement de ces deux variables déterminent les conditions d'interaction qui affectent les modalités de régulation que se donnent les individus pour réaliser la tâche. L'analyse des dialogues pendant l'exploration des situations montre que les modes de communication adoptés par les individus qui travaillent à deux face à l'ordinateur sont différents des modes de communication des sujets qui travaillent en réseau. Cette différence se traduit par une médiation des savoirs plus moins plus ou moins fonctionnelle au domaine de connaissances du exploré.
no abstract
L'information géographique est maintenant un produit de masse fréquemment manipulé par des utilisateurs non-experts en géomatique qui ont peu ou pas de connaissances de la qualité des données qu'ils utilisent. Ce contexte accroît significativement les risques de mauvaise utilisation des données et ainsi les risques de conséquence néfaste résultant de ces mauvaises utilisations. Cette thèse vise à fournir à des utilisateurs experts ou des experts en qualité une approche leur permettant d'évaluer la qualité des données et ainsi être à même de conseiller des utilisateurs non-experts dans leur utilisation des données. Cette approche se base sur une structuration des données de qualité dans une base de données multidimensionnelle et une communication dynamique et contextuelle utilisant des indicateurs de qualité affichés dans un système SOLAP (Spatial On-Line Analytical Processing) combiné à un système d'information géographique.
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"Les jeux sérieux sont de plus en plus utilisés pour la formation, et en particulier dans le domaine de la gestion de crise. Afin d'améliorer la formation, l'évaluation des apprenants peut fournir des indications sur ce qui a été bien ou mal appris au cours d'une session de formation. Cette appréciation se complexifie lorsqu'elle doit prendre en compte non seulement les actions individuelles des acteurs mais également le résultat de leurs interactions (actions collectives). De plus, selon la nature des phénomènes à émuler (incendie, pollution...) et le nombre d'acteurs impliqués, des besoins de simulations s'ajoutent (propagation incendie, simulation de comportement d'acteurs absents...). L'évaluation des apprenants implique ainsi l'acquisition d'un ensemble hétérogène d'informations à la fois par leur nature (données brutes, connaissances, procédures) et par leur source (niveau logiciel). Cet article propose une approche multi-agents répondant à deux objectifs: i) permettre la production et l'exploitation de ces informations hétérogènes en vue de produire des indicateurs qui alimenteront une évaluation multi-critères; et ii) mettre en œuvre le processus d'évaluation par un système multi-agents accompagnant la production d'évaluations individuelles et collectives. Cette approche est développée et illustrée sur le jeu sérieux SIMFOR dédié à la gestion de crise pour le faire évoluer vers un Système tutoriel."
no abstract
no abstract
"L'un des principaux défis posés à l'industrie du jeu vidéo est de mettre en scène des personnages non joueurs (PNJ) dont les comportements sont crédibles. Or, les recherches montrent que les émotions jouent un rôle déterminant dans le comportement des individus. Pour augmenter la crédibilité des comportements des PNJ, nous proposons dans cet article un modèle de la dynamique des émotions prenant en compte la personnalité et les relations sociales du personnage. Nous présentons tout d'abord les travaux de la littérature sur les émotions, la personnalité et les relations sociales en informatique et en sciences humaines et sociales. Nous soulignons l'influence de la personnalité sur le déclenchement des émotions et des émotions sur la dynamique des relations sociales. En nous appuyant sur ces travaux, nous proposons un modèle dynamique de l'état socio-émotionnel et son implémentation sous la forme d'un outil simple qui permet de simuler la dynamique de l'évolution des émotions et des relations sociales des PNJ suivant leur personnalité et leur rôle."
"This paper presents a review of different classification techniques used to recognize human activities from wearable inertial sensor data. Three inertial sensor units were used in this study and were worn by healthy subjects at key points of upper/lower body limbs (chest, right thigh and left ankle). Three main steps describe the activity recognition process: sensors' placement, data pre-processing and data classification. Four supervised classification techniques namely, k-Nearest Neighbor (k-NN), Support Vector Machines (SVM), Gaussian Mixture Models (GMM), and Random Forest (RF) as well as three unsupervised classification techniques namely, k-Means, Gaussian mixture models (GMM) and Hidden Markov Model (HMM), are compared in terms of correct classification rate, F-measure, recall, precision, and specificity. Raw data and extracted features are used separately as inputs of each classifier. The feature selection is performed using a wrapper approach based on the RF algorithm. Based on our experiments, the results obtained show that the k-NN classifier provides the best performance compared to other supervised classification algorithms, whereas the HMM classifier is the one that gives the best results among unsupervised classification algorithms. This comparison highlights which approach gives better performance in both supervised and unsupervised contexts. It should be noted that the obtained results are limited to the context of this study, which concerns the classification of the main daily living human activities using three wearable accelerometers placed at the chest, right shank and left ankle of the subject."
no abstract
"The analytical technologies development and simulation tools use increases day by day; leading to an increment of the data, information and knowledge associated to a product. Due to this, a wide spectrum of approaches (based in different contexts) during the study of the product are required. As well, during the process of design for manufacturing, an extensive number of uses cases are generated; where are contained a lot of behaviors, associations, aspects and inputs to consider. In consequence, this paper aims to propose a multi-scale modelling method to provide a better structure, better perception and better description regarding to the aspects implicated on a product and its manufacturing process. The model proposed is based on different scales representations, characterized through “representation axes”. In this the product data is decomposed and commit at different representation views or ranges. The use of manufacturing knowledge can be implemented on to the analysis and evaluation of the data (input values); providing new information based in the coherence among the inputs. In this way, its capitalization and coherences among the information can be used in product design. For this reason, different models are defined to represent the data and the knowledge during the evolution and structure of the project to develop."
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"La problématique est d'étudier la gestion d'un système multi-énergies, en fonction de la demande, dédié au bâtiment en milieu urbain. L'énergie peut être fournie par des panneaux photovolta ïques, une éolienne, des capteurs thermiques et peut être stockée dans des batteries et une pompe à chaleur. L'étude de ces systèmes complexes nécessite une bonne stratégie de contrôle, de commande et la mise au point de méthodes de gestion ecace. Le travail consiste à réaliser un modèle able et une simulation (réaliste en temps réel) du syst ème multi-sources et d'un bâtiment de trois pièces, avec diérentes charges domestiques, comme une machine à laver, un congélateur, un réfrigérateur et des systèmes de chauage. Un système de gestion des sources d'énergie qui permet d'ajuster et d'optimiser la consommation d'une façon adaptée aux ressources énergétiques disponibles tout en satisfaisant la demande et maximisant le confort des occupants dans le bâtiment est ensuite étudié. Ainsi, la dénition d'outils d'analyse permettra d'étudier le problème de la gestion de l'énergie pour réduire la consommation d'énergie primaire et baisser la production de gaz à eet de serre tout en couvrant les besoins énergétiques et en répondant à la demande. Après un état de l'art, le besoin en énergie d'un bâtiment et ses charges, en temps réel, est étudié pour développer un modèle de simulation (chapitre 2). Ensuite, dans le chapitre 3, le bâtiment est couplé à un système multi-sources pour la production de l'énergie nécessaire à la demande. L'avant dernier chapitre est dédié à l'amélioration de la poursuite du point de production maximale d'énergie qui peut être fournie par le générateur PV, en exploitant des techniques d'observation et de commande pour Systèmes Automatisés à Structure Variable (SASV). La production d'énergie est couplée, en plus des charges d'utilisation domestique, à un stockage sur batterie. Ces mêmes techniques SASV sont ensuite exploitées, dans le dernier chapitre, pour l'observation, l'estimation et la prédiction des réserves et la gestion de l'énergie stockées sur batterie."
"Cet article présente un aperçu des deux modes de dégradation de panneaux photovoltaïques: la dégradation induite par l'humidité et les fissures de la cellule. Les deux modes de dégradation mentionnés affectent les cellules photovoltaïques différemment en fonction de leur position dans le module. Lors d'une étape précédente, nous avons construit un modèle de PV qui prend en compte trois modes de dégradation, le potentiel induit la dégradation, la lumière induit la dégradation, et la dégradation Lumière ultraviolette. Dans cet article, nous mettons à jour notre modèle à prendre en considération tous les modes de dégradation."
"Dans cet article, nous présentons un aperçu des trois modes de dégradation: le potentiel induit la dégradation, la dégradation induite par la Lumière, et la dégradation par la Lumière ultraviolette. Ensuite, nous développons un modèle mathématique qui décrit les derniers modes de dégradation afin d'évaluer la durée de vie des modules photovoltaïques . A la fin on simule l'efficacité d'un module PV en fonction du temps lorsque les trois modes de dégradation sont pris en considération."
"Avec la croissance de panneaux photovoltaïques, de nombreux semi-conducteurs sont étudiés et exploités. De nouvelles formes et des générations de cellules photovoltaïques sont produites présentent des caractéristiques originales et particulières. Chaque technologie photovoltaïque est développée pour l'augmentation de l'efficacité ou pour la simplicité dans le processus de production. Dans cet article, nous allons présenter une étude sur les panneaux photovoltaïques. Etat de l'art de chaque technologie sera également présenté. Nous allons nous concentrer sur les matériaux utilisés et sur ​​le principe de fonctionnement de chaque type. L'intérêt principal de cette analyse, pour notre travail futur, est de savoir comment gérer l'utilisation de panneaux photovoltaïques pour augmenter leur durée de vie? Quelle technologie est plus approprié d'inclure les TIC (Technologies de l'information et des communications) pour la surveillance, l'entretien et l'extension du cycle de vie au moyen de techniques de contrôle adéquats?"
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"Nous proposons dans ce travail une étude de la fusion de connaissances et ses applications aux relevés de fouilles archéologiques sous‐marines. Le cadre de ce travail est la mesure fondée sur la connaissance, pouvant être décrite comme la synthèse entre des modèles théoriques élaborés par des experts du domaine étudié et d'un ensemble d'observations effectuées sur les objets à relever. Lors de l'étude d'un site archéologique, les relevés peuvent être effectués par des opérateurs différents et à des moments différents. Cette multiplication des observations induit des risques d'incohérence lors de l'agrégation de tous les résultats (objets mesurés deux fois, objets mal identifiés, ...). La construction d'un résultat final nécessite la mise en place d'un processus de fusion piloté par le responsable de l'étude. Un tel pilotage doit être automatisé tout en laissant à l'opérateur le choix des méthodes de rétablissement de la cohérence. Ce travail est divisé en trois parties : une étude théorique des méthodes de fusion connues, la mise en place de méthodes de fusion dans le cadre de la mesure fondée sur la connaissance et l'expérimentation des solutions proposées lors de relevés dans des applications grandeur nature. Dans la première partie, nous proposons une étude théorique des techniques de fusion de croyances existantes et nous proposons un nouveau cadre de fusion réversible permettant d'exprimer de manière équivalente ces méthodes de fusion d'un point de vue sémantique et syntaxique. Ce cadre est basé sur des pondérations par des polynômes qui permettent de représenter à la fois les priorités entre les croyances mais aussi l'historique des changements de ces priorités. Dans la deuxième partie, nous détaillons la mesure fondée sur la connaissance en décrivant une représentation des connaissances basée sur la notion d'entité. Cette représentation est exprimée dans le modèle Objet ainsi que sous forme semi‐structurée en XML. Nous proposons ensuite des techniques de fusion adaptées à cette représentation. Ces techniques sont basées sur la logique propositionnelle et la logique des prédicats instanciés. Des algorithmes de fusion sont décrits et étudiés. Dans la dernière partie, nous présentons les expérimentations des techniques de fusion mises en place. Nous proposons une description des outils développés et utilisés dans le cadre du projet Européen VENUS (http://www.venus‐project.eu) mais aussi leurs extensions à l'archéologie du bâti et à la biologie sous‐marine."
"L'approximation de fonctions et de données discrètes est fondamentale dans des domaines tels que la planification de trajectoire ou le traitement du signal (données issues de capteurs). Dans ces domaines, il est important d'obtenir des courbes conservant la forme initiale des données. L'utilisation des splines L1 semble être une bonne solution au regard des résultats obtenus pour le problème d'interpolation de données discrètes par de telles splines. Ces splines permettent notamment de conserver les alignements dans les données et de ne pas introduire d'oscillations résiduelles comme c'est le cas pour les splines d'interpolation L2. Nous proposons dans cette thèse une étude du problème de meilleure approximation au sens de la norme L1. Cette étude comprend des développements théoriques sur la meilleure approximation L1 de fonctions présentant une discontinuité de type saut dans des espaces fonctionnels généraux appelés espace de Chebyshev et faiblement Chebyshev. Les splines polynomiales entrent dans ce cadre. Des algorithmes d'approximation de données discrètes au sens de la norme L1 par procédé de fenêtre glissante sont développés en se basant sur les travaux existants sur les splines de lissage et d'ajustement. Les méthodes présentées dans la littérature pour ces types de splines peuvent être relativement couteuse en temps de calcul. Les algorithmes par fenêtre glissante permettent d'obtenir une complexité linéaire en le nombre de données. De plus, une parallélisation est possible. Enfin, une approche originale d'approximation, appelée interpolation à delta près, est développée. Nous proposons un algorithme algébrique avec une complexité linéaire et qui peut être utilisé pour des applications temps réel."
"La variété des concepts d'aéronef à voilure tournante n'a d'égal que l'étendue de leur hamp applicatif. Dès lors, se pose une question essentielle : quel concept est le plus adapté face à un certain nombre de missions ou de spécifications ? Une partie essentielle de la réponse réside dans l'étude des performances de vol et des impacts environnementaux de l'appareil. Le projet de recherche fédérateur C.R.E.A.T.I.O.N. pour "" Concepts of Rotorcraft Enhanced Assessment Through Integrated Optimization Network "" a pour but de mettre en place une plateforme numérique de calculs multidisciplinaires et multiniveaux capables d'évaluer de tels critères. La multidisciplinarité fait écho aux différentes disciplines associées à l'évaluation des giravions tandis que l'aspect multi-niveaux reflète la possibilité d'étudier un concept quelque soit l'état des connaissances sur ce dernier. La thèse s'inscrit dans ce projet. Une première implication est le développement de modèles de performances de vol et leur intégration dans des boucles de calcul multidisciplinaires. Au-delà de cet aspect de modélisation physique, la multidisciplinarité touche aussi le champ des mathématiques appliquées. Les méthodes d'optimisation multi objectifs multi paramètres, l'aide à la décision pour la sélection d'un optimum de meilleur compromis, l'exploration de bases de données, la création de modèles réduits sont autant de thématiques explorées dans cette thèse."
no abstract
"Nous revenons ici sur une méthode de résolution de CSP par décomposition introduite dans [16] et qui est appelée Regroupement Cyclique. Alors que [16] se limitait à présenter uniquement les principes de la méthode, dans cette contribution, nous montrons comment celle-ci peut être rendue opérationnelle, notamment par une exploitation idoine des propriétés des sous-graphes triangulés. Dans un second temps, nous présentons des résultats formels qui démontrent que le Regroupement Cyclique réalise effectivement un compromis temps-espace en termes de complexités théoriques. Nous concluons cet article en présentant quelques résultats expérimentaux qui montrent que le Regroupement Cyclique peut être efficace en pratique."
"Cet article présente une étude portant sur la minimisation de la réponse dynamique d'un système mécanique soumis à une excitation extérieure en pilotant son comportement. Dans un premier temps, on expose de manière générale comment on peut envisager de minimiser les amplitudes de la réponse d'un système excité. Ensuite on étudie le cas concret d'un atterrissage d'hélicoptère pour lequel on analyse comment la régulation de la dissipation de l'énergie due à l'impact de l'atterrissage, permet de minimiser la réponse de la poutre de queue. Puis on analyse les différentes méthodes de commande qui peuvent être adaptées à ce problème et on présente une mise en oeuvre expérimentale. De récentes études expérimentales sur des situations d'atterrissages d'hélicoptères à grande vitesse, révèlent que de part la courte durée de l'atterrissage et le couplage existant entre le fuselage et les trains d'atterrissage, la poutre de queue d'un appareil dont le premier mode de flexion se situe dans les basses fréquences pouvait être excitée. Afin d'assurer la pérennité de l'appareil, une solution passive consiste à rigidifier la liaison entre la cabine et la poutre de queue. Coûteuse en poids, celle-ci peut être évitée en pilotant le comportement des trains d'atterrissage."
no abstract
Non disponible
no abstract
"L'idée de l'argumentation est de rechercher dans une base de connaissances, pour chaque proposition dont on souhaite évaluer la validité, les raisons qui étayent cette proposition et celles qui l'infirment. Un argument est alors entendu comme une paire comprenant une proposition et les raisons qui la justifient. Notre propos est d'offrir des outils formels pour la génération automatique d'arguments par deux agents en situation de dialogue. Ces outils reposent sur les X-logiques, formalisme non-monotone proposé en 1996 par Siegel et Forget et déterminant un cadre fondateur autour de la notion de preuve pour le raisonnement non-monotone. En particulier l'ensemble X servant à paramétrer la relation d'inférence confère une souplesse inégalée à la gestion dynamique des arguments. Après un tour d'horizon des travaux passés en matière de représentations logiques pour l'argumentation, nous introduisons les X-logiques, à partir desquelles est composée la notion d'attitude d'un agent par rapport à une formule. Nous définissons ensuite des opérateurs de confrontation qui permettent d'associer des ensembles de formules aux attitudes d'un agent. Le concept de réponse d'un agent à un ensemble de formules est alors élaboré en tant que motivation de l'attitude de cet agent vis-à-vis de l'ensemble en question. Plusieurs formes de réponses sont distinguées parmi lesquelles les notions de réponse pertinente ou encore de mensonge. Une réponse représente les raisons qui justifient la conclusion d'un argument : c'est à partir du calcul de ces réponses que nous exhibons une procédure de génération automatique d'arguments. Enfin nous montrons que notre cadre argumentatif permet de générer les contre-arguments conservatifs maximaux de Besnard et Hunter (2001), arguments retenus pour leur pertinence."
"L'utilisation des technologies du Web dans le domaine de l'éducation permet d'envisager de nouvelles approches et de nouveaux contextes d'apprentissage. En effet, l'avènement du e-learning présente de nombreux intérêts. Toutefois, la qualité du service pédagogique rendu dépend de la capacité de ces nouvelles approches d'apprentissage à fournir aux apprenants, des contenus et des parcours pédagogiques adaptés à leurs besoins. Le développement de systèmes pédagogiques adaptatifs répond à cet objectif. Dans ce papier, nous proposons un cadre de référence permettant, de caractériser et de comparer les systèmes pédagogiques adaptatifs. Nous considérons qu'un système pédagogique adaptatif utilise trois types de connaissances : les connaissances sur les ressources pédagogiques, les connaissances sur les apprenants et les connaissances sur les processus d'apprentissage. Un système pédagogique adaptatif met aussi en oeuvre des méthodes et des techniques d'adaptation pour construire des parcours individualisés en utilisant ces trois formes de connaissances. Sur la base des trois types de connaissance et des méthodes et techniques d'adaptation, nous comparons plusieurs systèmes existants et nous montrons leurs limites. Cette étude aboutit au constat que les systèmes pédagogiques adaptatifs sont fortement centrés sur la gestion de contenus pédagogiques et peu sur les processus. La prise en compte de ces processus constitue un enjeu majeur pour le développement des nouveaux systèmes pédagogiques adaptatifs."
"La santé est une préoccupation mondiale et grandissante occupant une place prépondérante aussi bien dans les débats de la sphère publique que dans les conversations ordinaires. Toutefois, la vie numérique de l’usager est encore mal connue, ainsi que les ressorts des représentations qui régissent les comportements dans ce domaine. C’est donc à partir de la théorie des représentations sociales que nous proposons d’appréhender la e-médecine en tant que phénomène de société. La e-santé consécutive à l’essor du numérique et des technologies mobiles autorise non seulement la consommation de connaissances liées à la santé, mais également la production et diffusion de celles-ci. Aussi, dans le domaine de la médecine, le recours aux outils numériques a initié de nombreuses ruptures comportementales et entraîné de fait la création d’un univers représentationnel associé qui favorise ou non leur acceptabilité par les utilisateurs potentiels."
"In this paper, we deal with the problem of extracting and processing useful information from bibliographic references in Digital Humanities (DH) data. We present our ongoing project BILBO, supported by Google Grant for Digital Humanities that includes the constitution of proper reference corpora and construction of efficient annotation model using several appropriate machine learning techniques. Conditional Random Field is used as a basic approach to automatic annotation of reference fields and Support Vector Machine with a set of newly proposed features is applied for sequence classification. A number of experiments are conducted to find one of the best feature settings for CRF model on these corpora. RÉSUMÉ. L'extraction d'informations bibliographiques depuis un texte non structuré demeure un probléme ouvert que nous abordons, via des approches d'apprentissage automatique, dans le domaine des Humanités Numériques. Nous présentons dans cet article le projet BILBO, soutenu par un Google Digital Humanities Award avec le soutien du projet ANR CAAS : constitution de 3 corpus de référence correspondant à trois localisations des références, élaboration d'un modéle d'annotation puis évaluation. Les champs aléatoires conditionnels (CRFs) sont utilisés pour l'annotation des références bibliographiques et des machines à vecteurs supports (SVMs) pour l'identification des références au sein du texte. De nombreuses expériences sont conduites afin de déterminer les meilleures propriétés devant être exploitées par les modèles numériques."
no abstract
"Le développement des territoires est inséparable de la communication, elle-même pensée dans le cadre de dispositifs ambitieux. Au coeur de ces dispositifs, les technologies de l'information et de la communication (TIC) doivent s'appréhender dans une compréhension des usages et pratiques. Nous nous plaçons ici dans une approche artefactuelle de la communication qui nous conduit à envisager les TIC comme une des composantes de dispositifs pluriels qui construisent et représentent la médiation dans une collection de situations contrastées. A travers le déploiement d'un dispositif de gestion intégré des déchets (SITOM Sud-Gard), nous cherchons à éclairer le fonctionnement des acteurs et du système de communication, qui, dans un contexte général de développement durable, matérialise un enjeu majeur du développement de nos territoires."
"L'objectif de cet article est de proposer une approche théorique d'un partenariat entreprise-laboratoire fondée sur l'articulation entre un modèle de structure de la recherche et un modèle d'explication des actions sur le terrain. Nous montrons comment cette articulation pourrait augmenter les performances respectives des secteurs recherche et entreprise pour réaliser dans les meilleures conditions, les projets qui présentent un caractère complexe."
no abstract
"Un réseau Adaptive Resonance Theory a été utilisé pour simuler l'apprentissage des formes orthographiques des mots vus par les enfants. Il a été démontré qu'une modélisation par carte auto-organisatrice permet de rendre compte des performances de l'enfant si on conserve les probabilités d'apparition des mots de la base d'apprentissage. Nous montrons dans cet article avec ART2, que l'ordre d'apparition des mots joue aussi un rôle significatif dans la discrimination des mots plus ou moins orthographiquement voisins. Il en découle une hypothése de construction de corpus favorable é l'apprentissage de la lecture de ce type de mots."
no abstract
Pas de Résumé car Direction d'ouvrage
"Dans certaines conditions de vol, les aéronefs à voilure tournante souffrent parfois de l’émergence d’oscillations indésirables, phénomènes potentiellement instables connus sous le nom de Couplages Pilote-Aéronef aéroélastiques (CPA). Ces phénomènes affectent de manière critique la sécurité et la performance des aéronefs. Par conséquent, il est important d’être capable de prédire l’émergence de tels phénomènes dynamiques, le plus tôt possible dans le processus de conception des hélicoptères. Une revue de la littérature révèle que ces phénomènes sont le résultat d’interactions entre les comportements biodynamique du pilote et aéroélastique des hélicoptères. Afin d’avoir une plus grande modularité et granularité dans le processus de modélisation de systèmes complexes, une approche par bond graphs est adoptée. Un modèle aéromécanique d’hélicoptère et un modèle neuro-musculo-squelettique d’un des membres supérieurs du pilote sont développés en bond graphs. Parmi les représentations proposées, trois sont originales, notamment afin de modéliser : des efforts aérodynamiques quasi-statiques, la liaison traînée-battement-pas entre pale et moyeu rotor, et les efforts musculaires à partir d’un modèle de Hill qui tient compte d’une boucle de rétroaction neuromusculaire. Des résultats encourageants sont obtenus lorsque l’on compare la transmissibilité, entre l’angle de manche de pas cyclique imposé par le pilote et des accélérations latérales de la cabine, calculée à partir du modèle biodynamique, et à partir des résultats expérimentaux tirés de la littérature. Un modèle du système bioaéroélastique homme-machine est linéarisé, au voisinage d’un vol stationnaire, et analysé en termes de stabilité. L’étude révèle, comme conjecturé dans la littérature, que le mode régressif de traînée peut être déstabilisé. De plus, il apparaît que le mode progressif de traînée peut également être déstabilisé lors d’un CPA sur l’axe latéral-roulis. Un critère d’analyse de la stabilité d’un équilibre d’un système dynamique à partir d’un modèle linéaire limite la possibilité de prendre en compte certains comportements non-linéaires et donc réduit l’espace de conception. Les premières pierres vers une méthode basée sur des fonctions de Chetaev sont posées, afin de déterminer si l’équilibre d’un système dynamique est instable, directement à partir d’un modèle mathématique non-linéaire de grande dimension, à un coût de calcul potentiellement intéressant. Afin d’illustrer la pertinence de la proposition, le cas de la résonance sol d’un hélicoptère est présentée."
"Nous présentons dans ce texte, la démarche de modélisation de la performance que nous avons mise en oeuvre pour mettre en évidence les compatibilités communicationnelles des membres d'une équipe. Nous admettrons comme point de départ à cette démarche que la performance est étroitement liée aux caractéristiques des personnalités et aux compétences relationnelles des individus. Cette démarche avait pour finalité la création de deux briques technologiques d'un moteur d'aide à la décision pour un site web d'entreprise. La première brique permet de cibler les personnes qui ont des compétences communicationnelles (soft skills) liées à leur métier ; la seconde brique propose un moteur de prescription fonctionnelle pour la constitution d'équipes projet en fonction des attentes d'un donneur d'ordre. Le développement informatique de ces briques a nécessité la modélisation de fonctionnements et pratiques, articulée autour de niveaux de traduction et de codification des discours tenus par les professionnels sur leur métier et par les donneurs d'ordres sur leurs besoins. La mise en relation des deux niveaux de traduction donne les points d'interaction entre les individus qui favorisent la performance de l'équipe constituée. Le problème crucial soulevé ici par notre approche a été de traduire les données représentant des performances, des affinités, des personnalités et des compétences interpersonnelles afin qu'un codage informatique soit possible. Le codage en lui-même a été une des dernières étapes du développement des moteurs, mais sa mise en place a duré tout au long de la recherche et a particulièrement guidé le recueil des données (observations, questionnaires et entretiens, cf. Agostinelli et coll., 2015)."
no abstract
"Dans cet article, nous présentons un algorithme original de détection des séismes utilisant le capteur accéléromètre embarqué dans le smartphone. Nos travaux s'inscrivent dans le cadre du projet SISMAPP d'études et de recherche proposé aux étudiants du MBDS (www.mbds-fr.org) de l'Université de Nice – Sophia-Antipolis (UNS) en partenariat avec le Centre Sismologique Euro-Méditerranéen (CSEM, www.emsc-csem.org). Notre objectif est de montrer que la connectivité du smartphone et les capteurs embarqués pourraient devenir de facto une station sismique mobile pouvant être aisément déployée à grande échelle et à faible coût. ABSTRACT. In this paper, we present an original algorithm for detecting earthquakes using the accelerometer sensor embedded into the smartphone. Our work fits within SISMAPP research project proposed to MBDS (www.mbds-fr.org) students at University of Nice – Sophia-Antipolis (UNS) in partnership with the European-Mediterranean Seismological Centre (EMSC, www.emsc-csem.org). We aim to show that the smartphone connectivity and embedded sensors could turn into a mobile seismic station that can be easily and widely deployed at low cost."
"Notre communication porte sur l'usage des technologies numériques comme aide à la décision managériale. En d'autres termes, nous nous questionnons sur la place des technologies de l'information et de la communication dans le développement des innovations managériales. Nous présentons ici les fondements et les méthodes utilisés dans l'analyse exploratoire préalable à la réalisation d'un moteur d'affinités qui doit associer des professionnels rationnellement compatibles pour constituer une équipe susceptible de répondre aux attentes des chefs d'entreprise en matière de développement de projets pour le web. Cette expérimentation est conduite par le Laboratoire des Sciences de l'Information et des Systèmes 1 et de l'entreprise Nodalys 2 dans le cadre d'une réponse à candidature aux projets Pacalabs 3 dont l'objectif est de promouvoir l'innovation numérique et ses usages à travers l'expérimentation en Provence-Alpes-Côte d'Azur. Mots-clefs : moteur d'affinités, innovation managériale, équipes projet"
"Dans ce texte nous présentons les grandes lignes du mode de fonctionnement d'une équipe de chercheurs impliqué dans la conception, le développement et l'appropriation d'innovations technologiques. Nous exposons particulièrement le processus de pilotage de la recherche et les méthodes qui ont permis d'envisager l'innovation comme le résultat des interactions entre les acteurs de l'innovation, entre la médiation des connaissances scientifiques et des pratiques professionnelles, entre les intentions des commanditaires et les besoins des usagers. La recherche devient à la fois, la référence qui structure et l'opportunité qui fait découvrir. Abstract In this paper we present the outline of the operating mode of a team of researchers involved in the design, development and appropriation of technological innovations. We particularly expose the process of steering research and methods that were used to consider innovation as the result of interactions between innovation actors, mediation between scientific knowledge and professional practices between intentions sponsors and needs of users. Research becomes both the reference structure and the opportunity to discover."
"La visualisation 3D d'un terrain sur des médias de faibles capacités amène à résoudre des problèmes de compression de données et de synchronisation de ces données obtenues, à travers le réseau, à partir de serveurs distants. Le transfert doit aussi pouvoir se faire de façon progressive afin de permettre un affichage sur le média client bien qu'une faible partie des données aura été transmise. Ce travail cherche à résoudre ces problèmes en utilisant le standard JPEG2000 et en particulier le schéma de compression sans perte des données. La compression et la synchronisation des données est assurée grâce à une méthode d'insertion de données cachées permettant de placer toutes les données issues de plusieurs fichiers dans une seule image au format JPEG2000. La scalabilité de la résolution est issue de l'insertion du modèle numérique de terrain, lui même décomposé en ondelettes, dans l'image de texture associée, permettant ainsi un transfert hiérarchique et une synchronisation des données."
"The use of aerial photographs, satellite images, scanned maps and digital elevation models necessitates the setting up of strategies for the storage and visualization of these data. In order to obtain a three dimensional visualization it is necessary to drape the images, called textures, onto the terrain geometry, called Digital Elevation Model (DEM). Practically, all these information are stored in three different files: DEM, texture and position/projection of the data in a geo-referential system. In this paper we propose to stock all these information in a single file for the purpose of synchronization. For this we have developed a wavelet-based embedding method for hiding the data in a colored image. The texture images containing hidden DEM data can then be sent from the server to a client in order to effect 3D visualization of terrains. The embedding method is integrable with the JPEG2000 coder to accommodate compression and multi-resolution visualization. Résumé L'utilisation de photographies aériennes, d'images satellites, de cartes scannées et de modèles numériques de terrains amène à mettre en place des stratégies de stockage et de visualisation de ces données. Afin d'obtenir une visualisation en trois dimensions, il est nécessaire de lier ces images appelées textures avec la géométrie du terrain nommée Modèle Numérique de Terrain (MNT). Ces informations sont en pratiques stockées dans trois fichiers différents : MNT, texture, position et projection des données dans un système géo-référencé. Dans cet article, nous proposons de stocker toutes ces informations dans un seul fichier afin de les synchroniser. Nous avons développé pour cela une méthode d'insertion de données cachées basée ondelettes dans une image couleur. Les images de texture contenant les données MNT cachées peuvent ensuite être envoyées du serveur au client afin d'effectuer une visualisation 3D de terrains. Afin de combiner une visualisation en multirésolution et une compression, l'insertion des données cachées est intégrable dans le codeur JPEG 2000."
"L'utilisation de photographies aériennes, d'images satellites, de cartes scannées et de modèles numériques de terrains amène à mettre en place des stratégies de stockage et de visualisation de ces données. Afin d'obtenir une visualisation en trois dimensions, il est nécessaire de lier ces images appelées textures avec la géométrie du terrain nommée Modèle Numérique de Terrain (MNT). Ces informations sont en pratiques stockées dans trois fichiers différents : MNT, texture, position et projection des données dans un système géo-référencé. Dans cet article, nous proposons de stocker toutes ces informations dans un seul fichier afin de les synchroniser. Nous avons développé pour cela une méthode d'insertion de données cachées basée ondelettes dans une image couleur. Les images de texture contenant les données MNT cachées peuvent ensuite être envoyées du serveur au client afin d'effectuer une visualisation 3D de terrains. Afin de combiner une visualisation en multirésolution et une compression, l'insertion des données cachées est intégrable dans le codeur JPEG 2000."
"Dans cet article, nous présentons une méthode permettant de décomposer un maillage triangulaire en un ensemble de carreaux quadrangulés. Elle se base sur la construction de quadrangles par fusion de triangles. Les quadrangles sont ensuite regroupés afin de composer des zones quadrangulées qui sont redécoupées en carreaux. Cette méthode a la particularité de ne pas modifier les points du maillage triangulaire d'origine contrairement à de nombreuses méthodes de remaillage. Les carreaux quadrangulés extraits peuvent ensuite être utilisés comme support d'une surface paramétrique ou d'un schéma de subdivision"
"On ne disconviendra pas que les réseaux sont aujourd'hui omniprésents. Mais on précisera qu'ils le sont avant tout dans nos discours : réseaux informatiques, socio-numériques, relationnels, professionnels, entreprises en réseau, réseaux économiques, financiers, liens et réseaux sociaux..., tout n'est que réseau, réductible à la notion de réseau. Son avènement le confirme, la science des réseaux ouvre à ces universaux – physiques, biolo-giques, sociologiques… – que sont les réseaux, tissés des interactions qui y sont à l'oeuvre. Bien avant le numérique, Saint-Simon, pionnier d'une philosophie des réseaux, plus tard les courants interactionnistes, ou même Georg Simmel et son interactionnisme social, auraient-ils eu raison quand ils insistaient sur la dimension interactionnelle de l'action, et par là réticulaire de la société ? Pour certains, la société ne serait-elle alors que réseau ? Ou bien encore, tous les réseaux ne seraient-ils pas sociaux ? Non pas qu'ils soient le social, mais parce qu'ils le construisent ? Là où d'autres objecteront que les réseaux – quels réseaux ? – ne font ni ne façonnent en rien le social… Dès lors, parce qu'une telle approche épistémologique et transdisciplinaire ne l'exclut pas, l'avènement desdits « réseaux sociaux » – plus exactement réseaux socio-numériques – pourra être intéressant, parmi d'autres questionnements. Dossier coordonné par Jean-Thierry JULIA Acteur • Connaissance • École • Hodologie • Hybridation • Interaction • Lien • Objet technique • Pli • Réseau • Réseaux sociaux • Réseaux socio-numériques • Savoir • Science des réseaux Prix : 21 E w 3. sc so c. un iv-t ls e2 .fr / Revue publiée avec le concours du CNRS, du Centre national du livre"
"Dans un environnement international extrêmement concurrentiel, piloter avec efficience son Système de Production est un point fondamental pour une entreprise. Les méthodes traditionnelles de gestion et de pilotage montrent toutes leurs limites respectives face à l'inflation des contraintes de production, et il devient incontournable d'explorer de nouvelles approches de pilotage, plutôt en rupture avec les approches généralement utilisées à l'heure actuelle pour effectuer les prises de décision. Parmi ces champs exploratoires, le paradigme holonique offre un cadre conceptuel au sein duquel la communauté HMS (Holonic Manufacturing System) propose diverses solutions, selon des approches combinant à des degrés divers hiérarchisation et distribution de la prise de décision. Après un panorama du travail de ces équipes, nous étudions une architecture pour le pilotage des systèmes de production ne présentant plus aucune dimension hiérarchique dans la prise de décision. Pour cela, nous proposons le concept d'isoarchie, résultant du parti pris de pousser le raisonnement de la distribution à ses limites. Au delà du travail de ces équipes, nous proposons une architecture pour le pilotage des systèmes de production ne présentant plus aucune dimension hiérarchique dans la prise de décision. Pour cela, nous définisons le concept d'isoarchie, résultant du parti pris de pousser à ses limites ce raisonnement. Nous présentons donc notre vision de la prise isoarchique de décision dans le contexte d'un ensemble d'entités holoniques en interaction. Différentes applications montrant différents aspects des performances obtenues et de la mise en oeuvre de tels systèmes seront détaillées."
"Cette thèse développe des méthodes de diagonalisation conjointe de matrices et de tenseurs d’ordre trois, et son application à la séparation MIMO de sources de télécommunications numériques. Après un état, les motivations et objectifs de la thèse sont présentés. Les problèmes de la diagonalisation conjointe et de la séparation de sources sont définis et un lien entre ces deux domaines est établi. Par la suite, plusieurs algorithmes itératifs de type Jacobi reposant sur une paramétrisation LU sont développés. Pour chacun des algorithmes, on propose de déterminer les matrices permettant de diagonaliser l’ensemble considéré par l’optimisation d’un critère inverse. On envisage la minimisation du critère selon deux approches : la première, de manière directe, et la seconde, en supposant que les éléments de l’ensemble considéré sont quasiment diagonaux. En ce qui concerne l’estimation des différents paramètres du problème, deux stratégies sont mises en œuvre : l’une consistant à estimer tous les paramètres indépendamment et l’autre reposant sur l’estimation indépendante de couples de paramètres spécifiquement choisis. Ainsi, nous proposons trois algorithmes pour la diagonalisation conjointe de matrices complexes symétriques ou hermitiennes et deux algorithmes pour la diagonalisation conjointe d’ensembles de tenseurs symétriques ou non-symétriques ou admettant une décomposition INDSCAL. Nous montrons aussi le lien existant entre la diagonalisation conjointe de tenseurs d’ordre trois et la décomposition canonique polyadique d’un tenseur d’ordre quatre, puis nous comparons les algorithmes développés à différentes méthodes de la littérature. Le bon comportement des algorithmes proposés est illustré au moyen de simulations numériques. Puis, ils sont validés dans le cadre de la séparation de sources de télécommunications numériques."
no abstract
no abstract
"Une équipe de travail s'est constituée, afin de réaliser une plateforme d'apprentissage sur « la construction d'équipe » et la « gestion de conflits ». L'équipe s'est fédérée autour d'une question: Les contraintes et les opportunités du e-learning offrent-elles de nouvelles perspectives pour personnaliser la pédagogie, en maintenant l'apprenant au centre de la construction pédagogique? Les réponses ont pris en compte les caractéristiques des interactions et des relations ainsi que l'utilité de la médiation induite par un outil et par la nécessité d'appartenance à un groupe. Abstract: A working team has been set up in order to realise a learning plateform on « team building » and « conflict facilitation ». This team has gathered around a question: Do e-learning constrains and opportunities offer new prospects for pedagogical issues maintaining the learner at the center of the pedagogical construction? The answers have taken into account interactions and relationships features as well as the useful mediation induced by the tool and by the group belonging. Riassunto: È stato formato un gruppo di lavoro per la realizzazione di una piattaforma di apprendimento su: "" la costruzione di una squadra "" e la "" gestione dei conflitti "". Il gruppo si è concentrato su una domanda: le difficoltà e le opportunità dell'e-learning offrono nuove prospettive per personalizzare la pedagogia mantenendo lo studente al centro della costruzione pedagogica? Le risposte hanno tenuto in conto le caratteristiche delle interazioni e delle relazioni umane e l'utilità della mediazione indotta dalla presenza di un mezzo iinformatico e dall'appartenenza al gruppo."
no abstract
"Cette communication présente d'une étude bibliométrique réalisée en 2006 pour la tenue d'un comité d'orientation scientifique organisé à l'initiative des trois universités d'Aix Marseille. Cette étude bibliométrique était l'un des nombreux d'outils proposés au comité d'orientation scientifique (une quarantaine d'experts externes) pour l'assister dans son travail de recommandation. Les résultats de l'étude offrait une vision globale et << objective >> de l'activité et de l'évolution du pôle scientifique de la région d'Aix-Marseille. La méthodologie employée pour cette étude était fortement inspirée de la méthode d'analyse des portefeuilles d'activités stratégiques mise en œuvre lors d'une analyse concurrentielle. La transposition de cette méthode a nécessité de définir les unités scientifiques stratégiques, de choisir les pôles universitaires << concurrents >>, de choisir le critère de mesure de l'activité scientifique, d'étudier l'évaluation de la dynamique des deux précédents facteurs au fil du temps et au final la construction d'une représentation graphique du positionnement du portefeuille scientifique du pôle Aix-Marseille et de sa dynamique. Cette étude bibliométrique a permis de confirmer la politique scientifique affichée par le pôle d'Aix Marseille et de donner une vision globale de son positionnement international. Elle a également eu un rôle pédagogique. Elle a permis d'habituer les acteurs de la recherche à l'utilisation des indicateurs bibliométriques dans un cadre d'auto-évaluation."
no abstract
"Résumé. La diminution de l’efficacité de la mécanique ventilatoire entraîne une insuffisance respiratoire chronique. A ce jour, le traitement privilégié consiste en une assistance ventilatoire non invasive, essentiellement utilisée la nuit. L’objectif de ce travail est d’étudier les influences de la ventilation non invasive sur l’organisme non seulement du point de vue de la qualité de la respiration, mais aussi sur la qualité du sommeil. Les mécanismes sous-jacents aux interactions patient-ventilateur et leurs influences sont pris en compte et quantifiés. Pour cela, deux études basées sur des polysomnographies ont été effectuées. Lors d’une première étude rétrospective, des synoptiques permettant une visualisation globale des événements au cours de la nuit par la représentation simultanée des variables enregistrées lors de la polysomnographie ont été construits. Ensuite, nous avons quantifié les relations entre les différents asynchronismes patient-ventilateur et les fuites non intentionnelles. Quatre types d’interactions patient-ventilateur ont ainsi pu être mis en évidence. Une seconde, prospective, a été conduite sur l’adaptation des patients à la ventilation non invasive lors de sa mise en place chez des patients atteints d’insuffisance respiratoire chronique. Reposant sur trois polysomnographies respectivement réalisées lors de la première nuit à l’hôpital en ventilation spontanée, lors de la deuxième à l’hôpital sous assistance ventilatoire non invasive et lors d’une troisième nuit à l ?hôpital, 15 jours après l’appareillage. Une analyse individuelle a été effectuée par l’interprétation des synoptiques de chacun des patients, et une analyse globale a été effectuée par une approche statistique. Une entropie de Shannon, calculée à partir de diagrammes de proche-retour, a également été utilisée pour estimer la qualité du sommeil. La mise en place de la ventilation se traduit par une amélioration des paramètres ventilatoires (oxymétrie et capnographie), une amélioration voire une restauration du temps passé en sommeil paradoxal, et une diminution de la fragmentation du sommeil par la correction des apnées obstructives. Sous ventilation, la variabilité cardiaque, estimée à partir d’une entropie de Shannon calculée sur la base d’une dynamique symbolique, diminue significativement. Peu d’effets des asynchronismes sur la qualité de la ventilation ont été notés au cours de cette étude."
no abstract
no abstract
L'expérience présentée dans cet article s'appuie sur des études de psychologie cognitive portant sur le rôle des défaillances expectatives dans l'apprentissage à partir d'histoires. Les Environnements Informatiques d'Apprentissage Humain (EIAH) que l'on peut construire à partir de cette approche se révèlent bien adaptés à l'apprentissage des compétences comportementales. Nous présentons ici un exemple d'utilisation d'un tel système de formation à partir d'histoires dédié au développement des conduites sociales de consultants spécialisés.
no abstract
no abstract
no abstract
no abstract
no abstract
"L’article que nous proposons est articulé autour de la notion de classe : doit-on considérer la classe comme un ensemble statique composé d’individus possédant la même structure et le même comportement, ou opter pour une approche plus dynamique dans laquelle les contours de la classe sont définis en situation via la prise en compte des individus proches d’un prototype représentatif d’une catégorie donnée. Le choix d’une approche dynamique conduit les auteurs à revisiter deux théories « classiques », la théorie sémio-contextuelle et la théorie acteur-réseaux, en proposant les premiers éléments d’une théorie des prototypes réseaux avec pour objectif de faciliter l’appréhension des groupes d’acteurs dans un contexte de généralisation des communications numériques pour in fine une meilleure compréhension des situations."
"Le développement des territoires est inséparable de la communication, elle-même pensée dans le cadre de dispositifs ambitieux. Au cœur de ces dispositifs, les technologies de l’information et de la communication (TIC) doivent s’appréhender dans une compréhension des usages et pratiques. Nous nous plaçons ici dans une approche artefactuelle de la communication qui nous conduit à envisager les TIC comme une des composantes de dispositifs pluriels qui construisent et représentent la médiation dans une collection de situations contrastées. A travers le déploiement d’un dispositif de gestion intégré des déchets (SITOM Sud-Gard), nous cherchons à éclairer le fonctionnement des acteurs et du système de communication, qui, dans un contexte général de développement durable, matérialise un enjeu majeur du développement de nos territoires."
no abstract
"En dépit de leur caractère distribué, les chaînes logistiques peuvent se révéler très performantes dans les conditions idéales de production et d’échange. Toutefois, leur complexité les rend de plus en plus fragiles. Cette thèse propose des modèles et des méthodes pour l’analyse des risques, de façon à renforcer la robustesse et la résilience des chaînes logistiques. Pour nous aider à mieux positionner nos travaux et à tirer les caractéristiques essentielles des chaînes logistiques, nous avons analysé ce domaine suivant une démarche ontologique à l’aide de la méthode KOD. En nous appuyant sur un état de l’art du domaine des risques dans les chaînes logistiques, et sur les bases de cas réels, nous avons identifié les indicateurs des vulnérabilités les plus significatifs. A partir des connaissances extraites, et des modèles mathématiques proposés dans la littérature, nous avons construit un modèle de chaîne logistique multi-étages à l’aide de modèles ARIMA intégrant l’aspect aléatoire de la demande. Pour adapter ce modèle aux situations de vulnérabilité et de risques, nous avons ajouté des contraintes de capacité et de positivité sur les commandes et sur les stocks. Sous l’effet d’événements dangereux, certaines contraintes du système peuvent être atteintes et par conséquence, son évolution peut s’écarter fortement de la dynamique nominale. Nous avons proposé des indicateurs de vulnérabilités comme des indicateurs de fréquence des retards de livraison, ou de surcoût d’immobilisation de produits. Enfin, l’occurrence d’événements dangereux a été représentée par des scénarios. Nous avons alors obtenu des résultats de simulation sous MATLAB, qui nous ont permis d’évaluer leurs conséquences pour différentes configurations du système, en particulier sous perturbation des flux d’informations (demande) et des flux physique (qualité de produits approvisionnés)."
no abstract
"This communication focuses on the characterisation of a similarity measure between parts of 2D point clouds. This measure is defined thanks to the use of a general knowledge about real point clouds: they share a large amount of one-dimensional structures. These structures can be represented into a unified manner with a new type of primitives; then, we set the link between the existence of common information between parts of point clouds and the geometric relations of their primitives. Thus, we define a similarity measure that is rotationally invariant, and an algorithm to compute it."
no abstract
no abstract
"L’objectif de cette thèse était de proposer, valider et comparer des méthodes de fusion d’images provenant d’un capteur héliosynchrone multispectral et d’un capteur géostationnaire multispectral, pour produire des cartes de composition de l’eau détaillées spatialement et les mieux rafraîchies possibles. Notre méthodologie a été appliquée au capteur héliosynchrone OLCI sur Sentinel-3 et au capteur géostationnaire FCI sur Météosat Troisième Génération. Dans un premier temps, la sensibilité des deux capteurs à la couleur de l’eau a été analysée. Les images des capteurs OLCI et FCI n’étant pas encore disponibles, ont donc été simulées sur le Golfe du Lion, grâce à des cartes d’hydrosols (chlorophylle, matières en suspension et matières organiques dissoutes) et à des modèles de transfert radiatifs (Hydrolight et Modtran). Deux méthodes de fusion ont ensuite été adaptées puis testées à partir des images simulées : la méthode SSTF (Spatial, Spectral, Temporal Fusion) inspirée de la fusion de (Vanhellemont et al., 2014) et la méthode STARFM (Spatial Temporal Adaptative Reflectance Fusion Model) de (Gao et al., 2006). Les résultats de fusion ont alors été validés avec des images de référence simulées et les cartes d’hydrosols estimées à partir de ces images ont été comparées aux cartes utilisées en entrée des simulations. Pour améliorer le SNR des images FCI, un filtrage temporel a été proposé. Enfin, comme le but est d’obtenir des indicateurs de qualité de l’eau, nous avons testé les méthodes de fusion sur les cartes d’hydrosols estimées à partir des images FCI et OLCI simulées."
no abstract
"Les systèmes de production actuels sont de plus en plus complexes : les produits fabriqués sont de plus en plus techniques, les moyens de production de plus en plus précis, les règles de gestion de plus en plus élaborées. Les outils d’aide à la décision sont indispensables afin de guider le pilotage de ces systèmes, que ce soit au niveau stratégique pour dimensionner le système, au niveau tactique pour le piloter et/ou planifier les activités avec affectation de ressources ou au niveau opérationnel pour ordonnancer les activités. Nous proposons de développer un outil d’aide à la décision générique et modulaire."
"Cette étude porte sur des absorbeurs non linéaires de vibrations utilisés pour atténuer les irrégularités de rotation de systèmes tournants. Ils sont accordés sur un ordre de la vitesse de rotation du système vibrant. Cependant, leurs fortes non linéarités intrinsèques engendrent un désaccord de l’absorbeur pour de grandes amplitudes d’oscillation. On présente une méthode de suivi de fréquence d’antirésonance basée sur une technique de continuation numérique. Cette méthode permet une prédiction rapide et précise du point de fonctionnement du système vis-à-vis de l’amplitude d’oscillation de l’absorbeur."
no abstract
no abstract
no abstract
no abstract
"Pour résoudre les problèmes de satisfaction de contraintes pondérés, les méthodes basées sur une dé-composition arborescente constituent une approche inté-ressante selon la nature des instances considérées. Sou-vent, les décompositions exploitées visent à réduire la taille maximale des clusters, connue comme étant la lar-geur de la décomposition. En effet, l'intérêt de ce pa-ramètre est lié à son importance par rapport à la com-plexité théorique de telles méthodes. À ce niveau, Min-Fill constitue l'heuristique de référence pour le calcul de décompositions. Cependant, son intérêt pratique pour la résolution de problèmes demeure limité au vu de ses multiples défauts, notamment au niveau de la restriction de la liberté de l'heuristique de choix de variables. Ainsi, nous proposons, dans un premier temps, d'ex-ploiter de nouvelles décompositions pour le problème d'optimisation sous contraintes. Le but de ces décom-positions est de capturer des critères permettant d'aug-menter l'efficacité de la résolution. Dans un second temps, nous proposons d'exploiter ces décompositions plus dynamiquement dans le sens où la résolution d'un sous-problème ne se baserait sur la décomposition que lorsque cela semble utile. Les expérimentations réalisées montrent l'intérêt pratique des nouvelles décompositions ainsi que l'apport de leur exploitation dynamique. Abstract When solving weighted constraint satisfaction problems , methods based on a tree-decomposition constitute an interesting approach depending on the nature of the considered instances. The exploited decomposi-tions often aim to reduce the maximal size of the clusters , which is known as the width of the decomposition. Indeed, the interest of this parameter is related to its importance with respect to the theoretical complexity of these methods. However, its practical interest for the solving of instances remains limited if we consider its multiple drawbacks, notably due to the restriction imposed on the freedom of the variable ordering heuristic."
"Développer des applications multiplateformes pour les appareils mobiles est une tâche assez complexe car les différents systèmes qui les animent se sont rendus parfaitement incompatibles en termes de portage d'applications. Cordova, supporté par le groupe Apache, est une alternative de développement multiplateforme mobile se basant sur HTML5, CSS3 et JavaScript. Cordova est une forme de conteneur pour interfacer l'application Web avec les fonctionnalités natives de l'appareil mobile. Pour les étudiants R&T cette plateforme met en valeur les connaissances acquises dans les modules de développement Web et la programmation pour appareils mobiles."
"Ce travail s’inscrit dans une tentative de liaison entre la communauté classique de la Vision par ordinateur et la communauté du traitement d’images de documents, analyse être connaissance (DAR). Plus particulièrement, nous abordons la question des détecteurs de points d’intérêts et des descripteurs locaux dans une image. Ceux-ci ayant été conçus pour des images issues du monde réel, ils ne sont pas adaptés aux problématiques issues du document dont les images présentent des caractéristiques visuelles différentes.Notre approche se base sur la résolution du problème de la confusion entre les descripteurs,ceux-ci perdant leur pouvoir discriminant. Notre principale contribution est un algorithme de réduction de la confusion potentiellement présente dans un ensemble de vecteurs caractéristiques d’une même image, ceci par une approche probabiliste en filtrant les vecteurs fortement confusifs. Une telle conception nous permet d’appliquer des algorithmes d’extractions de descripteurs sans avoir à les modifier ce qui constitue une passerelle entre ces deux mondes."
"Dans cette thèse, nous nous intéressons au système d’identification automatique spatial lequel est dédié à la surveillancemaritime par satellite. Ce système couvre une zone bien plus large que le système standard à terre correspondant àplusieurs cellules traditionnelles ce qui peut entraîner des risques de collision des données envoyées par des navireslocalisés dans des cellules différentes et reçues au niveau de l’antenne du satellite. Nous présentons différentes approchesafin de répondre au problème de collision considéré. Elles ne reposent pas toujours sur les mêmes hypothèses en ce quiconcerne les signaux reçus, et ne s’appliquent donc pas toutes dans les mêmes contextes (nombre de capteurs utilisés,mode semi-supervisé avec utilisation de trames d’apprentissage et information a priori ou mode aveugle, problèmes liés àla synchronisation des signaux, etc...).Dans un premier temps, nous proposons des méthodes permettant la séparation/dé-collision des messages en modèle surdéterminé(plus de capteurs que de messages). Elles sont fondées sur des algorithmes de décompositions matriciellesconjointes combinés à des détecteurs de points temps-fréquence (retard-fréquence Doppler) particuliers permettant laconstruction d’ensembles de matrices devant être (bloc) ou zéro (bloc) diagonalisées conjointement. En ce qui concerneles algorithmes de décompositions matricielles conjointes, nous proposons quatre nouveaux algorithmes de blocdiagonalisation conjointe (de même que leur version à pas optimal) fondés respectivement sur des algorithmesd’optimisation de type gradient conjugué, gradient conjugué pré-conditionné, Levenberg-Marquardt et Quasi-Newton. Lecalcul exact du gradient matriciel complexe et des matrices Hessiennes complexes est mené. Nous introduisonségalement un nouveau problème dénommé zéro-bloc diagonalisation conjointe non-unitaire lequel généralise le problèmedésormais classique de la zéro-diagonalisation conjointe non-unitaire. Il implique le choix d’une fonction de coût adaptéeet à nouveau le calcul de quantités telles que gradient matriciel complexe et les matrices Hessiennes complexes. Nousproposons ensuite trois nouveaux algorithmes à pas optimal fondés sur des algorithmes d’optimisation de type gradientconjugué, gradient conjugué pré-conditionné et Levenberg-Marquardt.Finalement, nous terminons par des approches à base de techniques de détection multi-utilisateurs conjointe susceptiblesde fonctionner en contexte sous-déterminé dans lequel nous ne disposons plus que d’un seul capteur recevantsimultanément plusieurs signaux sources. Nous commençons par développer une première approche par déflationconsistant à supprimer successivement les interférences. Nous proposons ensuite un deuxième mode opératoire fondéquant à lui sur l’estimateur du maximum de vraisemblance conjoint qui est une variante de l’algorithme de VITERBI."
"L'´ etude des triangles cassés devient de plus en plus ambitieuse, par la résolution desprobì emes de satisfaction de contraintes (CSP) en temps polynomial d'un coté, et par la réduction de l'espace de recherchè a tra-vers l'´ elimination de variables et la fusion de valeurs de l'autre. Pour cela, plusieurs extensions de ce concept ontétéétudiées ontétéontétéétudiées dans le passé récent, tel que les triangles cassés duaux et les triangles légèrement cassés. Ces extensions ontétéontété introduites dans le but de maximiser soit le nombre de valeurs fusionnées et/ou le nombre d'ins-tances traitables capturées. Mais, aucune d'entre elles n'a préservé toutes les caractéristiques de BTP. Ici, nous introduisons une nouvelle version légère de BTP, que nous appelons m-fBTP (pour flexible broken-triangle property). m-fBTP permet la fusion de valeurs, l'´ elimination de variables et définit une plus grande classe polynomiale pour laquelle la cohérence d'arc est une pro-cédure de décision. Une version plus détaillée en langue anglaise a ´ eté publiéè a AAAI'17 [4]."
"Trois modules seront développés: Le module de détection a produit une analyse des problématiques figurations con, à savoir un ensemble de domaines où soit quelques nouveaux DDL ou des changements locaux dans les contraintes sont obligatoires. Le module de traitement permettra à la défi nition des mécanismes pour aider la décision sur modi cations. Le module de prédiction dire le degré de déformation en pré-analyser les caractéristiques des configurations de NURBS."
"Cette thèse commence avec un état de l’art des domaines d’études importants pour notre objectif (différentes techniques usuelles de réduction des vibrations en usinage, méthodes de contrôle actif) avant de valider le principe de contrôle actif du fraisage en se plaçant en repère fixe. On a alors développé un modèle d’état d’une poutre d’Euler Bernoulli perturbée en un point et corrigée en un autre via un actionneur piézoélectrique. Ce modèle a permis d’obtenir plusieurs compensateurs, suivant différentes stratégies de commande. Nous avons par la suite procédé, d’un point de vue expérimental, à l’étude sur un dispositif similaire à notre besoin d’un point de vue de l’actionnement et des ordres de grandeurs (amplification mécanique, gamme de fréquences etc.). Les stratégies de commande robustes que nous avons développé pour pouvoir atténuer les déplacements vibratoires de cette poutre ont conduit à des résultats concluants présentés dans le même chapitre, d’abord en simulation (qui nous a permis une étude comparative), avec ou sans la présence du processus d’usinage, puis expérimentalement. La robustesse de ces stratégies de commande a été étudiée (en simulation) en ajoutant des incertitudes au modèle étudié de différentes manières. Ensuite, nous avons identifié le modèle du système étudié, déterminé les correcteurs correspondants et testé ces derniers sur notre banc d’essai pour valider le bon fonctionnement des différentes stratégies de contrôle utilisées tout le long de cette thèse. Enfin, pour préparer un déploiement de ces stratégies en repère tournant (porte-outil de contrôle actif), nous avons modélisé et implémenté les mêmes démarches pour le cas où l’actionnement se situe en repère tournant et concerne deux axes simultanément, situés dans le plan XY du porte-outil. Nous avons d’abord étudié les vibrations transversales d’une poutre en rotation dans le cas général avant de négliger les phénomènes d’inertie et gyroscopique. En effet, on s’intéresse au contrôle actif du fraisage particulièrement dans les applications de finition, là où on utilise des outils longs de faibles diamètres. Les nouvelles expressions des deux fonctions de transfert de notre système usinant ont été déterminées pour obtenir sa représentation d’état, clé du contrôle actif. La projection du processus de coupe sur le repère tournant est indispensable pour effectuer les simulations du fraisage via le porte outil actif. Ce dernier chapitre met en relief les perspectives de cette thèse, à savoir le contrôle actif du fraisage quelque soit le type de l’opération ou du diamètre de l’outil avec un porte outil mécatronique destiné pour ce genre d’opérations."
"La théorie de l'homologie formalise la notion de trou dans un espace. Pour un sous-ensemble de l'espace Euclidien, on définit une séquence de groupes d'homologie, dont leurs rangs sont interprétés comme le nombre de trous de chaque dimension. Ainsi, β₀, le rang du groupe d'homologie de dimension zéro, est le nombre de composantes connexes, β₁ est le nombre de tunnels ou anses et β₂ est le nombre de cavités. Ces groupes sont calculables quand l'espace est décrit d'une façon combinatoire, comme c'est le cas pour les complexes simpliciaux ou cubiques. À partir d'un objet discret (un ensemble de pixels, voxels ou leur analogue en dimension supérieure) nous pouvons construire un complexe cubique et donc calculer ses groupes d'homologie. Cette thèse étudie trois approches relatives au calcul de l'homologie sur des objets discrets. En premier lieu, nous introduisons le champ de vecteurs discret homologique, une structure combinatoire généralisant les champs de vecteurs gradients discrets, qui permet de calculer les groupes d'homologie. Cette notion permet de voir la relation entre plusieurs méthodes existantes pour le calcul de l'homologie et révèle également des notions subtiles associées. Nous présentons ensuite un algorithme linéaire pour calculer les nombres de Betti dans un complexe cubique 3D, ce qui peut être utilisé pour les volumes binaires. Enfin, nous présentons deux mesures (l'épaisseur et la {largeur) associées aux trous d'un objet discret, ce qui permet d'obtenir une signature topologique et géométrique plus intéressante que les simples nombres de Betti. Cette approche fournit aussi quelques heuristiques permettant de localiser les trous, d'obtenir des générateurs d'homologie ou de cohomologie minimaux, d'ouvrir et de fermer les trous."
Nous introduisons une nouvelle méthode d'apprentissage de clauses dites nobetters pour les solveurs séparation etévaluationetévaluation pour Max-SAT. Elle s'inspire de l'apprentissage de clauses nogoods utilisé par les solveurs 5 SAT basés sur l'analyse de conflits (CDCL). Elle a pour objectif de permettre une meilleure résolution des instances industrielles par une meilleure prise en compte de leurs structures.
"Le contexte général de cette thèse est l’étude de problèmes inverses et directs en théorie du contrôle. Plus précisément, les trois problèmes étudiés sont les suivants.Le premier est un problème de contrôle optimal (approche directe). Il s’agit de fournir la synthèse temps minimum du modèle cinématique d'un drone volant à altitude constante, de vitesse linéaire non nécessairement constante voulant rejoindre une trajectoire circulaire de rayon de courbure minimum.Le deuxième problème concerne une approche inverse du contrôle optimal. Il s’agit d’élaborer des méthodes théoriques de reconstruction du critère optimisé dans un problème de contrôle optimal à partir d’un ensemble de solutions à ce problème, ainsi que caractériser les ""bons"" ensembles de trajectoires permettant la reconstruction du critère. Le contrôle optimal inverse connait un regain d’intérêt depuis une quinzaine d’années, en particulier dans l’étude des comportements moteurs humains. En effet, selon un paradigme largement accepté en neurophysiologie, parmi tous les mouvements possibles ceux effectivement réalisés sont solutions d’un processus d’optimisation.Le troisième problème traite de stabilisation par retour de sortie. Nous analysons, à travers un exemple académique tiré du contrôle quantique, le problème de stabilisation par retour de sortie (à l’aide d’un observateur) lorsque le point où l'on souhaite stabiliser le système correspond à un contrôle qui rend le système inobservable. L’idée générale est de perturber le retour d’état stabilisant afin de garantir l’observabilité du système tout en stabilisant le système sur la cible. L’analyse de cet exemple académique nous permet dans un second temps de dégager une méthode générale pouvant s’appliquer à une classe de système beaucoup plus large."
"A multitude of online repositories and a large amount of specific CAD model databases are currently available. Therefore, it is crucial to have access to these data in an easy and in a multi-perspective manner according to multiple access keys, not only in terms of annotation data or shape similarity, but also in terms of specific characteristics. In this perspective, this paper proposes a multi-level approach for CAD assembly model retrieval, which exploits assembly specific information related not only to the shape of the constituting components but also to peculiar assembly information, such as kinematic joints and component arrangements. In many cases, most of this information is not explicitly stored, thus tools for its extraction must be provided. In this paper, we focus on the detection of regular patterns of repeated elements in CAD assemblies and on their exploitation for the browsing and retrieval of assembly models."
"Leprobì eme du dénombrement de solutions d'une instance CSP, appelé #CSP, constitue unprobì eme ex-trêmement difficile qui possède de multiples applications en Intelligence Artificielle. S'il est le plus souvent résolu par des méthodes approchées, ici, nous nous focalisons sur le dénombrement exact. Nous montrons comment il est possible d'améliorer les méthodes basées sur les décompositions structurelles en améliorant la recherche d'une nouvelle solution, qui est uné etape essentielle, en particulier pour de telles méthodes. De plus, si les res-sources en temps ou en espace sont insuffisantes, nous montrons comment notre approche est capable de four-nir une borne inférieure du nombre de solutions. Des expérimentations sur des benchmarks CSP mettent en avant l'intérêt pratique de notre approche par rapport aux meilleures méthodes de la littérature. Ce papier est un résumé de [6]. Abstract The problem of counting solutions in CSP, called #CSP, is an extremely difficult problem that has many applications in Artificial Intelligence. This problem can be addressed by exact methods, but more classically it is solved by approximate methods. Here, we focus primarily on the exact counting. We show how it is possible to improve the methods based on structural decomposition by offering to enhance the search for a new solution which is a critical step for counting, particularly for such methods. Moreover, if the resources in time or in space are insufficient, we show that our approach is still able to provide a lower bound of the result. Experiments on CSP benchmarks show the practical advantage of our approach w.r.t. the best methods of the literature. This is a summary of [6]."
"Cette thèse se place dans le cadre de services web en dépassant leur description pour considérer leur structuration en réseaux (réseaux d'interaction et réseaux de similitude). Nous proposons des méthodes basées sur les motifs, la modélisation probabiliste et l'analyse des concepts formels, pour améliorer la qualité des services découverts. Trois contributions sont alors proposées: découverte de services diversifiés, recommandation de services et cohérence des communautés de services détectées. Nous structurons d'abord les services sous forme de réseaux. Afin de diversifier les résultats de la découverte, nous proposons une méthode probabiliste qui se base à la fois sur la pertinence, la diversité et la densité des services. Dans le cas de requêtes complexes, nous exploitons le réseau d'interaction de services construit et la notion de diversité dans les graphes pour identifier les services web qui sont susceptibles d'être composables. Nous proposons également un système de recommandation hybride basé sur le contenu et le filtrage collaboratif. L'originalité de la méthode proposée vient de la combinaison des modèles thématiques et les motifs fréquents pour capturer la sémantique commune maximale d'un ensemble de services. Enfin, au lieu de ne traiter que des services individuels, nous considérons aussi un ensemble de services regroupés sous forme de communautés de services pour la recommandation. Nous proposons dans ce contexte, une méthode qui combine la sémantique et la topologie dans les réseaux afin d'évaluer la qualité et la cohérence sémantique des communautés détectées, et classer également les algorithmes de détection de communautés."
"La spécialité ""Réseaux et Télécommunications"" forme des techniciens supérieurs capables de s'insérer dans les secteurs des réseaux informatiques, télécommunications et du web, ou de poursuivent leurs études en licence professionnelle orientée vers la sécurité et l'administration des réseaux informatiques (ASUR). Le programme pédagogique national a nettement mis l'accent sur l'administration des systèmes et des services de l'Internet en l'affectant d'une charge d'environ 700 heures réparties sur 7 modules dans les 3 premiers semestres. D'une part, parce que ce type d'enseignement est très difficile à mener dans des salles informatiques banalisées, d'autre part, parce que la mise en place d'une pédagogie par projet nous semble être un gage d'une formation de qualité, il nous semble donc important la mise en place du matériels adéquats (Serveurs, Stations de travail, Unités mobiles) à vocation purement pédagogique pour l'équipement d'une salle de travaux pratiques afin de mettre en oeuvre une solution répondant aux exigences pédagogiques de nos modules d'enseignements. Ainsi, dans ce papier nous présentons la solution technique mise en place dans notre département R&T pour favoriser la pédagogie par projet et l'évaluation par compétences dans les modules administration systèmes et services réseaux."
no abstract
no abstract
Nous présentons une interface de recommandation d'emojis porteurs de sentiments qui utilise un modèle de prédiction appris sur des messages informels privés. Chacun étant associé à deux scores de polarité prédits. Cette interface permet permet également d'enregistrer les choix de l'utilisateur pour confirmer ou infirmer la recommandation.
"Cette thèse traite de l'étude de méthodes de diagonalisation conjointe de matrices complexes, en vue de la séparation de sources, que ce soit dans le domaine des télécommunications numériques ou de la radioastronomie. Après avoir présenté les motivations qui ont poussé cette étude, nous faisons un bref état de l'art dans le domaine. Le problème de la diagonalisation conjointe, ainsi que celui de la séparation de source sont rappelés, et un lien entre ces deux sujets est établi. Par la suite, plusieurs algorithmes itératifs sont développés. Dans un premier temps, des méthodes utilisant une mise à jour de la matrice de séparation, de type gradient, sont présentées. Elles sont basées sur des approximations judicieuses du critère considéré. Afin d'améliorer la vitesse de convergence, une méthode utilisant un calcul du pas optimal est présentée, et plusieurs variantes de ce calcul, basées sur les approximations faites précédemment, sont développées. Deux autres approches sont ensuite introduites. La première détermine la matrice de séparation de manière analytique, en calculant algébriquement les termes composant la matrice de mise à jour par paire à partir d'un système d'équations linéaire. La deuxième estime récursivement la matrice de mélange, en se basant sur une méthode de moindres carrés alternés. Afin d'améliorer la vitesse de convergence, une recherche de pas d'adaptation linéaire est proposée. Ces méthodes sont alors validées sur un problème de diagonalisation conjointe classique. Puis les algorithmes sont appliqués à la séparation de sources de signaux de télécommunication numérique, en utilisant des statistiques d'ordre deux ou supérieur. Des comparaisons sont également effectuées avec des méthodes standards. La deuxième application concerne l'élimination des interférences terrestres à partir de l'estimation de l'espace associé, afin d'observer au mieux des sources cosmiques, issues de données de station LOFAR."
"Partage de compétences dans un réseau d'acteurs Cet article présente une méthode de recherche envisagée comme la construction d'un outil d'aide à la constitution d'équipe. Ces équipes sont constituées en fonction de projets particuliers et leur organisation doit tenir compte des connaissances, des compétences et des capacités que chacun des membres veut bien mettre à la disposition des autres. Nous cherchons donc à savoir comment choisir ses coéquipiers pour s'assurer de la réussite d'un projet lorsqu'on est en relation avec un groupe de personnes, de type réseau social fermé. En d'autres termes, comment optimiser ce réseau fermé pour prendre la bonne prise de décision sur un projet particulier, à partir des choix spécifiques des membres du réseau. Mots clés : Compétence, scénario, différentiateur sémantique, équipe, membre. Sharing of expertise in a network of actors This article is presenting a research method as a tool which is going to help for constituting a team. Teams are established on a project basis and are organized taking in consideration knowledge, skills, abilities that each member is willing to provide to others. The main question is how to choose the members in order to insure the project success when one is linked to a group of persons in a kind of closed social network. In other words, we are trying to know how to optimize this closed social network to make the right decision for the establishment of the project team from the choices of network members."
no abstract
no abstract
"Dans le contexte économique et compétitif actuel, les approches d’ingénierie intégrée ont apparu pour une meilleure gestion et organisation du cycle de vie des produits. Dans ce contexte la prise en compte des variabilités et leurs interdépendances a été démontré comme étant indispensable à l’amélioration de performance (coût, risque, qualité, …). Bien que l’amélioration de la conception, et la maîtrise de leurs variations soient souvent au coeur de ces travaux de recherche, il est impératif de poursuivre l’effet de ces variations redoutées au cours de la production. Pour cela le meilleur moyen reste l’inspection, par le contrôle de conformité du produit, et le suivi du processus de fabrication. Pour l’élaboration d’un plan d’inspection optimal, un cadre méthodologique est proposé qui permet une prise de décision opérationnelle par l’intervention des outils opérationnels et assure la satisfaction des objectifs stratégiques (réduction des coûts, amélioration de la qualité, augmentation de la productivité, …). Première activité de ce cadre est l’identification des caractéristiques clés à contrôler/suivre. Pour intégrer cette activité, les outils AMDEC, KC flowdown, sont retenus suite à une synthèse approfondie de la littérature. Cependant ces outils représentent certains manques à compenser et font l’objet de certaines modifications (adaptations) pour convenir au mieux aux besoins de cadre méthodologique proposé. L’intégration d’AMDEC et de KC flowdown aboutit à un nouvel outil présenté en détail nommé ACDE (Analyse de la Causalité, des Défaillance, et leurs Effets) dans le cadre de la planification d’inspection « au juste nécessaire »."
"Dans le cycle de conception du produit et de son processus, de fabrication jusqu'à l’industrialisation, il est inévitable de prendre en compte la variabilité des caractéristiques. Maitrise de l’évolution de celui-ci passe systématiquement par l’élaboration d’un processus d’inspection. Nous proposons à travers cet article, un processus décisionnel outillé qui intègre multiple aspects de la performance. Il est constitué d’une activité stratégique définissant un ensemble de critères de la prise de décision et trois activités opérationnelles dont l’objectif est de la conception conjoint des tâches de contrôle et de suivi de fabrication. Dans cet article, la modélisation et la formalisation de cette prise de décision ainsi que la capitalisation de la connaissance métier par les outils, intervenants dans la démarche décisionnelle, sont proposée."
"The development of analytical technologies and simulation tools used in the PLM increase day by day. There is a lot of data, information and knowledge associated to the product and its manufacturing plan. Precisely, during the process of design for manufacturing, the extensive number of solutions contains a lot of behaviours, associations, aspects and inputs to consider. For this reason, this paper aims at proposing a new multi-scale model as a way to provide a better structuring, better perception and better description of the many aspects involved in a product design and its manufacturing plan. The product and manufacturing plan models are based on different scale representations, characterized through “representation axes”, where the knowledge is decomposed and commit. At the same time, manufacturing knowledge is implemented to bring and evaluate the coherency among the model features."
"Dans le cycle de conception du produit et de son processus, de fabrication jusqu'à l'industrialisation, il est inévitable de prendre en compte la variabilité des caractéristiques. Maitrise de l'évolution de celui-ci passe systématiquement par l'élaboration d'un processus d'inspection. Nous proposons à travers cet article, un processus décisionnel outillé qui intègre multiple aspects de la performance. Il est constitué d'une activité stratégique définissant un ensemble de critères de la prise de décision et trois activités opérationnelles dont l'objectif est de la conception conjoint des tâches de contrôle et de suivi de fabrication. Dans cet article, la modélisation et la formalisation de cette prise de décision ainsi que la capitalisation de la connaissance métier par les outils, intervenants dans la démarche décisionnelle, sont proposée."
"Editorial ""Systèmes Contrôlés par le Produit"", numéro spécial SCP basé sur les communications présentées à MOSIM 2008"
"Nous proposons une tentative d'exploration du concept de mesures de cohérence. Par celles-ci, il s'agit d'attribuer un degré de cohérence à des ensembles finis de formules logiques, comme un pendant au concept bien connu de me-sures d'incohérence qui attribuent un degré d'incohérence à des ensembles finis de formules logiques. Nous introduisons un ensemble primitif de postulats pour des mesures de co-hérence. Nous nous penchons sur quelques correspondances avec les mesures d'incohérence. Nous posons également les bases d'une dualité entre les deux univers. Finalement, nous examinons de façon préliminaire ce que pourrait être une mesure mixte, à savoir, une mesure qui détermine un degré, sur un même référentiel, pour la cohérence (valeur positive) ainsi que pour l'incohérence (valeur négative). Nous abordons aussi, en comparaison, la question des super-modèles de Ginsberg et col., ainsi que ce qui peut en être considéré comme une généralisation, les morpho-logiques."
"L'importance des problèmes CSP, WCSP et #CSP est reflétée par la part considérable des travaux, théoriques et pratiques, dont ils font l'objet en intelligence artificielle et bien au-delà. Leur difficulté est telle qu'ils appartiennent respectivement aux classes NP-complet, NP-difficile et #P-complet. Aussi, les méthodes qui permettent de résoudre efficacement leurs instances ont une complexité en temps exponentielle. Les travaux de recherche de cette thèse se focalisent sur les méthodes de résolution exploitant la notion de décomposition arborescente. Ces méthodes ont suscité un vif intérêt de la part de la communauté scientifique du fait qu'elles soient capables de résoudre en temps polynomial certaines classes d'instances. Cependant, en pratique, elles n’ont pas encore montré toute leur efficacité vu la qualité de la décomposition employée ne prenant en compte qu'un critère purement structurel, sa largeur. Premièrement, nous proposons un nouveau cadre général de calcul de décompositions qui a la vertu de calculer des décompositions qui capturent des paramètres plus pertinents à l'égard de la résolution que la seule largeur de la décomposition. Ensuite, nous proposons une exploitation dynamique de la décomposition pendant la résolution pour les problèmes (W)CSP. Le changement de la décomposition pendant la résolution vise à adapter la décomposition selon la nature de l’instance. Finalement, nous proposons un nouvel algorithme de comptage qui exploite la décomposition d'une façon différente de celle des méthodes standards afin d'éviter des calculs inutiles. L'ensemble des contributions ont été évaluées et validées expérimentalement."
"Le recalage de deux nuages de points 3D est une étape essentielle dans de nombreuses applications. L’objectif de notre travail est d’estimer une transformation isométrique permettant de fusionner au mieux deux ensembles hétérogènes de points issus de deux capteurs différents. Dans cet article, nous présenterons une méthode de recalage 3D - 3D originale qui se distingue par la nature de la signature extraite en chaque point et par le critère de similarité utilisé pour mesurer le degré de ressemblance. Le descripteur que nous pr oposons est invariant à la rotation et à la translation et permet également de s’affranchir du problème de la multi - résolution relatif aux données hétérogènes. Dans le but de valider notre approche, nous l’avons testé sur des données synthétiques et nous l’avons appliqué sur des données réelles hétérogènes."
"Ce manuscrit synthétise une dizaine d'années de recherche pluridisciplinaire autour de la forme du cortex cérébral. Une première partie propose un certain nombre de méthodes et modèles génériques, issus des mathématiques appliquées ou de l'informatique, pour décrire, représenter et simuler des formes géométriques. Essentiellement je propose des contributions en termes de simulation et calcul numérique dans le cadre des équations aux dérivées partielles et de l'analyse spectrale sur des variétés. Dans une second partie, je cherche à montrer comment les outils précédents peuvent être appliqués à des questions fondamentales et cliniques relatives à la variabilité et à la morphogenèse du cortex cérébral. En particulier je m'intéresse à la variabilité de taille et à la variabilité du plissement cortical, deux phénomènes qui se confondent lors du développement cérébral précoce. La dissociation entre les deux parties doit moins être vue comme une dissociation dans la façon de traiter des problèmes pluri-disciplinaires que comme un parti pris pour s'adresser en même temps à plusieurs communautés scientifiques."
"Un objet peut contenir des zones autour desquelles on peut attacher une chaîne sans qu'elle puisse être enlevée. Le poignet ou le cou en sont des exemples sur le corps humain. Nous appelons ces zones « menottables ». Dans cet article nous formalisons mathématiquement cette notion sur des maillages 3D. Cette définition donne une visualisation naturelle des zones « menottables ». De plus, nous présentons une notion de persistance des zones « menottables » qui permet de distinguer les plus importantes et d'obtenir une signature géométrique de l'objet."
no abstract
no abstract
L’objectif de cette thèse est de proposer des stratégies de diagnostic dans le cas d'une commande en vitesse sans capteur mécanique (vitesse/position) d’une machine asynchrone triphasée en présence de défaut d'ouverture des transistors IGBT (Insulated Gate Bipolar Transistor) de l’onduleur. Une étude de l’impact de ces défauts sur les performances de ces structures sans capteur mécanique en termes de stabilité et de robustesse des observateurs en mode dégradé est présentée. Un observateur par mode glissant (Sliding Mode Observer) à base de modèle est développé et validé expérimentalement en vue de la commande sans capteur mécanique de la machine asynchrone triphasée. Les signaux issus de l’observateur (approche modèle) sont utilisés conjointement avec ceux mesurés (approche signale) pour former une approche hybride de diagnostic de défauts des transistors IGBT de l’onduleur. Un observateur par mode glissant d’ordre 2 à base d’un algorithme Super-Twisting est ensuite développé en vue d’améliorer la stabilité et d’assurer la continuité de fonctionnement du système en présence d'un défaut afin de pouvoir appliquer une stratégie de commande tolérante aux défauts dans les meilleures délais et conditions de fonctionnement.
"Cet article vise à analyser l’engagement sportif, associatif et politique des jeunes skateboarders en privilégiant l’approche diachronique. L’analyse croise des dossiers d’associations, des archives personnelles de skateboarders, des entretiens semi-directifs et articles de presse. Elle se focalise sur les transformations du contexte local de pratique et identifie trois générations de pratiquants. Le renouvellement du public s’inscrit dans une transformation du contexte local de pratique qui agit sur le renouvèlement des pratiques, des conditions de pratique, de la sociabilité et les relations avec le pouvoir municipal. On peut distinguer une évolution en trois étapes avec une ère des pionniers, d’institutionnalisation et une dernière marquée par des difficultés qui in fine interrogent le processus de légitimation de la pratique au niveau local."
"La thèse est consacrée à l'étude de diagnostic de pannes pour les systèmes pile à combustible de type PEMFC. Ce sujet de recherche est dans le cadre du project ANR DIAPASON2 (Jan. 2011- Sep. 2014).Le but est d’améliorer la fiabilité et la durabilité des systèmes pile à combustible. Afin d'assurer un diagnostic précis et applicable en temps réel, les approches explorées dans cette thèse sont celles du diagnostic guidé par les données. Plusieurs essais sont effectués sur les différentes piles PEMFC. Les données sont obtenues dans les expériences impliquant non seulement l'état de fonctionnement normal, mais aussi les états de défaut. Parmi les méthodes dédiées au diagnostic guidé par les données, les techniques basées sur la reconnaissance de forme sont les plus utilisées. Dans ce travail, les variables considérées sont les tensions des cellules et la démarche adoptée se compose de deux étapes. La première étape consiste à extraire les caractéristiques, et la deuxième étape permet d'établir une classification. Les résultats établis dans le cadre de la thèse peuvent être regroupés en trois contributions principales. La première contribution est constituée d'une étude comparative. Plus précisément, plusieurs méthodes sont explorées puis comparées en vue de déterminer une stratégie précise et offrant un coût de calcul optimal. La deuxième contribution concerne le diagnostic online sans connaissance complète des défauts au préalable. Il s'agit d'une technique adaptative qui permet d'appréhender l'apparition de nouveaux types de défauts. Cette technique est fondée sur la méthodologie SSM-SVM et les règles de détection et de localisation ont été améliorées pour répondre au problème du diagnostic en temps réel. La troisième contribution est obtenue à partir méthodologie fondée sur l'utilisation partielle de modèles dynamiques. Le principe de détection et localisation de défauts est fondé sur des techniques d'identification et sur la génération de résidus directement à partir des données d'exploitation. Toutes les stratégies proposées dans le cadre de la thèse ont été testées à travers des données expérimentales et validées sur un système embarqué."
"Le pilotage d'un avion à différentes étapes de vol est un problème de logique non monotone. Parce que les règles de pilotage peuvent changer en fonction des circonstances ex-ternes, telles que les perturbations atmosphériques ou les situations de sécurité ou d'urgence du pilote. Nous présen-tons une méthodologie pour mener un vol stable prenant en compte des facteurs d'incertitude et de contradiction des informations."
"Distributed systems spread widely in industrial environments. One of the key challenges is the exchange and the aggregation of data between these sys-tems. Although standards play an important role to solve data interoperability is-sues between systems, these standards do not completely address existing indus-trial problems. In fact, it is not granted to have an industrial environment that complies with a unique standard; therefore, ad-hoc solutions are used to solve this issue. In this article, the authors propose a generic architecture to address the in-teroperability between systems. This architecture is developed based on model-based techniques and principles. Besides, it reduces the need for human interven-tion and time by developing once and reusing the building blocks of the architec-ture. Finally, the architecture is described in its application to a case study."
"In this paper a generic and modular decision support tool developed to solve different planning, assignment or scheduling problems is presented. The utilization of this tool is illustrated by solving a real world multi-period job-shop scheduling problem proposed by a case study company which produces refrigerated foodservice equipment. The case study company problem and a list algorithm developed to integrate the proposed tool for this particular problem are presented. Preliminary results show that the proposed tool can be effectively used to solve the company problem. Besides the problem described in this paper, the proposed tool was used in the past to solve two other problems. Thus, it is demonstrated that the proposed tool can be easily adapted to several different planning or scheduling problems variants, overcoming the lack of flexibility generally associated to more problem-tailored methods proposed in the literature."
"Nous proposons un article articulé autour de la notion de classe : doit-on considérer la classe comme un ensemble statique composé d'individus possédant la même structure et le même comportement, ou opter pour une approche plus dynamique dans laquelle les contours de la classe sont définis en situation via une évaluation de la proximité de l'ensemble des individus par rapport à un prototype représentatif d'une catégorie (ou classe) donnée. Le choix d'une approche dynamique conduit les auteurs à revisiter deux théories « classiques », la théorie sémio-contextuelle et la théorie acteur-réseaux, en proposant les premiers éléments d'une théorie des prototypes réseaux avec pour objectif de faciliter l'appréhension des groupes d'acteurs dans un contexte de généralisation des communications numériques pour in fine une meilleure compréhension des situations."
"La collecte, l'organisation et la recherche de sources iconographiques est certainement un problème majeur au sein des communautés qui s'intéressent à l'étude, la conservation et la valorisation du patrimoine architectural. L'organisation et la structuration de ces ressources sont alors un problème essentiel si l'on s'inscrit dans l'idée de concevoir les archives de demain. Aujourd'hui, la méthode la plus couramment utilisée, pour classer et rechercher des contenus iconographiques au sein d'une base de données, est la recherche par mots-clés. En particulier, dans le domaine de la documentation architecturale, au vu du degré de complexité (en termes de richesse d'information) et d'hétérogénéité (en termes de variété de supports et de techniques d'exécution) des sources iconographiques (photos, dessins, peintures, etc.), les solutions existantes d'annotation sémantique d'images s'avèrent inefficaces. Cette thèse considère le modèle 3D interactif comme un moyen d'accès privilégié à l'information patrimoniale. Dans ce sens, ce travail se concentre sur l'exploitation de la représentation de la morphologie d'un édifice comme source principale pour distribuer / propager les attributs sémantiques sur l'ensemble des sources iconographiques (le représentant en deux dimensions) qui se trouvent dans une condition de cohérence géométrique/spatiale avec son modèle tridimensionnel. Un modèle de description sémantique (structurant l'ensemble de termes pouvant décrire la morphologie d'un édifice) est alors utilisé comme dénominateur commun permettant d'établir des relations entre la représentation 3D complète des formes architecturales qui composent l'édifice et l'ensemble des sources iconographiques (collections d'images 2D segmentées) correspondantes. Ce travail abouti à la définition et au développement d'un système d'informations permettant de rechercher visuellement, au sein d'un modèle 3D, les sources iconographiques corrélées en fonction de critères spatiaux, morphologiques et sémantiques. Il s'agit d'une application Web utilisable par des spécialistes du domaine de la documentation architecturale comme par le grand public."
"Actuellement la plupart des restitutions en patrimoine historique décrivent les édifices patrimoniaux comme un ensemble d'entités statiques et inaltérables. Toutefois, les sites historiques peuvent avoir une histoire très complexe, parfois riche d'évolutions, parfois seulement partiellement connue grâce aux sources documentaires. Trois aspects importants conditionnent l'analyse et l'interprétation du patrimoine historique. Tout d'abord, les bâtiments peuvent subir des transformations importantes ou ils peuvent disparaître au fil du temps. Deuxièmement, l'incertitude est très fréquente en patrimoine historique sous diverses formes : parfois il est impossible de définir la datation, parfois la forme originelle du bâtiment ou sa position spatiale. Troisièmement, la documentation historique concernant les états passés est hétérogène, douteuse, incomplète, et parfois contradictoire. Cette thèse propose une approche intégrée de modélisation capable d'une part de structurer les entités morphologiques spatiales en fonction du temps, d'autre part de conserver l'historique des évolutions architecturales. De plus, des hypothèses multiples à propos des sites historiques devraient être prises en compte. Comme la géométrie n'est pas suffisante pour comprendre les transformations des sites historiques, une interface de visualisation basée sur des graphes est intégrée pour manipuler les géométries et pour comprendre les transformations des édifices et leurs relations."
no abstract
"Nos sociétés industrialisées et occidentales, à la fois productrices et grandes consommatrices de technologie, sont actuellement prises dans l’effervescence du déploiement des Technologies de l’Information et de la Communication (TIC). La richesse des possibilités offertes par l’outil informatique et ses usages , du point de vue de l’accès à l’information, donc de la connaissance, de la communication ou du traitement de l’information, l’a propulsé au rang des « incontournables savoirs ». Si, dans une approche sociologique, un débat puisse être ouvert sur la nécessaire connaissance des TIC comme vecteur de lien social (Glassey, 2004), il n’en reste pas moins que la méconnaissance de ces technologies est un des facteurs du processus d’exclusion par rapport au développement de la société de l’information, comme le souligne Valendruc et Vendramin (2004). Cet effet d’exclusion peut être accentué dans le cas de personnes en recherche d’emploi, déjà dans une situation de fragilité, voire de rupture du lien professionnel et même social. Il semble alors essentiel d’envisager une démarche visant à réduire cet effet d’exclusion. Dans cette étude, nous discuterons notre hypothèse d’un apprentissage personnalisé des TIC, et plus particulièrement de l’outil informatique, auprès de personnes en recherche d’emploi, dans un objectif de revalorisation sociale et professionnelle. Plus généralement, nous souhaitons vérifier que l’approche personnalisée des TIC par ce public tend à favoriser l’appropriation durable des TIC et respecte ainsi la synergie de deux critères essentiels du développement durable: le progrès social et la viabilité économique."
"L’IA est d’une certaine manière au cœur des sciences du traitement de l’information. Ce troisième volume a pour objet l’examen des interfaces de l’IA avec différents champs de recherche de l’informatique, tant théorique qu’appliquée, comme les bases de données, le Web sémantique, la linguistique computationnelle, la bio-informatique, la reconnaissance des formes, le traitement d’images, la robotique, ou la communication homme-machine. Des chapitres sont également consacrés aux questions philosophiques, en particulier épistémologiques, liées à l’IA, ainsi qu’à la psychologie cognitive en matière de raisonnement et de décision. Une postface propose le regard d’un témoin de l’IA à ses débuts sur les résultats de plus de cinquante années de recherche. Enfin, un épilogue clôture les trois volumes de l’ouvrage, sous la forme d’un plaidoyer pour la recherche en IA."
"Ce deuxième volume présente les principales familles d’algorithmes développés ou utilisés en IA pour apprendre, inférer, décider. Des approches génériques pour la résolution de problèmes y sont présentées: la recherche heuristique ordonnée, particulièrement utile pour aborder certains jeux, ainsi que les métaheuristiques. Les problèmes de satisfaction de contraintes, éventuellement flexibles, complètent l’éventail de ces méthodes. Le traitement des représentations liées à la logique requiert des algorithmes spécialisés, qu’il s’agisse de déduction automatique, de satisfaisabilité d’ensembles de propositions, ou encore de programmation logique. Ils sont aussi présentés dans ce volume. L’algorithmique des modèles graphiques de représentation, en particulier celle des réseaux de type bayésien, ainsi que le développement d’algorithmes pour la planification ou l’apprentissage automatique sont également abordés. La postface dresse un parallèle entre les problématiques algorithmiques en recherche opérationnelle et en intelligence artificielle."
no abstract
"Cet ouvrage, organisé en 3 volumes, est issu de la communauté française des chercheurs en intelligence artificielle (IA). Il a pour objectif de dresser un panorama des recherches effectuées en IA allant de travaux fondamentaux aux applications et aux frontières, en mettant l’accent tout autant sur les résultats obtenus que sur les problématiques actuelles. Il s’adresse à un public d’étudiants de master et de doctorat, mais aussi de chercheurs et d’ingénieurs intéressés par ce domaine. L’ouvrage est organisé en trois volumes : - le premier volume regroupe vingt chapitres traitant des fondements de la représentation des connaissances et de la formalisation des raisonnements (Volume 1. Représentation des connaissances et formalisation des raisonnements) ; - le deuxième volume offre une vue de l’IA, en onze chapitres, sous l’angle des algorithmes (Volume 2. Algorithmes pour l’Intelligence Artificielle) ; - le troisième volume, en onze chapitres également, décrit les principales frontières et applications de l’IA (Volume 3. L’Intelligence Artificielle : frontières et applications)."
"L’étude des classes polynomiales constitue une question importante en intelligence artificielle, en particulier au niveau des problèmes de satisfaction de contraintes. Dans ce contexte, la propriété BTP fournit une classe importante de l’état de l’art. Dans cet article, nous proposons d’étendre et de généraliser cette classe en introduisant la propriété k-BTP (et la classe des instances satisfaisant cette propriété) où le paramètre k est une constante donnée. Ainsi, nous avons 2-BTP = BTP, et pour k > 2, k-BTP est une relaxation de BTP au sens où k-BTP ( (k + 1)-BTP. En outre, nous montrons que si k-TW est la classe d’instances ayant une largeur arborescente bornée par une constante k, alors k-TW ((k+1)-BTP. Au niveau de la complexité, nous montrons que les instances satisfaisant k-BTP et qui vérifient la k-cohérence-forte sont reconnaissables et résolubles en temps polynomial. Nous étudions aussi la relation entre k-BTP et l’approche de W. Naanaa qui a proposé un outil théorique connu sous le vocable directional rank afin d’´étendre les classes polynomiales de manière paramétrée. Enfin, nous proposons une étude expérimentale de 3-BTP qui montre l’intérêt pratique de cette classe."
"L'ingénierie à base de composants s'impose peu à peu dans le développement des systèmes d'information (S.I.). Pourtant, elle n'a pas encore atteint son niveau de maturité. Le terme même de composant est souvent défini de façon imprécise et parfois contradictoire, les modèles de composants proposés peuvent avoir des finalités et des contextes d'utilisation très différents. Dans ce papier, nous nous intéressons aux modèles sémantiques de composants, c'est-à-dire des modèles visant à associer aux composants une connaissance qui guide leur usage. Nous étudions trois types de modèle : les approches "" Web Services Sémantiques "", les approches "" patrons "" et les approches de modélisation de domaine. L'étude comparative de ces approches permet de dégager des différences importantes en terme de finalité, de spécification et d'aide à l'utilisation des composants."
"Proposition d'un système d'informations multimodal dédié au relevé d'art pariétal à travers une application web dotée de fonctionnalités de réalité augmentée video see-through permettant de réaliser des annotations spatialisées in-situ en temps réel, afin de centraliser l'ensemble des données, informations et connaissances relatives à l'objet d'étude."
"Decision support tools are essential to help in the management of industrial systems at different levels: strategic, to design the system; tactical to plan activities or assign resources; operational to schedule activities. In this paper, we present a generic and modular decision support tool to solve different planning, assignment, scheduling or lot-sizing problems. To the best of our knowledge, such generic tool does not exist. The methodology is illustrated by solving a real world lot-sizing and scheduling problem from a plastic injection company."
Les sons pulsés sont des exemples intéressants de sons biologiques complexes. Nous proposons une classification utile de ces sons en deux catégories : sons tonaux ou non-tonaux. Deux modèles mathématiques permettent de mieux cerner les propriétés de ces sons dans ces deux cas. Cette classification s'avère aussi utile pour les mesures de paramètres de ces sons et pour distinguer entre deux moyens de leur production. Nous avons appliqué cette méthode aux chants de baleines bleues du pacifique sud-est et ainsi trouvé que la fréquence de pulsation correspond à la fréquence fondamentale (qui n'est pas exprimée dans le spectre). Ainsi nous renforçons l'hypothèse que le son n'est produit que par un seul organe et ensuite filtré par le corps du géant.
"Dans cette thèse, nous développons une approche supervisée de reconnaissance d’objets basée sur l’utilisation de nouveaux descripteurs d’images globaux inspirés du modèle du cortex visuel humain primaire V1 en tant que groupe de roto-translations semi-discrètes SE (2,N)=R² x ZN produit semi-direct entre R² et ZN. La méthode proposée est basée sur des descripteurs de Fourier généralisés et rotationnels définis sur le groupe SE (2,N), qui sont invariants aux transformations géométriques (translations, et rotations). De plus, nous montrons que ces descripteur de Fourier sont faiblement complets, dans le sens qu’ils permettent de discriminer sur un ensemble ouvert et dense L² (SE(2,N)) de fonctions à support compact, donc distinguer entre des images réelles. Ces descripteurs sont ensuite utilisés pour alimenter un classifieur de type SVM dans le cadre de la reconnaissance d’objets. Nous avons mené une séries d’expérimentations dans le but d’évaluer notre méthode sur les bases de visages RL, CVL et ORL et sur la base d’images d’objets variés COIL-100, et de comparer ses performances à celles des méthodes basées sur des descripteurs globaux et locaux. Les résultats obtenus ont montré que notre approche est en mesure de concurrencer de nombreuses techniques de reconnaissance d’objets existantes et de surpasser de nombreuse autres. Ces résultats ont également montré que notre méthode est robuste aux bruits. Enfin, nous avons employé la technique proposée pour reconnaître des navires dans un contexte de surveillance maritime."
"Les mesures pour les groupes d'homologie ont été introduites récemment pour les objets discrets. Étant donné un volume binaire (un ensemble de voxels), on obtient deux mesures pour chaque générateur de groupes d'homologie associés à ce volume : l'épaisseur (thickness) et l'ampleur (breadth). Elles sont définies à partir de la transformée de distances signée et la persistance homologique et elles permettent de mesurer les trous du volume sous deux points de vue : est-il facile de le casser ou de le remplir ? Cette approche donne une heuristique aussi pour casser ou remplir les trous de façon minimale. Nous présentons dans ce court article un travail en cours sur l'adaptation de ces définitions au contexte des complexes simpliciaux. La difficulté consiste à définir la transformée de distances signée pour un tel complexe."
"Au cours des dernières années, innombrables produits ont été conçus en utilisant des modèles numériques 3D, où les courants logiciels pour la conception et le dessin technique utilisent des modèles CAO (Conception Assistée par Ordinateur). Ces logiciels sont utilisés dans de nombreux domaines, tels que l'automobile, la marine, l'aérospatiale et plus encore. À l'intérieur de d'une entreprise qui utilise ces systèmes, il est possible d'avoir accès à des modèles CAO de produits déjà développés puisque la conception de nouveaux produits fait souvent référence à des modèles existants depuis que produits similaires permettent à l'avance la connaissance des éventuels problèmes et leur solutions. Par conséquent, il est utile de disposer de solutions technologiques capables d'évaluer les similitudes de différents produits afin que l'utilisateur puisse récupérer des modèles existants et avoir ainsi accès à des informations utiles pour la nouvelle conception.Le concept de similarité a été largement étudié dans la littérature et il est bien connu que deus objets puissent être similaire de plusieurs façons. Ces multiples possibilités rendent complexe l'évaluation de la similarité entre deux objets. À ce jour, de nombreuses méthodes ont été proposées pour l’identification de différentes similitudes entre les pièces, mais peu de travaux abordent cet problème en évoquant d’assemblages de pièces. Si l’évaluation de la similarité entre deux pièces a beaucoup de points de vue, quand on va examiner des assemblages de pièces, les combinaisons de similarité augmentent vertigineusement puisqu'il y a plus de facteurs à considérer.Sur la base de ces exigences, nous proposons de définir un système qui permettant la récupération des assemblages des pièces similaires en fonction de multiple critères de similarité. Pour ce faire, il faut avoir un descripteur qui peut gérer les informations nécessaires pour caractériser les différentes similitudes entre les deux modèles. Par conséquent, l'un des points principaux de ce travail sera la définition d'un descripteur capable de coder les données nécessaires à l'évaluation des similarités. De plus, certaines des informations du descripteur peuvent être disponibles dans le modèle CAO, tandis que d'autres devront être extraites de manière appropriée. Par conséquent, des algorithmes seront proposés pour extraire les informations nécessaires pour remplir les champs du descripteur. Enfin, pour une évaluation de la similarité, plusieurs mesures entre les modèles seront définies, de sorte que chacune d'entre elles évaluent un aspect particulier de leur similarité."
"Avec l’essor de l’électronique embarquée, les câbles électriques constituentune part importante des pièces automobiles tandis que l’espace à bord n’a cessé de diminuer. Leur flexibilité requiert la prédiction de leur déformation durant leur montage afin d’éviter le contact avec d’autres pièces du véhicule et leur endommagement. Les outils actuels ne permettent pas une prédiction assez réaliste et précise de leur comportement, nécessaire dans un volume de travail très restreint. Les étapes de montage sont donc validées via la réalisation de maquettes réelles coûteuses. Cette thèsea pour but d’améliorer la simulation numérique de ces pièces souples. Nous proposonsici un code de simulation 3D basé sur un modèle de poutre géométriquement exact résolu par la méthode des éléments finis. Son originalité tient dans le couplage des quaternions pour modéliser les rotations 3D et de la méthode asymptotique numérique pour la continuation du système non linéaire qui lui confère une grande robustesse. Un banc d’essai permettant l’identification des paramètres homogénéisés nécessaires au modèle numérique et sa validation par comparaison de la géométrie finale et du chemin d’équilibre est présenté. Combinés à des développements analytiques sur les modèles de poutres avec cisaillement, les essais mènent à une évaluation critique du modèle deTimoshenko 3D pour la représentation des torons de câbles."
"La décroissance en fréquence des deux chants de baleine bleue de l'océan pacifique sud est est examiné sur plusieurs décennies en utilisant comme source des données acoustiques de l'Equateur à la Patagonie chilienne. La fréquence de pulsation et la fréquence pic des signaux sont mesurés en utilisant deux méthodes distinctes (auto-corrélation sommée et transformée de Fourier rapide). Les sources d'erreur associées à chaque mesure sont estimées. Il y a un déclin linéaire de ces deux fréquences pour le chant le plus commun de cette zone (chant du Pacifique Sud Est n°2, SEP2). Un analyse plus rapide montre aussi une baisse linéaire, entre 1970 et 2014, de la fréquence du chant SEP1, plus rarement enregistré dans cette zone. Ces deux baisses ont des amplitudes similaires. L'intérêt de mesurer la fréquence de pulsation et la fréquence pic de façon concomitante est estimé. Enfin, une comparaison globale des déclins en fréquence de tous les types de chants de baleines bleues est fournie."
no abstract
"It has been shown that indecisiveness is involved in many unwanted cognitive states, such as procrastination, distractibility, the lack of self-esteem, or even revenge. The purpose of this work is to propose a predictive model for the recognition of the indecisiveness class, from the analysis of the customer’s trajectory and his gripping. The movements are captured thanks to infra-red sensors. A structural behavioral architecture is built, based on eye-tracking methodology. Indeed, all the movements of a customer in a selling area can be assimilated to fixations and saccades. We show that the path in the selling area can be seen as a sequence of states. The final predictive classifier is built with a combination of Hidden Markov Models (HMM) through a logistic regression model (LRM) and leads to satisfying results, as it correctly predicts 88 % of the subjects’ indecisiveness classes."
"Les barrages sont caractérisés par des comportements complexes qui évoluent au cours du temps du fait d’un vieillissement naturel. Il est par conséquent pertinent de développer des approches de modélisation prenant en compte les aspects temporels. Le but de cette modélisation est de pouvoir ensuite conduire les tâches d’évaluation d’un barrage à un instant donné, de diagnostic des causes de la détérioration de sa sécurité dans le passé et d’analyser la fiabilité et la sécurité du barrage à différentes échelles de temps. Dans cet article, nous proposons une méthode de modélisation basée sur une approche multi-modèles appelée TOM4D (Timed Observations Modeling for Diagnosis) pour le diagnostic du comportement dynamique des barrages. Quatre modèles sont décrits : un modèle de perception, un modèle structurel, un modèle comportemental et un modèle fonctionnel. Nous utilisons ces modèles pour analyser le comportement du barrage à différents instants et en détecter l’éventuelle détérioration. Les modèles résultants de l’étape de modélisation, permet de caractériser et de calculer les diagnostics du barrage. Pour conduire les tâches d’évaluation, diagnostic, deux niveaux sont considérés : les composants individuels (diagnostic local) et le barrage dans sa globalité (diagnostic global). Une application sur un cas réel est effectuée."
"Cet article propose une approche automatisée d’inférence de connaissances basée sur l’analyse de relations extraites à partir de textes. Son originalité repose sur la définition d’un cadre tenant compte (i) d’une structuration des objets étudiés (e.g. syntagmes nominaux) sous la forme d’un ordre partiel et (ii) de l’exploitation possible d’une connaissance a priori formalisée dans un modèle de connaissances de type ontologie (taxonomie). Ce cadre permet notamment de définir des règles de propagation de l’information basées sur la théorie des croyances afin d’inférer de nouvelles connaissances à partir des relations extraites. Bien qu’à portée plus large, notre approche est ici illustrée et évaluée au travers de la définition d’un système automatique exploitant des textes issus du Web afin de répondre à des questionnaires générés. Nous montrons notamment l’intérêt de structurer les extractions et le gain apporté par la prise en compte d’une connaissance a priori au sein d’une telle chaîne de traitement."
"Dans cet article, nous traitons le problème d'approximation de nuages de points par une courbe spline ou surface au sens de la norme L1. L'utilisation de cette norme permet de préserver la forme des données même en cas de changement brutal de celle-ci. Dans nos précédents travaux, nous avons introduit une méthode par fenêtre glissante de cinq points pour l'approximation courbe spline L1 et une méthode de croix glissante de neuf points pour l'approximation surface spline L1 de données type grille. Malgré leur complexité linéaire, ces méthodes peuvent demeurer lentes lorsqu'elles sont appliquées sur un large flot de données. Par conséquent, sur la base de nouveaux résultats algébriques sur l'approximation L1 sur un nombre restreint de données, nous proposons ici des méthodes reposant sur des fenêtres de taille inférieure et nous comparons les différentes méthodes. In this article, we adress the problem of approximating scattered data points by C1-smooth polynomial spline curves and surfaces using L1-norm optimization. The use of this norm helps us to preserve the shape of the data even near to abrupt changes. In our previous work, we introduced a five-point sliding window process for L1 spline curve approximation and a nine-point cross sliding window process for L1 spline surface approximation of grid datasets. Nethertheless, these methods can be still time consuming despite their linear complexity. Consequently, based on new algebraic results obtained for L1 approximation on restricted sets of points in both planar and spatial cases, we define in this article methods with smaller windows and we lead a comparison between the methods."
"Cet article étudie la révision par ""r-ensembles"" de bases de connaissances exprimées en DL-Lite en présence d'une nouvelle information plus fiable, appelée ""entrée"". Cette stratégie de révision repose sur la minimisation de l'incohérence. Nous considérons différentes formes d'entrée : une assertion d'appartenance ou un axiome d'inclusion positif ou négatif. Nous donnons les propriétés logiques des opérateurs de révision proposés en termes de postulats. Nous montrons enfin comment utiliser la notion d'échantillon (""hitting sets"") pour calculer les r-ensembles. En particulier, nous montrons que pour certaines stratégies de révision et certains types d'entrées la révision par rensembles peut être réalisée en temps polynomial."
no abstract
no abstract
no abstract
"La configuration sous contraintes présente une nouvelle difficulté à prendre en compte par les méthodes d'élimination de symétries connues par la communauté CSP car elle y introduit un aspect dynamique. Nous présentons ici une amélioration significative d'un algorithme de génération de configurations canoniques. Cette nouvelle version exploite l'incrémentalité que l'on peut faire ressortir de la génération de solutions canoniques et de l'ordre total sur les arbres sur laquelle elle repose. La complexité du test de canonicité passe ainsi de O(Nlog(N)) à O(N). De plus, une technique de filtrage nous permet d'éliminer à l'avance des configurations non canoniques. Des résultats expérimentaux montrent l'intérêt de cette approche sur des problèmes classiques."
"Image registration is a major issue in the field of Remote Sensing because it provides a support for integrating information from two or more images into a model that represents our knowledge on a given application. It may be used for comparing the content of two segmented images captured by the same sensor at different times; but it also may be used for extracting and assembling information from images captured by various sensors corresponding to different modalities (optical, radar,). The registration of images from different modalities is a very difficult problem because data representations are different (e.g. vectors for multispectral images and scalar values for radar ones) but also, and especially, because an important part of the information is different from an image to another (e.g. hyperspectral signature and radar response). And precisely, any registration process is based, explicitly or not, on matching the common information in the two images. The problem we are interested in is to develop a generic approach that enables the registration of two images from different modalities when their spatial representations are related by a rigid transformation. This situation often occurs, and it requires a very robust and accurate registration process to provide the spatial correspondence. First, we show that this registration problem between images from different modalities can be reduced to a matching problem between binary images. There are many approaches to tackle this problem, and we give an overview of these approaches. But we have to take into account the specificity of the context in which we have to solve this problem: we must select those points of both images that are associated with the same information, and not the other ones, in order to process the pairing that will lead to the registration parameters. The approach we propose is a Hough-like method that induces a separation between relevant and non-relevant pairings, the Hough space being a representation of the rigid transformation parameters. In order to characterize the relevant items in each image, we propose a new primitive that provides a local representation of patterns in binary images. We give a complete description of this approach and results concerning various types of images to register."
no abstract
no abstract
no abstract
no abstract
no abstract
"Dans une optique de maximisation de la production et de réduction des coûts d’installation, de maintenance et d’entretien des trackers solaires, qui permettent d’orienter les modules photovoltaïques à haute concentration (HCPV), ces travaux de thèse se focalisent sur l’amélioration de la précision et la réduction du coût de la stratégie de génération de la trajectoire du tracker. Dans un premier temps, un simulateur de tracker HCPV est développé offrant une étude de l’influence de la performance du suivi du soleil sur la production des modules HCPV, permettant ainsi une étude et une comparaison des stratégies de génération de trajectoires. Le simulateur est basé sur un modèle comportemental de module HCPV monté sur tracker permettant de prédire la puissance maximale du module HCPV en fonction de l’erreur de position du tracker face au soleil, de l’ensoleillement direct et de la température. Une première stratégie de commande dite de référence a été implémentée sur ce simulateur. C’est une commande hybride qui repose sur un viseur solaire pour corriger l’erreur de poursuite par un calcul astronomique. Ensuite, afin d’améliorer les performances et de réduire les coûts de cette stratégie, une nouvelle approche sans capteur est développée en se basant sur une méthode d’optimisation du gradient de puissance pour la génération de la trajectoire du tracker. Une étude complémentaire est également exposée afin de mettre en évidence des algorithmes de recherche de la puissance maximale (MPPT) pouvant offrir des temps de réponse suffisamment rapides pour ne pas affecter la qualité de l’évaluation du gradient de puissance. Dans ce contexte, une commande MPPT P&O améliorée par un réseau de neurones à complexité réduite est proposée, assurant un compromis entre précision, simplicité et rapidité"
no abstract
no abstract
no abstract
"De récentes études expérimentales sur des situations d'atterrissage d'hélicoptères à grande vitesse dit dur (vitesse supérieure à 2 m/s), ont révélé que de par l'effort structural transmis par les trains d'atterrissage couplés mécaniquement au fuselage, la poutre de queue d'un appareil dont le premier mode de flexion se situe dans les basses fréquences pouvait être excitée. Les oscillations de celle-ci génèrent des contraintes mécaniques au niveau de la liaison entre la cabine et la poutre de queue qui portent atteinte à la pérennité de la structure. Afin de lutter contre ce phénomène problématique, une solution passive consiste à rigidifier la liaison entre la cabine et la poutre de queue. Coûteuse en masse et interférente avec le bon fonctionnement des dispositifs anti-vibratoires dimensionnés en fonction des fréquences propres initiales de l'appareil, celle-ci peut être évitée par une optimisation de l'effort transmis par les trains d'atterrissage en agissant sur le comportement dynamique de ceux-ci. Fort de ce constat, les travaux de recherche présentés dans ce mémoire, concernent l'étude et le développement de méthodes d'optimisation passive et active des trains d'atterrissage en vue de minimiser les efforts supportés par la poutre de queue et induits par l'impact de l'appareil sur le sol. Basé sur une constante synergie entre les aspects théoriques et les aspects expérimentaux appuyés par le développement d'un démonstrateur, cette étude formalise tout d'abord la problématique lié aux atterrissages des aéronefs et se propose d'analyser la physique du phénomène des atterrissages via des outils de modélisation utilisant des approches analytique et multi-corps. Ensuite après une analyse et une identification des paramètres d'optimisation de la dynamique des trains d'atterrissage, des méthodes d'optimisation passive et semi-active sont développées et validées expérimentalement sur un démonstrateur mécaniquement équivalent à l'hélicoptère considéré pour cette étude."
"La symétrie a été bien étudiée dans les logiques classiques et dans la programmation par contraintes depuis une décennie. Toutefois, en Intelligence Artificielle, nous avons l'habitude de manipuler des informations incomplètes et nécessité d'inclure l'incertitude dans la raisonnement sur la connaissance avec exceptions et la non-monotonie. Plusieurs logiques non classiques sont mises en place à cet effet, mais, selon nos connaissances, la symétrie dans ces logiques n'ont pas encore été étudiés. Ici, nous sommes intéressés à étendre la notion de la symétrie à des logiques non classiques telles que les logiques préférentielles, X-logiques et les logiques des défautss, puis donner des nouvelles règles d'inférence par symétrie pour les X-logiques et les logiques des défauts. Enfin, nous montrons comment le raisonnement par symétrie est rentable pour ces logiques et comment elles gèrent certaines symétries qui n'existent pas dans des logiques classiques."
no abstract
"Ce chapitre présente les étapes clés du processus de reconstruction en se focalisant sur certains ""points difficiles"". Une premiére partie traite de l'analyse d'images couleur en vue de sélectionner automatiquement la surface de jeu. Dans une seconde partie, l'extraction du marquage dans l'aire de jeu sera abordée en s'appuyant sur l'utilisation de la transformée de Hough. Une troisième et dernière partie traite de la mise en correspondance des primitives extraites des images avec celles du modèle de la scène."
"Nous nous intéressons à la création de surfaces à pôles (NURBS essentiellement), largement utilisées dans les systèmes de modélisation géométrique. Un des avantages de cette modélisation est de permettre d'appréhender la forme des surfaces par la position de points de contrôle. L'approche déclarative de la modélisation de surfaces est destinée à la réalisation rapide et facile d'ébauches de formes et de surfaces. Elle est aussi et surtout destinée à accélérer les processus de conception des spécialistes en leur proposant des solutions adaptées répondant à un ensemble de contraintes et de propriétés. Pour y parvenir, le travail réalisé dans la thèse se divise en quatre étapes : * Etude de faisabilité : réalisée en collaboration avec l'Ecole Nationale Supérieure des Arts et Métiers d'Aix-en-Provence, elle s'est focalisée sur la description et la modélisation d'objets de type pièces mécaniques. * Analyse conceptuelle : Cette étape primordiale dans un projet d'une telle envergure nous a permis de mettre au point l'architecture générale de notre processus déclaratif de surfaces. Dans le cadre de la thèse et en vue d'un premier prototype, nous décidons de focaliser notre étude sur la partie résolution qui se scinde en deux : la détermination des classes de solutions et leur construction sous contraintes. * Etude et développement de la ""détermination des classes de solutions"" : Notre approche repose sur le comportement d'une surface face aux déformations qui lui sont apportées lors de sa construction : deux surfaces appartiennent à la même classe si elles ont le même comportement face à la même succession de déformations. * Etude et développement de la ""construction sous contraintes"" : Afin d'obtenir une surface solution particulière (ou instance d'une classe de solutions) nous choisissons une construction par l'application successive de déformations. Ces déformations sont soumises à des contraintes plus ou moins fortes. Nous avons donc élaboré une méthode de déformations de surfaces capable de satisfaire des contraintes de passage tout en gardant un fort contrôle sur la forme de chaque zone d'influence. Deux applications mettant en oeuvre ce travail ont été réalisées en C++ et sont disponibles sous les versions 32-bits de MS Windows(R), Linux et MacOS X."
no abstract
"Dans ce papier, nous introduisons une nouvelle technique de filtrage pour les réseaux de contraintes. Elle est basée sur une propriété appelée cohérence structurelle. Il s'agit d'une cohérence paramétrable que nous noterons w-SC. Cette cohérence est basée sur une approche significativement différente de celles en usage. Alors que les cohérences classiques s'appuient généralement sur des propriétés locales étendues à l'ensemble du réseau, cette cohérence partielle considère à l'opposé la cohérence globale sur des sous-problèmes. Ces sous-problèmes sont définis par des graphes de contraintes partiels dont la largeur arborescente est bornée par une constante w, qui correspond au paramètre associé à la cohérence. Nous introduisons un algorithme de filtrage qui réalise un filtrage permettant d'obtenir la w-SC cohérence. Cette cohérence est ensuite analysée pour la positionner par rapport aux cohérences classiquement utilisées dans les CSP. Cette étude montre que cette nouvelle cohérence est généralement incomparable avec celles figurant dans la littérature. Enfin, nous présentons des résultats expérimentaux préliminaires pour évaluer l'utilité de cette approche."
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"Le savoir de métier est ici au centre de nos réflexions. Connaissance expérientielle collective formée et formulée par le biais de concepts propres aux métiers, sa transmission est aujourd'hui en mutation dans les entreprises, posant des problématiques complexes comme dans le cas de la Division Ingé-nierie Nucléaire d'EDF. La réponse est dans la mise en forme de ce savoir dé-sormais médiaté : le langage. Nous en arrivons donc naturellement à la termi-nologie qui, en tant que représentation linguistique de la science d'un champ social, en fixe le langage. Les ontologies qui modélisent et représentent for-mellement le système notionnel des terminologies, en sont la suite logique. Nous illustrons notre réflexion en présentant une application concrète sur un domaine de la Division Ingénierie Nucléaire d'EDF, par la construction d'une base de connaissances constituée d'une terminologie et d'une ontologie centrée sur le sens métier."
no abstract
no abstract
"Dans un contexte de concurrence accrue, la prise de décision est un acte managérial fondamental. Elle engage les décideurs dans des processus cognitifs tels que : la rigueur et la rapidité d'analyse, la capacité de synthèse, l'argumentation, la négociation et enfin, la conscience de ses responsabilités. Aussi, afin de combattre l'incertitude inhérente à leur prise de décision, les décideurs recherchent à obtenir une vision la plus précise possible des événements futurs. Afin de soutenir la stratégie d'innovation et aider les décideurs à disposer de toute l'information nécessaire à une prise de décision, le site de STMicroelectronics Tours s'est alors doté d'une cellule de veille, le Market Intelligence. Après avoir éprouvé et amélioré plusieurs processus de veille, la dernière méthode mise en place par l'équipe de veille, afin de soutenir l'effort d'innovation, s'appuie sur une participation active de groupes d'experts : les groupes experts. L'objectif de ces groupes experts est d'aider la direction dans l'identification de nouveaux axes stratégiques d'innovation et d'élaborer des recommandations en réponse à une problématique donnée dans une démarche d'investigation. Reposant sur trois entités, le groupe expert, le groupe de pilotage et la cellule de veille qui intervient à la fois comme animateur de réunion et expert en sciences de l'information, la démarche de groupe expert s'appuiera notamment sur plusieurs méthodologies : - Le cycle de l'information en entreprise, c'est-à-dire de l'analyse des besoins à la diffusion des connaissances acquises, en passant par la collecte et l'analyse d'informations. - L'étape de maturation des idées d'un processus d'innovation ""en entonnoir"", c'est-à-dire s'informer, analyser son potentiel et renforcer son potentiel d'innovation - Le processus de prise de décision c'est-à-dire de l'analyse de l'objectif ou identification du problème à la structuration du problème, puis à l'élaboration des options possibles, et enfin l'évaluation et le choix parmi ces options. Cette mixité des processus permet ainsi de créer une démarche complète d'aide à la décision. Comparée aux difficultés rencontrées par de nombreux hauts responsables d'entreprises et décideurs, la démarche de groupe expert apporte dés lors des solutions concrètes et efficaces au processus de prise de décision."
no abstract
no abstract
no abstract
"Lors des JFPC'2005, les auteurs de la présente contribution, ont montré l'intérêt que recèle l'exploitation des heuristiques pour s'assurer de l'efficacité pratique des méthodes de résolution de CSP par décomposition arborescente. Nous étendons ce travail en le géné- ralisant. Nous montrons qu'au-delà d'heuristiques sur le parcours de l'arborescence associé à la décomposition, une gestion dynamique de l'heuristique de choix des variables s'avère cruciale. Tout en conservant les bornes de complexité (O(exp(w+1)) où w est la tree-width du CSP), nous proposons une voie pour s'aranchir en partie du carcan imposé par la structure arborescente. Cette démarche nous conduit à dénir de nouvelles bornes de complexité qui orent un compromis entre bornes théoriques et libération de l'ordre. En particulier, nous introduisons un nouveau paramètre k indiquant le degré de liberté laissé à l'heuristique de choix de variables qui permet de borner la complexité par O(exp(2(w + k))). Les résultats expérimentaux présentés montrent l'intérêt d'une telle démarche."
"L'acquisition et la représentation des connaissances est un aspect central dans le domaine de l'Intelligence Artificielle car une machine intelligente doit avant tout s'appuyer sur des informations représentant le monde de façon suffisamment précise. Cette difficulté à disposer d'une représentation correcte du monde est particulièrement importante lorsque l'on a à faire à un monde changeant ou à des informations provenant de sources multiples. Nous proposons dans cette thèse une méthode de fusion syntaxique de croyances dans le cas où les croyances sont représentées dans le cas où il n'existe pas de priorités explicites ni entre les agents, ni entre les croyances exprimées par les agents. Cette méthode, appelée fusion par R-ensembles, repose sur la recherche des sous-ensembles de formules à retirer afin de restaurer la cohérence. Nous avons réalisé une mise en œuvre de cette méthode basée sur la traduction du problème de fusion en un programme logique avec sémantique des modèles stables. Nous avons d'abord proposé deux implantations : une adaptation de l'algorithme smodels ainsi qu'une autre implantation basée sur les instructions fournies par Lparse/Gringo. Nous avons testé cette dernière implantation avec des expérimentations portant sur des profils de croyances générés aléatoirement ainsi que sur les données issues d'un projet européen portant sur le relevé en archéologie sous-marine. Dans un deuxième temps, cette thèse propose une extension de la fusion par R-ensembles dans deux directions. Nous nous sommes ainsi intéressés au cas où des préférences sont exprimées entre les agents ou entre les croyances exprimées par chaque agent. Nous nous sommes également penchés sur le cas où les croyances sont exprimées sous forme de programmes logiques."
no abstract
no abstract
"Nous étudions les masques de chanfrein en 3 dimensions, dont les fondements théoriques s'appuient sur les triangulations de points visibles dans les ensembles de Farey. On définit les déplacements élémentaires et les cônes d'influence, et on se ramène à une boule rationnelle équivalente. On montre dans quelles conditions un masque de chanfrein induit une distance par une contrainte de convexité sur la boule ; la boule de chanfrein est alors un polyèdre discret, dont la triangulation est connue."
no abstract
no abstract
"Cet article présente le développement d'une méthode de diagnostic basée sur une approche multi-modèles pour le diagnostic du comportement des barrages. Trois modèles sont décrits : un modèle structurel, un modèle comportemental et un modèle fonctionnel. Le contexte particulier des barrages et notamment le fait que les modèles numériques sont peu nombreux nous a amenés à proposer un modèle structurel et un modèle comportemental originaux. La définition d'un modèle structurel unique pour l'ensemble des composants reposant sur un faible nombre de variables permet de concilier efficacité calculatoire et efficacité du diagnostic. Le diagnostic restitue plusieurs éléments : les composants, process et phénomènes impliqués mais également une identification des causes et une analyse des mécanismes au cours du temps. / This article presents the development of a diagnostic method for dam behaviour. This method is based on multimodelling reasoning. Three models are described: a structural model, a behavioural model and a functional model. The scarcity of numerical models leads us to propose original structural and behavioural models. A single structural model relying on few variables is defined for all the components. It allows a computing efficiency along with a reliable diagnosis. The diagnosis produces several elements: the components, processes and phenomena involved in the faulty behaviour, an identification of the causes of the faulty behaviour and an analysis of mechanisms through time."
"La réalisation technologique de la structure de chaine mécanique ouverte d'un porteur de robot industriel entraine l'apparition de défauts dans la géométrie de la trajectoire réellement effectuée. Ces phénomènes dégradent le niveau des performances métrologiques de la tâche robotisée. Aussi, après avoir effectué un inventaire des différentes causes à l'origine de ces phénomènes, il est proposé une modélisation de leur influence au niveau global. Il est présenté* ensuite une application logicielle permettant de reconstituer par simulation la trajectoire réelle, à partir des caractéristiques de la tâche et du robot. Cette application sera validée par une expérimentation sur une cellule robotisée industrielle. Enfin, en conclusion de ce mémoire, l'examen des différents axes d'application de cette approche technologique débouche sur de nouvelles perspectives de recherche, tant sur le plan de la CFAO Robotique que sur celui de la conception ou de la gestion d'exploitation des robots industriels."
"Deux des enjeux actuels de la simulation sont la réutilisation de modèles et la composition de simulations globales distribuées à partir de simulateurs répartis. Cette publication présente un environnement contribuant à modéliser et simuler de façon distribuée un système. Cet environnement utilise les concepts de spécification DEVS développés par B.P. Zeigler et la généralisation GDEVS définie par N. Giambiasi permettant ainsi d'utiliser les performances de la simulation à événements discrets. Il permet également la composition de modèles à partir d'éléments stockés en bibliothèques, évitant le redéveloppement de simulations existantes. De plus, la structure de simulation choisie est « mise à plat », réduisant ainsi l'échange de messages entre les composants par rapport à la structure des simulateurs DEVS existants. Enfin, sa compatibilité avec la norme HLA, grâce à l'utilisation de mécanismes de synchronisation d'échange des messages, lui permet de s'intégrer dans des simulations globales hétérogènes compatibles HLA."
no abstract
"Cet article montre que le problème de représentation de services web est crucial et analyse les différents facteurs qui l’influencent. Il discute une représentation classique et en propose deux nouvelles. La première représentation que nous proposons provient du domaine du traitement du langage naturel et est basée sur des règles pour annoter les descriptions de services et ainsi extraire les informations utiles pour l’indexation sémantique de services. La seconde méthode proposée, appelée réputation symbolique, est calculée à partir des relations entre les services considérés et est utilisée pour la recommandation de services web. L’impact de ces représentations pour la découverte et la recommandation est étudié et discuté à la lumière de nos expérimentations utilisant des services web réels."
"Dans le domaine de la planification de nombreux systèmes utilisent une représentation de type CSP (problèmes de satisfaction de contraintes). Le planificateur FDP que nous présentons dans cet article, fait partie de cette famille. Mais plutôt que de faire appel à un résolveur de CSP indépendant, FDP travaille directement sur une structure similaire au graphe de planification de GRAPHPLAN, appelée fdp-structure, avec ses propres mécanismes de filtrage et de décomposition, adaptés au domaine spécifique de la planification. Ainsi pour déterminer s'il existe un plan solution, FDP développe itérativement une fdp-structure jusqu'à trouver une solution ou atteindre une taille limite donnée. A chaque extension de la structure un plan est recherché. Différentes stratégies de recherches ont été envisagées. Actuellement, FDP utilise une recherche avant combinée à une décomposition par partitionnement des ensembles d'actions. FDP produit des plans séquentiels optimaux. Il intègre diverses techniques pour réduire l'espace de recherche, notamment pour limiter l'énumération de séquences d'actions redondantes, ou pour détecter des situations déja rencontrées. Comparé à d'autres planificateurs séquentiels optimaux sur des jeux de problèmes connus, FDP apparaît compétitif et régulier."
no abstract
no abstract
no abstract
"Les robots poly-articulés industriels sont un moyen de production moins couteux que les machines outils. De part leur structure, ils sont moins rigides, mais ils disposent d'une agilité et d'une zone de travail plus importante. L'exploitation de ces avantages pour la réalisation de certaines opérations continues, comme l'usinage par exemple, fait l'objet d'une demande croissante de l'industrie manufacturière. Ces nouvelles applications des robots poly-articulés pour l'usinage nécessitent de progresser sur le front de l'amélioration de la précision statique et dynamique de ces structures. Ainsi, afin d'améliorer la précision des robots, nous avons développé dans ce travail de thèse une méthode de planification de trajectoire basée sur l'interpolation paramétrique des courbes géométriques. Cette méthode permet de maîtriser le positionnement et la cinématique de l'outil pour les applications nécessitant un suivi de profil continu et notamment pour l'usinage. Nous proposons ainsi de qualifier les différentes souplesses des robots industriels 6 axes afin de déduire une cartographie de rigidité dans l'espace de travail cartésien. Une méthode exploitant cette cartographie permettant l'optimisation de la configuration géométrique du robot pour l'usinage est présentée. Les souplesses axiales des articulations sont intégrées dans un modèle élasto-statique utilisé pour la commande. Ce modèle permet d'anticiper les déviations statiques induites par ces souplesses articulaires. Enfin, nous mettons en évidence les défauts de transmission associés aux chaînes cinématiques des axes du robot. Nous montrons que ces défauts sont à l'origine d'une erreur de position au niveau de l'organe terminal de l'ordre de quelques dixièmes de millimètre. Un protocole d'identification de ces défauts est proposé. Ces défauts sont modélisés et intégrés dans une stratégie de correction hors ligne de position."
no abstract
"Cette contribution s'intéresse à la notion de recouvrement de problèmes (au sens des CSPs) par des hypergraphes acycliques (ou hyper-arbres). Elle introduit une méthode de résolution fondée sur l'exploitation d'ensemble d'hypergraphes acycliques recouvrants. Ces recouvrements peuvent être assimilés à une forme d'extension de la notion classique de décomposition arborescente de réseau de contraintes. Nous étudions ici les propriétés et les relations de ces recouvrements, puis nous évaluons leur intérêt théorique pour le cas de problèmes structurés. Nous montrons que cette approche rend possible une gestion dynamique de la structure des CSPs pendant la résolution, et facilite ainsi une exploitation aisée des heuristiques dynamiques d'ordonnancement des variables. De plus, nous proposons un résultat de complexité qui améliore significativement ceux fournis précédemment dans la littérature. Enfin, nous présentons des résultats expérimentaux qui donnent une idée de l'intérêt de cette nouvelle approche sur le plan pratique."
"Ce travail de recherche s'intéresse à la commande sans capteur mécanique du moteur synchrone à aimants permanents (MSAP) à pôles saillants, particulièrement en basse vitesse, avec détection de la position initiale du rotor. Après une présentation des techniques et approches qui ont initié nos travaux, en terme d'estimation de la vitesse et/ou de la position, nous avons choisi celles qui présentent plus d'intérêt de point de vue stabilité, robustesse, précision et simplicité d'implémentation. La première approche est basée sur le Système Adaptatif avec Modèle de Référence (MRAS). Quant à la deuxième, elle est réalisée autour d'un observateur non-linéaire pour l'estimation de la position et de la vitesse du MSAP à pôles saillants. Les deux techniques d'observation de la vitesse sont associées à une commande par orientation du flux rotorique avec la technique MLI vectorielle. Pour détecter la position initiale du rotor, nous avons utilisé une nouvelle approche qui permet d'estimer cette position avec une incertitude de 5° mécanique. Cette nouvelle approche est basée sur l'application de signaux tests aux bornes des phases statoriques du MSAP. Des résultats de simulation et expérimentaux sont présentés tout au long de ces travaux pour valider les études théoriques de la commande vectorielle sans capteur mécanique du MSAP. Enfin, nous avons étudié et analysé les performances de la commande tolérante aux défauts sans capteur mécanique du MSAP en présence de défaillances de types transistors à l'état-off. Les résultats expérimentaux obtenus avec les deux approches d'estimation de la vitesse en utilisant l'observateur MRAS et un observateur non linéaire ont permis d'améliorer la fiabilité du système de manière à rendre possible la commande vectorielle sans capteur mécanique en mode dégradé (alimentation avec deux bras de l'onduleur). En effet, les résultats de la commande sans capteur mécanique de la MSAP en mode dégradé montrent que l'observateur non linéaire est le mieux adapté pour ce type de fonctionnement car il présente de faible ondulation du couple et de vitesse. A l'aide d'un banc d'essais que nous avons développé au laboratoire LSIS-pôle Ecole Centrale de Marseille (ECM), nous avons pu valider expérimentalement les différentes approches proposées dans ce travail de recherche. Les résultats obtenus montrent l'efficacité des techniques mises en œuvre pour la commande vectorielle sans capteur mécanique du MSAP à pôle saillant en termes de robustesse, stabilité, précision et rapidité."
"La composition automatique ou assistée de workflows est un domaine d'intense recherche avec des applications au ""world wide web"" et à la modélisation de processus métiers (BPM), traditionnellement envisagée en utilisant des techniques de programmation logique et de démonstration automatique. L'originialité de cette recherche provient de l'observation que la construction d'un workflow composite est un problème de recherche de modèles finis, et que la plupart des languages de workflows peuvent être définis par des métamodèles objets contraints, comme UML ou YAWL. Cela conduit à envisager l'application de techniques de configuration à base de contraintes à ce problème, ce dont nous montrons la faisabilité. Nous présentons un modèle objet contraint pour la composition de workflows, s'appuyant sur un métamodèle contraint ainsi que des ontologies de processus et de flots de données. Des résultats expérimentaux sont donnés pour une implantation qui génère des workflows composites complexes mettant en jeu transformations, synchronisations et tests."
no abstract
no abstract
no abstract
"Après un rappel du type de systèmes considérés et d'un processus de conception système en phase avec l'état de l'art, nous décrivons ce que nous entendons par modèles et langage de modélisation. Nous rappelons ensuite les principaux éléments d'une théorie des exigences fondée sur le concept de propriété (PBR) compatible d'une approche d'ingénierie système basée sur des modèles (MBSE). Nous montrons ensuite comment définir, dans un langage de modélisation, des exigences basées sur le concept de propriété (PBRs) au sein même d'un modèle de conception de système, d'appliquer à ce modèle en développement un processus de conception système conforme à l'état de l'art. Nous pensons ainsi proposer des processus de définition des exigences et de solutions des systèmes qui renouvellent les pratiques actuelles du MBSE et les rendent plus adaptées au développement des systèmes d'aujourd'hui et de demain."
no abstract
"Nous étudions la communication sémiologique non-verbale et son jeu dans les interfaces des systèmes informatiques. Confronter les représentations sociales des interfaces graphiques avec celles des représentations mentales permet de mieux comprendre les symétries et asymétries qui se jouent dans la relation homme-machine. L'expérience que nous exposons ci-après ouvre le débat d'une gestion de l'information sous une forme sémiologique de sens commun, tridimensionnelle et dynamique."
no abstract
no abstract
"Le but de l'analyse du langage naturel est de construire des représentations traitables par une machine d'un texte donné en entrée. Nous présentons ici une nouvelle approche de ce problème qui combine l'analyse syntaxique (le parsage) et la construction de la sémantique en une unique tâche de configuration. L'utilisation d'un configurateur pour le parsage est possible en décrivant la grammaire comme un modèle objet sous contraintes. Nous étendons cette approche au traitement de la sémantique en introduisant deux nouveaux modèles objets contraints: un pour représenter la sémantique, et l'autre pour lier la syntaxe et la sémantique via la notion de ""schémas"". Nous mettons en avant les avantages de cette approche et présentons des résultats expérimentaux dans le cadre encore restreint de textes descriptifs simples, mais qui pourra être étendu pour capturer des sémantiques plus riches. Le système objet contraint sous-jacent est présenté formellement en utilisant le langage relationnel Z."
"La résolution étendue (ER) (c'est-à-dire, la résolution incorporant la règle d'extension) est un système de preuve plus puissant que la résolution standard (Res) car elle permet de résoudre en temps polynômial certaines classes d'instances que Res ne peut traiter qu'en temps exponentiel. Cependant, elle est très difficile à mettre en pratique car la règle d'extension accroit considérablement la taille de l'espace de recherche de la preuve. On dit qu'une résolution est étroite si l'application de la règle de résolution produit une résolvante contenant au maximum trois littéraux. Dans cet article nous présentons deux variantes de ER : la résolution étendue étroite (ER3) et la résolution à refragmentation. Ces deux systèmes de preuves p-simulent ER : toute preuve de l'une n'est que polynômialement plus longue que celle de l'autre. ER3 consiste simplement à n'autoriser que des résolutions étroites dans ER. Ceci permet une diminution exponentielle de l'espace de recherche d'une preuve dans ER. La résolution à refragmentation est une variante de ER3 qui permet une intégration facile de la résolution étendue dans n'importe quel solveur appliquant la résolution."
no abstract
no abstract
no abstract
"Notre étude, à travers l'exemple du jeu de l'individu chinois avec les normes, a pour but de montrer comment les niveaux micro et macro sont liés à travers la compétence de communication. Elle s'appuie sur la théorie des réseaux complexes (Green, 2000 ; Goldstein, 1999 ; Harvey & Reed, 1996) et sur la théorie de la structuration (Giddens, 1984) pour montrer que les phénomènes organisationnels sont indissociablement liés à l'émergence de configurations, de structures qui vont contraindre en retour l'action. Ces configurations sont des méta-matrices liant et organisant, à des échelles hiérarchiques supérieures, des éléments apparaissant dans différents réseaux sociaux, matériels et cognitifs (Hardy & Agostinelli, 2008). Dans le cadre des organisations socio-matérielles, il y a coorientation organisationnelle, car la communication humaine relie deux environnements: un environnement social-matériel et un environnement conceptuel médié par le langage (Taylor, 2006 ; Taylor & Van Every, 2000). Les êtres humains vivent dans les deux mondes à la fois, d'où la nécessité d'effectuer sans cesse des "" traductions "" d'un monde à l'autre. C'est là que l'on retrouve le rôle essentiel de la communication, dont l'étude permet de "" capturer cette dynamique de la traduction en cours"" (Taylor, 2006, p.147). Nous concevons donc le monde social comme constitué de deux principaux réseaux interreliés, un réseau socio-matériel reliant des actants humains et non-humains, mais physiques, et un réseau conceptuel, constitué de règles, par définition culturelles, et dont les normes sont un des éléments. Nous définissons la culture comme structure sociale, c'està-dire un système cognitif socialement distribué, formé par des règles et des ressources (que nous comprenons comme étant les valeurs, ou représentations mentales), et médiatisé par le langage. Si les règles culturelles organisent le social, comme le rappelle Sapir (1967/1927, p.38), "" il n'est de comportement qu'individuel "". Dans ce cadre, les variations individuelles sont comprises comme étant les pratiques individuelles guidées, mais non déterminées, par le système de règles. La culture est donc un réseau cognitif en constante émergence, à divers degrés de structuration de niveaux hiérarchiques, plus ou moins locaux ou globaux. C'est dans la pratique individuelle cependant qu'émergent ces niveaux hiérarchiques, de manière situationnelle. Le système cognitif d'un individu peut donc être représenté comme une méta-matrice émergente qui associe sur plusieurs dimensions des éléments de divers réseaux cognitifs socialement distribués, et c'est bien l'individu qui "" développe des compétences dans de multiples structures de règles "" (Sigman, 1980, p.38). La compétence communicationnelle peut être considérée comme cette méta-matrice émergente. Dans les règles du jeu social interactionnel, la compétence revient à la capacité à exprimer un chemin individuel (aspect efficient) tout en respectant les règles sociales, le cadre de référence commun (aspect approprié). Puisque les règles constitutives, qui sont des systèmes de sens (cf. Searle, 1972), sont à la base des règles normatives et définissent notamment le sens des sanctions, la compétence sociale va consister pour l'individu à parvenir à ses objectifs même dans le cas où ils seraient en contradiction avec les conventions et normes en vigueur, sans être sanctionné. Pour ce faire, l'individu peut contourner les règles, ou les modifier. Contourner les règles signifie trouver un ""jeu"", un interstice, dans le système de sens des règles constitutives. Modifier les règles signifie modifier les configurations entre les règles dans le système de règles, voire modifier le système lui-même. Ainsi, la compétence communicationnelle permet de jouer avec les normes et introduit la variété et la stratégie individuelle à l'intérieur de la structure sociale. Notre étude s'appuie sur le cas de la communication chinoise. Le choix du contexte chinois a été effectué pour deux raisons. Tout d'abord, parce qu'une culture nationale constitue un système de règles souvent institutionnalisées et donc plus facilement visibles si elles sont mises en comparaison avec les règles d'autres cultures nationales. Ensuite parce que la culture chinoise est décrite par la littérature interculturelle comme étant "" collective "" face à une culture occidentale individualiste. Dans le cadre d'une culture dite collective, l'individu n'aurait d'autre choix que de suivre les règles qui lui sont imposées. Or certains travaux montrent que l'individu chinois existe comme dans toute culture et utilise des stratégies communicationnelles (Hardy et Jian, 2011 ; Chang, 2010 ; 2001). La compétence communicationnelle, ou le jeu de l'individu avec les normes, est donc d'autant plus intéressante à étudier dans le contexte chinois où les normes communicationnelles sont particulièrement saillantes."
"This thesis deals with the development of Monte-Carlo methods to compute Feynman-Kac representations involving divergence form operators with a piecewise constant diffusion coefficient. The proposed methods are variations around the walk on spheres method inside the regions with a constant diffusion coefficient and stochastic finite differences techniques to treat the interface conditions as well as the different kinds of boundary conditions. By combining these two techniques, we build random walks which score computed along the walk gives us a biased estimator of the solution of the partial differential equation we consider. We prove that the global bias is in general of order two with respect to the finite difference step. These methods are then applied for tumour detection to the forward problem in electrical impedance tomography. A variance reduction technique is also proposed in this case. Finally, we treat the inverse problem of tumours detection from surface measurements using two stochastics algorithms based on a spherical parametric representation of the tumours. Many numerical tests are proposed and show convincing results in the localization of the tumours."
no abstract
no abstract
no abstract
"Le problème de satisfiabilité (SAT) est le premier problème de décision à avoir été montré NP-complet. Il est central en théorie de la complexité. Une for- mule mise sous forme CNF contient un nombre inté- ressant de symétries. En d'autres termes, la formule reste invariante si l'on permute quelques variables. De telles permutations sont les symétries de la formule et leurs éliminations peuvent conduire à une preuve plus courte pour la satisfiabilité. D'autre part, de nom- breuses améliorations ont été apportées dans les sol- veurs actuels. Les solveurs de type CDCL sont aujour- d'hui capables de résoudre de manière efficace des problèmes industriels de très grande taille (en nombre de variables et de clauses). Ces derniers utilisent des structures de données paresseuses, des politiques de redémarrage et apprennent de nouvelles clauses à chaque échec au cours de la recherche. Bien que l'uti- lisation des symétries et l'apprentissage de clauses s'avèrent être des principes puissants, la combinai- son des deux n'a encore jamais été exploitée. Dans cet article, nous allons montrer comment la symétrie peut être utilisée afin d'améliorer l'apprentissage dans des solveurs de type CDCL. Nous avons mis en ap- plication l'apprentissage par symétries dans MiniSat et nous l'avons expérimenté sur différents problèmes. Nous avons comparé MiniSat avec et sans apprentis- sage par symétries. Les résultats obtenus sont très en- courageants et montrent que l'utilisation des symétries dans l'apprentissage est profitable pour des solveurs à base de CDCL."
no abstract
no abstract
no abstract
"La composition est un des principaux challenge pour la communauté des services web sémantiques (SWS). Parmi les approches existantes, il a été montré efficace d'utiliser des techniques à base de contraintes (telle que la configuration) pour créer des orchestrations à partir des choréographies. Des expérimentations supplémentaires ont révélé les limitations et les ambiguités sémantiques qui peuvent survenir à partir de la requête à un composeur. Cet article propose une approche originale où la requête est vue comme un problème en lui-même, et montre comment la configuration peut être utilisée pour le résoudre grâce à un modèle objet contraint."
no abstract
no abstract
no abstract
no abstract
no abstract
De nomb reux CSP contiennent un mélange de contraintes symétriques et asymétriques. Nous présentons une approche générale qui permet d'appliquer des méthodes d'élimination de symétries connues à la partie symétrique d'un CSP puis de chercher une solution au problème entier en intégrant postérieurement les contraintes asymétriques. Nous étudions aussi le cas particulier des problèmes d'optimisaition où seule la fonction de coût à minimiser empêche les symétries. Nous montrons expérimentalement que dans ce contexte là nous pouvons accélérer la résolution de certains problèmes
no abstract
"Les distances de chanfrein sont largement utilisées en analyse d'image, et leur propriétés sont bien établies en 2 dimensions. Nous proposons dans cet article une étude des masques de chanfrein en 3 dimensions, dont les fondements théoriques sont plus complexes. Le but est de faire apparaître ces nouvelles structures et leurs propriétés, qui s'appuient sur les triangulations de points visibles dans les ensembles de Farey. On définit les déplacements élémentaires et les cônes d'influence, et on se ramène à une boule rationnelle équivalente. Par une contrainte de convexité sur la boule, on montre dans quelles conditions un masque de chanfrein induit une distance ; la sphère de chanfrein est alors un polyèdre discret. Enfin nous présentons quelques exemples."
no abstract
"Dans le cadre de l'ACP, les concepts de détectabilité et d'isolabilité de défauts ont été développés plus particulièrement pour quelques indices de détection. Dans ce papier, nous avons étendu ces concepts afin d'être unifiés et valables pour tout indice ayant une forme quadratique. L'approche RBC a été utilisée pour le diagnostic de défauts unidimensionnels de grandes amplitudes. En revanche, les défauts peuvent être dans plusieurs directions. Pour cala, nous avons proposé une RBC multidimensionnelle. Ce papier présente également une nouvelle approche nommée RBC ratio (RBCr). Elle est dédiée au diagnostic des défauts détectables de faibles amplitudes. Une diagnosabilté qui s'appuie sur cette méthode garantit l'identification de tels défauts. Toutefois, l'isolation de ces derniers n'est garantie que si leurs amplitudes satisfassent une condition suffisante d'isolabilité. Un exemple simulé est présenté afin d'illustrer la théorie d'une telle diagnosabilité."
"Les modèles polyédriques, très utilisés dans les processus d'ingénierie, constituent une représentation privilégiée au sein des maquettes numériques de produits. Les approches et méthodes de manipulation et d'exploitation de ces modèles, sont le plus souvent pilotées par un critère géométrique lié à la forme des objets (normale, courbure ...) mais très peu prennent en compte des informations de type perceptuelles. Parallèlement, les techniques de traitement d'images existantes extraient des données (contours, primitives géométriques, textures ...) relatives à la description des formes des objets qu'elles représentent. Ainsi, le but de cette thèse est de réaliser un couplage modèle polyédrique 3D / images numériques 2D pour manipuler les modèles 3D avec des critères extraits d'image(s). Une méthode de simplification et deux méthodes de déformation de polyèdres sont présentées. Dans la première méthode, la simplification est basée sur un processus itératif de suppression de sommets piloté par un critère de tolérance de simplification. Cette tolérance est liée au filtrage de contours d'image(s) qui sont projetés sur le modèle 3D afin d'identifier les zones plus ou moins proches de ces lignes de caractère. Les méthodes de déformation de polyèdres sont appliquées au cas du remplissage de trous. Une triangulation est insérée au modèle puis déformée par la résolution d'un problème d'optimisation numérique sous contraintes. La fonctionnelle à minimiser simule la variation de courbure entre le maillage inséré et le modèle initial. La solution est obtenue par un algorithme itératif basé sur un modèle mécanique de réseau de barres. Les contraintes imposent le respect de lignes de caractère 3D obtenues par triangulation stéréoscopique dans la première méthode, ou bien calculées en fonction de l'intensité lumineuse des pixels et qui imposent le déplacement des sommets correspondants suivant une certaine élévation (problème inverse du Shape From Shading). Les trois méthodes implémentées sont complètement modulaires."
"Cet article porte sur une application qui permet de simuler le comportement d'un officier dans un sousmarin. Nous nous sommes intéressés plus particulièrement à ses réactions en cas de détection d'un sousmarin adverse. Cette application a été implémentée en Prolog. Dans les simulations de combat naval, les performances opérationnelles de navires militaires sont estimées pour un scénario donné. Dans les modèles courants, les réactions de l'opérateur sont prédéfinies. Ces réactions ne sont pas réalistes : la décision de l'opérateur peut conduire à des réactions inattendues. Cet article présente une méthode pour modéliser le comportement d'un opérateur dans les simulations. Cette méthode permet de raisonner sur des informations incertaines et révisables : un opérateur a une vue partielle de son environnement et il doit réviser ses décisions avec l'arrivée et le changement d'informations. Notre méthode utilise une logique non-monotone : les règles de comportement sont formalisées avec la logique des défauts, à laquelle nous ajoutons une gestion du temps. Nous utilisons des préférences pour gérer le choix entre plusieurs règles, avec une technique simple de probabilité. Cette méthode a permis de modéliser les réactions d'un officier dans un scénario faisant intervenir deux sous-marins adverses. Cette application, implémentée en Prolog, a été interfacée avec un atelier de simulation de combat naval de DCNS."
"Le cachalot, Physeter macrocephalus le plus grand des odontocètes, a été longtemps exposé à la pêche pour extraire l'huile contenue dans sa tête (spermaceti), et est maintenant vulnérable. Les cachalots nagent dans divers endroits du globe et atteignent les plus grandes profondeurs, où ils chassent par écholocation. Ils émettent des sons, des clics à large bande, à structure multi-pulsée générée par des réflexions intra-tête. Ces clics contiennent de l'information sur la taille de l'animal et sur son orientation, reposant sur les délais entre les pulses qui le composent (intervalle inter-pulse 'IPI'). Ces mesures sont utiles dans la préservation et les efforts de suivi populationnel du cachalot, puisqu'il est impossible de filmer ces mammifères nageant en grandes profondeurs.Les avancées technologiques permettant un essor considérable du nombre d'enregistrements sous-marins. Des algorithmes d'analyse automatiques sont alors requis pour le suivi de cette population ou des études comportementales et mesures de protection. La littérature offre une variété de méthodes pour le calcul de l'IPI. Cependant, elles souffrent d'un mélange des différents délais entre pulses résultant en une estimation de l'IPI peu précise et, par conséquent, de la taille de l'animal. De plus, ce mélange des différents délais entre pulses ne permet pas l'extraction de données sur l'orientation de l'animal. Dans cette thèse, une nouvelle méthode pour une analyse fine de l'IPI est présentée. Elle ne mélange pas les différents délais entre pulses, mais sélectionne les pulses à travers une analyse combinatoire et statistique. Il en résulte une meilleure estimation de la taille du cachalot. De plus une information supplémentaire, sur son orientation, est extraite. Notre algorithme est comparé avec ceux de l'état de l'art. Nous en discutons ses forces et faiblesses.Les résultats expérimentaux sont donnés autant sur des exemples avec un ou plusieurs cachalots émettant en même temps, que sur des données obtenues en différentes zones géographiques (France, Italie, Etats-Unis) et à partir de différents systèmes d'enregistrement. Ceci démontre la robustesse de la méthode proposée, et donne des perspectives pour le suivi des cachalots à l'échelle du globe."
no abstract
"La conception de situations de communication des connaissances pour l'e-learning demande de préciser quels sont les modèles didactiques, pédagogiques, informatiques, les pratiques et usages liés aux technologies, les relations communautaires sur les réseaux qui sont pris en compte."
"Les méthodes de vol de travail permettent la re-distribution et une équilibrage équitable de la charge de travail sur des machines parallèles. Lorsqu'une unité de calcul termine ses travaux, elle va voler des travaux des unités qui ne sont pas encore terminées. On s'intéresse dans cet article à la modélisation et à l'évaluation de performance d'une méthode de vol de travail stochastiques sur des machines multi-coeurs. On propose des modèles génériques pour l'algorithme de vol de travail. Ces modèles génériques seront en suite utilisés pour modéliser une architecture constituée de trois coeurs de calcul. On termine par une analyse de mesures obtenues lors de la résolution du modèle."
Method and system for estimating a similarity between two binary images.
no abstract
no abstract
no abstract
no abstract
no abstract
"Les récentes découvertes et avancées technologiques dans la compréhension des matériaux ainsi que l'essor des outils informatiques d'aide au calcul ont contribué à la prolifération de matériaux intelligents avec un champ d'applications très large. Cette thèse s'inscrit dans le contexte d'utilisation des actionneurs piézoélectriques plutôt qu'une vision purement matériau. Le but est d'enrichir les bibliothèques de modèles de ces types d'actionneurs afin de faciliter leur prise en compte dans les phases de conception des systèmes complexes les intégrant. Le cahier des charges est que ces modèles incluent le plus possible les non-linéarités tout en restant aisés d'utilisation. Pour atteindre ces objectifs, nous proposons de faire un pont entre le domaine des experts des matériaux et celui de l'ingénieur en suivant une méthodologie claire. Dans un premier temps nous passons en revue les approches existantes dans la littérature ainsi que les solutions offertes par certains logiciels commerciaux. Une analyse des équations constitutives de la piézoélectricité associées aux conditions de fonctionnement de l'actionneur nous permet d'en déduire un premier modèle analogique. Ce dernier est ensuite traduit en bond graph pour en déduire des modèles blocs-diagramme. En plus de cet effort de formalisation, ces premiers modèles se distinguent de ceux proposés par les logiciels commerciaux en prenant mieux en compte la dynamique propre à l'actionneur. Nous proposons deux types de modèles. L'un rend uniquement compte du premier mode de résonance alors que le second rend compte de deux modes de résonance. Ensuite nous proposons des modèles prenant en compte les non-linéarités : l'approche de Preisach pour la modélisation de l'hystérésis statique et l'approche de Voigt dans le cas dynamique. Ces deux approches sont ensuite fusionnées dans le but d'avoir un model plus complet."
no abstract
no abstract
no abstract
no abstract
"Une des difficultés inhérentes à la recherche énumérative est l'explosion combinatoire. Parmi les algorithmes incomplets qui tentent de résoudre ce problème, l'optimisation par colonie de fourmis (ACO - Ant Colony Optmisation), qui combine des méthodes aléatoires et heuristiques avec l'apprentissage par renforcement, a prouvé son efficacité sur de nombreux problèmes de satisfaction de contraintes (CSP). Cet article présente une application d'un algorithme basé sur ACO pour la configuration, ce qui à notre connaissance n'avait pas encore été étudié. Nous décrivons comment la nature des problèmes non-bornés de configuration influe sur l'approche ACO, notamment à cause de la présence de variables ensemblistes et de domaines ouverts. Nous proposons un algorithme et un modèle phéromonal original permettant de traiter ces difficultés. Nous montrons également l'utilisation de l'optimisation par essaim de particules (PSO) pour converger vers des ensembles de paramètres optimaux. Enfin, nous fournissons des résultats expérimentaux, à la fois pour des instances aléatoires et pour le problème d'optimisation des racks."
no abstract
no abstract
no abstract
no abstract
no abstract
"Ce rapport porte sur le sujet de recherche de l'identification boîte noire du système non linéaire. En effet, parmi toutes les techniques nombreuses et variées développées dans ce domaine de la recherche ces dernières décennies, il semble toujours intéressant d'étudier l'approche réseau de neurones dans l'estimation de modèle de système complexe. Même si des modèles précis ont été obtenus, les principaux inconvénients de ces techniques restent le grand nombre de paramètres nécessaires et, en conséquence, le coût important de calcul nécessaire pour obtenir le niveau de pratique de la précision du modèle désiré. Par conséquent, motivés pour remédier à ces inconvénients, nous avons atteint une méthodologie complète et efficace du système d'identification offrant une précision équilibrée, la complexité et les modèles de coûts en proposant, d'une part, de nouvelles structures de réseaux de neurones particulièrement adapté à une utilisation très large en matière de modélisation système pratique non linéaire, d'autre part, un simple et efficace technique de réduction de modèle, et, troisièmement, une procédure de réduction de coût de calcul. Il est important de noter que ces deux dernières techniques de réduction peut être appliquée à une très large gamme d'architectures de réseaux de neurones sous deux simples hypothèses spécifiques qui ne sont pas du tout contraignant. Enfin, la dernière contribution importante de ce travail est d'avoir montré que cette phase d'estimation peut être obtenue dans un cadre robuste si la qualité des données d'identification qu'il oblige. Afin de valider la procédure d'identification système proposé, des exemples d'applications entraînées en simulation et sur un procédé réel, de manière satisfaisante validé toutes les contributions de cette thèse, confirmant tout l'intérêt de ce travail."
"Lřobjet de cet article est de démontrer comment des éléments (substantif générique choisi sciemment) se sont enchevêtrés pour faire en sorte que la technologie et les acteurs dřun projet progressent dans un milieu (substantif préféré à celui de contexte pour inclure des aspects climatologiques ou géologiques, par exemple) a priori défavorable ou peu favorable. Comment la mise en évidence de ces éléments et la compréhension du milieu pourraient-elles permettre de transférer ou dřadapter des projets réussis en Afrique, à un moment donné, à des projets du pourtour méditerranéen, pour des populations socialement ou géographiquement distantes ou isolées ? Pour accomplir cette démonstration, à partir de la petite histoire dřune ONG dédiée à la diffusion audiovisuelle mobile, où sřentremêlent différents « actants » (Latour, 2001) et les intentions des acteurs, nous avons mené à bien une réflexion autour de la technologie, pour comparer les intentions premières des principaux acteurs et les difficultés rencontrées dans la mise en place dřun projet situé au Liban. Nous présupposons que le croisement de notions théoriques ou dřobservations théorisées et de lřexpérience de terrain apporte un éclairage propice à la compréhension de lřexpérience non aboutie et au transfert dřune expérience aboutie à une autre expérience dans un nouveau milieu. Nous devons spécifier que nous avons participé au travail de lřONG Cineaction (que nous présentons en suivant), devenue la société Nomadic Dre@m Machines-Cineaction (NDM-Cineaction) et que nous sommes en possession des archives (une documentation organisée et classée) de cette ONG que nous utilisons comme corpus, riche en formes et contenus. Lřanalyse de lřexpérience de lřONG Cineaction peut servir à la mise en place de nouveaux projets qui se fondent sur lřimplantation dřune haute technologie numérique pour des populations, socialement ou géographiquement isolées."
"Dans le cadre de la réflexion menée sur la mise en oeuvre d'un système d'information d'aide au pilotage de la recherche et de la création d'un Infocentre Recherche pour l'Université Paul Cézanne, nous en présentons, ici, les premiers résultats appliqués à l'Unité de Recherche pilote (UMR6171 - Systèmes Chimiques Complexes), puis nous discuterons de l'ajustement des indicateurs quantitatifs mis en oeuvre en vue de l'élaboration d'indicateurs élaborés étendus à un domaine scientifique, enfin nous reviendrons sur les attentes des différents acteurs de la recherche et à l'évaluation de l'Infocentre."
no abstract
no abstract
"This paper deals with the integrated facility location and supplier selection decisions for the design of supply chain network with reliable and unreliable suppliers. Two problems are addressed: (1) facility location/supplier selection; and (2) facility location/supplier reliability. We first consider the facility location and supplier selections problem where all the suppliers are reliable. The decisions concern the selection of suppliers, the location of distribution centres (DCs), the allocation of suppliers to DCs and the allocation of retailers to DCs. The objective is to minimise fixed DCs location costs, inventory and safety stock costs at the DCs and ordering costs and transportation costs across the network. The introduction of inventory costs and safety stock costs leads to a non-linear NP-hard optimisation problem. To solve this problem, a Lagrangian relaxation-based approach is developed. For the second problem, a two-period decision model is proposed in which selected suppliers are reliable in the first period and can fail in the second period. The corresponding facility location/supplier reliability problem is formulated as a non-linear stochastic programming problem. A Monte Carlo optimisation approach combining the sample average approximation scheme and the Lagrangian relaxation-based approach is proposed. Computational results are presented to evaluate the efficiency of the proposed approaches."
no abstract
no abstract
no abstract
no abstract
"Le but de cet article est de proposer un cadre de conception des systèmes complexes aussi cohérent et complet que possible qui permette à la fois : (1) d'intégrer les activités de développement et les activités d'évaluation de la sûreté, (2) de satisfaire aux exigences émises par les autorités de certification telles que la l'EASA et FAA , (3) de répondre à des standards tels que l'ED-79/ARP 4754 [9], la ED-80/DO-254 [10], ou la ED-12/DO-178B [8], (4) de préparer la voie à une ingénierie des systèmes aéronautiques basée sur des modèles qui est encore en cours d'élaboration. Dans un premier temps, nous rappelons un certain nombre de résultats concernant l'ingénierie des exigences que nous avons publié dans un article [14] du "" Journal of Systems Engineering "" en 2008. Dans un second temps, nous présentons les éléments architecturaux du processus d'ingénierie tel qu'il est présenté par le standard EIA-632 [3]. Nous l'adoptons comme cadre de notre travail. Dans un troisième temps, nous montrons comment il est possible d'étendre le cadre proposé par l'EIA-632 pour y intégrer nos propositions relatives à l'ingénierie des exigences, et de rendre compte des différents attendus exprimés dans les réglementations et standards aéronautiques."
no abstract
"Dans ce papier nous étudions deux approches dédiées à la détection de défauts dans les puces micro-électroniques dans le domaine des semi-conducteurs. Ces approches s'appuient fondamentalement sur des aspects multivariés pour réaliser la tâche de diagnostic du système à partir de données volumineuses et hétérogènes issues d'une chaîne de production. Nous proposons ces approches multivariées qui tirent avantage de leurs capacités de traiter des données complexes et qui permettent de mettre en évidence les effets de corrélation entre les variables éventuellement indécelable dans une approche univariée. Ces thématiques portent sur l'aide au diagnostic de systèmes complexes et l'aide à la décision, plus particulièrement sur des aspects de classification et de réduction de dimensionnalité. Les résultats de ces approches pour la détection ont été validés avec succès sur des données réelles issues d'une phase de production de plusieurs produits de la société STMicroelectronics. Dans l'objectif de localiser les défauts, nous avons considéré l'approche du calcul des contributions relatives aux deux statistiques de détection de l'analyse en composantes principales (ACP). Pour cela, nous avons d'abord considéré ce qui existe dans la littérature. Cependant, le calcul de la contribution à l'erreur quadratique SPE (Squared Prediction Error) exprimée dans le sous-espace résiduel de l'ACP présente un inconvénient. Ce problème a été évité en proposant une nouvelle forme de cette contribution."
no abstract
"Les servocommandes aident le pilote à contrôler l'appareil avec précision et peu d'effort au manche. Le travail de thèse concerne le design et la fabrication de servocommandes à entrée mécanique et puissance hydraulique. Le distributeur est la pièce la plus coûteuse et la plus difficile à concevoir et à fabriquer de la servocommande. Cette pièce est également celle qui influence principalement les performances de l'actionneur. Le principal objectif de la thèse est de concevoir une servocommande faible coût qui possède des performances similaires à une servocommande actuellement utilisée. Cet objectif a été scindé en trois étapes :  Modéliser une servocommande et en particulier l'étage pilote de celle-ci  Concevoir le distributeur à partir de ce modèle  Fabriquer des prototypes de servocommande et valider la conception grâce à des essais Dans le premier chapitre, des solutions techniques du futur design sont sélectionnées pour la servocommande et le distributeur afin de répondre aux exigences de l'application. Le second chapitre présente les modèles et outils pour le design et la fabrication du distributeur de servocommande. Le troisième chapitre concerne le pré-design et la fabrication des premiers distributeurs. La méthode choisie pour le pré-design du distributeur est basé sur une exploitation de modèle. Une nouvelle méthode de fabrication bas coûts est développée basée sur la représentation asymptotique de la courbe de gain en pression. Le dernier chapitre présente le design final du distributeur. L'évaluation à partir d'essais de l'exigence de mixabilité n'étant pas concluante, une nouvelle géométrie de fente pour le distributeur est proposée. Le design est donc mis à jour grâce à une approche basée sur le modèle puis validé par des essais."
no abstract
no abstract
"La modélisation et l'identification floues de systèmes avec de multiples entrées et une seule sortie (MISO), non linéaires, non stationnâmes et avec perturbations sont présentés. A cet effet, des techniques d'apprentissage floues sont utilisées. C'est ainsi qu'un modèle est mis en place à partir des mesures d'un système bio-climatique, qui concerne l'étude d'une serre expérimentale de l'Université du Sud Toulon Var (USTV) en France. L'analyse multi-modèles est utilisée, avec la structure de règles floues proposée par Takagi-Sugeno-Kang (TS), où les prémisses des règles sont identifiées au moyen de l'algorithme flou de C-Means. La démarche d'apprentissage local et global est introduite pour identifier les paramètres linéaires des conséquences des règles floues. Ainsi, des modèles flous TS sont obtenus avec une démarche pluri-objectif. Dans le cadre de la technique de la modélisation et de F identification floues TS, le développement d'un algorithme est détaillé, pour modéliser des systèmes SISO, pour lesquels l'algorithme flou de Gustafson-Kessel (G-K) sera mis en oeuvre afin d'identifier les prémisses des règles floues. L'innovation réside dans le fait que les conséquences des règles floues sont des polynômes d'ordre cubique. Enfin, avec la technique LMI et avec un modèle développé pour un système bio-climatique, la synthèse d'un contrôleur stable avec l'approche de Lyapunov qui régule le chauffage de la serre, est réalisée pour maintenir le paramètre VPD dans un domaine fiable pour la plantation."
"Nous proposons une nouvelle classe de CSP binaires appelés CSP extrêmaux. Les CSP de cette classe sont inconsistants mais deviendraient consistants si n'importe quel couple de valeurs interdit devenait autorisé. Etant inconsistants, ils ne sont pas traitables avec des méthodes de réparation locale. Comme ils autorisent un nombre très élevé de solutions partielles presque complètes, ils peuvent être très difficiles à résoudre à l'aide de méthodes de recherche arborescente intégrant le filtrage des domaines. Il faudra donc trouver de nouvelles méthodes pour les résoudre. Nous présentons un algorithme simple de génération de CSP extrêmaux. Nous constatons expérimentalement que les CSP extrêmaux équilibrés sont beaucoup plus longs à résoudre que les CSP aléatoires dits difficiles de même taille. Nous présentons aussi un schéma d'algorithme susceptible d'être performant sur des problèmes difficiles, dès lors qu'on sera en mesure de générer suffisamment rapidement des CSP extrêmaux."
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"Nous présentons une étude de cas clinique d’un enfant autiste sévère suivi lors d’un atelier thérapeutique à médiation corporelle (bain-massages) au long des quinze séances produites. Pour mener cette étude de cas, nous proposons une méthode d’analyse clinique fondée sur une approche psychosomatique du fonctionnement de la pensée et de l’identité. Cette méthode permet d’observer et d’évaluer concrètement les effets d’un atelier thérapeutique avec des enfants en grande difficulté de développement. L’étude s’appuie sur un matériel clinique pictural original, il s’agit de quinze dessins spontanés du bonhomme issus de chacune des séances d’atelier. Des pistes thérapeutiques/cliniques sont proposées et discutées."
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"La simulation numérique de comportement des futurs produits est largement utilisée sur les modèles virtuels de produits avant leur fabrication physique. Cependant, le processus pourrait encore être optimisé en particulier pendant la phase d'optimisation du comportement de produit. Ce processus implique la répétition de quatre étapes principales de traitement : conception de CAO, création de maillage, enrichissement de sémantique par la modélisation du comportement physique et enfin calcul par éléments finis (EF). L'analyse de comportement de produit est effectuée à partir de la première solution de conception puis sur les nombreuses boucles successives d'optimisation de produit. Chaque évaluation de solution nécessite le même volume de temps que celui nécessaire pour la première conception de produit, cela est particulièrement crucial dans le contexte de maintenance de produit et d'évaluation de cycle de vie de produit. Cette thèse propose un nouveau cadre de travail pour l'optimisation de produit à partir de simulation par EF menées successivement sans retour à la CAO initiale du produit, ce qui réduit les activités de préparation de maillages et d'enrichissement sémantique E.F. Plus concrètement, l'idée est d'opérer directement le maillage enrichi par la sémantique E.F pour optimiser le produit. Dans cette thèse, les concepts sous-jacents et les composants conçus pour le développement de ces opérateurs de modification sont présentés et analysés. Une spécification d'opérateur de haut niveau est proposée selon une structure modulaire qui permet ensuite une réalisation facile des différents opérateurs de modification de maillage. Enfin, quatre déclinaisons de cet opérateur de maillage de haut niveau sont présentées: la fusion, la fissuration, le perçage et le congé d'arête. Ces opérateurs ont été prototypés et validés sur des modèles E.F. académiques et industriels, permettant de démontrer leur efficacité et la pertinence de l'approche proposée."
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"L'objectif de cet article est d'explorer trois façons/moyens de rendre public : les tweets, les vidéos d'un discours, mises en ligne sur un site tel que Daily Motion et la publication d'un livre de photographies sur des « phrases/tweets » tracés au crayon ou au charbon de bois sur la pierre, par un berger dans la première moitié du 20 ème siècle. Par l'analyse anthropologique de la production et de la diffusion des signes, nous observons la perméabilité entre la sphère privée et la sphère publique, l'influence de l'écriture et de l'oralité ainsi que les distinctions entre le publié, le public et la popularité des trois formes analysées. Abstract The aim of this article is to study three ways to make public: the tweets, the videos of discourses posted on Daily Motion and the publication of a book where sentences as tweets were drawn by a shepherd, with a pencil on stones, at the beginning of the 20 th Century. Through anthropological analysis of signs production and diffusion, we observe the permeability between private and public field, the influence of the writing and oral activity as well as the distinctions between the published, the public and the popularity regarding these three mentioned forms."
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"Nombreux dans l’entreprise sont ceux qui croient encore que les phénomènes observables entre les générations de « baby boomers » (BB), les « X » et les « Y » sont normales et s’estomperont alors que les derniers auront « mûri ». Si l’on peut retrouver des points communs aux trois générations, les perceptions et les modes de fonctionnement divergent nettement et il serait suicidaire pour l’entreprise, qui doit désormais faire face à de nouveaux enjeux économiques, sociétaux et technologiques, de ne pas intégrer cette réalité dans sa stratégie, élément essentiel dans la cohésion et dans les processus de transmission du savoir et de la culture dans les organisations."
no abstract
no abstract
no abstract
no abstract
"Longtemps considérés comme polluants, les moteurs diesels sont aujourd’hui autant, voire plus, propres que les moteurs à essence. Afin de respecter au mieux les normes environnementales, même en présence de dysfonctionnements, les constructeurs automobiles mettent en œuvre des systèmes de détection et de localisation de défauts. La plupart des recherches précédentes pour le diagnostic des moteurs diesel étudie des parties bien déterminées du moteur et non pas le moteur complet. Cette thèse propose deux stratégies différentes de diagnostic sur un modèle de connaissance d’un moteur diesel ce qui permet de détecter, d’isoler et d’estimer six défauts dans les différentes parties d’un moteur diesel équipé d’un turbocompresseur à géométrie variable : fuite d’air dans le collecteur d’admission, mauvais fonctionnement du compresseur, défaut d’ouverture des soupapes d’admission, défaut dans l’échangeur, détérioration dans le couplage turbine compresseur, défaut dans la commande de la turbine à géométrie variable et défaut dans le capteur de vitesse. L'idée de base de la première stratégie est d'utiliser l’apprentissage récursif d’un observateur à modes glissants. La seconde stratégie utilisée pour concevoir un FDI (Fault Detection and Isolation) est basée sur la théorie de contrôle par «Séquencement de gain» appliquée sur un système multilinéaire de type Takagi_Sugeno. Des simulations avec un modèle diesel non-linéaire validé, en présence de bruit, ont été effectuées et qui a démontré l'efficacité de l'algorithme proposé."
no abstract
no abstract
no abstract
Non disponible
no abstract
"Le SIG, archétype du système d'aide à la décision spatiale, est généralement statique. La dynamique des systèmes spatiaux est souvent représentée à l'aide de systèmes multi-agents, moins présents chez les décideurs. Une analyse des modes de couplages entre ces deux types de systèmes est réalisée, et les limites du couplage étroit sont mises en évidence au travers de l'étude de cas du système Camargue. Un couplage intelligent à base d'agents dans un environnement d'information géographique est envisagé pour la modélisation de la dynamique des structures spatiales."
"Dans ce papier, nous proposons une nouvelle approche pour calculer un strong backdoor pour des formules mises sous forme normale conjonctive (CNF). Elle est basée sur une utilisation originale d'une méthode de recherche locale qui fournit un renommage maximisant la sous-formule horn-renommable d'une CNF donnée. Plus précisément, à chaque étape, on choisit de renommer la variable qui fait le plus diminuer le nombre de clauses non-horn. S'il ne reste plus de clauses strictement positives (ou strictement négatives) ou de clauses non-horn dans la formule, notre méthode répond au problème de satisfaisabilité de la formule originale; sinon, on utilise la plus petite sous-formule qui ne soit pas de horn pour en extraire un ensemble de variables (strong backdoor) tel qu'une fois ces variables instanciées, le reste du problème appartient à une classe polynômiale. Les premiers résultats expérimentaux montrent que notre approche est prometteuse sur un grand nombre d'instances SAT."
no abstract
no abstract
no abstract
"Nous pr&#233;sentons dans ce papier les conditions pour lesquelles un syst&#232;me artificiel constitu&#233; d&rsquo;entit&#233;s m&#233;catroniques peut &#234;tre lui-m&#234;me qualifi&#233; de m&#233;catronique. Nous montrons que c&rsquo;est le cas lorsque l&rsquo;intelligence conf&#233;r&#233;e aux dites entit&#233;s leur permet de faire fonctionner ce syst&#232;me sans l&rsquo;intervention d&rsquo;un centre de pilotage de niveau hi&#233;rarchique sup&#233;rieur. Le principe g&#233;n&#233;ral de l&rsquo;affectation de t&#226;ches est bas&#233; sur un m&#233;canisme de s&#233;lection de la meilleure r&#233;ponse &#224; un appel d&rsquo;offres diffus&#233; aupr&#232;s de toutes les entit&#233;s pour chaque t&#226;che &#224; effectuer. L&rsquo;approche holonique nous fournit un support conceptuel pour proposer un syst&#232;me g&#233;n&#233;rique de pilotage auto-organis&#233; et isoarchique, adapt&#233; aux syst&#232;mes m&#233;catroniques complexes et fond&#233; sur l&rsquo;existence d&rsquo;un pilotage autonome associ&#233; au sein de chaque entit&#233; m&#233;catronique. Pour conclure, nous pr&#233;sentons un &#233;ventail de syst&#232;mes que nous pouvons consid&#233;rer de par leur syst&#232;me de pilotage comme &#233;tant des syst&#232;mes m&#233;catroniques complexes&hellip;"
"Les descriptions utilisées par les services web sont en langage naturel, multilingues et inter-domaines. Générer des représentations de services web est donc un défi majeur. Cet article présente une représentation classique et en propose deux nouvelles. L’impact de ces représentations pour la découverte et la recommandation est étudié et discuté à la lumière de nos expérimentations utilisant des services web réels."
no abstract
"Le Laboratoire Commun de Métrologie LNE-CNAM (LCM) souhaite affiner sa maîtrise des références de pression afin de réaliser des appareillages de tout premier rang au niveau international. L'incertitude relative visée sur les références de pression est de l'ordre de 10-6. Cet objectif se traduit par une problématique de métrologie dimensionnelle où une mesure de la forme des pistons/cylindres utilisés dans les balances manométriques doit être menée. La mesure de cylindricité est également impliquée dans un très grand nombre d'applications industrielles comme la qualification d'étalons de référence destinés à la qualification d'appareillages de mesure. Notre travail de recherche, réalisé dans le cadre d'une convention CIFRE avec la SAS GEOMNIA, concerne la réalisation d'un instrument de référence de très haute précision permettant la mesure de forme de cylindres creux ou pleins. Nous proposons un saut technologique pour satisfaire un niveau d'incertitude sur la mesure de l'écart de cylindricité de l'ordre de 10 nanomètres dans un volume de mesure cylindrique de Ø350 mm et de hauteur 150 mm. La mesure de forme est habituellement pratiquée en déplaçant un capteur par rapport à la surface à mesurer par un guidage de haute précision. Il n'est cependant pas possible de réaliser un guidage entre deux solides d'un niveau de précision permettant de garantir les incertitudes souhaitées, même en utilisant les techniques de correction d'erreurs dont la précision est limitée par le défaut de répétabilité des guidages. Pour satisfaire à ce niveau d'incertitude, nous proposons une démarche basée sur le concept de structure métrologique dissociée. La mesure d'une pièce consiste alors à comparer sa forme à celle d'une pièce cylindrique de référence. Cette dernière doit seulement présenter une stabilité de forme parfaite. La cartographie d'écart de forme de la référence cylindrique doit cependant être identifiée au même niveau d'incertitude visé.Le travail de recherche développé propose une analyse détaillée des machines actuelles et de leurs limitations. Suite à cette analyse, une architecture de machine a été proposée pour lever ces limitations. Cette architecture tient compte des écarts « secondaires » liés à la position des capteurs et des effets de second ordre, pour satisfaire le niveau de précision visé. Une procédure complète d'étalonnage de la machine a été élaborée en s'inspirant des méthodes de séparation d'erreurs. Cette procédure originale permet de séparer les défauts de forme du cylindre de référence de ceux d'une pièce de qualification cylindrique mesurée simultanément. La méthode employée ne présente pas de limitations en termes d'exactitude. Cette procédure a été expérimentalement validée. Une analyse des effets liés à la mesure de surfaces cylindriques par des capteurs capacitifs a été menée. Ces essais ont conduit au développement de stratégies d'étalonnage de ces capteurs in situ utilisant des interféromètres à laser intégrés dans la machine. La traçabilité métrologique des résultats des mesures est ainsi garantie. Deux bancs de tests ont été développés pour caractériser les diverses influences et valider les procédures d'étalonnage des capteurs. La conception détaillée de l'instrument est issue de la synthèse des réflexions menées sur l'architecture, sur l'étalonnage et sur la maîtrise de la mesure de déplacements par capteurs capacitifs. Ce travail a abouti à la réalisation de ce nouvel instrument de référence ; sa conception, son montage et son réglage sont présentés."
no abstract
no abstract
no abstract
no abstract
no abstract
"This paper uses as starting point the transformation matrix defined in the homogeneous space that associates the points of a 2D plane (that represents the model) with those of another 2D space (the image one), this transformation characterizing the camera capture process. This transformation (an homography from 2D to 2D) is coming from previous work and is used within the scope of the SimulFoot project. The final objective is to reconstruct a 3D model from TV soccer scenes, making it important to characterize the transformation between a 2D plane (the soccer field) and the camera image. We suppose the transformation (from image to field) is a conic projection whose center is S and projection plane is P in the model 3D space. We formulate two additional hypotheses related to the reference system of P: its origin is the orthogonal projection of S on P, and its first basis vector is parallel to the horizontal plane xOy. In fact, these conditions are often verified in soccer scenes since the camera is fixes on a tripod. In this communication, we give the camera location and aperture expressions on the only basis of the transformation matrix values."
no abstract
no abstract
no abstract
no abstract
Non disponible
no abstract
no abstract
no abstract
no abstract
"Dans un premier temps, nous présentons une classe de distances discrètes : les distances de chanfrein. Nous montrons que les propriétés de ces fonctions dépendent de la géométrie de l'enveloppe convexe des points du masque de chanfrein servant à les définir. Nous présentons une méthode permettant de construire des masques de chanfrein réguliers qui définissent des normes discrètes (vérifiant la propriété d'homogénéité). Nous effectuons, sur la base des contraintes ainsi déterminées, une optimisation afin de trouver des exemples pratiques dans les cas 3D de normes de chanfrein optimales (minimisant l'erreur par rapport à la distance euclidienne). Dans un deuxième temps, nous définissons l'axe médian d'une forme, qui est l'ensemble des centres des boules maximales inscrites dans cette forme. Nous détaillons son calcul \`a partir de la carte de distance de la forme, par la méthode des tables de correspondance. Nous présentons une méthode de détermination des valeurs de cette table pour toute distance discrète en 2D ou 3D, puis nous présentons une méthode permettant de calculer, ainsi que de valider le voisinage de test, dont dépend le calcul local de l'axe médian. Nous donnons enfin plusieurs exemples de tables et de voisinages obtenus dans le cas des normes de chanfrein 3D, ainsi que dans celui du carré de la distance euclidienne."
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"Cette thèse contribue à la modélisation et à létude de la dynamique de la coupe vibratoire, ayant comme application immédiate et comme objectif, la validation du perçage vibratoire par tête auto-vibrante. Les modèles de la coupe vibratoire qui ont été développés ont fait lobjet dune analyse de stabilité et dune étude détaillée par simulations numériques. Les phénomènes non linéaires engendrés par la coupe vibratoire ont été mis en évidence par simulations et par une campagne dexpérimentations. Les modèles et les études issus de cette thèse donnent une vue densemble de la dynamique compliquée de la coupe vibratoire et apportent une compréhension fine des aspects pratiques du perçage vibratoire par tête auto-vibrante. Les moyens expérimentaux développés et mis en uvre constituent un outil dinvestigation performant et indispensable à lanalyse systématique du procédé, dans le but de son industrialisation."
no abstract
"La détection dynamique et l'élimination des valeurs symétriques dans les CSPs quelconques est en générale une tâche difficile, mais dans le cas des CSPs à contraintes de différence (NECSPs) , les conditions de symétrie peuvent être simplifiées. Dans cet article, nous étendons le principe de la symétrie à la dominance dans le cas des CSPs à contraintes de différence et nous montrons comment les valeurs dominées sont détectées et éliminées efficacement à chaque noeuds de l'arbre de recherche. Nous proposons un algorithme de détection de valeurs dominées de complexité linéaire. Nous avons implémenté cet algorithme dans un Forward Checking adapté au cas des CSPs à contraintes de différence. Nous avons comparé cette méthode à la méthode DSATUR sur deux classes de problèmes de coloration de graphes : les aléatoires ainsi que sur des instances issues du challenge de DIMACS. Les résultats montrent que notre méthode est en générale la plus performante."
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"Dans ce papier, nous proposons un algorithme de recherche locale stochastique (SLS) pour résoudre le problème de la détermination du gagnant (PDG) dans les enchères combinatoires. Des expérimentations numériques sont réalisées sur des benchmarks de diverses tailles dans le but de tester et de prouver l'efficacité de notre approche. Les résultats trouvées par la méthode SLS sont nettement meilleurs que ceux fournis par les algorithmes de recherche tabou et Casanova."
"Cette thèse présente la conception, la fabrication et l'étude analytique d'un nouveau concept basé sur la barre de Hiller pour réaliser la commande du rotor sans plateau cyclique. Les pales sont couplées à des palettes. Un aileron, commandé par des actionneurs piezo-électriques, est situé derrière le bord de fuite de chaque palette. L'aileron est incliné par l'actionneur, et génère ainsi une portance. Le moment en pas de la palette change ainsi que le pas, la portance et le battement de la palette. L'angle de battement de la palette et l'angle de pas de la pale étant couplés, ce dernier varie. Chaque ensemble {pale palette aileron} est indépendant d'un autre. La palette peut donc générer du pas collectif et cyclique en entrée de la pale. Comparé aux rotors conventionnels, un tel système présente divers avantages tels la réduction de la complexité mécanique, de la traînée et du poids. La consommation en fuel de l'hélicoptère devrait donc décroître fortement et la disponibilité de l'aéronef augmenter. Un hélicoptère de modélisme a été piloté en milieu extérieur et sert à valider le concept de couplage. Le système a pu maintenir un vol stationnaire stable malgré la présence de vent. Une analyse comprenant la dynamique de l'aileron et quatre degrés de liberté est développée pour évaluer le comportement dynamique et apprécier la faisabilité d'un tel concept de rotor sans plateau cyclique. L'analyse est utilisée pour investiguer l'effet des paramètres du système sur l'influence que la palette et l'aileron peuvent exercer sur la pale. Des tests en stationnaire ont été réalisés sur un banc rotor principal qui représente un environnement plus maîtrisé. Le but de ces tests est de valider l'étude théorique et d'évaluer l'effet de différentes variables de conception sur la réponse en pas de la pale. Pour ce faire, le système est équipé de capteurs. Dans le cas des essais en vol comme au banc rotor principal, la commande en pas de la palette est réalisée par de petits plateaux cycliques assurant une conception rapide, simple et peu coûteuse. Le reste du système est inchangé. Pour une palette d'envergure égale à 40% du rayon de la pale, avec un pas cyclique de g o, un angle de pas cyclique de pale de 5° a été obtenu."
no abstract
"Nous présentons un outil effectif de traduction automatique d'une grammaire de propriété en un problème de configuration sous contraintes. Un template Latex (utilisant les AVMs1 standards) est utilisé pour spécifier la grammaire et le générateur prend en entrée les fichiers Latex utilisés pour la documenter. Ainsi, notre contribution propose à la fois une validation syntaxique des grammaires de propriété (l'outil détecte les inconsistances dans la grammaire au niveau de la formulation) et une manière de vérifier la correction et la consistance de la grammaire (en détectant les erreurs et/ou les étiquetages manquant des phrases de test). L'approche pourrait se voir étendue à d'autres théories linguistiques basées sur les contraintes."
no abstract
no abstract
no abstract
no abstract
no abstract
"Basé sur le concept d'actants (Latour) et d'espace lisse et strié (Deleuze et Guattari), l'hypothèse serait que l'espace d'actants ouvrirait des possibles en terme d'échanges et de création d'action. Parmi ces actants, la présence du 5ème écran mobile (Scakan 1) participe de façon notable à l'action mise en oeuvre au sein de l'espace d'actants et représente une forme d'empowerment (Rappoport, 1984). Nous verrons l'application de cette analyse conceptuelle dans une situation significativement soutenue par des organisations internationales en vue de la mise en oeuvre d'un projet en faveur de populations socialement et géographiquement isolées du Sud Liban. Dans cet espace agricole, le sol planté d'oliviers et jonché de mines à défragmentation, est resté potentiellement « risqué ». Quel espace d'actants permet-il aux populations concernées d'obtenir une «expansion de liberté de choix et d'action » (World Bank 2002, p.11) au sein de cet espace? Serait-ce l'espace créé qui met en route l'action ou est-ce l'action qui détermine l'espace créé?"
no abstract
no abstract
"Ce papier présente un schéma générique d'algorithmes énumératifs pour la résolution de CSP. Ce schéma exploite des propriétés sémantiques et topologiques du réseau de contraintes afin de produire des goods et des nogoods. Il repose sur un ensemble de séparateurs du graphe de contraintes et plusieurs fonctions et procédures paramétrables de sorte à exploiter des heuristiques, des méthodes de filtrage, des techniques de retour en arrière intelligent, d'enregistrement de nogoods classiques ou de (no)goods structurels et des bornes de complexité théorique héritées des méthodes basées sur les décompositions de graphes. Selon les choix effectués, nous obtenons une famille d'algorithmes dont la complexité en temps est comprise entre $O(exp(w+1))$ et $O(exp(n))$ avec $w$ la largeur d'arbre du graphe de contraintes et $n$ le nombre de variables."
"Radar Imaging using SAR systems provides specific information that is very useful in the frame of “Digital Earth ” applications (i.e. flood supervision, forestry or agriculture watch,). The main interest of such active systems is their capability to gather relevant data whatever the weather and the illumination conditions may be (cloudy, misty, during the night,). In addition, these systems give a useful “distance map ” thanks to the wave coherence. Most applications require a follow-up of the situation during weeks or months. Such a follow-up can only be performed if we are able to register images captured at different times. This registration problem is a very classical one and has been widely studied in Remote Sensing, but the proposed solutions are often dedicated to specific contexts (sensors, type of scenes, known relevant elements).Many algorithms have been proposed to register SAR images, and we give, in this paper, a global overview of these methods depending on the chosen approach. They may use filtering or not prior to registration, and they may use landmarks or not; but, in all cases, there will be to take into account the speckle that reduces the efficiency of classical methods for extracting features (e.g. landmarks,) to be paired in both images. During the last years (since 2000), a new set of methods, related to the Hough Transform concept, have been proposed: the algorithm we introduce in this communication can be considered as being in this class of approaches."
no abstract
"La plupart des méthodes de recherche locale pour le problème de satisfaisabilité traitent une interprétation complète et souvent inconsistante des variables du problème, et essaient de la réparer en changeant la valeur de vérité de certaines variables jusqu'à atteindre une solution. Nous proposons une nouvelle méthode de recherche locale qui gère des interprétations incomplètes, mais toujours consistantes, au lieu de complètes et inconsistantes. Cette méthode tente de prolonger l'interprétation partielle courante comme le ferait une méthode complète. Cependant, au lieu de déclencher un retour arrière (backtrack) lorsqu'un conflit sur vient, elle libère au moins une variable impliqué dans chaque clause falsifiée a in de restaurer la consistance. Ainsi, le voisinage exploré est toujours consistant alors que ce n'est pas le cas pour les algorithmes de recherche locale classiques. Notre méthode peut aussi tirer profit de certaines techniques efficaces issues des méthodes complètes comme la propagation unitaire et les heuristiques de choix de variables. Les résultats expérimentaux montrent la compétitivité de notre méthode par rapport à d'autres méthodes de recherche locale."
"Dans ce papier, nous introduisons les cadres argumentatifs avec nécessités (CANs), une extension des cadres argumentatifs de Dung (CAs) qui prend en compte une relation de nécessité comme sorte de support entre arguments (un argument est nécessaire pour un autre). Nous redéfinissons les sémantiques d'acceptabilité pour ces cadres étendus et nous montrons que la relation de nécessité assure une correspondance directe et immédiate entre un fragment des programmes logiques (PLs) et les CANs. Nous introduisons ensuite une généralisation des CANs qui étend la relation de nécessité pour lui permettre de porter sur des ensembles d'arguments. Nous présentons une adaptation naturelle des sémantiques d'acceptabilité à ce nouveau contexte et nous montrons que ce cadre généralisé permet de capter des PLs arbitraires."
"2 LSIS-Aix-Marseille Université patrice.bellot@lsis.org RÉSUMÉ. Nous proposons dans cet article une méthode non supervisée pour l'identification et la modélisation de concepts associés à une recherche d'information. Nous utilisons l'alloca-tion de Dirichlet latente (LDA), un modèle génératif probabiliste, pour détecter les concepts implicites de la requête en utilisant les documents obtenus par un processus de retour de perti-nence simulé (ou documents de feedback). Notre approche estime automatiquement le nombre de concepts ainsi que le nombre de documents de feedback sans aucun apprentissage préalable ni paramétrage. Les concepts implicites sont pondérés afin de refléter leur importance relative par rapport à la requête et sont utilisés pour modifier l'ordre des documents renvoyés à l'utili-sateur. Nous utilisons quatre sources d'information générales de natures différentes (web, jour-nalistique, encyclopédique) à partir desquelles les documents de feedback sont extraits. Nous comparons différentes approches état-de-l'art sur deux collections ad-hoc de TREC, et les ré-sultats montrent que l'utilisation de concepts implicites identifiés par notre méthode améliore significativement les performances de recherche documentaire. ABSTRACT. In this paper we introduce an unsupervised method for mining and modeling latent search concepts. We use Latent Dirichlet Allocation (LDA), a generative probabilistic topic model, to exhibit highly-specific query-related topics from pseudo-relevant feedback documents. Our approach automatically estimates the number of latent concepts as well as the needed amount of feedback documents, without any prior training step. Latent concepts are then weighted to reflect their relative adequacy and are further used to automatically reformu-late the initial user query. We also explore the use of different types of sources of information for modeling the latent concepts. For this purpose, we use four general sources of information of various nature (web, news, encyclopedic) from which the feedback documents are extracted. We evaluate our approach over two large ad-hoc TREC collections, and results show that it significantly improves document retrieval effectiveness while best results are achieved by combining latent concepts modeled from all available sources. MOTS-CLÉS : Recherche contextuelle, modélisation thématique, retour de pertinence"
"Le cadre général du travail de recherche est l'amélioration de la sécurité routi ère pour les véhicules automobiles et les conducteurs en utilisant les outils automatique. Dans le cadre de la dynamique du véhicule, les e orts d'interaction entre le pneumatique et la chaussée sont des données indispensables d'où la nécessité d'estimer en temps réel les variables qui caractérisent ces e orts. Cette estimation est réalisée à l'aide des observateurs non linéaires tel que les observateurs à mode glissant d'ordre 1 et d'ordre supérieure. Une proposition de découpage de modèle dynamique complet de véhicule en trois sous-systèmes (caisse, suspensions et roues) en se basant sur la théorie de passivité. Tous ces théories sont validées à l'aide de deux simulateur de véhicule SCANeRstudio et SIMK106N."
"La programmation par ensembles r éponses (Answer Set Programming) est un cadre bien étudi é en programmation logique. Plusieurs travaux ont été faits pour d éfinir une s émantique pour les programmes logiques. La plupart de ces s émantiques sont en fait des s émantiques de point fi xe. L'id ée principale est le calcul de mod èles canoniques du programme logique consid ér é, appel és mod èles stables. Les mod èles stables sont dans un certain sens des mod èles minimaux des programmes r éduits. Nous introduisons une nouvelle s emantique pour les programmes logiques, à partir d'une notion d'extension d'une formule propositionnelle classique. Ces extensions peuvent être calcul és de mani ère it érative. Un programme logique est alors cod é par un ensemble de clauses de la logique propositionnelle. On prouve que chaque formule consistante admet au moins une extension et que, pour chaque mod èle stable d'un programme logique, il existe une extension de son codage qui l'implique logiquement. Certaines des extensions ne correspondent pas à un mod èle stable mais sont int eréssantes. Nous donnons une condition discriminante simple qui permet de reconnaitre de telles extensions. En fin, nous d écrivons un algorithme qui calcule les extensions de la formule CNF codant le programme logique. De cet ensemble d'extension on peut extraire les mod èles stables du programme logique initial."
no abstract
"Mettre son CV sur Internet ou offrir des emplois par ce même canal pourrait sembler multiplier les chances de rencontre, ce n'est pas si simple. Internet étant par essence générateur d'un flux continu d'informations, il est indispensable de savoir trier et sélectionner les champs utiles tant pour l'employeur que pour le demandeur d'emploi. Des enquêtes auprès d'entreprises, de recruteurs et d'intermédiaires révèlent que les effets induits par ce nouveau marché excèdent largement les buts recherchés par les partenaires. Transposer une situation de communication (comme celle d'un recrutement lorsque les partenaires sont en face à face) sur Internet crée une configuration inédite qui requiert une analyse particulière. Cette interactivité est génératrice d'éléments contextuels nouveaux souvent difficiles à anticiper mais qu'il faut analyser avec précision."
no abstract
"Dans ce papier, nous avons étudié les conditions suffisantes de détectabilité de défauts multidimensionnels. Pour cela, nous avons établi une expression unifiée de la condition de détectabilité des indices ayant une forme quadratique. Partant de l'idée que la distance combinée est la somme de deux indices de détection (SPE et T2 de Hotelling), nous avons analysé sa performance de détectabilité vis-à-vis de ces deux derniers. Nous avons montré théoriquement que si un défaut est garanti détectable par la distance combinée, il n'est pas forcément garanti détectable par les indices T2 de Hotelling et SPE."
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"RÉSUMÉ L'utilisation de sources externes d'informations pour la recherche documentaire a été considéra-blement étudiée dans le passé. Des améliorations de performances ont été mises en lumière avec des corpus larges ou structurés. Néanmoins, dans ces études les ressources sont souvent utilisées séparément mais rarement combinées. Nous présentons une évaluation de la combinaison de quatre différentes ressources générales, standards et accessibles. Nous utilisons une mesure de distance informative pour extraire les caractéristiques contextuelles des différentes ressources et améliorer la représentation de la requête. Cette évaluation est menée sur une tâche de recherche d'information sur le Web en utilisant le corpus ClueWeb09 et les topics de la piste Web de TREC. Les meilleurs résultats sont obtenus en combinant les quatre ressources, et sont statistiquement significativement supérieurs aux autres approches. ABSTRACT Query Contextualization and Reformulation by Combining External Corpora Improving document retrieval using external sources of information has been extensively studied throughout the past. Improvements with either structured or large corpora have been reported. However, in these studies resources are often used separately and rarely combined together. We present an evaluation of the combination of four different scalable corpora over a web search task. An informative divergence measure is used to extract contextual features from the corpora and improve query representation. We use the ClueWeb09 collection along with TREC's Web Track topics for the purpose of our evaluation. Best results are achieved when combining all four corpora, and are significantly better than the results of other approaches. MOTS-CLÉS : Combinaison de ressources, RI contextuelle, recherche web."
"L'identification des systèmes dynamiques complexes reste une préoccupation lorsque les erreurs de prédictions contiennent des outliers d'innovation. Ils ont pour effet de détériorer le modèle estimé, si le critère d'estimation est mal choisi et mal adapté. Cela a pour conséquences de contaminer la distribution de ces erreurs, laquelle présente des queues épaisses et s'écarte de la distribution normale. Pour résoudre ce problème, il existe une classe d'estimateurs, dits robustes, moins sensibles aux outliers, qui traitent d'une manière plus « douce » la transition entre résidus de niveaux très différents. Les M-estimateurs de Huber font partie de cette classe. Ils sont associés à un mélange des normes L2 et L1, liés à un modèle de distribution gaussienne perturbée, dit gross error model. A partir de ce cadre formel, nous proposons dans cette thèse, un ensemble d'outils d'estimation et de validation de modèles paramétriques linéaires et pseudo-linéaires boîte-noires, avec extension de l'intervalle de bruit dans les petites valeurs de la constante d'accord de la norme de Huber. Nous présentons ainsi les propriétés de convergence du critère d'estimation et de l'estimateur robuste. Nous montrons que l'extension de l'intervalle de bruit réduit la sensibilité du biais de l'estimateur et améliore la robustesse aux points de levage. Pour un type de modèle pseudo-linéaire, il est présenté un nouveau contexte dit L-FTE, avec une nouvelle méthode de détermination de L, dans le but d'établir les linéarisations du gradient et du Hessien du critère d'estimation, ainsi que de la matrice de covariance asymptotique de l'estimateur. De ces relations, une version robuste du critère de validation FPE est établie et nous proposons un nouvel outil d'aide au choix de modèle estimé. Des expérimentations sur des processus simulés et réels sont présentées et analysées.L'identification des systèmes dynamiques complexes reste une préoccupation lorsque les erreurs de prédictions contiennent des outliers d'innovation. Ils ont pour effet de détériorer le modèle estimé, si le critère d'estimation est mal choisi et mal adapté. Cela a pour conséquences de contaminer la distribution de ces erreurs, laquelle présente des queues épaisses et s'écarte de la distribution normale. Pour résoudre ce problème, il existe une classe d'estimateurs, dits robustes, moins sensibles aux outliers, qui traitent d'une manière plus « douce » la transition entre résidus de niveaux très différents. Les M-estimateurs de Huber font partie de cette classe. Ils sont associés à un mélange des normes L2 et L1, liés à un modèle de distribution gaussienne perturbée, dit gross error model. A partir de ce cadre formel, nous proposons dans cette thèse, un ensemble d'outils d'estimation et de validation de modèles paramétriques linéaires et pseudo-linéaires boîte-noires, avec extension de l'intervalle de bruit dans les petites valeurs de la constante d'accord de la norme de Huber. Nous présentons ainsi les propriétés de convergence du critère d'estimation et de l'estimateur robuste. Nous montrons que l'extension de l'intervalle de bruit réduit la sensibilité du biais de l'estimateur et améliore la robustesse aux points de levage. Pour un type de modèle pseudo-linéaire, il est présenté un nouveau contexte dit L-FTE, avec une nouvelle méthode de détermination de L, dans le but d'établir les linéarisations du gradient et du Hessien du critère d'estimation, ainsi que de la matrice de covariance asymptotique de l'estimateur. De ces relations, une version robuste du critère de validation FPE est établie et nous proposons un nouvel outil d'aide au choix de modèle estimé. Des expérimentations sur des processus simulés et réels sont présentées et analysées."
no abstract
no abstract
Ce papier présente une nouvelle approche pour la description de grammaires du langage naturel et pour leur utilisation lors de l'analyse. Nous définissons un modèle objet contraint qui peut être étendu pour implémenter les formalismes grammaticaux et être ensuite exploité par un configurateur pour analyser syntaxiquement des phrases. Le cadre général se nomme Grammaires de Configuration et peut être utilisé pour la description de grammaires de dépendences comme pour celle de grammaires syntagmatiques. Nous proposons un exemple détaillé de cette application à un formalisme génératif à base de contraintes nommé les grammaires de propriétés.
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"Après avoir exposé toute la complexité des processus de production dans l'industrie du semi conducteur, les auteurs montrent comment la simulation distribuée peut apporter une aide au pilotage des installations. L'approche proposée tend à s'appuyer sur les capacités de différents cadres de modélisation formelle pour décrire les processus opérationnels et les processus décisionnels, à l'aide d'éditeurs graphiques adéquats. Ensuite, ces différents modèles de processus sont transformés en modèles G-DEVS (Generalized Discrete EVent Specification) interconnectés. Enfin, tous ces modèles sont exécutés simultanément par simulation dans un environnement HLA (High Level Architecture). Il devient alors possible d'observer le comportement de tout ou partie du système de production, et d'en tirer les informations propres à aider à la prise de décision lors du pilotage des installations. Les différentes briques de cette approche seront illustrées via l'exemple simple d'une production dans une usine de ‘Front End'."
no abstract
no abstract
no abstract
Cet article présente une nouvelle approche d'approximation de la bordure négative et de la bordure positive de motifs fréquents. Notre approche s'appuie sur le passage d'une bordure à l'autre par le calcul de traverses minimales d'hypergraphes. Nous proposons alors une nouvelle méthode de génération de traverses minimales approchées reposant sur la réduction d'hypergraphes. Les expérimentations sur différents jeux de données montrent que notre proposition pour approcher les bordures produit des résultats prometteurs.
no abstract
"L'Apprentissage Automatique tire ses racines d'un large champ disciplinaire qui inclut l'Intelligence Artificielle, la Reconnaissance de Formes, les Statistiques ou l'Optimisation. Dès les origines de l'Apprentissage, les questions computationelles et les propriétés en généralisation ont toutes deux été identifiées comme centrales pour la discipline. Tandis que les premières concernent les questions de calculabilité ou de complexité (sur un plan fondamental) ou d'efficacité computationelle (d'un point de vue plus pratique) des systèmes d'apprentissage, les secondes visent a comprendre et caractériser comment les solutions qu'elles fournissent vont se comporter sur de nouvelles données non encore vues. Ces dernières années, l'émergence de jeux de données à grande échelle en Apprentissage Automatique a profondément remanié les principes de la Théorie de l'Apprentissage. En prenant en compte de potentielles contraintes sur le temps d'entraînement, il faut faire face à un compromis plus complexe que ceux qui sont classiquement traités par les Statistiques. Une conséquence directe tient en ce que la mise en place d'algorithmes efficaces (autant en théorie qu'en pratique) capables de tourner sur des jeux de données a grande échelle doivent impérativement prendre en compte les aspects statistiques et computationels de l'Apprentissage de façon conjointe. Cette thèse a pour but de mettre à jour, analyser et exploiter certaines des connections qui existent naturellement entre les aspects statistiques et computationels de l'Apprentissage. Plus précisément, dans une première partie, nous étendons l'analyse en stabilité, qui relie certaines propriétés algorithmiques aux capacités de généralisation des algorithmes d'apprentissage, la matrice de confusion, que nous suggérons comme nouvelle mesure de performance (fine). Dans une seconde partie, nous présentons un nouvelle approche pour apprendre une fonction de régression basée sur les noyaux, où le noyau appris sert directement la tâche de régression, et qui exploite la structure du problème pour offrir une procédure d'optimisation peu coûteuse. Finalement, nous étudions le compromis entre vitesse de convergence et coût computationel lorsque l'on minimise une fonction composite avec des méthodes par gradient-proximal inexact. Dans ce contexte, nous identifions des stratégies d'optimisation qui sont computationellement optimales."
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
Une difficulté intrinsèque à la résolution de problèmes de configuration réside dans l'existence de nombreux isomorphismes structurels dans les solutions. Nous définissons deux procédures de recherche permettant la suppression de grandes portions de l'espace de recherche dont on montre qu'elles ne renferment que des solutions non canoniques. On y parvient grâce à un test en chaque noeud de l'arbre de recherche de complexité temporelle linéaire. Nous présentons des résultats sur un exemple de configuration simple mais représentatif de ce qu'on pourra obtenir sur des problèmes réels.
"Ce rapport d’activité concerne « l’Étude Relative à la Protection de la Zone Centre d’Orléans située en bordure des quais». Il constitue le dernier livrable d’un projet de recherche contractuel établi entre la société Altoo, Aix-­‐Marseille Université (AMU) et le Centre National de la Recherche Scientifique (CNRS). Le Plan Loire Grandeur Nature est l’organisme financeur de ce projet. L’objet de ce projet de recherche consistait à établir une méthode générique (la méthode Indigoo) d’évaluation des impacts d’une inondation sur un territoire, en terme de dommages monétaires, et des apports de mesures de protection, en terme de bénéfices ou d’économies monétaires. L’étude qui fait l’objet de ce rapport est destinée à tester et valider la méthode Indigoo. Une étude similaire a été réalisée sur le territoire de la ville de Tarascon. Ce travail a été mené de façon conjointe à une étude technique qui a permis de caractériser expérimentalement les performances du système Altoo en présence de contraintes hydrodynamiques. Les modèles expérimentaux issus de cette étude ont servis de base à la construction d’un simulateur de scénarii d’inondation. Les résultats de ces simulations sont intégrés à Indigoo en vue d’évaluer l’apport du système Altoo."
"Les managers sont des témoins et des acteurs privilégiés de l'évolution des entreprises. Cet article a pour objectif de montrer leurs représentations du sacré à travers le témoignage de leurs expériences et de leurs pratiques quotidiennes du management. Dans une perspective volontairement très empirique, nous avons effectué des entretiens auprès de managers et d'autres personnes proches de la sphère organisationnelle des entreprises. Cet article propose une synthèse des entretiens réalisés selon cinq thèmes : sacré et valeurs morales, les managers : leurs valeurs et leurs rôles pour leurs collaborateurs, la dimension implicite du sacré et du religieux, les rôles du manager pour le mouvement des Entrepreneurs et Dirigeants Chrétiens, les managers des entreprises participent-ils à une évolution du sacré ?"
no abstract
"Le problème SAT est connu pour être le premier problème de décision de classe NP-complet (Cook,71). C'est un problème central dans la théorie de la complexité. Dans la dernière décennie, les procédures prouvant la satisfiabilité ont été améliorées par l'élimination de la symétrie. Une formule CNF contient usuellement un nombre intéressant de symétries. Il y a deux types d'exploitions des symétries. La première correspond à l'élimination des symétries globales, c'est à dire, seules les symétries initiales du problème (le problème à la racine de l'arbre de recherche) sont détectées et éliminées. La seconde exploite toutes les symétries locales qui apparaissent à chaque noeud de l'arbre de recherche. Les symétries locales doivent être détectées et éliminées dynamiquement durant la recherche. Exploiter ce genre de symétrie semble être une tâche difficile. Quasiment tous les travaux sur l'exploitation de la symétrie dans le problème de la satisfiabilité traitent uniquement le cas des symétries globales. En dépit de leur importance en pratique, seuls quelques travaux étudient les symétries locales. Détecter et éliminer efficacement les symétries locales durant la recherche est un challenge important. Le travail que nous présentons ici est une contribution pour répondre à ce difficile challenge. Nous présentons une nouvelle méthode pour l'élimination des symétries locales qui consiste à réduire l'instance partielle SAT, non encore résolue correspondante à chaque noeud de l'arbre de recherche, à un graphe dont le groupe d'automorphismes est équivalent au groupe de symétries de l'instance partielle SAT. Nous avons utilisé l'outil Saucy pour le calcul du groupe d'automorphisme et nous avons implémenté une technique de coupure de symétrie dans un solveur SAT. Nous avons expérimenté cette méthode sur plusieurs instances SAT et nous l'avons comparé à une méthode qui exploite les symétries globales. Les résultats obtenus sont prometteurs. L'exploitation des symétries locales améliore l'exploitation des seules symétries globales dans la résolution de nombreux problèmes difficiles et elles leurs sont complémentaires si nous combinons les deux techniques."
"Le problème de livraisons jointes de produits (JDP) consiste à planifier les livraisons de différents produits à différents sites de consommation ou de distribution en traitant les problèmes de groupement, de livraison et de stockage. Il s’agit de construire des tournées de livraison sur un horizon de planification, en satisfaisant les demandes et en minimisant le coût total de commande, de livraison et de stockage. Les coûts fixes de commande portent d’une part sur le lancement d’une tournée, d’autre part sur chaque couple (produit, site) présent ou non dans la tournée. Les taux de demandes étant supposées fixes et connus, le problème en horizon infini admet une solution périodique, le plan de livraison optimal sur une période-type pouvant se répéter indéfiniment. Dans notre approche, le problème est formulé en temps discret et nous choisissons comme période-type commune de cyclicité un multiple de la période élémentaire, et cette période-type sert d’horizon de planification. Ainsi, les livraisons restent périodiques à travers la répétition de l’horizon de planification, mais les livraisons pendant l’horizon de planification ne sont pas contraintes à être périodiques. Les résultats numériques montrent en particulier la supériorité de cette approche sur une solution cyclique pour chaque couple (produit, site)."
"La prise de décision collective conduit à l'interaction de plusieurs agents afin d'élaborer une décision commune cohérente. D'un point de vue informatique, ce problème peut se ramener à celui de la fusion de différentes sources d'informations. Dans le domaine de la représentation des connaissances pour l'intelligence artificielle, plusieurs approches ont été proposées pour la fusion de bases de croyances propositionnelles, cependant, la plupart d'entre elles l'ont été sur un plan sémantique et sont peu utilisables en pratique. Ce papier propose une nouvelle approche syntaxique pour la fusion de bases de croyances, appelée Fusion par Rensembles (ou RSF). La notion de R-ensemble, initialement définie dans le contexte de la révision de croyances, est étendue à la fusion et la plupart des opérations classiques de fusion sont capturées syntaxiquement par RSF. Afin d'implanter efficacement RSF, ce papier montre comment RSF peut être codé en un programme logique avec sémantique des modèles stables, puis présente une adaptation du système Smodels permettant de calculer efficacement les R-ensembles. Finalement, une étude expérimentale préliminaire montre que la mise en œuvre utilisant la programmation logique avec sémantique des modèles stables semble prometteuse pour réaliser la fusion de bases de croyances sur des applications réelles. Collective decision making leads to interaction between agents in order to elaborate a consistent common decision. From a data-processing point of view, this problem can be brought back to the merging of different sources of information. In knowledge representation for artificial intelligence, several approaches have been proposed for propositional bases fusion, however, most of them are de- paper proposes a new syntactic approach of belief bases fusion, called Removed Sets Fusion (RSF). The notion of removed-set, initially defined in the context of belief revision is extended to fusion and most of the classical fusion operations are syntactically captured by RSF. In order to efficiently implement RSF, the paper shows how RSF can be encoded into a logic program with answer set semantics, then presents an adaptation of the smodels system devoted to efficiently compute the removed sets in order to perform RSF. Finally a preliminary experimental study shows that the answer set programming approach seems promising for performing belief bases fusion on real scale applications."
"Cet article porte sur la caractérisation d'une mesure de similarité entre sous-parties de nuages de points 2D. Cette mesure est définie à partir d'une hypothèse généralement vérifiée sur des nuages de points issus de cas réels: ceux-ci possèdent des groupes de points qui s'organisent en structures linéiques, et qui apparaissent en même temps dans les différents nuages. Après avoir défini des primitives qui permettent une représentation unifiée de ces structures, nous montrons le lien qui existe entre la présence d'une information commune entre sous-nuages et la distribution des relations géométriques entre leurs primitives. Nous donnons alors une mesure de similarité invariante par rotation, ainsi qu'un algorithme permettant de la calculer."
no abstract
no abstract
"Les surfaces complexes ont des applications dans divers domaines tels que ceux de la photonique, de l'énergie, du biomédical, du transport... Par contre, elles posent de véritables défis quant à leur spécification, fabrication et mesure ainsi que lors de l'évaluation de leur défaut de forme. Les processus de fabrication et de mesure de surfaces complexes sont fortement tributaires des dimensions, des tolérances et des formes spécifiées. Afin de rendre exploitable les informations données par le système de mesure, une étape importante de traitement s'impose. Il s'agit ici de la reconstruction de surfaces afin de reconstituer la géométrie et la topologie de la surface sous-jacente et d'en extraire les informations nécessaires pour des besoins de métrologie dimensionnelle (caractéristiques dimensionnelles et évaluation des défauts de forme). Dans la catégorie des surfaces asphériques pour lesquelles un modèle mathématique est associé, le processus de traitement de données géométriques, non nécessairement organisées, se fait par l'association du modèle aux données. Les résidus d'association recherchés en optique sont typiquement de l'ordre du nanomètre. Dans ce cadre, nous proposons l'utilisation de l'algorithme L-BFGS qui n'a encore jamais été utilisé en métrologie. Ce dernier permet de résoudre des problèmes d'optimisation non-linéaires, sans contraintes et d'une manière robuste, automatique et rapide. La méthode L-BFGS reste efficace pour des données contenant plusieurs millions de points. Dans la catégorie des surfaces gauches et notamment des aubes de turbines, la fabrication, la mesure et le traitement sont à une toute autre échelle, sub-micrométrique. Les surfaces gauches ne sont généralement pas définies par un modèle mathématique mais sont représentées par des modèles paramétriques de type B-Spline et/ou NURBS. Dans ce cadre, nous exposons un état de l'art détaillé et proposons une nouvelle approche itérative d'association B-Spline. L'algorithme s'affranchit de tous les problèmes liés à l'initialisation et au paramétrage initial. Par conséquent, un tel algorithme constitue une nouveauté dans ce domaine. Nous établissons une étude approfondie en évoquant les avantages et les limites actuelles de cette approche sur des exemples de courbes fermées en 2D. Nous complétons ensuite cette étude par des perspectives d'amélioration et de généralisation aux surfaces en 3D."
no abstract
"L'assemblage de structures aéronautiques nécessite de nombreuses opérations de perçage et de fraisurage. Les deux problématiques principales concernant ces opérations sont que les alésages réalisés correspondent aux standards de qualité exigés, et que les outils coupants soient utilisés de manière optimale afin de réduire les coûts. Ces deux objectifs nécessitent l'implémentation d'une solution de surveillance en ligne des opérations de perçage. De nombreuses études ont été réalisées à ce sujet. Pourtant, une grande partie des méthodologies développées ont peu de chance de quitter les laboratoires au profit des sites de production industrielle en raison de leur difficulté d'implémentation et de leur manque de robustesse. L'utilisation de plusieurs capteurs, couplés à des techniques avancées de traitement de l'information a permis une meilleure appréhension de la complexité du procédé de perçage et une augmentation de la flexibilité des systèmes de surveillance. Cependant, la majorité des études ont été réalisées en laboratoire et dans des conditions favorables, et les problématiques relatives à la flexibilité des conditions opératoires, ou encore à la qualité des données issues des capteurs n'ont pas été abordées. Cette étude a pour but de démontrer les améliorations potentielles que peuvent apporter les développements récents concernant la modélisation et la fusion de connaissances imparfaites pour la surveillance robuste des opérations de perçage. Une approche sera proposée pour l'implémentation industrielle de systèmes de surveillance de procédés. La méthodologie proposée doit pouvoir être transposée à un champ d'application plus large incluant la plupart des procédés de fabrication automatisés."
"Les robots industriels représentent un moyen de production sophistiqués pour l'industrie manufacturière d'aujourd'hui. Ces manipulateurs sont plus agiles, plus flexibles et moins coûteux que les machines-outils spécialisées. L'exploitation de ces avantages fait l'objet d'une demande croissante de l'industrie. La dynamique de ces manipulateurs est soumise à des nombreuses sources d'imprécision. En effet les défauts de la chaîne de transmission, ou encore les éléments de liaisons peuvent être le siège de déformations et de vibrations dégradant sensiblement leur précision. Ces phénomènes physiques sont d'autant plus difficiles à compenser que seul un sous ensemble des états du système est mesuré par les codeurs moteurs. La structure de commande industrielle actuelle d'un robot n'agit donc pas directement sur ces phénomènes. Il est nécessaire alors de progresser sur le front de l'amélioration de la précision par l'adaptation de la commande à ces nouvelles exigences. Un état de l'art met en évidence un manque de travaux qui traitent de l'élaboration d'anticipations adaptées aux axes d'un robot et intégrant les phénomènes de déformation. En outre, la planification de trajectoire n'est classiquement pas remise en cause et peu évoquée. Elle représente pourtant un moyen d'action éprouvé afin d'améliorer les performances dynamiques en suivi de profil. L'approche proposée dans ce mémoire se veut une alternative à ces méthodes. Elle est basée sur une exploitation d'un modèle dynamique représentatif et détaillé. Il intègre les principaux phénomènes physiques mis en évidence tels que les effets de la gravité, les systèmes mécaniques de compensation, les forces de frottement et la flexibilité articulaire. Cette modélisation associée à des méthodes d'identification expérimentale est exploitée afin de déduire une structure de commande. Elle permet la réduction des déformations élastiques et des vibrations par une action sur la précommande et sur la loi de mouvement adaptée. Ainsi, nous introduisons une méthode d'estimation non asymptotique appliquée en robotique, afin d'estimer rapidement les paramètres vibratoires de ce dernier et contribue à une réactualisation des modèles exploités. Des résultats expérimentaux montrent que cette méthodologie mène à une amélioration des performances de positionnement par rapport à la commande industrielle."
no abstract
no abstract
"Plusieurs approches exploitant l'élimination des symétries dans la résolution des CSPs sont apparues récemment. La grande majorité de ces méthodes exploitent les symétries globales du problème étudié et ne tente pas d'exploiter les symétries locales. Il a été montré que l'élimination des symétries globales peut être utile dans la résolution des CSPs. Mais exploiter uniquement ces symétries peut ne pas suffire pour résoudre des problèmes difficiles contenant de nombreuses symétries locales. En effet, un problème peut avoir peu ou pas du tout de symétries initiales (globales) et devenir très symétrique à certains noeuds durant la recherche. Dans ce papier, nous étudions le principe général de la symétrie sémantique et on définit la symétrie syntaxique qui est une condition suffisante de la symétrie sémantique. Nous montrons comment la symétrie syntaxique est détectée et éliminée localement pour améliorer l'efficacité des méthodes de résolution de CSPs. Les expérimentations confirment que l'exploitation des symétries locales est profitable dans la résolution des CSPs."
no abstract
no abstract
"En planification on distingue les plans optimaux en le nombre d'étapes (plans parallèles) et les plans optimaux en le nombre d'actions (plans séquentiels). Il est généralement admis que le calcul d'un plan séquentiel est plus couteux que le calcul d'un plan parallèle. Büttner et Rintanen ont proposé une procédure de recherche qui calcule des plans dont le nombre d'étapes est fixé et le nombre d'actions minimal. Cette procédure est utilisée pour le calcul d'un plan séquentiel optimal partant d'un plan parallèle optimal. Nous décrivons dans cet article une approche de ce type, développée à partir du système de planification FDP. L'idée consiste à maintenir deux structures, l'une représentant le plan parallèle et l'autre le plan séquentiel, en répercutant les choix effectués pendant la recherche simultanément dans les deux structures. Les techniques développées dans FDP pour le calcul de plans séquentiels ou de plans parallèles permettent de détecter les échecs dans les deux structures. Les résultats expérimentaux montrent que cette approche est très compétitive comparée au calcul de plans séquentiels optimaux obtenus avec FDP."
"Étant donné une formule booléenne mise sous forme normale conjonctive (CNF), le problème de satisfiabilité XSAT, une variante du problème de satisfiabilité (SAT), consiste à trouver une interprétation des variables pour laquelle chaque clause est satisfaite par exactement un littéral. Le meilleur algorithme pour résoudre ce problème possède une complexité en temps en (<i>O</i>)(2<sup>0:2325n</sup>) ((<i>O</i>)(2<sup>0:1379n</sup>) pour X3SAT) [12]. Une autre possibilité pour résoudre ce problème consiste à transformer chaque clause dans un ensemble de clauses équivalentes pour le problème de satisfiabilité et d'utiliser des solveurs SAT modernes et puissants (zChaff [14], Berkmin [6], MiniSat [5], etc) pour trouver une telle interprétation. Dans ce papier, nous introduisons un nouveau codage du problème XSAT dans SAT qui contient beaucoup d'informations structurelles (en particulier les chaînes d'équivalences), qui sont cachées dans la transformation habituelle. Certains solveurs (LSAT [15], march_dl [7]) prennent en compte ce type d'informations structurelles pour faire des simplifications en prétraitement et accélérer la résolution. Puis, nous montrons l'intérêt de traiter le problème XSAT en introduisant un codage des CSP binaires et du problème de coloration de graphes sous forme de problèmes XSAT. Les résultats préliminaires sur le problème de coloration de graphes montrent l'importance du raisonnement sur les équivalences pour le problème XSAT."
"Cet article présente un algorithme de recherche locale guidée par l'analyse de conflits pour résoudre le problème SAT. L'usage d'une telle analyse, permettrait d'exploiter les dépendances entre les variables particulièrement présentes dans des instances structurées et d'accroître l'effet de la propagation unitaire. Les premiers résultats expérimentaux sont prometteurs."
no abstract
"Le système de surveillance (HUMS) installé dans les hélicoptères permet d'anticiper les anomalies et de donner la possibilité d'effectuer des tâches de maintenance prédictive avant l'apparition de défauts critiques. Par ailleurs, HUMS est également destiné à détecter la propagation de défauts émergents. Ceci consiste à comparer les caractéristiques vibratoires en vol de l'hélicoptère aux caractéristiques d'un état normal prédéfini. L'inconvénient majeur de cette approche est que les caractéristiques de l'état normal sont relatives au type de l'hélicoptère et changent après les tâches de révision et de maintenance, ce qui nécessite un réapprentissage de ces caractéristiques. Cette étude présente des méthodes d'évaluation de la progression temporelle des signatures vibratoires. L'étude de l'évolution de la signature vibratoire dans le temps permet de détecter des événements comme des interventions de maintenance ou des propagations de défauts sans avoir à définir un modèle de l'état de bon fonctionnement de l'appareil. Des méthodes fondées sur des modèles paramétriques et des bancs de filtres d'analyse vibratoire ont été testées et validées. Finalement, une méthode de détection de défauts a été mise en oeuvre et a donné de meilleurs résultats que les méthodes traditionnelles utilisées."
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"Les méthodes exploitant les décompositions arborescentes pour résoudre des réseaux de contraintes semblent constituer les meilleures approches en termes de complexité théorique en temps. Néanmoins, on peut estimer qu'elles n'ont pas démontré, à ce jour, un véritable intérêt pratique. Aussi, dans cette contribution, nous étudions tout d'abord différentes stratégies d'approximations de décompositions arborescentes optimales, et nous analysons ensuite leur pertinence dans le cadre de la résolution de CSP. Dans une seconde partie, nous étudions le problème du choix de la meilleure stratégie pour le parcours de l'arbre de clusters associé dans le cadre de la résolution du CSP. Les stratégies sont relatives notamment au choix du cluster racine, c'est-à-dire, celui à partir duquel débutera la recherche. Un deuxième aspect concerne l'ordre dans lequel les fils d'un cluster seront visités."
no abstract
no abstract
"Ce travail présente un modèle orienté envers le développement d’un simulateur pour le moteur diesel marin. Dans le but d’avoir un simulateur qui répond aux besoins du moteur étudié au niveau de l’efficacité, la rapidité et la prise en considération des défauts, le modèle étudié est basé sur des modélisations physiques, semi physique, empirique, mathématique et thermodynamique. Le modèle du moteur est divisé en plusieurs sous-modèles dont chacun présente un système réel. Ces systèmes sont : le refroidissement, la lubrification, l’admission de l’air, l’injection, la combustion et les émissions. Les sous-modèles et les caractéristiques dynamiques de chaque bloc sont conçus en respectant les équations principales du fonctionnement du moteur ainsi que les données expérimentales collectées sur un banc d’essai pour un moteur marin diesel fabriqué par la société SIMB sous la référence 6M26SRP1. Ce modèle a été implémenté sur Matlab/Simulink, et la simulation permet d’obtenir les variables suivantes dans les différentes sous-blocs: pression, température, efficacité, échange de chaleur, angle de vilebrequin, débit du fuel et émissions. Le simulateur sera utilisé pour montrer les dégradations dans les performances du moteur lors de l’occurrence des défauts et peut aider dans l’application des stratégies de diagnostic et de pronostic. Les différents défauts considérés dans le simulateur sont : défauts injecteur, fuite cylindre, avarie pompe fuel, casse du segment de piston, dégradation de turbocompresseur, dégradation filtre à air, dégradation refroidisseur d’air, fuite d’air, fuite d’eau, fuite et contamination d’huile, encrassement des échangeurs, dégradations pompes."
no abstract
"Cette thèse est une contribution à la réduction de la consommation d’énergie primaire et à une meilleure utilisation des sources d’énergie renouvelables dans le cadre des systèmes de refroidissement utilisés dans le bâtiment. Après un état de l’art sur les systèmes de refroidissement, un modèle dynamique d’un système de rafraîchissement solaire à base de machine à absorption est développé et simulé. Ensuite, un facteur d’efficacité (EF) pour comparer la pertinence de ce système dans différentes régions du monde est défini. Dans la troisième partie, la notion des systèmes de refroidissement hybride -une méthode efficace contribuant à la réduction de la consommation d’énergie primaire- est présentée. Puis, les systèmes hybrides de refroidissement sont classés en catégories et sont comparés avec les systèmes de refroidissement individuels. Ensuite, un schéma permettant de sélectionner le meilleur système de refroidissement hybride dans des conditions données est proposé. Dans la dernière partie, une méthode de dimensionnement d’un système hybride est établie. Cette approche est fondée sur des critères économiques et vise une optimisation des énergies renouvelables disponibles localement. Ainsi, le dimensionnement est réalisé en tenant compte de la région spécifique d’utilisation. Pour ce faire, un système de refroidissement hybride, conçu pour une maison standard, est modélisé puis simulé en utilisant le logiciel Trnsys. Finalement, et pour illustrer la méthode proposée, la problématique de dimensionnement est considérée pour deux régions différentes du globe ; à savoir Marseille-France et Beyrouth-Liban. Le but est d’évaluer les performances de la méthode, à travers des données effectives, pour diverses conditions climatiques, prix des composants et tarif d’électricité."
"En vue, d’introduire un nouveau produit dans une usine existante, plusieurs décisions concernant les évolutions possibles du système de production sont à prendre. Ces changements sont de plus en plus fréquents et nécessitent de reconfigurer aussi souvent le système, ce qui constitue de véritables perturbations durant son cycle de vie. Dans la phase avant-projet d’industrialisation, différentes solutions process sont à évaluer en termes de coût d’investissement, de délai de mise en œuvre, de qualité et de flexibilité du système de production. Les objectifs de conception sont multiples et parfois conflictuels. Ce qui rend difficile l’obtention d’un consensus lors de l’évaluation des solutions. De plus, il est difficile d’avoir une vision partagée de l’impact des modifications sur la configuration initiale d’une ligne de production. Les décisions prises dans cette phase ont un impact considérable sur les autres phases du projet (Développement, intégration, démarrage). Dans le but d’améliorer ces décisions, cette thèse se propose dans un premier temps, d’étudier les processus de reconfiguration d’une ligne de production afin d’identifier les solutions et leurs impacts sur la configuration initiale de la ligne, ensuite, d’identifier et d’organiser les critères de performances pertinents afin d’évaluer les solutions par une approche multicritère dans les phases amont du projet, enfin, d’élaborer un processus de revue numérique d’atelier afin d’identifier au plus tôt les problèmes et de valider les solutions retenues à différents jalons du projet."
"Cette thèse présente l’étude d’un problème de contrôle optimal dont le coût est non-différentiable pourcertaines valeurs du contrôle ou de l’état, tout en restant Lipschitz. Ce problème nous a été inspiré par laproblématique générale de la minimisation de l’énergie dépensée par un véhicule ou robot de type voiture lelong d’un trajet dont le profil de route est connu à l’avance. Cette problématique est formulée à l’aide d’unmodèle simple de la dynamique longitudinale du véhicule et une fonction coût qui englobe la notiond’efficacité du processus de conversion énergétique. Nous présentons un résultat de régularité des contrôles,valable pour la classe des systèmes non-linéaires, affines dans les contrôles, classe à laquelle appartient notreproblème. Ce résultat nous permet d’exclure les phénomènes de chattering de l’ensemble des solutions. Nousréalisons trois études de cas pour lesquelles les trajectoires optimales sont composées d’arcs bang,d’inactivations, d’arcs singuliers et, dans certains cas, de retours en arrière."
"Un aéronef à voilure tournante est un système physique dynamique complexe. Le développement de ce type de système nécessite méthodes d’analyse (structurelle et comportementale) et de commande afin de maîtriser ses comportements. L’approche énergétique (bond graph et formalisme hamiltonien à port) permet une représentation multi-physique non linéaire, modulaire (acausale) et à différents niveaux de granularité. Parmi ses organes, les commandes de vol de l’aéronef permettent la transmission du pilotage aux rotors : canaliser la puissance motrice (2 MW) à partir d’une commande manuelle est impossible sans organes actifs d’assistance. Afin de représenter les cheminements et traitements des informations nécessaires aux organes actifs, la représentation multi-physique est complétée par une représentation informationnelle causale (schéma bloc).Les travaux exposés dans ce mémoire visent à ajouter le niveau de granularité intermédiaire et nécessaire entre la représentation multi-physique pure et une représentation combinée physique et informationnelle. Basée sur la démarche du PMBC (Physical Model Based Control), ils proposent une méthode originale permettant de représenter les organes d’assistance et leur commande par un modèle physique équivalent. La méthode est ici enrichie dans une démarche de conception des Systèmes d’Assistance à Opérateur : nous déterminons où doivent agir les organes actifs, selon quelles mesures et suivant quelles lois de commande. La méthode est illustrée sur un cas d’étude industriel : nous obtenons deux représentations de l’espace des solutions (les représentations physico-informationnelle détaillée et globale de son comportement) incluant la solution industrielle actuelle."
no abstract
no abstract
"Aujourd’hui, sur le marché, on peut trouver une vaste gamme de produits différents ou des formes variées d’un même produit et ce grand assortiment fatigue les clients. Il est clair que la décision des clients d’acheter un produit dépend de l'aspect esthétique de la forme du produit et de l’affection émotionnelle. Par conséquent, il est très important de comprendre les propriétés esthétiques et de les adopter dans la conception du produit, dès le début. L'objectif de cette thèse est de proposer un cadre générique pour la cartographie des propriétés esthétiques des formes gauches en 3D en façon d'être en mesure d’extraire des règles de classification esthétiques et des propriétés géométriques associées. L'élément clé du cadre proposé est l'application des méthodologies de l’Exploration des données (Data Mining) et des Techniques d’apprentissage automatiques (Machine Learning Techniques) dans la cartographie des propriétés esthétiques des formes. L'application du cadre est d'étudier s’il y a une opinion commune pour la planéité perçu de la part des concepteurs non-professionnels. Le but de ce cadre n'est pas seulement d’établir une structure pour repérer des propriétés esthétiques des formes gauches, mais aussi pour être utilisé comme un chemin guidé pour l’identification d’une cartographie entre les sémantiques et les formes gauches différentes. L'objectif à long terme de ce travail est de définir une méthodologie pour intégrer efficacement le concept de l’Ingénierie affective (c.à.d. Affective Engineering) dans le design industriel."
"Model generation operations are important artifacts in MDE applications. These approaches can be used for model verification, model finding, and others. In many scenarios, model transformations can as well be represented by a model generation operation. This often comes with the advantage of being bidi- rectional and supporting increments. However, most part of model generation approaches do not target several operation kinds, but narrower scenarios by mapping the generation problem into solver specific problems. They are efficient, but often don’t have a supporting framework. In this paper, we present an approach and framework that allows to specify and to execute model operations that can be represented in terms of model generation operations. We first introduce a model search layer that can be used with different solvers. We illustrate this layer with a driving example implemented using Alloy/SAT solver. On top of this, we introduce a transformation layer, which specification are translated into the model search layer, independently from any solver. The solution is natively bidirectional, incremental and it is not restricted to one-and-one scenarios. The approach is illustrated by two use cases and with 3 different scenarios, backed by a full, extensible and free implementation."
"L’étude du comportement collectif d’un grand nombre d’agents en interaction, souvent dénommé “foule”, a suscité un grand intérêt de la part des communautés scientifiques. Ce sujet de recherche touche aussi bien le Génie civil (évacuation de bâtiments et problèmes du trafic routier), la Robotique (coordination de robots volants), l’Informatique et la Sociologie (réseaux sociaux), que la Biologie (groupes, troupeaux et vols d’oiseaux). Des difficultés majeures sont intimement liées à l’utilisation de ces modèles. En effet, d’un point de vue théorique, la présence d’un grand nombre d’agents rend les outils classiques de l’analyse mathématique peu utiles, car l’espace d’état est de très grande dimension. De plus, pour les foules humaines ou d’animaux, la dynamique de chaque agent n’est pas clairement identifiée, car elle est très sensible aux facteurs intérieurs et extérieurs (comme le stress, la panique, la présence d’obstacles). C’est pour cela que, stimulés par les multiples défis théoriques et applicatifs, de nombreux chercheurs travaillent sur les modèles de foules. La première question qu’il est nécessaire de se poser, en ce contexte, concerne le choix d’un cadre mathématique pour la description de la dynamique des agents. La population peut se décrire de trois façons : microscopique, macroscopique ou multiéchelle. Dans l’approche microscopique, la foule est représentée par la position de chaque agent, et sa dynamique est un système d’équations aux dérivées ordinaires de dimension très grande. Dans l’approche macroscopique, la foule est donnée par la densité d’agents, et sa dynamique est une équation aux dérivées partielles (EDP dans la suite), souvent de type transport. Dans l’approche “multi-échelle”, dite aussi “granulaire”, la population se compose tout autant d’une partie microscopique d’agents “significatifs” (tels que les leaders) que d’une partie macroscopique pour le reste de la foule. Dans cette troisième approche, dans laquelle je développe la plupart de mes recherches, les mesures sont l’outil principal utilisé. La Section 1 présente mes résultats d’analyse dans ce contexte. Je m’intéresse à une classe particulière d’EDP pour la dynamique des mesures. Cette classe d’équations de transport avec vitesses non-locales est au coeur des modèles pour les foules. En effet, chaque agent dans une foule est en interaction avec ses voisins, engendrant ainsi une dynamique qui ne dépend pas seulement de sa position (terme locale), mais aussi des positions des autres (terme non-local). En Section 1.1, j’expose d’abord un cadre assez rigoureux et riche dans lequel les équations de transport avec vitesses non-locales ont de bonnes propriétés : existence et unicité de la solution, dépendance continue, etc. Puis, j’étudie certains schémas numériques pour ces équations, décrits en Section 1.2, et je démontre leur convergence. Je définis ensuite en Section 1.3 une généralisation de la distance de Wasserstein aux mesures de masse variable. Pour cette distance, je prouve des propriétés intéressantes en lien avec l’EDP de transport avec source, et notamment une généralisation de la formule de Benamou-Brenier. Enfin, je présente en Section 1.4 certains résultats spécifiques pour l’équation de transport dans un cadre non-lisse, qui est utilisée dans des modèles de trafic routier. Au-delà de la description et de l’analyse du comportement collectif, il est intéressant de se demander quels changements un agent extérieur – un gouvernement régulateur ou des leaders, par exemple – peut induire sur une foule. La plupart des recherches dans ce domaine ont été consacrées à la création de structures ou de règles efficientes, avec un point de vue statique. Aujourd’hui cependant, ce point de vue est remis en question par une vision dynamique et variable dans le temps. On assiste à un changement de paradigme : d’un problème d’optimisation statique on passe à un problème de commande, dépendant du temps et des configurations. Si l’on considère les problèmes d’évacuation, par exemple, avec un point de vue statique, l’infrastructure est conçue dans une configuration donnée et elle ne peut subir aucune modification. Avec le nouveau paradigme dynamique, quand une sortie de secours est congestionnée, on peut introduire des signaux lumineux ou des dispositifs portables pour envoyer la foule vers la direction la plus convenable. Cette approche introduit le problème de la commande des foules : on souhaite comprendre quels changements de comportement peuvent être produits sur la foule par des leaders ou par un régulateur extérieur. Ces problèmes ont déjà été étudiés en automatique pour la coordination d’agents dans des situations très variées, comme les formations de drones en vol, le routing dans les réseaux de télécommunication, les problèmes d’énergie avec les “smart grids”. Dans ce cadre très général, j’ai travaillé à la commande de l’équation de transport avec vitesse non-locale. La Section 2 en présente les résultats. Je discute d’abord en Section 2.1 d’un problème conceptuel pour la commande des foules. Pour passer d’un modèle microscopique à un modèle macroscopique, l’indistinguabilité des agents est nécessaire : cette propriété s’oppose au fait que les commandes agissent normalement sur des agents précis. Pour cette raison, je présente des problèmes de commande dans deux cas particuliers. En Section 2.2, je montre la commande du modèle de champ moyen de Cucker et Smale vers une configuration d’alignement. C’est l’un des rares résultats en littérature de commande de l’équation de transport avec vitesse non-locale, et le seul avec commande localisée. En Section 2.3, j’étudie un problème de commande optimale où la dynamique est donnée par le couplage d’un système contrôlé pour des leaders avec une EDP de transport pour le reste de la foule. Le résultat principal est la généralisation du Principe de Maximum de Pontryaguine à ce problème de mesures, dans lequel l’équation de Hamilton est écrite comme un gradient de Wasserstein. Ce mémoire contient aussi plusieurs autres résultats de recherche dans le domaine de l’automatique, de la commande et de l’analyse des EDP. La Section 3 les décrit plus brièvement. Ces résultats sont assez indépendants par rapport aux sujets présentés dans les sections précédentes. Il est à noter, cependant, que les instruments de recherche utilisés dans les trois sections relèvent tous de la commande géométrique. Je focalise également mon attention sur les modèles d’EDP, et en particulier sur les méthodes de limite permettant de passer d’un système en dimension finie à une EDP associée. Voire des exemples en Sections 1.1, 2, 3.1 et 3.2. Enfin, un CV détaillé est présenté en Section 4."
"Dans les messageries sociales les emojis sont parmi les principaux vecteurs d'émo-tions et de sentiments des individus. Aujourd'hui, les utilisateurs naviguent dans des biblio-thèques contenant souvent des milliers d'emojis pour sélectionner celui correspondant à ce qu'ils souhaitent transmettre. Nos travaux visent à développer un système de recommandation automatique d'emoji permettant à l'utilisateur d'identifier un panel réduit d'emojis pertinents étant donnée sa conversation en évitant le parcours de bibliothèques conséquentes d'emojis. Cette recommandation pouvant permettre à l'utilisateur de requêter les phrases susceptibles de contenir cet emoji, et l'émotion qui y est associée. Pour ce faire, dans un premier temps, notre objectif est de développer un outil permettant de prédire automatiquement les emojis d'une phrase à partir d'un modèle de classification appris sur un corpus de messagerie sociale conte-nant des emojis. Plusieurs caractéristiques sont considérées pour l'apprentissage telles que le sentiment de l'utilisateur mais aussi son humeur. Dans cet article, nous décrivons l'impact de ces caractéristiques et les performances des modèles résultants. ABSTRACT. Emojis are among the main carriers of emotions and sentiment in social messaging applications. Nowadays users have to scroll down libraries of thousands of emojis in order to select the one they wanted to use. Our work aims to build an emoji automatic recommendation system to avoid scrolling emoji libraries. And which will allow the user to request emojis by the current sentence based on the emotion it conveys. To do so, we first contribute by building an emoji automatic prediction in sentences based on a classification model. This classification model is learned on an informal text messages corpus based on real data containing emojis. Several features are used to train the classifier. Such as the sentiment value of the text and the user's mood. In this paper we describe the features and models impact on the emoji prediction task. MOTS-CLÉS : Classification multi-étiquette, recommandation d'emoji, analyse de sentiment."
no abstract
"Cette thèse s’inscrit dans le cadre de conception d’une stratégie de gestion de l’énergie dans un système hybride de génération d'énergie électrique composé d’une pile à combustible (PC) et un module de supercondensateurs (SC). La source hybride fournit une puissance maximale de 1,2 kW et sa conception implique des décisions concernant la sélection de l’architecture du système hybride ainsi que le choix de la topologie et le dimensionnement d’une unité de convertisseurs. La stratégie de gestion vise à satisfaire la demande d’énergie électrique de la charge et favoriser la consommation énergétique efficiente ; sa performance est évaluée en développant un simulateur qui comprend la dynamique des éléments mis en jeu : deux sources et l’unité de convertisseurs. Le générateur hybride est supposé alimenter un profil de consommation correspondant à un véhicule électrique, de ce fait un cycle standard de conduite en ville en échelle est demandé lors des simulations, ce qui permet d’évaluer la performance du générateur hybride et plus spécifiquement de la stratégie de gestion énergétique.Dans une première étape de cette thèse, un simulateur intégral a été construit avec des librairies de Simscape. Le simulateur est constitué des blocs de différents domaines, contenant des modèles fondamentaux des composants du système. Le block de pile à combustible modèle la dynamique d’un système BAHIA® (400 W - 1100 W, 0 A - 70 A nominale) et le block de supercondensateur modèle les cycles charge-décharge d’un module Maxwell de 400 F et 16 V. Un onduleur de tension pont complet avec convertisseur élévateur conditionne l’énergie délivrée par la pile à combustible et un convertisseur bidirectionnel (buck-boost) est connecté au module de supercondensateurs afin de conditionner les cycles de charge-décharge. L’unité des convertisseurs a été dimensionné, puis, un modèle moyen de petits signaux a été formulé afin de décrire la dynamique de ces dispositifs. Les différents composants ont été intégrés dans l’environnement Simulink. Dans une deuxième étape, la stratégie de gestion énergétique a été conçue en considérant les caractéristiques et performances des sources ; le résultat est une stratégie de trois niveaux hiérarchiques, dont l’aspect principal es la définition des lois de commande locales et globale. Dans une troisième étape, le système complet est évalué en termes du niveau d’utilisation des sources, du domaine d’opération de la pile à combustible, et de l’accomplissement des objectifs des commandes locales et global, qui engagent notamment le SOC des supercondensateurs et la régulation de la tension du générateur hybride."
no abstract
"Avec la dépendance du prix du pétrole et ses variations ainsi que les préoccupations environnementales, les acteurs du transport maritime sont de plus en plus nom-breuxàbreuxà chercher des outils d'aidè a la décision leur per-mettant d'optimiser le temps de trajet en même temps que la consommation de carburant (optimisation bi-objectif). Dans leprobì eme traité, la donnée d'entrée est un itinéraire, et nous nous intéressonsintéressonsà optimiser le temps de trajet et la consommation en fonction d'un unique paramètre : la vitesse le long de cet itinéraire. Nous présentons ainsi une modélisation duprobì eme, puis nous expliquons pourquoi une approche exacte ne semble pas pertinente. Nous proposons ensuite trois mé-thodes de résolution qui semblent adaptées au contexte et qui devront dans un second tempsêtretempsêtre implémentés puis testés afin de juger de leur intérêt pratique."
Une nouvelle méthode de résolution numérique du modèle de poutre géométriquement exact de Reissner-Simo pour la simulation d’assemblage de câbles est présentée dans cet article. Les quaternions sont choisis comme paramètres cinématiques de description des rotations 3D pour leur caractère algébrique. La méthode asymptotique numérique permet une résolution numériquement très efficace des équations polynomiales obtenues. La robustesse de la méthode est illustrée sur des exemples présentant des courbes d’équilibres très complexes.
"La diffusion massive de données géographiques soulève des inquiétudes quant aux risques d'utilisations inappropriées, de méprises ou de mauvaises interprétations. Il se pose donc avec acuité la question des normes de comportement que devrait adopter le producteur en pareilles circonstances. Dans le cas des objets de type bona fide, le producteur serait tenu d'une obligation de résultat quant à la satisfaction des besoins du consommateur. Dans le cas des objets de type fiat, il ne serait tenu qu'à une obligation de moyen assortie toutefois d'une obligation de conseil et de mise en garde ainsi qu'une obligation d'identification et de divulgation des risques. ABSTRACT. The massive diffusion of geographic data raises many concerns with regards to the risks of inappropriate uses, mistakes or bad interpretations. It strongly challenges the questions regarding rules of conduct that producers should adopt in such circumstances. In regard to bona fide objects, the producer should be under a duty of result to satisfy the consumer. In regard to fiat objects, the producer should be only under a duty of means matched nevertheless with a duty of advices and warnings as well as a duty of identification and disclosure of risks. MOTS-CLÉS : données géographiques, incertitude, producteur, norme, obligation légale."
"Introduction Comprendre un texte est un but que l'Intelligence Artificielle (IA) s'est fixé depuis ses débuts et les premiers travaux apportant des réponses ont vu le jour dans les années 70s. Depuis lors, le thème est toujours d'actualité, bien que les buts et méthodes qu'il recouvre aient considérablement évolués. Il est donc nécessaire de regarder de plus près ce qui se cache derrière cette dénomination générale de « compréhension de texte ». Les premiers travaux, qui ont eu lieu du milieu des années 70 jusqu'au milieu des années 80 [Charniak 1972; Dyer 1983; Schank et al. 1977], étudiaient des textes relatant de courtes histoires et comprendre signifiait mettre en évidence les tenants et aboutissants de l'histoire-les sujets traités, les événements décrits, les relations de causalité les reliant-ainsi que le rôle de chaque personnage, ses motivations et ses intentions. La compréhension était vue comme un processus d'inférence visant à expliciter tout l'implicite présent dans un texte en le retrouvant à partir des connaissances sémantiques et pragmatiques dont disposait la machine. Cela présupposait une modélisation préalable de ces connaissances. On rejoint ici les travaux effectués sur les différents formalismes de représentation des connaissances en IA, décrivant d'une part les sens associés aux mots de la langue (réseaux sémantiques vs logique, et notamment graphes conceptuels [Sowa 1984] et d'autre part les connaissances pragmatiques [Schank 1982]. Tous ces travaux ont montré leur limite dès lors qu'il s'agissait de modéliser manuellement ces connaissances pour tous les domaines, ou de les apprendre automatiquement. Le problème de la compréhension automatique en domaine ouvert restait donc entier. Puisque le problème ainsi posé est insoluble en l'état des connaissances, une approche alternative consiste à le redéfinir et à le décomposer en sous-tâches potentiellement plus faciles à résoudre. Ainsi la compréhension de texte peut être redéfinie selon différents points de vue sur le texte qui permettent de répondre à des besoins spécifiques. De même qu'un lecteur ne lit pas un texte de façon identique selon qu'il veut évaluer sa pertinence par rapport à un thème qui l'intéresse (tâche de type recherche documentaire), qu'il veut classer des documents, prendre connaissances des événements relatés ou rechercher une information précise, de même les processus automatiques seront multiples et s'intéresseront à des aspects différents du texte en fonction de la tâche visée. Suivant le type de connaissance cherché dans un document, le lecteur n'extraira du texte que l'information qui l'intéresse et s'appuiera pour cela sur les indices et sur les connaissances qui lui permettent de réaliser sa tâche de lecture, et donc de compréhension, sans avoir à tout assimiler. On peut alors parler de compréhension à niveaux variables, qui va permettre d'accéder à des niveaux de sens différents. Cette démarche est bien illustrée par les travaux en extraction d'information, évalués dans le cadre des conférences MUC [Grishman and Sundheim 1996], qui ont eu lieu de la fin des années 1980 jusqu'en 1998. L'extraction d'information consistait alors à modéliser un besoin d'information par un patron, décrit par un ensemble d'attributs typés, et à chercher à remplir ces attributs selon l'information contenue dans les textes. C'est ainsi que se sont notamment développées les recherches sur les « entités nommées » (à savoir le repérage de noms de personne, d'organisation, de lieu, de date, etc.) et sur les relations entre ces entités. C'est aussi dans cette optique que se sont développées les approches se situant au niveau du document, que ce soit pour la recherche d'information ou pour en déterminer la structure"
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"Cet article s'intéresse à l'évaluation de la contextualisation de tweets. La contextualisation est définie comme un résumé permettant de remettre en contexte un texte qui, de par sa taille, ne contient pas l'ensemble des éléments qui permettent à un lecteur de comprendre son contenu. Nous définissons un cadre d'évaluation pour la contextualisation de tweets généralisable à d'autres textes courts. Nous proposons une collection de référence ainsi que des mesures d'évaluation adhoc. Ce cadre d'évaluation a été expérimenté avec succès dans le contexte de la campagne INEX Tweet Contextualization. Au regard des résultats obtenus lors de cette campagne, nous discutons ici les mesures proposées et les résultats obtenus par les participants."
"La représentation du temps et de l'espace ainsi que les modèles de raisonnements associés sont des thèmes largement étudiés en informatique, d'une manière générale, et en intelligence artificielle, en particulier. Ces thèmes sont de plus en plus importants dans de nombreux domaines de notre société, en particulier là où est disponible une très grande quantité d'informations et de services évoluant au cours du temps ou dans l'espace. Les techniques temporelles et/ou spatiales sont, par exemple, importantes dans : la gestion des grandes quantités de données, l'analyse et la fouille de ces données, la simulation et l'analyse de l'évolution temporelle de processus ; l'évaluation de la sécurité et la sûreté ; la gestion dynamique des connaissances ; la gestion de l'espace, la prévention des risques naturels, la modélisations des systèmes dynamiques et complexes, etc. Elles offrent une alternative ou un complément aux méthodes statistiques et mathématiques de modélisation de l'espace et du temps."
"Les conditions de plus en plus exigeantes de la production industrielle imposent de rechercher de nouvelles techniques de pilotage d&rsquo;atelier. Nous pr&#233;sentons ici une mani&#232;re radicalement novatrice pour organiser le pilotage, reposant sur l&rsquo;approche holonique, l&rsquo;isoarchie et l&rsquo;analyse multicrit&#232;re. Apr&#232;s avoir abord&#233; les diff&#233;rents concepts sous-tendant cette approche, nous d&#233;taillons les m&#233;canismes de d&#233;cision multicrit&#232;re utilis&#233;s et la fa&#231;on de la mettre en &oelig;uvre et de l&rsquo;instrumenter. Les premiers r&#233;sultats obtenus par simulation via Arena et un module de calcul multicrit&#232;re AHP sont ensuite pr&#233;sent&#233;s et montrent les excellentes performances obtenues. Enfin, nous d&#233;taillerons la plateforme de test que nous venons d&rsquo;achever. L&rsquo;objectif est de pouvoir explorer et analyser plus facilement les performances de cette nouvelle approche du pilotage d&rsquo;atelier."
"Ce numéro est issu de l'édition 2013 des Journées Doctorales du Groupement de Recherche en Modélisation, Analyse et Commande des Systèmes, dites JDMACS. Dix-neuf articles ont été sélectionnés, parmi ceux qui avaient été unanimement appréciés par les évaluateurs des JDMACS, après une nouvelle phase de réécriture et de relecture C'est l'occasion de faire le point sur les nouvelles idées et les innovations technologiques issues de la communauté MACS."
no abstract
"L’objectif principal de ce travail est l’étude de la planification de trajectoires pour des drones de type HALE ou MALE. Les modèles cinématiques de ces drones sont étudiés. Les drones HALE sont modélisés par le système de Dubins. Pour les drones MALE, le modèle est construit en étudiant le repère cinématique du drone. Nous considérons les problèmes de planification de trajectoires point-point et point-pattern. Il s’agit, à partir de la position courante du drone, de rejoindre un point ou une figure prédéfinie dans l’espace. La planification point-point est abordée sous forme d’un problème de contrôle optimal. Deux méthodes sont proposées pour résoudre le problème point-pattern. D’abord nous présentons la synthèse en temps minimal pour le système de Dubins. Ensuite, nous développons une méthode basée sur le principe de LaSalle. La première méthode est utilisée au sein d’un algorithme de planification pour des drones HALE. La deuxième permet de stabiliser les deux types de drones considérés vers un pattern. Nous proposons une extension des algorithmes de planification développés, basée sur une discrétisation del’espace grâce aux graphes de Voronoï et une méthode de planification discrète, pour construire des trajectoiresdans des milieux encombrés. Nous étudions également le problème de couplage drone/capteur. Il s’agit de calculer une trajectoire permettant de satisfaire les objectifs du drone et de son capteur (une caméra). L’algorithme proposé est construit à partir de la résolution d’un problème quadratique sous contraintes.Dans une seconde partie, nous analysons un problème de contrôle optimal inverse. Celui-ci permet d’améliorer les résultats des méthodes de planification en s’inspirant du comportement des pilotes. Après avoir posé le problème, les résultats théoriques sont exposés et le cas particulier du système de Dubins est étudié en pratique."
no abstract
no abstract
"Malgré leur volume important et leur accessibilité, de nombreuses données numériques ne peuvent être correc-tement exploitées car elles sont contenues dans des textes sous des formes peu ou pas structurées. L'extraction de relations est un processus qui rassemble des techniques pour extraire des entités et des relations à partir de textes, nous donnant la possibilité d'enrichir des bases de connais-sances de façon automatique. Cependant le langage na-turel est de façon intrinsèque porteur d'ambiguïté, ce qui constitue un premier niveau d'incertitude auquel on peut rajouter l'imprécision due aux formulations telles que ""je crois que"", ""il semble que"", etc. La base de connaissances doit donc tenir compte de cette incertitude par exemple en associant à chaque nouvelle connaissance extraite un score de confiance dépendant du degré de certitude. Cet article est une communication de synthèse qui détaille les diffé-rentes problématiques liées à l'incertitude et à l'impréci-sion au cours de la chaîne de traitement allant de l'extrac-tion d'information dans les textes à l'inférence de connais-sances. Il y sera notamment question de stratégie d'agré-gation des différentes sources d'incertitude et d'impréci-sion et de leur prise en compte dans les traitements ulté-rieurs (par exemple la recherche d'information ou l'aide à la décision). Mots Clef TALN, extraction d'information, incertitude, inférence de règles Abstract Among the increasing volume of electronic resources available , non-structured texts expressed through natural language are difficult to process automatically. In this context, relation extraction techniques propose to combine various approaches to extract entities and their relations from texts, e.g. to automatically enrich a knowledge base. Nevertheless , the natural language is per se ambiguous, which makes extraction results uncertain. It can also be used to express imprecise or uncertain statements, ""It seem to me"", ""I believe"", etc. Therefore, any knowledge base enriched through text analyses must consider these uncertainties , for instance by combining a confidence score to each knowledge extraction according to its associated level of uncertainty. This information will be of major importance to infer additional knowledge from these extractions. However , how to characterize, capture and integrate the uncertainty and imprecision of natural language ? In addition , how to take into account this uncertainty to infer new knowledge ? This paper is a synthesis communication related to the consideration of uncertainty and imprecision in the context of Information Extraction from texts and knowledge inference from these extractions. We propose in particular to define the terminology, to characterize the several sources of uncertainty and to discuss strategies that can be used to capture and consider the uncertainty in knowledge extraction and knowledge inference treatments."
no abstract
"http://nkms.free.fr/ Polytech Marseille, Aix Marseille Université ; Domaine Universitaire Saint Jérôme, av Escadrille Normandie Niémen ; 13397 Marseille Cedex 20 RESUME : L'objectif de cette communication est double : il présente l'approche pragmatique (EAPAPA) que nous proposons (en ingénierie de l'automatique) pour notre formation d'ingénieurs à Polytech Marseille et le projet Erasmus+ nommé WESET (dans lequel cette approche est développée et appliquée) auquel Polytech Marseille contribue comme acteur responsable du module Contrôle, Commande, Optimisation et Diagnostic."
"Afin d'accroître leur compétitivité les entreprises ont recours de plus en plus à des systèmes robotisés pour realiser différentes tâches complexes. Ces robots sont très attractifs par leur coût mais nécessitent d'introduire des capteurs externes afin de garantir une plus grande précision de mouvement. Un enjeu majeur concerne la co-activité avec l'homme. Fort de l'acquisition très récente du système de vision low-cost, Leap Motion, qui présente des caractéristiques de précision inégalées à ce coût, nous proposons un premier travail d'apprentissage, par un système robotisé, de la gestuelle d'un opérateur. L'objectif est de reproduire des tâches complexes en 3D sans contraintes pour l'opérateur. Cette interaction permet d'engendrer un nuage de points et de directions très précises. Afin de garantir une bonne répétabilité du mouvement sur notre robot UR10, nous réalisons une interpolation de ces données par des splines polynomiales minimisant la norme L1. Ce formalisme développé récemment présente une complexité de calcul linéaire avec les données et permet de conserver la forme des données même lorsque le pas de discrétisation n'est pas uniforme en espace."
no abstract
no abstract
no abstract
"Ce chapitre introduit le domaine des méthodes d'ingénierie de systèmes d'information (ISI). Il présente notamment les notions de modèle de produit et de modèle de processus qui sous-tendent la définition d'une méthode. Après avoir rappelé les grandes stratégies d'adaptation, ce chapitre traite des approches adaptables. En utilisant une méthode de base (ou un élément méthodologique), ces approches proposent différentes techniques pour l'adapter. L'adaptation est ici vue comme une ―modification‖ d'une méthode de référence. Ce chapitre présente différentes propositions qui relèvent de l'approche adaptable et les compare sur plusieurs critères présentés dans l'introduction et qui sont pertinents du point de vue de l'adaptation."
"Ce chapitre introduit le domaine de l'ingénierie des méthodes et présente des approches dans lesquelles la dimension ―adaptation‖ est prise en compte au moment de la conception/construction d'une méthode. Dans l'introduction à cette partie du livre, ces approches ont été qualifiées d'adaptatives ; elles considèrent l'adaptation comme une activité à part entière de l'ingénierie d'une méthode. Elles proposent à la fois des modèles de description de méthodes permettant leur adaptation à chaque situation à traiter et des processus pour contrôler et guider l'adaptation. Nous distinguons trois familles d'approches adaptatives : les approches dirigées par les modèles, les approches à base de composants et les approches à base de services. Les différentes approches sont comparées selon un ensemble de critères relatifs à la dimension adaptation qui ont été présentés dans l'introduction."
"L'âge a de tout temps constitué un attribut identitaire important. Nous avons développé au fil de l'évolution une aptitude innée à classer les individus en fonction de leur âge. Cette classification s'appuie en grande partie sur le visage et sur les transformations anatomiques qu'il subit au cours du temps. De plus en plus de traitements cosmétiques, dermatologiques et d'interventions chirurgicales s'attaquant à un signe ou un groupe de signes spécifiques du vieillissement sont mis en oeuvre pour annuler, ou tout au moins masquer partiellement l'effet du temps sur le visage. On peut dès lors s'interroger sur l'influence de chacun des signes sur notre capacité à prédire l'âge d'un individu en observant son visage. Afin de construire un algorithme capable de déterminer l'âge d'individus à partir de leurs photos, nous nous sommes intéressés aux signes du vieillissement et à leur impact sur l'âge apparent. Dans un premier temps, nous avons déterminé et analysé les transformations anatomiques qui altèrent le visage à partir de l'âge adulte (au-delà de 20 ans). Puis nous avons étudié les signes sur lequel on se base pour prédire l'âge d'une personne. Enfin, nous avons construit et validé un modèle prédictif de l'âge en s'appuyant sur les observations précédentes. Transformations anatomiques du visage avec l'âge : La prévalence d'un certain nombre de signes de vieillissement (rides, tâches brunes, forme du visage...) a été mesurée sur un panel représentatif de femmes volontaires âgées de 20 à 74 ans. Ces données ont permis d'établir la cinétique d'apparition de ces signes. Appréciation subjective de l'âge: Il s'agissait de déterminer les signes sur lesquels un observateur s'appuie lorsqu'il évalue l'âge d'un sujet. Pour ce faire, nous avons demandé à un panel constitué de 48 observateurs d'attribuer un âge aux volontaires sur lesquelles nous avions précédemment mesuré les signes du vieillissement. Nous avons confirmé avec ce groupe d'observateurs que la perception de l'âge est liée au sexe et à l'âge de l'observateur. De plus, à l'aide d'une régression PLS (Partial Least Square régression), nous avons établi des relations entre les signes du vieillissement et l'âge observé et démontré que selon que l'on soit jeune ou âgé, un homme ou une femme, on n'exploite pas les mêmes signes de vieillissement pour prédire l'âge.Modèle de prédiction : Enfin, nous avons proposé un modèle s'appuyant sur la régression PLS pour prédire automatiquement l'âge à partir des photos du visage. Ce modèle présente la particularité d'associer, dans une approche unifiée, les signes relatifs à la couleur, à la forme et à la texture du visage, à l'âge des sujets. A l'instar des Modèles Actifs D'apparence (AAM), le modèle construit vise à réduire fortement l'information portée par l'ensemble des pixels du visage. Toutefois, ce dernier est supervisé : Il est donc très approprié dans notre contexte puisque que l'on peut mettre en oeuvre une procédure d'apprentissage pilotée par le but. Les performances sont de fait comparables à celles des humains."
no abstract
no abstract
no abstract
Non disponible
"Cet ouvrage traite des formalismes de représentation et de raisonnement sur l'espace et le temps, tels qu'ils sont développés actuellement en intelligence artificielle. Il est organisé en trois parties principales. Nous présentons tout d'abord des formalismes logiques et algébriques de représentation de l'espace et du temps. Dans une deuxième partie, nous décrivons les techniques de raisonnement associés à ces représentations : raisonnements logiques, résolution de contraintes, treillis et modèles graphiques. Une troisième partie s'attache à décrire quelques applications concrètes de ces techniques, en particulier à l'analyse et à la gestion de territores agricoles."
"Parmi les différentes espèces volantes présentes dans la nature, les insectes font sans nul doute montre de capacités des plus impressionnantes en termes de vol stationnaire ou de vive accélération et leur diversité est source de nombreuses solutions pour les systèmes bio-inspirés. En s'intéressant à la cinématique de l'aile d'un insecte volant, on observe qu'elle peut être décomposée en quatre mouvements basiques : le battement vers l'arrière, la supination, le battement vers l'avant et la pronation. Dans le cas d'ailes flexibles, les deux battements impliquent alors un mouvement de flexion de l'aile tandis que la supination et la pronation se résument à un mouvement de torsion de l'aile en quadrature de phase avec le battement précédent. On peut également voir, bien que cela soit sujet à de nombreuses discussions, que l'usage de mécanismes résonants par les insectes volants est démontré par différentes études : notamment l'utilisation des fréquences propres de l'aile, permettant d'excellentes performances pour un minimum de consommation énergétique, ou encore l'exploitation d'un mécanisme résonant reposant sur un autre organe, tel que le thorax, pour certains insectes dont les fréquences de battements diffèrent des fréquences propres de leurs ailes. Cet article propose donc de coupler un mouvement résonant de flexion et un mouvement résonant de torsion d'une structure flexible d'aile artificielle pour reproduire cette cinématique particulière et générer de la portance. Alors que les mécanismes de vol aujourd'hui utilisés par les nano-véhicules aériens (NAV) s'appuient sur un bord d'attaque rigide articulé lié au thorax et une membrane flexible, notre solution se distingue par l'utilisation d'une aile totalement flexible dont la conception est telle que le mouvement du bord d'attaque soit directement induit par le comportement dynamique de la structure en résonance. Plus précisément, la problématique est de déterminer la géométrie et les propriétés élastiques d'ailes artificielles telles qu'un couplage entre une déformée de flexion et une déformée de torsion en quadrature de phase puisse être obtenu avec l'utilisation d'un unique actionneur. Qualitativement, cela implique de rapprocher les fréquences propres des deux modes visés. Dans l'optique de ce défi, une configuration optimale de l'aile a été déterminée à l'aide d'une analyse paramétrique réalisée sur les paramètres géométriques du squelette de l'aile modélisé par un assemblage de poutres d'Euler-Bernoulli. Cette configuration d'aile présentant alors des fréquences auxquelles les deux modes sont couplés en quadrature de phase tout en conservant une amplitude non-négligeable en flexion comme en torsion. Des validations expérimentales ont alors été menées sur des prototypes intégrant cette géométrie d'aile, présentant ainsi une envergure de 22 mm pour un poids total de 22 mg. Ces prototypes sont réalisés à l'aide de procédés de fabrication dédiés aux micro-technologies et sont pourvus d'un actionneur électromagnétique permettant l'actionnement simultané des deux ailes. Une analyse modale expérimentale a été effectuée à l'aide d'un vibromètre laser et démontre la proximité fréquentielle des modes de flexion et de torsion en excellent accord avec les résultats de la modélisation en termes de déformées opérationnelles comme de fréquences propres. Dans un second temps, un banc de mesure de force de portance dédié a été développé et utilisé pour démontrer la génération par le prototype d'une force de portance équivalente à plus de 110% de son poids. Les maximums de poussée se produisant, comme attendu, aux fréquences de quadratures entre les deux modes. Enfin, des enregistrements à la caméra rapide ont permis de confirmer que ces maximums de force de portance étaient bien obtenus pour des mouvements de grandes amplitudes mêlant mouvement de flexion et mouvement de torsion en quadrature de phase."
"Pour modéliser des paysages et leur dynamique, il est largement admis que les phénomènes géographiques ne peuvent être traités que sous la forme d'entités ou de champs (Goodchild, 1992 ; Peuquet, 2001). Ces concepts induisent des contraintes qui, lorsque combinées à d'autres, comme la difficulté de gérer le temps, des échelles multiples, ou des frontières floues, posent des problèmes pour la modélisation de la dynamique des paysages qui sont toujours d'actualité. Des études récentes (e.g. Rietsma and Albrecht, 2005; Parent et al., 2006) suggèrent cependant que l'amélioration des primitives de modélisation plutôt que les modèles eux-mêmes est une nouvelle voie qui mérite d'être explorée. Nous présentons ici une tentative pour dépasser les principales limites rencontrées en modélisation de la dynamique des paysages avec une approche basée sur le développement de nouvelles primitives de modélisation d'une part, et un langage métier (DSL) pour construire et manipuler ces primitives d'autre part. Nous faisons l'hypothèse que bien des aspects auxquels on fait appel pour modéliser des paysages et leur dynamique sont en fait liés entre eux et ne devraient pas être traités séparément comme c'est souvent le cas . Nous observons en particulier que les structures de données utilisées pour porter l'information spatiale contraignent fortement l'activité de modélisation, ce qui suggère que des primitives pour lesquelles les concepts et leurs représentations sont séparés, devraient être plus appropriées . Nous nous orientons donc vers la redéfinition de primitives spatiales et temporelles, en les dotant d'un certain nombre de propriétés de base visant à prendre en compte les principales limites rencontrées dans les études actuelles . Les différentes étapes de la construction du langage métier sont exposées dans cette présentation. Les propriétés de base que doivent posséder les primitives de modélisation sont d'abord identifiées . On constate que ces propriétés peuvent être exprimées à l'aide d'un ensemble restreint de structures et d'opérateurs plus fondamentaux . Ces derniers sont utilisés pour établir le méta-modèle du langage métier, en spécifiant les opérateurs et les règles de dépendance entre ces opérateurs indépendamment de toute syntaxe . La structure et la logique du langage sont présentées, ainsi que la façon dont on peut l'utiliser pour construire des primitives . Enfin, des exemples d'utilisation du DSL et un ensemble initial de quelques primitives sont illustrés à travers des expériences de modélisation de paysages."
Pas de résumé
no abstract
no abstract
"L’article décrit la problématique et les solutions proposées par le pro-jet QUADRIS (ARA-05MMSA-0015)dont l’objectif est d’offrir un cadre d’évaluation de la qualité dans les systèmes d’information multisources (SIM). Ce cadre a permis de définir un méta-modèle pour étudier en particulier les inter-dépendances entre les dimensions de la qualité d’un modèle conceptuel de don-nées et celles de la qualité des données instanciant ce modèle. Nous étudions la possibilité de définir des patterns d’évaluation de la qualité dans le but de : 1)formaliser les corrélations entre les facteurs de qualité, 2) représenter les processus, et 3) analyser la qualité des données, du système et son évolution. Le projet QUADRIS s’est engagé à valider ses propositions dans les trois domaines d’application suivants : le domaine biomédical, le domaine commercial et le domaine géographique."
"Notre article s’intéresse aux techniques et enjeuxd’une recherche de mot ifs généralisée dans les documentsnumériques. La première partie rappelle que pour êtrepleinement un document (au sens de preuve),le document numérique doit être accessible y compris dans ses fragments.La seconde partie porte sur la recherche de motifsdans des documents nativement numériques (expressions rationnelles,XPath, cas des arborescences multiples). La troisième portesur les documents numérisés (formulaires, textes imprimés,textes manuscrits, graphiques, photographies, enregistrementsaudio et vidéo)"
fr_abstract_s
"L’étude traite de la détection de cibles mobiles dans un contexte de radar passif bistatique utilisant les émetteursde télévision numérique TNT (DVB-T) comme émetteurs d’opportunité. Outre leur présence généralisée sur leterritoire, l’intérêt de ces émissions réside dans leur relative largeur de bande permettant une bonne précisiond’estimation. Le principal inconvénient de ce type d’approche réside dans l’éblouissement par le signal en trajetdirect, des échos de très faible intensité des cibles d’intérêt. Après un rappel du principe du radar bistatique etde la norme OFDM utilisée par les signaux TNT, une première étude donne une construction originale du signalde référence dans le cas multi-capteurs : le signal de référence est construit par un traitement d’antenne de typeCAPON où le balayage des paramètres optimaux est remplacé par la connaissance de signaux pilotes inséré dansles symboles OFDM. Ensuite le rapport se focalise sur l’estimation d’un filtre de canal multitrajet à partir dela connaissance de la modulation OFDM utilisée. Ce filtre, d’abord étudié comme réjecteur de fouillis originalavant détection par la fonction d’ambiguïté, donne des résultats semblables aux méthodes classiques de réjectionde fouillis standard. Étendu à toutes les fréquences Doppler, son module au carré est utilisé comme un nouveaudétecteur présentant un très faible niveau de clutter, surpassant ainsi la fonction d’ambiguïté. Une interprétationen terme de traitement d’antennes du nouveau détecteur ouvre la voie à des variantes haute-résolution de celui-ci.La validité du nouveau détecteur est illustrée par des résultats sur données réelles."
"Fission products and actinides arising from the spent UOX fuel reprocessing are vitrified in borosilicate glass matrices. Among the fission products, platinum-group metals (Pd-Rh-Ru) exhibit very low solubility and partly precipitate as metal or oxide phases in the glass melt. Molybdenum can form molybdate phases which are known to precipitate in the glass as a complex molybdate phase called yellow phase. These molybdate phases may induce modifications of the physico-chemistry of the glass melt and have an impact on the final glass confinement properties.To understand the relative stability of these phases depending on both temperature and oxygen potential of the melt, a thermodynamic database is being developed using the Calphad method.This database includes the metallic and oxide complex platinoid system and the interactions with tellurium Pd-Rh-Ru-Te-(O). To consider the formation of molybdates, the CaO-MoO$_3$ and of Na$_2$O-MoO$_3$ pseudo binary systems are taken into account. The modeling of Na$_2$O-SiO$_2$ and of the ternary SiO$_2$-Na$_2$O-MoO$_3$ system was carried out based on the literature and on new experiments performed at the CEA.Using this tool, the thermodynamic state of the molybdate phases is calculated as a function of temperature and composition and the ruthenium redox behavior is predicted as a function of temperature and oxygen pressure in the melt. This study throws new light on the interactions between molybdenum and platinum-group metals with the glass melt during the vitrification process of high level nuclear waste."
"En quête de miniaturisation, rapidité et faible consommation électrique, les nanotechnologies se réinventent constamment autour de nouveaux matériaux. Dans cet article nous présentons une classe de matériaux hybrides appelés « multiferroiques », qui ont comme particularité d'intriquer à l'échelle atomique plusieurs propriétés physiques : électrique-magnétique-élastique. Plus particulièrement, nous verrons comment la résonance paramagnétique électronique permet de comprendre pourquoi ces propriétés peuvent exister dans les matériaux organiques ou métallo-organiques qui sont bien moins polluants que leurs homologues inorganiques mais présents en masse dans l'industrie. Abstract : In the quest of downsizing, speed up and low electrical consumption,"
"Ce travail de thèse est constitué de deux grandes parties. Dans la première partie, nous avons étudié la redistribution du bore dans le silicium (001), à l'ambiante et après recuit thermique, à l'aide de la sonde atomique tomographique (SAT), la microcopie électronique en transmission (MET) et la spectrométrie de masse d'ions secondaires (SIMS). Pour cette étude, le silicium a été fortement implanté en bore. La concentration en B dans le Si peut alors dépasser la limite de solubilité. On est donc dans le cas d'un système sursaturé. Dans ce cas, nous avons observé qu'à la formation de défauts (BIC's, défauts {113} etc...) s'ajoute la germination d'amas riches en B ou même la précipitation d'une nouvelle phase après recuit thermique. Dans la deuxième partie, nous avons étudié la redistribution du B et du Pt dans le NiSi, utilisé lors de la miniaturisation des transistors MOS, afin de réduire la résistance de contact. A part l'accumulation du bore à l'interface NiSi/Si et à la surface de NiSi, nous avons observé, la précipitation du bore dans le monosiliciure de nickel, pour un recuit à 450°C. En revanche, pour le platine nous n'observons plus un phénomène de précipitation. Il a plutôt tendance de ségréger aux interfaces à 290°C (observation du phénomène de ""chasse-neige ""), tandis qu'au delà de 350°C, le Pt s'accumule majoritairement dans la phase NiSi."
"La diffraction cohérente permet de caractériser les hétérogénéités de déformation dans un polycristal. La figure de diffraction dans l'espace réciproque dépend de la forme du grain illuminé et de son champ de déformation. Dans ce travail, La déformation d'un film mince polycristallin est calculée par élément finis et les figures de diffraction de certains grains en sont déduites en fonction de la déformation imposée. L'influence de la densité de maillage, de la taille et de la forme du grain illuminé est étudiée."
"Pendant la solidification d'alliages se développe au niveau de l'interface solide-liquide une microstructure dont les caractéristiques influencent fortement les propriétés macroscopiques du matériau. Sa formation est un processus dynamique dans lequel le réseau se développe, s'organise et s'ordonne progressivement. Une étude détaillée des mécanismes physiques qui contrôlent la formation de cette microstructure est fondamentale pour maîtriser la qualité du matériau. Sur terre, la convection dans la phase liquide affecte fortement la microstructure de solidification en créant par exemple des hétérogénéités dans les paramètres de contrôle le long de l'interface. Dans le cadre du projet scientifique MISOL3D (MIcrostructures de SOLidification 3D) sélectionné par le CNES, nous avons participé au développement de l'Instrument DECLIC et de son insert DSI (Directional Solidification Insert) dédié à l'étude in situ de la formation des microstructures colonnaires cellulaires et dendritiques 3D sur des analogues transparents et installé à bord de la Station Spatiale Internationale. En microgravité, le dispositif sert à établir une base de données de référence sur la dynamique des phénomènes dans la limite du transport diffusif et à étudier les mécanismes physiques qui gouvernent la dynamique de formation et de sélection de la microstructure interfaciale. Ces travaux bénéficient d'une collaboration scientifique avec les équipes américaines du Pr. Trivedi (expériences en échantillons minces) et du Pr. Karma (simulations champ-de-phase), sélectionnées par la NASA. Deux campagnes spatiales ont eu lieu sur des échantillons de compositions différentes en 2010-2011 (DSI) et 2017-2018 (DSI-R) qui ont permis d'explorer largement la carte des microstructures en fonction des paramètres de contrôle. Dans ce résumé, nous présenterons des résultats sur l'évolution de l'espacement primaire mettant en évidence l'importance de cette collaboration entre expérience et simulation numérique."
"Mes travaux depuis mon recrutement au CNRS et actuellement au sein de l'équipe MCA de l'IM2NP concernent principalement la solidification des matériaux. Les propriétés des matériaux sont largement contrôlées par la microstructure de solidification, les structures de grains et les ségrégations laissées dans le solide avant toute mise en forme et traitement postérieurs. Pour élaborer des matériaux possédant des propriétés définies, sur mesure et de façon reproductible, il est donc nécessaire de maîtriser les mécanismes qui lient les procédés d'élaboration à la structure interne à diverses échelles des matériaux. Mes travaux de recherche vont dans le sens de l'approfondissement de la connaissance de ces mécanismes et, à plus long terme, de l'application de ces recherches aux procédés industriels. La nécessité d'améliorer les procédés pour obtenir des pièces de plus en plus performantes et qui permettent des économies en énergie soulève de nombreuses questions métallurgiques pour la recherche et l'industrie. L'amélioration des procédés nécessite de comprendre les mécanismes physico-chimiques qui entrent en jeu pendant la phase de solidification. De plus, pour être prédictives et quantitatives, les simulations numériques largement utilisées en milieu industriel doivent être nourries par la connaissance de ces mécanismes, leur formulation mathématique et les paramètres entrant en jeu. Dans les alliages métalliques, j'étudie plus particulièrement les mécanismes de la formation de la structure de grains, de la transition colonnaire équiaxe (CET), des ségrégations induites par cette structure. Les grains colonnaires et équiaxes résultent de dendrites avec différentes morphologies. Les dendrites colonnaires sont allongées dans une direction tandis que les grains équiaxes n'ont pas de direction privilégiée. A cause de leur forme, les grains équiaxes permettent d'obtenir des propriétés mécaniques isotropes pour le matériau final et des champs de concentration plus homogènes que dans le cas de la croissance colonnaire. En fonction de l'application, l'un ou l'autre type de grain est préféré et doit donc être favorisé par le procédé de solidification (par exemple : les grains équiaxes dans les pièces de moteur, les grains colonnaires voire un monograin dans les aubes de turbines). En conséquence, la compréhension des mécanismes physico-chimiques qui contrôlent la CET est une question critique en métallurgie et qui reste d'actualité. Les grains équiaxes peuvent apparaître de deux manières au cours de la solidification. La première est la germination hétérogène sur des particules incluses volontairement dans l'alliage comme cela est fait couramment dans l'industrie de l'aluminium par exemple ou, sur des impuretés ou des précipités présents naturellement dans l'alliage. La seconde est le détachement de branches dendritiques secondaires dans la zone pâteuse ce qui est admis comme la cause de l'apparition d'une zone équiaxe au centre des lingots de fonderie. Afin de comprendre et de caractériser les mécanismes de la dynamique de formation de la structure de grains dans les alliages métalliques, mon programme de recherche comporte trois volets : * l'étude de la structure de grains et de la fragmentation (Chapitre 1) * l'étude de la CET en présence d'affinants dans des alliages à base aluminium (Chapitre 2) * l'influence de la convection au cours de la solidification de ces alliages (Chapitre 3). Mon approche est expérimentale et comporte des expériences originales: 1- Caractérisation in situ et en temps réel de la dynamique de la solidification d'alliages métalliques proches des alliages industriels par imagerie X synchrotron. 2- Etude des effets de la convection naturelle, de la convection contrôlée par un champ ou un stimulus externe ou, utilisation de la microgravité (absence de convection naturelle et de phénomènes de sédimentation). Par ailleurs, depuis 2008, je développe au sein de l'équipe MCA une nouvelle thématique de recherche pour laquelle j'ai mis en place un projet (Si-X : Caractérisation et compréhension de la cristallisation du SiIicium photovoltaïque: imagerie X synchrotron) financé par l'ANR HABISOL. Les cellules photovoltaïques (PV) sont amenées à devenir une des composantes majeures de l'habitat écologique de demain. Les différentes étapes d'élaboration des cellules PV à base de silicium (purification, cristallisation, traitements intermédiaires, procédé cellules) concourent au rendement des cellules PV. Dans ce cadre, je m'intéresse à la phase de cristallisation/solidification. Jusqu'à présent, du silicium en provenance de l'industrie microélectronique était employé pour fabriquer les cellules PV mais cette filière est très coûteuse et est tributaire de l'industrie microélectronique pour l'approvisionnement en silicium de qualité suffisante. D'autres voies d'approvisionnement et de fabrication du matériau silicium de qualité suffisante pour les applications PV sont explorées mais ces matériaux silicium sources doivent être considérés comme de nouveaux matériaux vis-à-vis des procédés d'élaboration de lingots et de cellules. En conséquence, un certain nombre de problèmes liés à la solidification de ces matériaux doivent être (ré)-examinés avec attention même pour des procédés établis pour les matériaux en provenance de l'industrie microélectronique. D'une manière générale, dans le Si multi-cristallin utilisé massivement pour la fabrication des cellules photovoltaïques, le rendement PV de la cellule est complètement différent en fonction de la structure de grains du lingot. Par conséquent, il est indispensable de contrôler et donc de comprendre la formation de la structure de grains issue de l'étape de solidification du Si multi-cristallin. Ces travaux sur le Si multi-cristallin font l'objet du chapitre 4. Pour les deux principaux types de matériaux que j'étudie (alliages métalliques, Si PV) la problématique de la solidification et en particulier de la formation de la structure de grains est essentielle. En revanche, la croissance du silicium multi-cristallin, en général facettée, est totalement différente de celle des alliages métalliques classiques ce qui ouvre des perspectives intéressantes pour la compréhension de mécanismes peu abordés jusqu'à présent dans nos travaux : effet de l'orientation cristallographique, macles, croissance facettée."
"La procédé STI (« Shallow Trench Isolation ») est couramment utilisé dans la microélectronique afin d'isoler électriquement les dispositifs entre eux. Les nombreuses étapes de ce procédé engendrent des contraintes mécaniques très importantes qui peuvent nuire à la fiabilité. L'originalité de la diffraction X haute résolution est d'utiliser l'intensité diffractée par le silicium déformé périodiquement comme empreinte du champ de déformation locale. Les mesures ont porté sur des structures de période allant de 2 µm à 200 nm, avec des lignes de silicium de largeur inférieure à 100 nm pour les plus petites périodes. Pour les échantillons de période submicronique, un second pic de diffraction apparaît sur les cartographies du réseau réciproque. Ce pic est attribué à une déformation homogène du silicium entre les tranchées et permet une mesure directe et sans modèle des déformations. L'effet sur les déformations et les contraintes de variations géométriques et de procédé ont été ainsi étudiées. Cette méthode expérimentale s'appuie sur des simulations numériques par éléments finis."
"Both zinc tungstate and zinc molydate were synthesized by the conventional co-precipitation method. The as-prepared materials were then characterized by X-Ray diffraction (XRD), thermogravimetric and differential thermal analyses TGA/DTA and the scanning electron microscopy (SEM). The visible photocatalytic activities of the samples were evaluated by photodegradation of rhodamine B ; the zinc tungstate reached an average efficiency of 43% and 30% for zinc molybdate. Additionally many parameters were discussed."
"Les mémoires à nanocristaux de silicium sont considérées comme l’une des solutions les plus intéressantes pour remplacer les grilles flottantes dans les mémoires Flash pour des applications de mémoires non-volatiles embarquées. Ces nanocristaux sont intéressants pour leur compatibilité avec les technologies de procédé CMOS, et la réduction des coûts de fabrication. De plus, la taille des nanocristaux garantie un faible couplage entre les cellules et la robustesse contre les effets de SILC. L’un des principaux challenges pour les mémoires embarquées dans des applications mobiles et sans contact est l’amélioration de la consommation d’énergie afin de réduire les contraintes de design de cellules. Dans cette étude, nous présentons l’état de l’art des mémoires Flash à grille flottante et à nanocristaux de silicium. Sur ce dernier type de mémoire une optimisation des principaux paramètres technologiques a été effectuée pour permettre l’obtention d’une fenêtre de programmation compatible avec les applications à faible consommation d’énergie. L’étude s’attache à l’optimisation de la fiabilité de la cellule à nanocristaux de silicium. On présente pour la première fois une cellule fonctionnelle après un million de cycles d’écriture et effacement dans une large gamme de températures [-40°C;150°C], et qui est capable de retenir l’information pendant dix ans à 150°C. Enfin, une analyse de la consommation de courant et d’énergie durant la programmation montre l’adaptabilité de la cellule pour des applications à faible consommation. Toutes les données expérimentales ont été comparées avec les résultats d’une cellule standard à grille flottante pour montrer les améliorations apportées."
"Cette communication présente d'une étude bibliométrique réalisée en 2006 pour la tenue d'un comité d'orientation scientifique organisé à l'initiative des trois universités d'Aix Marseille. Cette étude bibliométrique était l'un des nombreux d'outils proposés au comité d'orientation scientifique (une quarantaine d'experts externes) pour l'assister dans son travail de recommandation. Les résultats de l'étude offrait une vision globale et << objective >> de l'activité et de l'évolution du pôle scientifique de la région d'Aix-Marseille. La méthodologie employée pour cette étude était fortement inspirée de la méthode d'analyse des portefeuilles d'activités stratégiques mise en œuvre lors d'une analyse concurrentielle. La transposition de cette méthode a nécessité de définir les unités scientifiques stratégiques, de choisir les pôles universitaires << concurrents >>, de choisir le critère de mesure de l'activité scientifique, d'étudier l'évaluation de la dynamique des deux précédents facteurs au fil du temps et au final la construction d'une représentation graphique du positionnement du portefeuille scientifique du pôle Aix-Marseille et de sa dynamique. Cette étude bibliométrique a permis de confirmer la politique scientifique affichée par le pôle d'Aix Marseille et de donner une vision globale de son positionnement international. Elle a également eu un rôle pédagogique. Elle a permis d'habituer les acteurs de la recherche à l'utilisation des indicateurs bibliométriques dans un cadre d'auto-évaluation."
"Des couches ultra-minces céramiques nanoperforées ont été élaborées par une approche bottom-up en combinant la chimie sol-gel, l’auto-assemblage de copolymères à blocs et le procédé d’enduction par trempage (dip-coating). En plus de présenter des propriétés associées à une hétérogénéité structurée à l’échelle nanométrique et de rendre accessible la surface des substrats à travers le réseau hexagonal de nanoperforations, ces Inorganic Nano Patterns (INP) présentent l’avantage d’être chimiquement, thermiquement et mécaniquement stables. La méthode développée est transposable à de grandes surfaces et est adaptable à une large gamme de matériaux utiles dans le domaine des nanotechnologies."
"Les mécanismes de formation de phases dans des films minces du système ternaire Al-Cu-Fe et des systèmes binaires Al-Cu, Al-Fe et Cu-Fe ont été étudiés. Dans chacun des systèmes, plusieurs échantillons avec des compositions distinctes ont été préparés par pulvérisation cathodique. Des couches d'aluminium, de cuivre et de fer ont été déposées séquentiellement sur des substrats de silicium oxydé et ont été traités thermiquement par différentes méthodes puis caractérisés. Des mesures de diffraction de rayons X et de résistivité in-situ ont été effectuées pour suivre la formation des phases. Des recuits thermiques suivis de trempe ont été réalisés et les échantillons ont été caractérisés par diffraction des rayons X. L'analyse enthalpique différentielle a également été utilisée ainsi que des mesures simultanées in-situ de résistivité et de diffraction des rayons X. L'ensemble des résultats obtenus nous a permis de proposer des mécanismes de formation de phases pour chacun des échantillons étudiés et en utilisant des modèles théoriques de croissance de phases nous avons pu déterminer des données cinétiques sur la formation de phases dans ces films."
"L'obtention de réseaux supramoléculaires hautement cristallins étendus sur de grandes surfaces de substrats isolants et stables à température ambiante constituerait une percée pour l'élaboration de matériaux fonctionnels pour des applications en nanoélectronique, notamment pour la conversion de l'énergie solaire en électricité (cellules solaires organiques) ou le stockage de l'information (mémoires moléculaires). Au cours de ce travail, nous avons étudié l'adsorption d'une molécule organique, spécialement conçue, sur trois différentes surfaces d'halogénures d'alcalins (NaCl, KCl et RbCl) à température ambiante et sous ultra vide, en combinant la microscopie à force atomique (nc-AFM) et des calculs théoriques basés sur la théorie de la Fonctionnelle de la Densité (DFT). Le but de cette étude est de caractériser structuralement les films moléculaires obtenus et de comprendre l'influence du substrat sur le processus de croissance de ces réseaux supramoléculaires."
"Les jonctions tunnel (JT) constituent des éléments essentiels dans la conception de cellules solaires à multijonction, puisqu'elles assurent les interconnexions permettant de mettre en série les cellules élémentaires et ce, sans perte de performances. Les cellules élémentaires absorbant différentes gammes du spectre solaire peuvent être empilées de façon monolithique par épitaxie sur un substrat. Les cellules multijonction couvrent ainsi plus efficacement le spectre solaire et peuvent atteindre, grâce à la réduction des pertes par thermalisation, des rendements élevés, le record étant actuellement de 46% [1]. Les JT assurent donc les interconnexions électriques et doivent satisfaire plusieurs critères pour minimiser les pertes dans la multijonction : une capacité de conduction (évaluée par le courant tunnel pic) bien supérieure au courant photogénéré par la cellule multijonction ; une très faible résistance ; une absorption optique minimale pour assurer la transmission sans perte dans la bande spectrale de la (ou des) cellule(s) sous-jacente(s) ; ces JT doivent enfin présenter d'excellentes qualités structurales et ne pas contenir de dislocations dans le composant qui seraient générées par la relaxation des contraintes paramétriques."
"Dans le cadre du développement de nouvelles technologies pour la protection environnementale, et tout particulièrement pour la dépollution de l’eau ou de l’air, le présent travail de thèse porte sur la mise en œuvre de matériaux semiconducteurs à morphologies contrôlées, susceptibles d’activités photocatalytiques permettant la dégradation ou la transformation de molécules en milieux aqueux. Plusieurs types de synthèses conduisant à des morphologies diversifiées ont été mises en œuvre. Chaque matériau a été caractérisé par diffraction de rayons X, microscopies électroniques à balayage et en transmission, et parspectroscopie Raman. La réflectance diffuse a été utilisée pour déterminer les énergies de bandes interdites des matériaux. Compte tenu des propriétés déjà connues pour les tungstates de type MWO4, notre choix s’est orienté vers trois matériaux : le trioxyde WO3, le tungstate SrWO4 et un nouveau tungstate NaCe(WO4)2 ou Na0,5Ce0,5WO4. L’oxyde WO3 a été choisi comme matériau de référence. Pour ce matériau, deux types de morphologies ont été obtenues : des nanoplaquettes et des nanosphères. Le tungstate SrWO4 de structure scheelite a été synthétisé sous deux formes microstructurales : des sphères et des navettes. Un nouveau matériau a été synthétisé et caractérisé : le tungstate double Na0,5Ce0,5WO4 de structure scheelite. Pour cette nouvelle phase, trois morphologies 3D hiérarchisées ont été élaborées en utilisant la méthode hydrothermale en présence d’EDTA. Pour chaque morphologie observée, un mécanisme de germination-croissance est proposé. Les performances photocatalytiques des différentes formes morphologiques ont été évaluées lors de la dégradation de la rhodamine B (RhB) et du bleu de méthylène (BM), sous rayonnements UV et visible. L’efficacité photocatalytique des différentes microstructures a été étudiée en fonction du pH du milieu réactionnel. À partir des résultats obtenus, nous avons pu montrer la forte corrélation entre largeur de bande interdite et réactivité photocatalytique, mais aussi entre morphologies, tailles et propriétés photocatalytiques. Il est apparu que la dégradation reposesur deux mécanismes complémentaires : l’adsorption des molécules due à la porosité des microstructures et à leur morphologie, et la réaction photocatalytique due aux radicaux actifs générés par les paires e-/h+ photogénérées. Ainsi, la RhB se décompose en présence de SrWO4 et WO3 sous UV-C (254 nm) et UV-Vis (365 nm) respectivement. Le bleu de méthylène se dégrade en présence de NaCe(WO4)2 sous rayonnement solaire UV-Vis."
"L'anisotropie structurelle, par exemple la texture, peut régir d'importantes propriétés physiques d'un film mince, telles que les propriétés électriques, magnétiques et/ou mécaniques. La texture (information d'orientation) est typiquement observée et quantifiée par la mesure de ce que l'on appelle les figures polaires. Une approche expérimentale optimisée mise en œuvre sur la ligne DiffAbs (Synchrotron SOLEIL) est présentée ici. À l'aide d'un détecteur de zone de pixels à rayons X et de sources de rayonnement synchrotron, une figure de pôle complète (avec des résolutions adaptées aux films minces texturés métalliques, généralement de l'ordre de quelques degrés) peut être mesurée à des intervalles de temps aussi courts qu'une minute. Les corrections nécessaires permettant la récupération complète de la figure de pôle à partir des données expérimentales en utilisant cette approche optimisée sont fournies et discutées. On constate un gain de temps de mesure jusqu'à deux ordres de grandeur par rapport à l'utilisation d'un détecteur ponctuel (approche classique) dans les mêmes conditions expérimentales. Les données mesurées à l'aide de ces deux approches sont présentées, comparées et discutées."
"Directional solidification of a cast mono silicon seed and of a float-zone (FZ) silicon seed was performed and the grain and defect structures of the seeds as well as of the regrown parts are analyzed. In situ X-ray diffraction imaging enabled the observation of the dislocation arrangements. During the heating process, in the FZ seed, mobile dislocations glide on {111} planes, whereas in the cast mono seed dislocations are arranged in a mainly immobile cellular structure. Ex situ grain orientation mappings reveal the presence of subgrains with misorientations up to 3◦ in the regrown part of the cast mono-seeded sample, which are not observed in the regrown part of the FZ-seeded sample. Subgrain boundaries characterized by misorientations around the [001] growth axis propagate roughly along the growth axis and increase their misorientation by merging with new subgrain boundaries appearing in their vicinity. Although the first inception of subgrain formation cannot be revealed, the comparison of the dislocation arrangements in the two seeds strongly suggests an influence of the latter on subgrain formation. In the regrown part, interactions between subgrain boundaries and twin boundaries show that they can follow Σ3{111} and Σ9{221} grain boundaries or cross Σ3{111} grain boundaries. Whether Σ3 {111} GBs are crossed or not depends among other things on the orientation of the grains on either side of the twin. It demonstrates that the grain orientation relationship and not only the grain boundary character play an important role in the subgrain structure evolution and redistribution in a multicrystalline silicon ingot."
"To control the final grain structure and the density of structural crystalline defects in silicon (Si) ingots is still a main issue for Si used in photovoltaic solar cells. It concerns both innovative and conventional fabrication processes. Due to the dynamic essence of the phenomena and to the coupling of mechanisms at different scales, the post-mortem study of the solidified ingots gives limited results. In the past years, we developed an original system named GaTSBI for Growth at high Temperature observed by Synchrotron Beam Imaging, to investigate in situ the mechanisms involved during solidification. X-ray radiography and X-ray Bragg diffraction imaging (topography) are combined and implemented together with the running of a high temperature (up to 2073 K) solidification furnace. The experiments are conducted at the European Synchrotron Radiation Facility (ESRF). Both imaging techniques provide in situ and real time information during growth on the morphology and kinetics of the solid/liquid (S/L) interface, as well as on the deformation of the crystal structure and on the dynamics of structural defects including dislocations. Essential features of twinning, grain nucleation, competition, strain building, and dislocations during Si solidification are characterized and allow a deeper understanding of the fundamental mechanisms of its growth."
"This work is dedicated to the advanced in situ X-ray imaging and complementary ex situ investigations of the growth mechanisms when silicon solidifies on a monocrystalline seed oriented ⟨110⟩ in the solidification direction. It aims at deepening the fundamental understanding of the phenomena that occur throughout silicon crystal growth with a particular focus on mechanisms of formation of defects detrimental for photovoltaic applications. Namely, grain nucleation, grain boundary formation and evolution, grain competition, twining occurrence, dislocation generation and interaction with structural defects are explored and analysed. Nucleation of twin crystals preferentially occurs on {111} facets at the edge of the sample where solid e liquid e vapor triple point lines exist in interaction also with the crucible as well as, at grain boundary grooves at the solid e liquid interface (solid e solid e liquid triple lines), where two grains are in competition, either on the {111} facets of the groove or in the groove. Enhanced undercooling and/or stress accumulation levels are found to act as driving forces for grain nucleation. Additionally, it is demonstrated that twin formation has the property to relax stresses stored in the crystal during the growth process. However, grains formed initially in twin position can undergo severe distortion when they are in direct competition or when they are squeezed in e between grains. Moreover, we show by X-ray Bragg diffraction imaging that on the one hand, coherent S3 ⟨111⟩ grain boundaries efficiently block the propagation of growth dislocations during the solidification process, while on the other hand, dislocations are emitted at the level of incoherent and/or asymmetric S27a ⟨110⟩ at the encounter with either S3 ⟨111⟩ or S9 ⟨110⟩ grain boundaries. Indeed, grain boundaries that deviate from the ideal coincidence orientation act as dislocation sources that spread inside the surrounding crystals."
"La manipulation des propriétés physiques des nanostructures, telles que leur forme ou leur composition, suscite de plus en plus l’intérêt des recherches à cause des propriétés exceptionnelles des matériaux à cette échelle. L’ingénierie des contraintes a pour objet d’utiliser la déformation pour contrôler les propriétés. Cela est particulièrement intéressant dans les nano-objets car ils peuvent supporter des déformations élastiques élevées. Dans ce travail, nous étudions la déformation et l’influence de la température dans des nanofils uniques de type coeur/coquille. Ceci est possible en utilisant la diffraction cohérente des rayons X (CDI) en condition de Bragg, une technique d’imagerie qui remplace les lentilles optiques par des algorithmes d’inversion capables de reconstruire l’amplitude (densité électronique) et la phase (projection du champ de déplacement atomique) de l’échantillon à partir des clichés de diffraction. Cette méthode a également été appliquée à des particules facettées de platine qui ont des propriétés catalytiques exceptionnelles. Des expériences CDI in situ ont permis d’étudier l’évolution du champ de déformation dans les particules pendant des réactions chimiques et donc de progresser vers le découplage entre leur déformation intrinsèque et leur activité chimique."
"La technologie des cellules solaires tandem (MJSCs) GaAs (1.42 eV)/GaInP (1.87 eV) sur substrat GaAs est très mature, et l'ajout de sous­cellules solaires de plus petit gap à cette structure bien maîtrisée a permis d'obtenir de très hauts rendements au­delà de 40 %. Les alliages GaInAsN accordés sur GaAs (rendement de 43.5% [1]), GaInAs métamorphique (rendement de 44.4 % [2]) ainsi que le collage par ""wafer­bonding"" de sous­cellules fabriquées sur InP (rendement de 46% [2]) ont jusqu'ici été exploités. Dans l'optique de dépasser cette valeur record dans des cellules solaires métamorphiques, la maîtrise d'un matériau à 1 eV ayant de bonnes propriétés structurales et optoélectroniques est indispensable, et constitue un défi majeur pour la filière des MJSCs sur substrat GaAs. Nous visons à exploiter un alliage GaAsBi, avec une concentration de bismuth de 7%. En effet, cet alliage présente un désaccord de maille avec le GaAs plus faible (0.6%) que l'alliage Ga 0.69 In 0.31 As (2.2%) pour atteindre 1eV. De ce fait, la couche graduelle métamorphique AlGaInAs élaborée avant l'absorbeur 1eV qui ne contiendra que 15% d'indium au lieu de 35%, pourra être plus fine et sera de meilleure qualité structurale. Pour encore améliorer les performances de ces sous­cellules, il est aussi nécessaire de disposer de jonctions tunnel (JT) de hautes performances permettant la connexion électrique entre les sous­cellules [3]. Nous proposons ici une géométrie originale de JT et présentons les résultats obtenus. Nous démontrons la fabrication par Epitaxie par Jets Moléculaires (EJM) d'une JT AlGaInAs/AlGaAsSb hautes performances intégrée au tampon graduel relaxé réalisant l'accord de maille du GaAsBi (xBi =7%), comme présenté sur la Figure 1. Cette solution a été développée à partir d'un travail expérimental et théorique autour des hétérojonctions tunnel de type II GaAsSb/GaInAs, qui sera donc aussi détaillé. Comme preuves de concept, la croissance de cellules solaires métamorphiques GaInAs (x In =11%) (1.25 eV) et GaAsSbN (1 eV) a été réalisée et les composants sont en cours de fabrication. Les premiers résultats obtenus sur ces composants seront discutés. Références: [1] Sabnis, V., Yuen, H., Wiemer, M. (2012,). High­efficiency multijunction solar cells employing dilute nitrides."
"De nos jours, les maladies cardiovasculaires engendrent autant de décès que le cancer en Europe. Dans le but d'être en mesure de déceler ces maladies, de nombreux projets de télésurveillance médicale ont été lancé. Dans le cas de la surveillance du rythme cardiaque à l'aide d'un signal électrocardiographique, des impulsions appelées complexes QRS et synchrones avec le battement cardiaque doivent être détectées. Pour permettre la détection de ces complexes QRS dans un environnement non maîtrisé contrairement à une salle d'examen médical, un détecteur de complexes QRS tolérant au bruit est proposé dans cet article. Il repose sur un étage de mise au carré intégrable dans un circuit analogique faible coût qui a été simulé à l'aide de la technologie CMOS 0,35µm d'AMS."
"Grâce à sa sensibilité et son environnement très bas bruit, les couplages Terre–ionosphère sont observables par le magnétomètre [SQUID]2 (Superconducting QUantum Interference Device with Shielding QUalified for Ionosphere Detection). Notamment : – un mode de résonance de la mésopause excitable par les ondes P ou par champ électrique comme dans lmodifier letter apostropheheure précédant le séisme de Sichuan en mai 2008 ; – les modes S et T de respiration du globe pendant des périodes de calme magnétique et sismique ; – lmodifier letter apostropheintégrale mondiale du signal des orages magnétiques y compris les contributions polaires ; – des signaux associés aux sylphes. Ceci permet de modifier d'envisager un réseau mondial de quelques stations de même type."
"Les derniers déploiements de la technologie Internet concernent l’utilisation des objets connectés destinés notamment à la mesure localisée des grandeurs environnantes (température, éclairement, pression,…). Le défi énergétique pour assurer l’autonomie de ces capteurs peut tout-à-fait être relevé par l’utilisation de modules photovoltaïques fonctionnant dans différentes conditions d’éclairement. Plus spécifiquement, pour les utilisations en intérieur de ces objets où l’éclairement ambiant est assuré par des sources lumineuses, la question qui est prédominante concerne le comportement des technologies PV vis-à-vis des différents spectres et irradiations des sources utilisées. Durant la dernière décennie les cellules solaires organiques ont fait preuve d’un constant développement au niveau des performances de conversion énergétique et au niveau de la facilité de mise en œuvre de ces dispositifs grâce à leur flexibilité, leur faible masse et la possibilité de réaliser des « designs » attrayants pour différentes applications (taille, transparence et coloration). Dans cette thématique, la société DRACULA TECHNOLOGIES (DT) produit des cellules et modules photovoltaïques par la technique d’impression jet d’encre. Le grand avantage de l’impression à jet d’encre inhérent à la technologie numérique est la liberté des formes réalisables pour obtenir des modules organiques à surface et contour différents répondant ainsi à des besoins spécifiques. Les modules organiques fabriqués par DT sont principalement destinés à des applications intérieures sous éclairage artificiel. L’objectif de cette étude réalisée au sein du laboratoire IM2NP (équipe LUMEN-PV) est de caractériser les modules de DT dans les environnements intérieurs irradiés par des sources lumineuses LED et de suivre la stabilité et la fiabilité des modules testés. Un banc de caractérisation instrumenté spécialement conçu pour la mesure de haute précision de la récupération d'énergie photovoltaïque en intérieur avec une irradiance ultra faible (inférieure à 10 W / m²) est utilisé."
"Dans le cadre général de l'amélioration de la sélectivité de capteurs et microcapteurs de gaz, les hydroxycarbonates, dioxycarbonates et oxydes à base de terres rares font partie de catégories de matériaux évolutifs, susceptibles, de part leur changements de phases, d'être sensibles à la vapeur d'eau, au gaz carbonique et enfin à un gaz type CH4 ou CO, toxique ou d'intérêt industriel. L'étude de la stabilité des phases LaOHCO3, La2O2CO3, La2O3 puis des phases CeOHCO3, CeO2, a été entreprise afin d'évaluer d'une part leurs comportements catalytiques vis-à-vis de CH4 et CO, et d'autre part, leurs réponses électriques sous air et sous flux de CO2. Les diverses phases LaOHCO3 et CeOHCO3 ont été élaborées par voie humide à basse température. Les phases La2O2CO3, La2O3, CeO2 ont été obtenues par décomposition thermique des hydroxycarbonates. La phase CeO2 a également été obtenue sous forme nanostructurée par voie humide et à température ambiante. Chaque phase a fait l'objet d'une analyse par diffraction de rayons X, microscopies électroniques à balayage et en transmission afin de déterminer les natures des phases, les morphologies et tailles de cristallites. L'étude des interactions solide gaz a été réalisée en fonction de la température et du temps de réaction, en utilisant un réacteur tubulaire traversé par des flux air-CH4 ou air-CO. La spectroscopie infrarouge à transformée de Fourier a été utilisée pour déterminer les quantités relatives de CO2 issues de la conversion de CH4 ou CO. Les efficacités catalytiques sont mesurées en normant les intensités IRTF absorbées par rapport aux surfaces spécifiques BET. L'oxyde de lutécium peut être considéré comme meilleur catalyseur parmi les trois oxydes de terres rares étudiés, vis-à-vis de CO et de CH4. La cinétique de carbonatation sous flux de CO2 pur de La2O3 a été étudiée en analysant les prises de masses à températures fixées. En utilisant le modèle d'Avrami, nous avons mis en évidence l'existence de deux régimes lors de la formation du carbonate La2O2CO3 : un régime réactionnel puis un régime diffusionnel. Les mesures par spectroscopie d'impédance électrique ont été effectuées afin d'évaluer l'amplitude des réponses électriques liées aux divers changements de phase, en montée en température, sous air ou sous flux de CO2. Les variations électriques sont très significatives lors de la décomposition thermique de LaOHCO3 sous air. Les changements de phase (LaOHCO3ﰁLa2O3CO3ﰁLa2O3) sont identifiés au travers des variations des logarithmes de la résistance électrique, et comparés aux variations de masses observées lors des mesures ATD-TG. Les processus de carbonatation puis de décarbonatation ont été mis en évidence par thermogravimétrie sous flux de CO2 pur, à température croissante puis décroissante. Les mesures électriques sous CO2 ont de même été effectuées à température croissante : dans la phase de carbonatation, les deux régimes réactionnel et diffusionnel sont à nouveau observés. La décarbonatation observée à 800°C a permis de clairement identifier le caractère majoritaire de la conduction ionique en ions CO32- dans le composé La2O2CO3. Un ordre de grandeur de la mobilité ionique des ions carbonates à 750°C a pu ainsi être proposé, pour la première fois. La série La2O3-La2O2CO3-LaOHCO3 semble être un ensemble "" évolutif "" prometteur permettant le développement futur d'un capteur, sensible soit à la vapeur d'eau à basse température, soit à CO vers 200°C, soit à CO2 vers 500°C, soit enfin à CH4 à des températures élevées (T> 425°C)."
"Les cellules solaires en couches minces permettent de produire de l'énergie à bas-coût et sans émission de gaz à effet de serre. Dans le but de réaliser des dispositifs toujours plus performants, nous étudions l'impact de l'intégration de nanostructures métalliques (NSs) au sein de cellules solaires organiques (CSO). Ces NSs peuvent alors générer des effets diffusifs et des résonances issues de plasmons de surface. A l'aide d'un modèle numérique FDTD, nous démontrons que l'ingénierie plasmonique peut servir à augmenter l'absorption dans le matériau photoactif tout en limitant l'énergie perdue sous forme de chaleur dans les NSs. L'influence de paramètres opto-géométriques de structures associant matériaux organiques et effets plasmoniques est étudiée (diamètre, position des particules dans la couche et période du réseau de particules sphériques). Expérimentalement, des NSs d'argent ont été réalisées par évaporation sous vide puis intégrées dans des couches organiques. Nous avons mesuré une exaltation de l'absorption optique dans la gamme spectrale utile à la photo-conversion. Trois architectures différentes de CSO plasmonique ont été fabriquées et caractérisées par MEB, TEM et ToF-SIMS, puis modélisées, permettant d'identifier des verrous technologiques et de proposer des pistes d'amélioration. Nous avons aussi intégré des NSs au sein d'un empilement transparent et conducteur de type oxyde/métal/oxyde, dans le but de remplacer l'électrode classique en oxyde d'indium et d'étain d'une CSO. Le rôle de chaque couche de l'empilement sur le comportement optique de l'électrode est discuté. Les épaisseurs des couches d'une électrode de type ZnO/Ag/ZnO ont été optimisées."
"Le positionnement d'un engin sous-marin s'appuie sur des systèmes dits ""acoustiques"". Ces derniers renseignent la position relative de l'engin immergé par rapport au navire support. Les performances de ces systèmes sont définies en termes de limite de portée et de précision. Le principe de ces systèmes repose sur les notions de distance-métrie et de goniométrie, qui s'appuient toutes deux sur l'estimation du temps de propagation et donc de la date d'arrivée du signal utile. Cela est classiquement réalisé par une opération de Compression d'Impulsion. Cette technique qui est largement utilisée dans les domaines du SONAR, RADAR et imagerie bio-médicale, repose sur une application sous-optimale du Filtrage Adapté. En effet, le Filtrage Adapté est une technique d’estimation ou de détection optimale lorsque le bruit et blanc et gaussien et lorsque le signal utile est déterministe, c’est-à-dire que le signal reçu est bien connu. Cependant, il est bien connu que dans le monde sous-marin, le bruit n’est pas blanc, et pas toujours gaussien. Aussi, le signal utile étant déformé soit par le milieu de propagation soit par des phénomènes physiques tels que l’effet Doppler, celui-ci n’est pas déterministe. On peut alors considérer que le bruit est coloré et que le signal utile est une réalisation d’un processus aléatoire. Ainsi, en vue d’étendre les hypothèse d’application de la Compression d’Impulsion classique, nous proposons de construire une nouvelle forme de Compression d’Impulsion basée sur l’utilisation du Filtrage Adapté Stochastique. En effet, ce dernier est une extension naturelle du Filtrage Adapté pour des bruits colorés et des signaux déterministes. Toutefois, le Filtrage Adapté Stochastique suppose que les signaux sont stationnaires au second ordre. Or, cela n’est pas toujours le cas pour le bruit en milieu marin, et cela n’est jamais le cas pour un signal modulé en fréquence tel que ceux utilisés par les systèmes de positionnement acoustiques. Ainsi, nous proposons une nouvelle technique de Compression d’Impulsion alliant les qualités du Filtrage Adapté Stochastique et celle des techniques Temps-Fréquence. Ces dernières, et en particulier la transformée de Wigner-Ville, permettent de contourner l’hypothèse de stationnarité imposée par le Filtrage Adapté Stochastique. D’autre part, en vue de contrer l’apparition d’interférences générées par ces techniques, nous développons ici une approche par « décomposition atomique » sur une base de DCT. Ainsi donc, ces trois années de thèse, ont donné naissance à de nouvelles méthodes de Compression d'Impulsion qui permettent d'améliorer les performances des systèmes de positionnement sous-marin."
"Les cartes à puce, mondialement appelées Smart Cards, sont de véritables ordinateurs embarqués dont le but est d'effectuer des opérations de cryptographie et de stocker des données confidentielles, telles que des sommes d'argent, des informations biométriques, des droits d'accès. Il n'est donc pas étonnant que les pirates tentent de s'accaparer ces données, par des failles où la carte laisse fuir des informations capitales, appelées canaux secondaires. Le but de cette thèse est de concevoir des techniques de traitement du signal de masquage de signaux de consommation de courant. Un premier chapitre introductif présente l'univers de la carte à puce ainsi que les enjeux de sa sécurisation. Cela permet de fixer l'objectif à atteindre qui est de concevoir des techniques de masquage des signaux compatibles avec la technologie Smart Card. Le second chapitre présente l'étude et la caractérisation statistique d'un système dynamique chaotique servant à la genèse de nombres pseudo-aléatoires. Le chapitre suivant présente la méthode de masquage par décomposition des signaux, consistant à remplacer les échantillons du signal à masquer par les coefficients de son développement de Karhunen-Loève. Enfin, un dernier chapitre présente une autre technique de masquage des signaux où le modèle utilisé pour la consommation de courant est paramétrique. Le paramètre est estimé selon le critère du maximum de vraisemblance par une technique originale basée sur le couplage du filtrage adapté stochastique utilisé en détection avec l'algorithme Expectation-Maximization. Toutes les techniques sont validées à l'aide de signaux réels."
"La diffusion des dopants du Si dans les dispositifs de la microélectronique a été étudiée en 1 et 2 dimensions. Les effets de codiffusion de l'As et du P ont été caractérisés dans le but de la fabrication des « sources » et « drains » des dernières technologies de transistors (90 nm). Nous observons une accélération de la diffusion de l'As et du P lorsque ces 2 dopants sont présents en même temps dans le Si. Cet effet, qui dépend principalement de la dose d'As, semble provenir d'une modification des caractéristiques des clusters AsnV et d'un excès de lacunes dans la zone de coexistence. De plus, nous montrons que la diffusion des dopants peut être étudiée en 2 dimensions dans les dispositifs de la microélectronique, en utilisant les techniques de champ proche électriques (SCM, SSRM) et topographique (AFM). Du fait de leurs principes différents, ces techniques sont complémentaires. Elles trouvent une application en métrologie et en analyse de défaillance."
"Le besoin securitaire lie au developpement des cartes a puce intelligentes impose de fortes contraintes quant a la robustesse de fonctionnement de ces dispositifs afin de garantir des performances optimales dans un environnement sans cesse perturbe. Depuis Trois ans, des effets d'annonces, suivies de prudentes introductions commerciales, se sont multiplies pour promouvoir aupres des operateurs telecoms un nouveau concept de carte SIM : une carte, Mega, Very Large ou SuperSIM, offrant des capacites memoires etendues aptes a supporter de nouveaux services de gestion de contenus et, surtout, disposant d'un protocole de communication bien plus rapide que celui specifie par l'ISO7816-3 (9, 6kbits/s en standard). L'augmentation de ce taux de transfert est donc un enjeu important pour ce marche. Une des solutions envisagees serait d'utiliser la performance de la norme USB (Universal Serial Bus) qui est une interface rapide, bidirectionnelle, isochrone et de faible cout, dont les connections sont gerees dynamiquement. En depit de sa simplicite, cette solution a un cout. En effet, l'isochronisme n'est pas assure par le transfert d'une base de temps au travers de la connexion. Ainsi le lecteur et l'element connecte (host et device) doivent generer leurs propre reference. Cependant, celles-ci doivent avoir une precision compatible, aussi bien au niveau des taux de transfert que du nombre d'elements faisant partie de la chaine de communication. L'objectif de cette these est d'une part, la recherche de solutions innovantes et de faible cout permettant la recuperation d'horloge lors de la transmition de donnees entre la carte a puce et son lecteur en se servant du protocole USB, et d'autre part, de demontrer la faisabilite de la solution par l'implementation d'une structure robuste, a faible puissance, pour les applications Smart-Card."
"Ce travail est consacré à l’élaboration et l’étude des propriétés catalytiques, électriques et magnétiques denanomatériaux à base de ferrite de cobalt. Les nanopoudres de ferrite de cobalt (CoxFe3-xO4 , x=0.6,1,1.2,1.8 ) ont étéélaborées par une nouvelle méthode chimique solvo-thermale. Les nanopoudres obtenues sont très bien cristallisées ontdes tailles de particules qui varient avec le taux de cobalt entre 4 et 7 nm et sont très homogènes en composition. Lesnanopoudres de ferrites de cobalt sont monophasées, de structure spinelle avec un paramètre de maille qui varie enfonction du taux de cobalt. Les nanopoudres de ferrites de cobalt ne s’oxydent pas sous air et en température .Lesnanopoudres de composition proches de x=1 sont stables jusqu’à 900°C, alors que pour de plus forts écarts à lastoechiométrie, des transformations de phase ont lieu au delà de 550°C.Les mesures catalytiques ont mis en évidence l’oxydation de CH4 en CO2 après passage sur le catalyseur pour tous leséchantillons. L’efficacité catalytique est maximale et l’énergie d’activation est la plus faible pour l’échantillon x=1.8 ;ceci est lié à la plus grande surface spécifique, et au plus fort taux de sites actifs pour cette composition.Les ferrites de cobalt élaborées présentent une conduction de type électronique avec un comportement semi conducteurjusqu’à 500-600°C et un comportement métallique au-delà. Les variations de conductivité d’une composition à l’autres’expliquent par les variations du nombre de paires [Co2+,Fe3+].Les nanoparticules ont un comportement superparamagnétique quelle que soit la composition. Ce comportement estdû principalement à un effet de taille et de forme, et à une distribution cationique différente entre les deux types desites tétraédriques et octaédriques de la structure spinelle. Ces ferrites présentent une aimantation à saturation prochede celle de l’état massif, du fait de la grande qualité cristalline attribuée à la méthode d’élaboration mise au point."
"Les antireflets permettent d'accroître l'efficacité des cellules photovoltaïques, d'augmenter la sensibilité des détecteurs optroniques, et même d'améliorer l'extraction lumineuse des diodes électroluminescentes. Traditionnellement, des empilements de matériaux en couches minces sont utilisés pour les fabriquer. Nous avons étudié une technique alternative qui s'appuie sur la microstructuration de l'interface air-substrat. Il s'agit, plus précisément, de modéliser et de fabriquer des surfaces microstructurées bi-périodiques sur silicium et sur germanium présentant un effet antireflet très efficace dans l'infrarouge en bandes II et III respectivement. Ces structures nécessitent une description rigoureuse des phénomènes de propagation de la lumière. L'influence des paramètres opto-géométriques est examinée sous le point de vue des cristaux photoniques en utilisant les diagrammes de bandes. Pour réaliser ces structures, des techniques à bas coût, basées sur une gravure humide anisotrope du semi-conducteur cristallin à travers un masque obtenu par photolithographie, ont été utilisées. Nous obtenons expérimentalement sur silicium un facteur de réflexion inférieur à 4% sur l'ensemble du spectre IR II. Un très bon accord calcul/mesure permet de valider les résultats numériques obtenus précédemment."
"Cette thèse a pour but de définir et concevoir de nouvelles techniques de représentation des signauxacoustiques sous-marins. Notre objectif est d’interpréter, reconnaître et identifier de façon automatique lessignaux sous-marins émanant du système sonar. L’idée ici n’est pas de substituer la machine à l’officiermarinier, dont l’expérience et la finesse d’ouïe le rendent indispensable à ce poste, mais d’automatiser certainstraitements de l’information pour soulager l’analyste et lui offrir une aide à la décision.Dans cette thèse, nous nous inspirons de ce qui se fait de mieux dans ce domaine : l’humain. A bord d’un sousmarin,ce sont des experts de l’analyse des sons à qui l’on confie la tâche d'écoute des signaux afin de repérerles sons suspects. Ce qui nous intéresse, c’est cette capacité de l’humain à déterminer la classe d’un signalsonore sur la base de son acuité auditive. En effet, l’oreille humaine a le pouvoir de différencier deux sonsdistincts à travers des critères perceptuels psycho-acoustiques tels que le timbre, la hauteur, l’intensité.L’opérateur est également aidé par des représentations du signal sonore dans le plan temps-fréquence quiviennent s’afficher sur son poste de travail. Ainsi nous avons conçu une représentation qui se rapproche de laphysiologie de l’oreille humaine, autrement dit de la façon dont l’homme entend et perçoit les fréquences. Pourconstruire cet espace de représentation, nous utiliserons un algorithme que nous avons appelé l’Hearingogramet sa version débruitée le Denoised Hearingoram. Toutes ces représentations seront en entrée d’un systèmed’identification automatique, qui a été conçu durant cette thèse et qui est basé sur l’utilisation des SVM."
Ce manuscrit d'HDR comprend trois parties comprenant pour certaines plusieurs chapitres. Mes travaux de recherche depuis le post-dosctorat (2001) jusqu'à 2012 y sont présentés synthétiquement. Ceux-ci concernent les thématiques de microscopie à force atomique en mode non-contact et de microscopie de sonde de Kelvin appliquées à la caractérisation structurale et à la mesure des propriétés électroniques de phases de molécules organiques adsorbées sur des surfaces de sels alcalins sous ultra-vide.
"La filière des cellules solaires organiques est une alternative très intéressante pour convertir l'énergie solaire en raison des propriétés mécaniques des matériaux organiques et de leurs faibles coûts de fabrication. Des améliorations en termes d'efficacité de conversion photovoltaïque ont été récemment obtenues sur les cellules solaires organiques. Cependant, dans l'optique d'atteindre des plus hauts rendements de conversion, de l'ordre de 10%, les performances optiques et électriques de ces cellules doivent être simultanément améliorées. La photonique organique est l'une des voies très prometteuses pour répondre à cette problématique."
"Mes activités de recherches sont regroupées en trois parties principales allant de la couche sensible jusqu'au système. La première partie porte sur les plasmas froids. Le plasma, en tant que milieu physiquement et chimiquement très réactif, a essentiellement été utilisé pour le dépôt de couches minces, mais a aussi servi d'outil de caractérisation de surface ou encore de traitement de gaz. La deuxième partie traite du développement d'un capteur de gaz à partir du dépôt de la couche sensible jusqu'au dispositif. Il est présenté à partir des hypothèses de départ jusqu'aux caractéristiques du capteur. La troisième partie concerne les systèmes multicapteur. L'étude se situe cette fois au niveau du système, constitué de plusieurs capteurs, de l'acquisition des signaux et du traitement des données. Pour terminer, mes activités dans des sujets récents sont rapidement exposées dans la quatrième partie de ce mémoire."
"Les fluctuations électriques des composants sont une limitation à la miniaturisation des circuits. Malgré des procédés de fabrications en continuelle évolution, les variations des caractéristiques électriques dues au désappariement entre deux dispositifs limitent les performances des circuits. Concernant les applications à faible consommation, ces fluctuations locales peuvent devenir très critiques. Dans le contexte du développement d'une technologie CMOS 90nm avec mémoire Flash embarquée pour des applications basse consommation, l'appariement de transistors MOS est étudié. Une analyse de l'impact du dopage de grille des transistors NMOS est menée. L'étude se focalise sur l'appariement en tension des paires différentielles polarisées dans la zone de fonctionnement sous le seuil. Il est démontré que cet appariement peut être dégradé à cause de l'effet "" hump "", c'est-à-dire la présence de transistors parasites en bord d'active. Un macro-modèle permettant aux concepteurs de modéliser cet effet est présenté. Il est étudié au niveau composant, au niveau circuit et en température. Enfin, une étude de la dégradation de l'appariement des transistors MOS sous stress porteurs chauds est réalisée, validant un modèle de dégradation. Des transistors octogonaux sont proposés pour supprimer l'effet "" hump "" et donnent d'excellents résultats en termes d'appariement ainsi qu'en fiabilité."
"Ce manuscrit expose des travaux effectués entre 1994 et 2004 sur la fiabilité des composants à base de structures MOS et la fiabilité des oxydes ultra-minces de SiO2 (<10nm) utilisés comme isolant de grille dans ces composants. Nous avons établi un lien entre courants de fuite dans l'oxyde (SILC) et injection de porteurs chauds, principalement les trous chauds, dans les oxydes de 3.8 et 4.7nm. La dépendance en champ et en température du SILC soutient un modèle d'effet tunnel assisté par des défauts neutres barycentriques dans l'oxyde, même si une composante partielle de type Schottky est identifiable. Pour les claquages de type Soft-breakdown relevés, nous avons proposé un modèle simple, fondé sur un rétrécissement local de l'épaisseur d'oxyde. Le phénomène LVSILC, typique de la structure MOS en déplétion, est mis en évidence suite à des stress à tension constante pour des oxydes entre 2.5 et 1.2 nm. Nous proposons de l'interpréter comme un effet tunnel assisté par des niveaux proches des bandes de conduction ou de valence de la densité d'états d'interface. Les mécanismes de génération sont principalement déterminés par l'énergie des porteurs injectés (y compris dans le cas d'injections de porteurs chauds), et génèrent une loi d'accélération en VG pour le vieillissement en mode tunnel direct. On établit une loi générale, donnant la probabilité de création de défauts en fonction des paramètres qui déterminent l'énergie des porteurs injectés. Nos études sur les porteurs chauds nous ont aussi amené à étudier la fiabilité de transistor MOSFET lors de contraintes dynamiques (AC), caractéristiques des séquences de polarisation en mode normal de fonctionnement. Le résultat pratique de ce travail est la mise en oeuvre d'une méthodologie s'inspirant de l'hypothèse quasi-statique pour la prévision des durées de vie AC. Cette méthodologie, éprouvée et comparée aux résultats de mesure dans un certains nombre de cas où sa validité est reconnue, est appliquée au cas plus complexe du transistor de passage NMOS. L'accord reste satisfaisant, mais nous avons également mis en évidence les limitations de cette technique lors de séquences faisant intervenir des relaxations, des périodes de dépiégegage ou des dégradations bi-directionnelles. Concernant le lien entre les étapes du procédé et la fiabilité, nous avons étudié l'influence d'une étape d'implantation ionique à haute énergie, qui induit un dégât dans le volume du semi-conducteur détecté électriquement par C(V), mais aussi des courants de fuite similaires au SILC (IILC Implantation Induced Leakage Current). Nous avons mis au point une méthodologie optimisée de détection du Wafer Charging, utilisant des injections très courtes de porteurs chauds (au pic de courant électronique) dans le transistor PMOS. Cette méthode s'est révélée plus sensible et plus révélatrice que les injections pratiquées en régime Fowler-Nordheim ou la simple étude paramétrique pour détecter les défauts latents issus du charging dans les oxydes minces. Enfin, nous avons identifié par DLTS les défauts issus d'une contamination au Fer dans le Silicium (paire Fe-B et Fer interstitiel Fei) et avons observé la re-transformation spontanée du Fei en paire Fe-B en quelques heures."
"La microélectronique a montré une évolution rapide motivée par l'accroissement des performances et par l'abaissement des coûts. Le marché des mémoires est un domaine clé de ce secteur. L'enjeu majeur est d'accéder à la mémoire universelle qui remplacera toutes les autres en associant la densité et l'endurance ""illimitée"" des DRAM, la rapidité des SRAM et la non-volatilité des Flash. Nous nous sommes intéressés aux technologies MRAM et OxRRAM possédant l'avantage d'être, comme la technologie Flash, non volatile et compatible avec la technologie MOS. Elles promettent également, suivant l'architecture adoptée, d'être aussi rapides qu'une SRAM, aussi dense qu'une DRAM et avoir une endurance quasi-illimitée. Ces technologies reposent sur des concepts dans lesquels la discrimination des deux états du point mémoire est assurée par un changement de résistance. La première partie de cette thèse a été consacrée à la technologie MRAM et notamment à la fiabilité de l'oxyde tunnel intégré dans la jonction magnétique, élément de base des cellules mémoires MRAM. La seconde partie a été axée sur le développement et la compréhension des mécanismes physiques de programmation des mémoires OxRRAM intégrant un oxyde binaire NiO dans l'élément de mémorisation. Un accent particulier a été porté sur le développement d'une solution technologique simple dans son mode de fabrication et permettant d'aboutir à un empilement présentant des performances électriques conformes aux spécifications. Il est alors possible d'envisager l'intégration de l'oxyde de nickel dans des structures de très faibles dimensions et de viser une réduction substantielle de la taille de la cellule mémoire"
"La surface occupée par la mémoire et la circuiterie digitale dans une carte à puce est prépondérante, ce qui motive l'utilisation de technologies à forte densité, mais impliquant une tension d'alimentation Vdd en dessous du volt. Par ailleurs, les cartes à puces étant destinées à des applications nomades, leur consommation est limitée alors que les fonctionnalités demandées deviennent plus nombreuses, ce qui nécessite de diminuer la consommation de chaque fonction élémentaire. Ainsi, le concepteur est amené à dimensionner des cellules analogiques fonctionnant sous spécifications nanowatt (faible tension d'alimentation - au plus 1 Volt - et faible consommation - quelques dizaines à quelques centaines de nano-ampères - ). Cette étude traite de l'élaboration d'une méthodologie de conception de circuits analogiques nanowatt, et de son application au domaine de la carte à puce. La méthodologie développée a été appliquée à des architectures autopolarisées et à polarisation fixée. Les circuits dimensionnés sous spécifications nanowatt, ont été simulés avec Spectre et les paramètres BSIM3v3 de la technologie CMOS 0.15µm de la société ATMEL. Les transistors qui ont été utilisés sont des transistors haute tension (HV oxyde épais). Les résultats de simulation se sont révélés cohérents avec les performances prédites par la méthodologie. Les mesures expérimentales ont confirmé l'aptitude de la méthodologie au dimensionnement de circuits sous spécifications nanowatt."
"Les circuits à haut technologie d'aujourd'hui requièrent toujours plus de services et de sécurité. Le marché correspondant est orienté vers de la reconfigurabilité. Dans cette thèse je propose une nouvelle solution de coprocesseur cryptographique multi-algorithmes, appelé Celator. Celator est capable de crypter et décrypter des blocs de données en utilisant des algorithmes cryptographiques à clé symétrique tel que l'Advanced Encryption Standard (AES) ou le Data Encryption Standard (DES). De plus, Celator permet de hacher des données en utilisant le Secure Hash Algorithm (SHA). Ces algorithmes sont implémentés de façon matérielle ou logicielle dans les produits sécurisés. Celator appartient à la classe des implémentations matérielles flexibles, et permet à son utilisateur, sous certaines conditions, d'exécuter des algorithmes cryptographiques standards ou propriétaires. L'architecture de Celator est basée sur un réseau systolique de 4x4 Processing Elements, nommé réseau de PE, commandé par un Contrôleur réalisé avec une Machine d'États Finis (FSM) et une mémoire locale. Cette thèse présente l'architecture de Celator, ainsi que les opérations de base nécessaires pour qu'il exécute AES, DES et SHA. Les performances de Celator sont également présentées, et comparées à celles d'autres circuits sécurisés."
"Au cours des dernières décennies, les chromophores organiques “push-pull” (c’est-à-dire donneur-espaceur-accepteur) ont vu leur intérêt grandir en raison de leurs applications potentielles dans les domaines des transistors { effet de champ, de l'optique non linéaire, des OLEDs, et du photovoltaïque. Parmi les différents systèmes pi-conjugués connus, les structures basées sur le thiophène ont conduit à une large variété d'applications en science des matériaux en raison notamment de la flexibilité de leur synthèse permettant de modifier aisément leurs propriétés optiques et électrochimiques. En revanche, à ce jour les couches auto-assemblées des chromophores ”push-pull” sur une surface et leurs applications n’ont pas fait l’objet d’études approfondies. Dans le cadre de la conception de cellules photovoltaïques, ces structures moléculaires correctement organisées sur une surface devraient permettre d’améliorer l’interface donneur/accepteur, l’absorption optique, et d’augmenter le volume de la couche active. Ces deux derniers points peuvent bénéficier de multicouches organiques, qui ouvrent la voie aux cellules PV tandem ou multi-jonctions, et d’effets plasmoniques par l’insertion de nanoparticules de métaux nobles. Dans cette perspective nous avons développé une synthèse en plusieurs étapes de nouvelles molécules “push-pull” comportant une tête réactive thiol autorisant la formation de monocouches moléculaires auto-assemblées (SAM) sur surfaces d’or ou d’ITO. En variant les groupements donneur, accepteur, et l’espaceur il a été possible de moduler les propriétés optiques et électroniques des “push-pull” comme le montrent la voltampérométrie cyclique et la spectroscopie d’absorption UV-Visible. En particulier, la position de la LUMO peut être contrôlée par le choix de l’accepteur. Les produits obtenus possèdent une forte absorption de lumière (λmax près de 550 nm et 610 mn) et peuvent donc être efficaces pour le photovoltaïque. Les SAMs incorporant des nanoparticules d’or ont été préparées sur or et ITO et la cinétique de croissance des couches successives a été suivie par voltampérométrie cyclique et par spectroscopie d’abosrption UV-Vis. Les monocouches moléculaires finales des chromophores avec ou sans nanoparticules d’or ont été étudiées principalement par angles de contact, techniques de spectroscopie IR, UV-Vis, XPS, et par microscopie à sonde locale (STM, AFM). Les matériaux ainsi obtenus à base de SAMs de chromophores “push-pull” et de nanoparticules de métaux nobles ont ensuite caractérisés électriquement et optiquement pour évaluer leur utilisation potentielle pour la conversion de l’énergie photovoltaïque."
"Ce travail est effectué dans le cadre d'une thèse Conventions Industrielles de Formation par la Recherche (CIFRE) entre l’entreprise Sophia Antipolis Énergie Développement (SAED) à Valbonne et l'Institut Matériaux Microélectronique Nanosciences de Provence (IM2NP) – CNRS – Université du Sud Toulon-Var.L’objectif de cette collaboration est l’évaluation du potentiel technico-économique de divers matériaux pour le stockage de l’énergie thermique par chaleur latente, adapté aux niveaux de température des capteurs solaires développés par SAED. En effet, le stockage de l’énergie est un des principaux verrous technologiques reconnus pour les procédés ayant recours à des énergies renouvelables intermittentes et en particulier pour les centrales héliothermodynamiques.Après une introduction sur le potentiel et l’intérêt des centrales solaires thermodynamiques à basse température, un bref état de l’art des principaux types de stockage de l’énergie est présenté. Le deuxième chapitre aborde plus en détail le principe du stockage thermique par chaleur latente et recense une centaine de matériaux sélectionnés dans la littérature pour leur changement de phase dans la gamme de température 70 - 140°C. Les critères de sélection retenus y sont exposés.Des analyses thermiques par calorimétrie différentielle à balayage sont effectuées de façon systématique sur les différents Matériaux à Changement de Phase (MCP) sélectionnés. Les résultats de ces mesures, présentés dans le chapitre III, caractérisent avec précision le comportement de ces matériaux au chauffage. La transformation au refroidissement est étudiée au moyen d’un dispositif conçu spécifiquement pour représenter au mieux les conditions imposées dans une enceinte industrielle. Cette étude, présentée dans le chapitre IV, permet d’affiner la sélection des MCP pour ne garder que ceux dont la réversibilité du changement d’état est compatible avec une utilisation industrielle en tant que milieu de stockage de l’énergie thermique. Les chapitres V et VI permettent d’étudier plus en détails les spécificités de deux types de MCP que sont les polyols et les mélanges eutectiques de nitrates.Le dernier chapitre est consacré à la modélisation des échanges thermiques au sein d’une cuve de stockage contenant un MCP encapsulé. L’objectif est de disposer d’un outil de prédiction des performances d’une unité de stockage par chaleur latente, afin d’analyser l’influence des différentes solutions envisagées sur le productible d’une centrale thermodynamique solaire et leur impact sur le coût du kWh électrique produit."
Cette thèse présente des méthodes de conception d'amplificateurs faible bruits en technologie CMOS standard. Des méthodes pour la conception d'amplificateurs bande étroite et large bande sont abordées.
"La nanostructuration de la matière parauto-assemblage est actuellement un des domaines de recherche fondamentale les plus dynamiques et ouvre de vastes perspectives technologiques. Cette thèse se propose d'étudier l'auto-assemblage de nanocristaux d'oxalate de cuivre. Ce composé peut être considéré comme système modèle dont les propriétés permettent une transposition à l'élaboration de nanostructures complexes. Une étude bibliographique portant d'une part sur le phénomène d'auto-assemblage à l'échelle mésoscopique, d'autre part sur le cas particulier des oxalates de métaux divalents constitue la première partie de ce travail. Puis, la caractérisation structurale des nanocristaux d'oxalate de cuivre et l'influence des conditions de synthèse sur leur auto-assemblage permettent d'aboutir à un modèle de mésocristaux issus d'une orientation des nanocristaux par reconnaissance de faces cristallines. Enfin, les modifications morphologiques des mésocristaux en présence d'additifs et l'étude spectroscopique de ces nanostructures confirment le modèle proposé par la mise en évidence d'une adsorption sélective des additifs sur certaines faces des nanocristaux."
"L'intégration du siliciure de nickel allié à un faible pourcentage de platine dans un environnement de transistors CMOS génère des difficultés à contrôler sa formation. Ces problèmes peuvent se traduire par une migration anormale du nickel court-circuitant le transistor, impactant les rendements de fabrication. L'objectif de cette thèse est d'améliorer la compréhension de ce phénomène physique apparaissant de manière aléatoire à l'échelle d'un circuit intégré pour la microélectronique avancée. L'étude de ce phénomène rare a été conduite à l'aide de méthodes de caractérisation locales aux limites des possibilités techniques actuelles : détection des fuites par contraste de tension, SIMS, Microscopie électronique et sonde atomique tomographique. L'ensemble des résultats statistiques et des caractérisations réalisées ont permis de proposer un scénario de formation des défauts du siliciure en fonction des conditions de sa formation et de la redistribution des éléments chimiques en présence."
"Depuis une vingtaine d'années, l'industrie de la microélectronique et en particulier le marché des mémoires non-volatiles connaît une évolution considérable, en termes d'augmentation de la capacité d'intégration et de diminution du prix de revient. Ceci a permis au grand public d'accéder aux produits électroniques (téléphones portables, baladeurs MP3, clés USB, appareils photos numériques...) qui connaissent actuellement un énorme succès. Cependant, la miniaturisation des mémoires Flash risque de rencontrer des limitations. C'est pourquoi les industriels et les laboratoires recherchent actuellement de nouvelles voies qui permettraient de prolonger la durée de vie de ces dispositifs. Dans ce contexte, l'objectif premier de cette thèse est l'étude expérimentale et théorique des mémoires non-volatiles à nanocristaux de silicium. Nous avons montré les différentes possibilités d'intégration des nanocristaux de silicium à partir d'un procédé de fabrication standard. Un démonstrateur Flash NOR 32 Mb à nanocristaux de silicium a été réalisé à partir d'un produit ATMEL. Nous nous sommes ensuite intéressés à la caractérisation électrique des cellules et matrices mémoires. Une étude exhaustive de l'influence des conditions de programmation ainsi que des paramètres technologiques sur les performances électriques a été menée. La modélisation de l'effacement Fowler-Nordheim et du « gate disturb » a permis de comprendre l'influence de certains de ces paramètres. Concernant l'écriture par porteurs chauds, nous avons étudié l'influence des conditions d'écriture sur la localisation de la charge à l'aide de simulations TCAD et d'un modèle analytique couplé à des mesures expérimentales."
"L'autocalibration des positions des capteurs formant une grande antenne réseau aéroportée s'appuie sur les enregistrements de sources d'opportunité de directions d'arrivée inconnues, bande-étroite, émettant simultanément sur une même fréquence porteuse. Ce problème non-observable peut le devenir localement si l'on dispose de suffisamment de sources d'opportunité ou d'un modèle de déformations de voilure. Une étude de deux approches de la littérature est proposée. La première, basée sur le principe du Maximum de Vraisemblance est itérative ; la seconde, basée sur une méthode de sous-espace/modules constant (SEMC) identifie algébriquement la matrice de transfert du réseau. Leurs limites sont montrées quand le niveau de déformation est supérieur à une demi-longueur d'onde. Dans ce cas, des ambiguïtés de phase engendrent des positions erronées. Des solutions originales sont proposées pour estimer les positions des capteurs dans le cas de déformations statiques importantes. Trois sources d'opportunité et l'utilisation d'un modèle polynomial de déformation ou plus simplement des contraintes physiques couplées à une méthode de résolution des ambiguïtés de phase, permettent d'autocalibrer l'antenne. Enfin, pour autocalibrer une antenne vibrante grandement déformée une approche basée sur SEMC est proposée. Elle autorise la résolution des ambiguïtés de phase en intégrant suffisamment d'échantillons et permet ensuite de suivre l'antenne au cours des vibrations en utilisant un temps d'intégration plus court. Une extension pour des sources de fréquences porteuses différentes est finalement présentée."
"L'utilisation de machines à mesure tridimensionnelle s'est généralisée dans les grosses entreprises et chez les sous traitants. Mais la mise en place des normes ISO 9000 et les calculs de capabilité qui découlent de ces normes impliquent une connaissance approfondie des incertitudes de mesures. Dans le domaine de la machine à mesurer, l'incertitude globale et les normes utilisées sont indiquées. Actuellement, les incertitudes de mesure de ces différentes machines sont exprimées sous la forme : ± A+ bL ou A est une incertitude représentative de la fidélité et bL est une incertitude de position en fonction de la longueur. La géométrie et l'environnement de ces machines étant actuellement pris en compte par les logiciels, il reste un grand nombre de paramètres méconnus (relatif au capteur) : longueur, diamètre, vitesse ainsi que le type et nombre de points de l'élément mesuré. Nous proposons de donner une méthode d'obtention rapide de l'incertitude de mesure et de l'indice de capabilité suivant les critères choisis par l'opérateur et d'obtenir rapidement un indice de capabilité plus proche de la réalité. Après un historique sur les mesures et de l'importance de la métrologie dans la mise en place des ISO 9000, le troisième chapitre est une présentation du matériel utilisé. Dans le quatrième, le plus important, on trouvera les méthodes statistiques utilisées et l'analyse des différents paramètres étudiés. Grâce à une analyse systématique des résultats, nous validerons les différents paramètres étudiés et de proposer le tableau récapitulatif permettant d'obtenir instantanément l'indice de capabilité. Dans les chapitres suivants des exemples et une proposition de modification des logiciels actuels afin d'intégrer ces calculs de capabilité. Les annexes comprennent normes, certificats d'étalonnage, valeurs de la pièce test et programmes développés."
"Lors de ce cours, nous discuterons des principes de base de la microscopie à force atomique en mode de non-contact (nc-AFM, ou FM-AFM), technique de microscopie de force dynamique utilisée essentiellement dans un environnement ultra-vide. Nous nous restreindrons aux instruments basés sur une détection optique de l'amplitude d'oscillation du cantilever, i.e. à des amplitudes d'oscillations supérieures au nanomètre. Cette présentation sera découpée en deux parties. L'une, fondamentale, dans laquelle nous détaillerons le principe de fonctionnement de l'instrument en relation avec les forces auxquelles la pointe est sensible ainsi que les modélisations qui en ont été faites depuis son développement instrumental, débuté dès 1991 ; l'autre, plus pratique, dans laquelle nous illustrerons "" pratiquement "" comment utiliser un nc-AFM pour imager une surface. Les grands points abordés lors de cette présentation seront donc : - Schéma bloc d'un nc-AFM : principe de fonctionnement - Les principales composantes des forces impliquées entre la pointe et la surface en nc-AFM - Modélisation du principe de fonctionnement d'un nc-AFM : quels sont les signaux de mesure ? - Aspect partiellement quantitatif des mesures : calibration de l'amplitude d'oscillation, courbes spectroscopiques et déconvolution de la force - Illustration pratique d'une séquence complète de manip"
"Les décharges électrostatiques (ESD) constituent un problème majeur de fiabilité pour les entreprises de semi-conducteurs. Pour enrayer les défauts générés par les ESD sur les circuits intégrés (ICs), des éléments de protection sont implantés directement dans les puces. La constante poussée de l'intégration des circuits a pour conséquence la réduction des dimensions des cellules technologiques élémentaires ainsi que l'accroissement du nombre d'applications supportées par les ICs. Les conditions restrictives imposées par les procédés technologiques et par la complexité croissante des systèmes entraînent un défi considérablement accru pour le développement de produits robustes aux ESD. Dans ce travail de recherche, le problème émergeant des défaillances des couches d'oxydes minces d'épaisseur Tox = 8 à 1.1nm sous contraintes ESD est adressé dans les technologies CMOS les plus avancées, par une contribution à la compréhension des mécanismes de dégradation de la fiabilité du diélectrique et des dispositifs sous contraintes ESD. Une nouvelle approche de caractérisation des oxydes minces sous des stress à pulses ultra-courts (20 ns) est décrite jusqu'à la modélisation complète de la dépendance temporelle du claquage du diélectrique. Basé sur un ensemble cohérent de modélisations, une nouvelle méthodologie est proposée pour ajuster la détermination de la fenêtre ESD de façon mieux adaptée aux intervalles de tension et d'épaisseur d'oxyde de grille pour l'ingénierie des concepts de protection. Ceci a permis d'améliorer la prise en compte des problèmes ESD pour une meilleure fiabilité et robustesse des produits conçus en technologies CMOS fortement sub-microniques vis-à-vis des décharges électrostatiques."
"Les travaux présentés dans ce manuscrit, sont basés sur l'étude de l'auto-organisation de la matière à l'échelle nanométrique. A cette échelle, les énergies de surfaces jouent un rôle prépondérant dans cette organisation. Pour comprendre au mieux ses mécanismes nous avons étudié plusieurs types de structures à base de Silicium et de Germanium. Nous avons expérimentalement étudié la croissance cristalline ou amorphe sur différents types de substrats (amorphe : SiO2 et cristallins Si ou SOI). Certain de ces substrats furent nano-structurés en utilisant un faisceau d'ions focalisés de type Gallium ou Or-Silicium. De plus nous avons pu utiliser des surfaces différentes telle que le TiO2 ou le Silicium poreux, afin d'étudier l'organisation de la matière sur des pores de petites tailles (inférieurs à 50nm)."
"Les nanocristaux de semi-conducteurs, ou boites quantiques, trouvent leur application dans de nombreux domaines. Pendant cette thèse nous avons étudié les propriétés optiques de couches minces nanocomposites de polymère PMMA contenant différentes concentrations de boites quantiques CdSe/ZnS. Les spectres d'absorption et de luminescence peuvent être expliqués par la mécanique quantique. A partir des spectres de luminescence mesurés nous montrons clairement l'effet du couplage entre les boites quantiques. Sous l'effet d'un faisceau pompe à 514nm le spectre de luminescence centré à 560nm évolue fortement au cours du temps. Nous montrons que ces couches luminescentes qui convertissent ainsi les fréquences optiques peuvent permettre d'augmenter l'efficacité de cellules solaires. Par ailleurs, pour bénéficier au mieux du fort rendement de photoluminescence, il est nécessaire de contrôler la répartition spatiale de la lumière émise. Pour contrôler cette répartition une nanostructure bi périodique a été réalisée dans des couches de PMMA contenant les boites quantiques par nano-impression, en utilisant un moule en silicium gravé. La caractérisation de la structure réalisée met en évidence la qualité de la méthode utilisée. On montre également, par la théorie, à la fois que le champ local est résonant dans la structure et que la lumière se répartie en champ lointain dans les directions de diffraction contrôlées par la période du réseau."
"Le premier chapitre de ce mémoire intitulé ""Amplificateurs faible bruit accordés pour systèmes intégrés CMOS"" s'intéresse aux méthodes de conception permettant l'intégration complète de l'amplificateur faible bruit d'une (LNA) depuis la gamme des radiofréquences jusqu'à la gamme des fréquences millimétriques. Ces travaux ont été menés dans le cadre de la Thèse de Mathieu Egels et dans le cadre d'une convention de recherche avec la société ST-Microélectronics financée par le Conseil Général des Bouches du Rhône. Le deuxième chapitre est intitulé ""Amplificateurs bas niveau large bande pour systèmes intégrés CMOS"". Ce chapitre présente les solutions que nous avons développées au laboratoire qui permettent de contrôler la bande passante des amplificateurs faible bruit pour systèmes intégrés destinés aux applications utilisant les normes UWB ainsi que des études plus prospectives sur l'amplification distribuée CMOS pour des applications à très grandes bandes passantes. Dans la dernière partie de ce chapitre nous décrivons nos travaux concernant la mise en boîtier des circuits et systèmes intégrés haute fréquence et large bande. Ces différents travaux ont été réalisés d'une part dans le cadre des Thèses de Mathieu Egels, et de Marc Battista, dans le cadre d'une convention de recherche avec la société ST-Microélectronics financée par le Conseil Général des Bouches du Rhône, et d'autre part dans le cadre de la thèse de Romen Cubillo avec le soutien de la plateforme conception du Centre Intégré de Microélectronique de la région PACA (CIMPACA). Le troisième chapitre ""Convertisseurs RF/DC pour la téléalimentation haute fréquence en RFID"" décrit nos activités de recherche concernant les circuits et architectures pour la télé-alimentation des circuits intégrés au moyen d'une onde électromagnétique. Les applications ciblées concernent essentiellement les étiquettes électroniques sans contact dans le domaine des fréquences UHF pour lesquelles nous avons développé des circuits et des architectures pour les technologies CMOS standard. Ces travaux ont été réalisés dans le cadre de la Thèse de Emmanuel Bergeret dans le cadre d'une convention de recherche avec la société ST-Microélectronics soutenue par le Conseil Général des Bouches du Rhône. Dans ce mémoire nous nous attacherons à décrire l'état de l'art des différents thèmes de recherche abordés et à situer nos travaux vis-à-vis de cet état de l'art. Le détail de nos travaux de recherche étant disponible dans les différents articles et thèses référencés, nous donnerons dans ce mémoire uniquement les grandes lignes de nos études et les principaux résultats obtenus."
"Ce travail de thèse porte sur l'étude de l'interdiffusion et des contraintes dans des systèmes métalliques modèles en couplant la diffraction des rayons X connue pour sa très grande sensibilité aux variations de distances interréticulaires et la modélisation. Les systèmes modèles choisis sont des multicouches Cu/Ni et Mo/V épitaxiées sur MgO. Dans ces deux systèmes le coefficient de diffusion dépend fortement de la concentration ce qui doit donner lieu lors de l'interdiffusion à des profils de concentration très asymétriques. Pour étudier l'évolution des profils de concentration et de distance lors de la cinétique d'interdiffusion, un programme couplant cinétique d'interdiffusion et évolution des spectres de diffraction symétrique coplanaire a été mis en place avec succès. Il s'appuie sur le modèle de Martin (modèle d'Ising cinétique dans l'approximation de champ moyen) pour simuler l'interdiffusion et utilise la théorie cinématique pour calculer le diagramme de diffraction. De plus, il intègre une relation d'élasticité entre champs de déformation et champs de concentration en tenant compte de la cohérence des interfaces. Ce programme a permis d'établir l'existence d'une forte asymétrie de diffusion dans les systèmes Cu/Ni et Mo/V avec des paramètres contrôlant l'asymétrie d'interdiffusion similaires pour les différents échantillons étudiés. De plus ces paramètres sont très proches de ceux donnés par la littérature établies sur des systèmes non contraints. Ce constat indique que le fort état de déformation de ces multicouches n'affecte pas la cinétique d'interdiffusion."
"La nécessité d’économiser de l'énergie est l’un des axes importants de ces dernières décennies, d’où le besoin de surveiller la consommation d'énergie des processus résidentiels et industriels. Le travail de recherche présenté dans ce manuscrit s’inscrit plus particulièrement dans le suivi de la consommation électrique afin de permettre l’économie d’énergie. Le but final étant d'avoir une connaissance précise et fiable d'un réseau électrique donné. Cela passe par la décomposition de la consommation électrique globale du réseau électrique étudié afin de fournir une analyse détaillée de l'énergie consommée par usage. L’objectif de cette thèse est la mise en place d’une approche non-intrusive permettant de réaliser les étapes de détection d’évènements et d’extraction de caractéristiques, qui précédent les étapes de classification et d’estimation de la consommation électrique par usage. L’algorithme résultant des travaux effectués durant cette thèse permet de détecter les évènements qui surviennent sur le courant et d’y associer un vecteur d’information contenant des paramètres caractérisant le régime permanent et le régime transitoire. Ce vecteur d’information permet ensuite de reconnaître tous les évènements liés à la même charge électrique."
"Cette thèse étudie divers aspects de la fiabilité des mémoires, notamment les tests en endurance et les tenues en rétention sur des mémoires Flash, en architectures NOR et NAND. Nous abordons différentes méthodes de programmation existantes dans la littérature, à savoir l'utilisation de signaux très courts et un algorithme de programmation intelligent, que nous avons appliquées sur nos cellules mémoires afin de réduire la dégradation qu'elles subissent lors des phases successives de programmation /effacement. Les améliorations observées n'étant pas significatives, nous n'avons pas choisi d'utiliser de tels signaux dans la suite de notre étude. Nous présentons également une théorie des signaux optimisés qui n'a pas été approfondie ici mais que nous avons étudiée dans une étude préalable à cette thèse. Nous présentons ensuite une modélisation des pertes de charges en rétention à partir d'équations simples de types Fowler-Nordheim et Poole-Frenkel qui se superposent et respectivement prépondérantes à des temps de rétention élevés (t>200h) et courts (t<200h). Nous proposons enfin une étude des perturbations intervenant dans une matrice mémoire, à la fois du point de vue des tensions électriques appliquées sur les cellules mais aussi du point de vue des capacités de couplages parasites. Nous avons dans un premier temps évalué les valeurs de perturbation de grille sur des cellules mémoires Flash en architecture NOR puis NAND avant de traiter des capacités parasites entre cellules dans une matrice. Nous avons été amenés à étudier ces capacités dans la cadre de l'étude des dégradations excessives des cellules inhibées lors de tests en endurance pour certaines conditions process non-optimisées. Nous avons pour cela développé une simulation TCAD bidimensionnelle à partir des étapes process réelles que nous avons ensuite calibrée sur des mesures sur silicium. Enfin cette simulation a été complétée par une prise en compte des capacités parasites de couplage, extraites sur une simulation tridimensionnelle d'une matrice 3x3 de cellules mémoires. Les valeurs de ces capacités ont été validées par des mesures sur des structures de test spécifiques et par calcul géométrique. Notre simulation bidimensionnelle émule donc un comportement tridimensionnel tout en restant dans une rapidité de calcul liée à une simulation 2D. Nous avons ainsi pu développer des simulations électriques permettant de visualiser le phénomène d'inhibition des cellules, tout au long de l'application des diverses polarisations sur la structure."
"Dans cette thèse, nous étudions l’effet du désordre magnétique sur les propriétés de transport électronique du graphène et des isolants topologiques 2D de type HgTe. Le graphène et les isolants topologiques sont des matériaux dont les excitations électroniques sont assimilées à des fermions de Dirac sans masse. Ces matériaux présentent un grand intérêt pour remplacer le silicium dans les dispositifs électroniques, de par leurs propriétés de conduction : grande mobilité électronique (résistance nulle), possibilité de fabriquer des transistors à base de graphène fonctionnant à des fréquences de l’ordre du térahertz, manipulation du spin des porteurs avec les isolants topologiques pour fabriquer des dispositifs de spintronique. Nous nous proposons d’étudier l’influence du désordre magnétique pour deux raisons principales : la première est que les matériaux bi-dimensionnels contiennent une grande concentration de défauts et d’impuretés qui peuvent posséder un spin (une lacune dans le graphène peut par exemple acquérir un spin), la seconde est que le dopage de matériaux avec des espèces magnétiques peut être une façon de changer les propriétés de conduction (manipulation de courants de spin, modification de la conductance) à volonté en jouant sur la magnétisation du désordre. L’influence des impuretés magnétiques sur les propriétés de transport du graphène est étudiée dans le régime de forts champs électriques. En conséquence de la production de paires électron-trou, la réponse devient non linéaire et dépend de la polarisation magnétique. Dans une phase paramagnétique, la symétrie par renversement du temps est statistiquement préservée, et les propriétés de transport sont similaires au cas sans impuretés. Au contraire, dans la phase antiferromagnétique, le système subit une transition entre un étalement super-diffusif et sous-diffusif du paquet d’ondes, mettant en évidence le développement d’états localisés. Ce régime critique est caractérisé par l’apparition d’états électroniques de géométrie multi-fractale près du gap. la densité locale d’états se concentre en larges patches ayant une corrélation charge-spin définie. Dans cet état, la conductivité tend vers la moitié de la conductivité linéaire du graphène pur. Nous étudions une transition entre un isolant topologique bi-dimensionnel conducteur, caractérisé par une conductance G = 2 (en quantum de conductance) et un isolant de Chern avec G = 1, induite par des impuretés magnétiques polarisées. Deux types de couplages, ferromagnétique et antiferromagnétique, sont considérés entre les bandes d’électrons et les bandes de trous. Nous démontrons que pour un désordre fort, une phase G = 1 existe même pour l’ordre ferromagnétique, contrairement aux prédictions de l’approximation en champ moyen. Ce résultat est soutenu par des calculs numériques directs en utilisant la formule de Landauer du transport, et par des calculs analytiques sur la renormalisation du potentiel chimique et de la masse topologique en fonction de l’intensité du désordre, dans l’approximation de Born auto-cohérente au second ordre. La transition est reliée à la suppression d’un des canaux de conduction lié à un spin donné, pour un désordre assez fort, par la diffusion sélective en spin et la localisation."
"Afin de continuer l'amélioration des performances du transistor MOSFET à l'échelle décananométrique, la recherche en microélectronique explore différentes solutions. Les travaux menés au cours de cette thèse se sont plus particulièrement orientés vers l'étude de transistors innovants avec une architecture Double-Grille (DGMOSFET) et l'utilisation de “nouveaux” matériaux tels que les diélectriques de grille à forte permittivité dits “high-κ” et les semiconducteurs à forte mobilité intrinsèque (Ge et III-V). Grâce au développement de codes de simulation numérique basés sur la résolution auto-cohérente du couple d'équations Poisson-Schrödinger ou en utilisant le formalisme des fonctions de Green (NEGF), nous étudions le comportement électrique de différentes structures. Dans un premier temps, le fonctionnement des capacités Métal-Isolant-Semiconducteur et Métal-Isolant-Métal est simulé afin d'évaluer l'influence des propriétés des matériaux innovants et de la composition de l'empilement de grille sur les caractéristiques capacité-tension et sur le courant de fuite tunnel à travers la grille. Puis, les performances en termes de courant de drain face à la réduction de la longueur de grille (effets électrostatiques) et de l'épaisseur du canal de conduction (effet de confinement quantique) sont comparées dans le transistor MOS Double-Grille (à grilles indépendantes ou connectées) avec plusieurs matériaux aux propriétés très différentes (Si, Ge, GaAs et In0.53Ga0.47As). Enfin, nous avons développé une approche simplifiée (modélisation compacte) pour le calcul du courant de drain en dérive-diffusion ou balistique dans les transistors MOS Double-Grille à grilles indépendantes, validée par nos codes de simulation numérique."
"Cette étude de la fonctionnalisation du graphène se base principalement sur la monocouche de graphène épitaxiée sur SiC, dont nous avons notamment modifié les propriétés électroniques via le dopage par exposition à un plasma d'azote et par l'hydrogènation. Les propriétés électroniques, structurales et la composition chimique du graphène fonctionnalisé sont étudiées in situ et ex situ par des techniques de spectroscopies électroniques, principalement la photoémission inverse résolue angulairement (ARIPES). L'incorporation d'azote dans le graphène réalisée par l'exposition à un plasma d'azote entraîne un décalage des niveaux inoccupés du graphène vers le niveau de fermi. Ce dopage-n est attribué à la présence d'atomes d'azote substitutionnels entourés de trois voisins carbone (N-graphitique). D'autres centres azotés, associés à des lacunes, sont présents dans le graphène, sous forme d'azotes pyrroliques et pyridinique, mais la configuration majoritaire peut tre contrlée efficacement par l'énergie du plasma, les espèces d'azote incidentes (atomes ou/et ions), et l'épaisseur de la couche de graphène de départ. Nous étudions ensuite l'hydrogénation de la couche tampon de graphène (BLG) sur SiC en fonction de la température. A l'ambiante, l'hydrogène adsorbé sur le graphène sature les liaisons pendantes de Si de l'interface par un processus indirect impliquant la formation de nouvelles liaisons C-Si. Le BLG ainsi hydrogéné est un isolant dont la bande interdite (EGAP ~5 eV) est proche de celle du graphane, alors que le BLG est un isolant de Mott-Hubbard (EGAP ~1.6 eV). A haute température, l'hydrogène s'intercale sous le BLG, exposant ainsi une monocouche de graphène quasi flottante (QFSG) non-dopée, les liaisons pendantes du substrat étant complètement saturées par l'hydrogèene intercalé. Sur la base de ces propriétés, nous proposons un nouveau concept de fabrication de dispositifs à base de graphène sur SiC. En fin, nous avons également étudié la réaction entre des molécules pi-conjuguées et le graphène vierge ou dopé à l'azote. Les états inoccupés des molécules dérivées du pérylène (PTCDA, PTCDI) sont légèrement modifiés sur le graphène dopé N à cause d'un renforcement du transfert électronique vers la molécule. Des réactions chimiques entre ces molécules et le graphène sont observées après exposition aux électrons de basse énergie. En résumé, cette étude permettra une meilleure maîtrise des propriétés électroniques des matériaux 2D comme le graphène, et facilitera le développement de nouvelles applications à base de graphène, notamment en nano-électronique."
"La réaction de films minces métalliques avec un substrat de silicium reste encore peu étudiée dans le domaine des très faibles épaisseurs. Afin de suivre les cinétiques de réaction et l'évolution des contraintes lors de la formation d'une phase, des mesures de diffraction du rayonnement X synchrotron, et de courbure de substrat ont été couplées. Le système Pd/Si constitue un système modèle ; un siliciure unique se forme : Pd2Si. Les résultats obtenus prouvent que le modèle de Zhang et d'Heurle permet d'expliquer l'évolution des contraintes résultant de la compétition de deux mécanismes : le développement de contraintes en compression dû à la formation d'une nouvelle phase et la relaxation des contraintes du siliciure déjà formé. Néanmoins, la microscopie électronique en transmission et les figures de pôle révèlent que la texture de cette phase change selon l'orientation du substrat. Sur Si(111), Pd2Si est en épitaxie alors que sur Si(001), la phase présente une texture qui évolue au cours du traitement thermique. Cette évolution serait activée par un mécanisme de fluage diffusionnel puis par de la déformation plastique. L'étude de films ultra-minces de Ni montre qu'il existe une épaisseur critique (<6 nm) en dessous de laquelle la séquence de phases et la texture des siliciures formés sont modifiées. Différentes techniques révèlent qu'à partir de 200 °C, la phase NiSi croît sous la forme d'une couche homogène et continue. En augmentant la température, les phases NiSi et NiSi2 coexistent avec différentes morphologies : îlots pénétrant dans le substrat ou bâtonnets."
"Les progrès de la microélectronique imposent de faire évoluer les mémoires vers des dispositifs rapide et à haute densité d'intégration. Cependant, l'obtention de produits fiables passe en premier lieu par le développement des procédés de fabrication, la compréhension des problèmes de fiabilité et l'analyse physique de défaillances. Les travaux réalises durant cette thèse portent ainsi sur l'analyse de défauts et la caractérisation physique de cellules mémoires par microscopie électronique en transmission. Quatre thèmes de recherche ont été abordés. Le premier porte sur l'étude des dégradations microstructurales de cellules EEPROM produites par la société STMicroelectronics après sollicitations électriques et thermiques. Ensuite, l'architecture innovante SQeRAM, développée par STMicroelectronics, a été caractérisée, le but étant d'appréhender la microstructure des zones de stockage de charges, et de comprendre l'origine physique des performances en rétention de ces dispositifs. Une collaboration avec Crocus Technology nous a permis ensuite de participer au développement des procédés de fabrication d'une nouvelle génération de mémoires magnétorésistives (TA-MRAM). Ici, la microstructure de différents empilements magnétiques constituant les éléments de mémorisation de ces dispositifs a été caractérisée. Enfin, le dernier axe de recherche abordé concerne une nouvelle génération de mémoires macromoléculaires non volatiles à commutation de résistance basée sur le complexe organométallique CuTCNQ et sa croissance dans des structures d'interconnexion a été étudiée selon divers procédés développés à l'IMEC et à l'Université technique d'Aachen"
"Les récentes avancées des applications de télécommunication radio-fréquences (RF), l'augmentation des fréquences d'opération des microprocesseurs et les possibilités de stockage de données rapides ont pour conséquence une expansion exponentielle du volume de données échangées. Ce développement a été permis et a engendré une demande croissante de systèmes de télécommunication de plus en plus performants, que ce soit en terme de débit, de flexibilité des réseaux, et bien évidement de coût des systèmes. Tous les systèmes de communication modernes requièrent un signal périodique stable pour fournir une base de temps nécessaire à la synchronisation, à l'alignement des horloges d'échantillonnage, à la récupération d'horloge ou encore à la synthèse de fréquence. Le verrouillage de phase est une des principales techniques pour répondre à ces besoins. L'enjeu de ce travail de thèse est de concevoir, réaliser et caractériser une boucle à verrouillage de phase capable de s'intégrer dans un système de télécommunication développé en partenariat entre la société STMicroelectronics et l'Institut Matériaux Microélectronique Nanosciences de Provence (IM2NP). Ce système faible coût, faible consommation, réalisé en technologie CMOS est destiné à des applications de type réseaux personnels sans fils. Des contraintes fortes en terme de surface silicium, consommation, réactivité de la boucle et de précision fréquentielle sont les éléments directeurs de la conception de cette PLL. La boucle réalisée devra être capable de fonctionner en synthétiseur de fréquence et en modulateur FSK. Une attention particulière sera portée à l'oscillateur contrôlé en tension, véritable coeur de la PLL proposée."
"Le contexte énergétique actuel est un enjeu sociétal. L'utilisation de l'énergie solaire au travers de cellules solaires photovoltaïques à bas-coût et à haut rendement, est une des voies envisagées pour répondre aux besoins énergétiques. Ce travail de thèse a permis de démontrer la faisabilité de cellules solaires hybrides, basées sur une jonction de type "" cœur/coquille "" entre des nanofils de silicium obtenus par gravure chimique et du PEDOT polymérisé par voie électrochimique. Les principaux avantages d'une telle structure sont à la fois la simplicité et le faible coût des méthodes utilisées pour la réalisation de la cellule. Les nanofils de silicium, grâce à leur capacité à piéger la lumière, conduisent à des propriétés d'anti-reflet très intéressantes avec notamment des valeurs de réflexion inférieures à 3% sur toute la gamme spectrale du visible. La réalisation de telles jonctions a fait l'objet d'une étude poussée sur les différentes caractéristiques de dépôt du polymère, tels que l'intensité lumineuse, le potentiel appliqué et la durée du procédé. L'influence de ces paramètres sur la mesure I(V) de la cellule solaire hybride complète a également été étudiée. On peut noter en particulier que l'on obtient ainsi une densité de courant de fuite très faible et une résistance de fuite très élevée, permettant d'émettre l'hypothèse d'une bonne passivation des états de surface. Ceci constitue une voie prometteuse pour obtenir un bon transport de charges en polarisation inverse."
"L'objectif de cette étude est de regarder l'influence du Pt sur la formation des siliciures de Ni dans le procédé Salicide et en particulier sur la phase basse résistivité NiSi, envisagée par l'industrie pour réaliser les contacts avec les zones actives de transistors de type Flash. Pour cela, nous avons étudié la nature, la séquence et la cinétique des phases formées, d'une part sur le système Ni1-xPtx/Si(100) (0% ≤ x ≤ 30%), et plus particulièrement sur un système intéressant pour certaines de ces propriétés, Ni(13%Pt)/Si(100). Deux types de dépôts ont été confrontés : dépôts réalisés avec une cible alliée Ni(13%Pt) ou par codéposition (cibles Ni et Pt dissociées). Ainsi nous avons couplé différentes techniques de caractérisation in situ (diffraction des rayons X, Réflectivité des rayons X (RRX), résistivité 4 pointes) pour essayer de comprendre les mécanismes liés à ce système. En particulier des expériences de RRX in-situ, associées à une analyse par transformée de Fourier inverse, ont été mises en œuvre, en utilisant le rayonnement synchrotron (ESRF), et aboutissent à des résultats originaux : la séquence des phases est modifiée dans le cas du Ni(13%Pt). Enfin, des premières mesures de résistance sur lignes étroites ont été réalisées, soulignant les avantages et les limites associées à l'utilisation d'un tel système."
"Ce cours en deux parties (première partie ici) introduit les deux modes dynamiques utilisés en microscopie à force atomique (AFM): mode de modulation d'amplitude (AM-AFM), plus connu sous le nom de ""Tappling"" et mode de modulation de fréquence (FM-AFM), également connu sous le nom de non-contact AFM. Les aspects pratiques et théoriques propres à chaque mode sont discutés et illustrés d'un point de vue expérimental. Concernant le non-contact AFM, nous détaillons point par point l'électronique de contrôle de l'instrument et illustrons son fonctionnement à partir des résultats d'un code numérique qui reprend très précisément une électronique de contrôle existante."
"La première partie du manuscrit présente un état de l'art des différentes visions du DFM au sein du processus de conception industriel. Une nouvelle méthode de conception orientée DFM baptisée le DFM² est définie, se basant principalement sur l'intensification des interactions entre la conception et la fabrication. L'étape de conception de cellules se place au coeur de ces interactions afin d'appliquer les améliorations DFM en amont de la conception du circuit. La suite de l'étude présente des résultats visant à s'affranchir de la variabilité observée lors de la fabrication, concernant des domaines comme la planéité ou la lithographie à la fois pour le FEOL et le BEOL. Une étude statistique sur l'étape de métallisation est ensuite proposée, visant à modéliser l'impact de la géométrie du motif des dispositifs de remplissage métalliques sur les performances électriques des circuits. Les résultats permettent de définir de nouvelles conditions de simulation afin de prendre en compte cet effet dès la conception des cellules. Enfin, un outil d'aide à la conception de cellules (DUTY) est proposé. Son objectif est en premier lieu d'accompagner les concepteurs dans la mise en place du DFM² en leur proposant des améliorations DFM basées principalement sur les résultats obtenus précédemment. De plus, son but à long terme est de corréler les modifications DFM réalisées avec les améliorations de rendement attendues."
"La caractérisation par mesure spectrale de l'environnement neutronique atmosphérique ambiant s'avère être spécifiquement primordiale dans le contexte de la problématique actuelle liée à l'évaluation des effets de radiations naturelles dans les matériaux semi-conducteurs. Ces "" effets singuliers "" identifiés sont susceptibles d'altérer le bon fonctionnement des technologies sur silicium fortement intégrées jusqu'au niveau du sol terrestre. Les travaux menés au cours de cette thèse de doctorat ont ainsi porté sur le développement d'un spectromètre de neutron, basé sur le principe du système généralisé des sphères de Bonner, de sensibilité adaptée à la mesure en environnement radiatif atmosphérique naturel et d'efficacité étendue de surcroît jusqu'au domaine des hautes énergies. Le développement du spectromètre s'est alors grandement appuyé sur la simulation numérique de type Monte Carlo au moyen du code de transport MCNPX. Une fois le système multi-détecteurs défini sur le support d'une modélisation détaillée, la matrice de réponse en fluence a été déterminée sur un large spectre en énergie, depuis les énergies thermiques jusqu'à plusieurs GeV. Une phase d'extension puis d'optimisation de la réponse aux neutrons d'énergies supérieures à la dizaine de MeV ont été menées pour aboutir à la configuration et au dimensionnement finals du système de spectrométrie, jusqu'à sa propre réalisation. Une phase de caractérisation, par simulations Monte Carlo, du spectromètre s'est ensuite consacrée à l'évaluation des possibles déviations et incertitudes associées aux réponses en fluence calculées. Une analyse de la sensibilité du spectromètre aux composantes radiatives atmosphériques chargées complète et finalise cette étude exhaustive de caractérisation. Des tests de mesures réalisés au moyen du spectromètre auprès d'une source 241Am-Be de référence ont apporté des éléments de validation expérimentale préliminaire de la matrice de réponse calculée."
"Dans le cadre général des études de matériaux multifonctionnels, électrolytiques et catalytiques, susceptibles d’être utilisés au sein de dispositifs de détection de gaz, un système d’oxydes (1-x)CeO2. x/2Bi2O3 avec 0≤x≤1 a été élaboré par coprécipitation puis traitement thermique à 600°C. Le système ainsi obtenu correspondrait à un diagramme de phases original, constitué d’un domaine de solutions solides (Ce1-xBixO2-z pour x ≤ 0,20), d’un domaine multiphasé pour 0,3≤x≤0,7 comportant une phase de type quadratique b’-Bi2O3 et une phase cubique substituée limite (x=0,20), d’un autre domaine multiphasé pour les compositions 0,8≤x≤1, comportant une phase quadratique b-Bi2O3 et une phase monoclinique. Ces deux phases ont déjà été considérées dans la littérature comme phases métastables résultant de divers modes de refroidissement de la phase pure Bi2O3. Dans le cas présent, la stabilisation de ces deux phases b’ et b en présence d’une phase substituée cubique Ce1-xBixO2-z pourrait être due à la présence d’ions cérium au sein duréseau cristallin de Bi2O3. Les interactions catalytiques entre des échantillons polycristallins de ce système avec x variable et des mélanges air-CO et air CH4 ont été étudiées par spectroscopie infrarouge à transformée de Fourier dans le domaine 100 à 525°C. Il apparait que les composés riches en cérium ou riches en bismuth n'ont pas la même réactivité vis-à-vis des gaz CH4 ou CO. Cette diversité de propriétés catalytiques pourrait être utilisée au sein de systèmes multicapteurs de gaz.Une étude de la conduction électrique du système pour x variable a été effectuée par spectroscopie d’impédance électrique entre 100 et 750°C. Les représentations Nyquist des impédances électriques ont été interprétées en mettant en jeu des modèles de type élément de phase constante ou de type Warburg pour prendre en compte l’hétérogénéité des échantillons ainsi que les phénomènes de réaction-diffusion aux électrodes. La conductivité en volume (coeur de grains) augmente avec la composition, avec deux types d’évolutions distinctes : une évolution caractéristique de la phase substituée liée à l’augmentation du taux de lacunes, une évolution dans le système biphasé avec une forte augmentation de conductivité au-dessus de x=0,3 et un maximum atteint pour x=0,7. La phase quadratique de type b’-Bi2O3 connue comme phase métastable est ainsi stabilisée au sein de ce système mixte, au moins à 600°C: elle serait à l’origine de la forte conductivité ionique observée pour la composition proche de x = 0,7."
"L'objectif de cette étude est de caractériser la redistribution d'éléments d'alliages et de dopants au cours des premiers stades de formation des siliciures de Ni. Pour cela, nous avons étudié la nature, la séquence et la cinétique des phases formées, dans un premier temps pour les systèmes binaires Pd/Si, Pt/Si et Ni/Si, puis pour les systèmes ternaires (Ni,Pt)/Si et Ni/(Si, As) présentant un intérêt technologique pour la nanoélectronique. Ainsi, nous avons couplé des techniques de caractérisation originales (calorimétrie différentielle à balayage sur films minces, sonde atomique tomographique, diffraction des rayons X in situ) pour mesurer la redistribution du Pt dans les phases formées et leurs cinétiques de croissance. Nous avons pu développer un modèle pour décrire les premiers stades de croissance de ces siliciures alliés et dégager les mécanismes mis en jeu ainsi que les facteurs limitant la redistribution des éléments d'alliage et des dopants."
"Cet article présente une nouvelle méthode d'écoute passive destinée à trajectographier un objet en mouvement appelé la "" source "" ou la "" cible "" , avec les contraintes suivantes : le réseau de capteurs est fixe et imposé, et les méthodes classiques de traitement d'antenne ne peuvent pas s'appliquer. La méthode proposée ici a été utilisée avec succès pour trajectographier un bâtiment de surface (problème à deux dimensions) ou une cible sous-marine (problème à trois dimensions). Tous les résultats présentés ont été obtenus sur des signaux réels. Le calcul de la position de la cible nécessite l'estimation des temps de propagation, c'est-à-dire du temps écoulé entre l'instant d'émission du signal et sa réception sur chaque capteur. La fonction d'intercorrélation est un outil opportun lorsque la cible est immobile mais elle doit être étendue à la notion d'ambiguïté quand la source est en mouvement. Le rapport signal à bruit, l'uniformité de la densité spectrale de puissance et la durée d'intégration sont des facteurs déterminants pour la précision de la localisation. Nous montrons qu'une méthode de blanchiment et une compensation de l'effet Doppler sont nécessaires et nous proposons un moyen d'éliminer le problème des trajets multiples. Par ailleurs, une nouvelle configuration des capteurs est proposée, fondée sur l'idée d'associer des hydrophones séparés d'une distance déterminée de manière expérimentale. Des résultats de trajectographies sont présentés et comparés aux trajectoires obtenues par une méthode active."
"Dans le contexte actuel d'optimisation des performances des dispositifs de microélectronique, le transistor MOSFET, brique de base, est soumis à des contraintes géométriques telles que son architecture même est remise en cause. L'augmentation du nombre de grille afin d'accentuer le contrôle électrostatique de la grille sur le canal a mis en avant des architectures ultimes telles que le nanofil dont la grille enrobe totalement le canal. Dans ce travail, une étude du nanofil de silicium a été réalisée afin d'estimer les potentialités de cette architecture au niveau transistor jusqu'à l'étude de petits circuits. Pour cela, un modèle analytique en courant a été mis en place et implémenté en Verilog-A afin de simuler des petits circuits dans un environnement de type ELDO. Toutefois, les paramètres du modèle telles que les masses effectives de transport (ou de confinement) ou le transport dans le film sont la clé de la prédictibilité au niveau circuit. C'est pourquoi des simulations avancées de type liaisons fortes ou Kubo-Greenwood ont été développées afin d'étudier finement l'évolution des caractéristiques du nanofil notamment vis-à-vis de son intégration géométriques. Issues de ces approches numériques, des expressions analytiques ont été établies afin d'inclure dans le modèle toute la physique observée en amont. Des effets comme l'évolution de la structure de bande ou l'impact des mécanismes d'interaction ont ainsi pu être apportés jusqu'au niveau circuit. Les résultats en courant acquièrent une certaine pertinence en créant un lien entre simulations numériques et données expérimentales."
"Le transistor MOSFET atteint aujourd'hui des dimensions déca nanométriques pour lesquelles les effets de balisticité ne peuvent plus être négligés. Le challenge actuel est d'être capable d'introduire le transport (quasi-)balistique dans la modélisation des dispositifs innovants et d'évaluer son impact au niveau système. Dans ce contexte, notre travail porte sur l'introduction du transport (quasi-)balistique dans une modélisation analytique des transistors MOS multigrilles pour la simulation d'éléments de circuit. Dans un premier temps, la redécouverte de la méthode de McKelvey appliquée au transistor MOSFET a permis de synthétiser l'ensemble des travaux concernant la modélisation analytique du transport balistique/quasi-balistique. Nous avons alors construit une modélisation appelée « mobilité quasi-balistique » (à partir des travaux de Rhew et al), issue du rapprochement entre la méthode des moments et la méthode de McKelvey permettant de décrire le transport (quasi-)balistique de façon macroscopique dans un environnement TCAD. L'ensemble des résultats issus de cette première modélisation nous a dirigé dans la construction de notre modèle analytique de courant (quasi-)balistique en adaptant ou en créant de nouvelles approches pour prendre en compte les divers effets des dispositifs nanométriques : les effets de canal court, le confinement quantique et la description des interactions. Nous avons donc pu quantifier l'impact des propriétés de transport électronique sur le fonctionnement d'éléments de circuit et cela en fonction du type d'architecture."
"Reçu le 21 mars 2005 ; accepté le 1 août 2005 Résumé Objectif. – Préciser, dans la succession des opérations cognitives aboutissant à une action, la localisation des troubles attentionnels consé-cutifs à un traumatisme crânien. Méthode. – Les potentiels évoqués cognitifs ont été enregistrés au cours d'un test de Stroop, après les stimulations et lors des réponses, chez 25 traumatisés crâniens et chez 25 sujets témoins, environ quatre mois après l'accident. La latence et l'amplitude des différentes ondes ont été comparées entre les groupes et corrélées à différentes données cliniques. Résultats. – Il existe une augmentation des temps de réaction des traumatisés crâniens, sans augmentation ni du nombre d'erreurs ni de l'interférence. Les enregistrements électrophysiologiques précisent l'existence d'un trouble précoce se produisant entre 100 et 200 millise-condes suivant la présentation d'une stimulation. La région responsable pourrait être la face occipitotemporale interne en relation avec les régions frontales. Ils montrent également une perturbation du monitorage du programme moteur qui implique les zones frontomédianes. Conclusion. – Ce protocole permet de mettre en évidence des altérations cognitives relativement précises dans leur localisation tempo-relle. Des études ultérieures devraient préciser les fonctions neuropsychologiques atteintes en rapport avec ces anomalies, et leur possibilité de régression avec l'évolution. © 2005 Elsevier SAS. Tous droits réservés. Abstract Objective. – To specify, in cognitive processing leading to an action, the localization of difficulties in attention following severe, traumatic brain injury. Method. – Stimulus-locked and response-locked event-related potentials were recorded during a Stroop task in 25 patients with traumatic brain injury and 25 control subjects approximately 4 months after the accident. The latency and amplitude of the waves were compared between the two groups and correlated with clinical data. Results. – The reaction times of brain-injured patients were significantly longer than those of the control group, but neither the number of errors nor the interference differed between the groups. Electrophysiological recordings showed early abnormalities between 100 and 200 ms after stimulus onset. The key area could be the medial occipitotemporal side connected with frontal regions. Recordings also showed disruptions in motor program monitoring, which implied frontomedial areas. Conclusion. – This protocol allows for precisely dating cognitive abnormalities. Future studies should relate cognitive with neuropsycho-logical abnormalities and examine the possibilities of later regression."
"Note d'application: Intégration du RHK R9 à un Omicron VT-AFM. Avec l'achat de notre Omicron VT-AFM (modèle XA) en 2007, nous avons décidé d'équiper le microscope avec le contrôle RHK SPM 100 et le RHK PLLPro2 NC-AFM électronique. En 2015, nous avons amélioré notre système de contrôle RHK avec la nouvelle version R9 entièrement numérique, car dans la plupart de nos projets de recherche, nous devions améliorer les performances de la R9, notamment les mesures de spectroscopie contrôlées par ordinateur et les techniques de modulation de fréquence. Dans le document, nous allons d'abord détailler les modifications apportées à notre VT-AFM, puis nous décrirons comment nous avons intégré le système à la RHK R9 et enfin nous présenterons la performance du système final au moyen d'une analyse de bruit thermique et une démonstration de la qualité d'image obtenue."
"L' intérêt des phases oxydes à base de terres rares est certes multiple et leurs propriétésont été explorées depuis longtemps: mais nous nous intéressons ici aux comportements de cesphases en tant que phases thermiques, catalytiques ou conductrices ioniques à hautetempérature. Il s'agit en particulier de développer des systèmes innovants de matériauxpouvant intervenir dans la conception de dispositifs pour la microélectronique, pour capteursde gaz ou membranes sélectives ou pour systèmes dépolluants.Les phases de structure pyrochlore ou fluorine de type TR2Ce207 où TR désigne unélément de Terre Rare présentent divers potentiels d' applications: elles ont été considéréescomme des phases pouvant résister à de hautes températures. Les oxydes de structurespérovskites, de formule générale ABO), présentent de multiples applications potentielles,notamment en tant que phases diélectriques pour condensateurs, ou phases conductricesioniques (en ions oxygène ou en protons) pour électrolytes solides, du fait même de leur hautestabilité chimique à haute température.Ce travail a été divisé en deux parties. La première a consisté à élaborer la phase« thermique)} de type pyrochlore TR2Ce20 7 (TR = La, Ce, ... ) en utilisant un minéralcomplexe à base d'allanite-monazite et de silico-aluminates issus de déchets industriels, doncà bas coût.La deuxième partie a consisté à élaborer BaCe03 et à étudier ses propriétéscatalytiques et conductimétriques en fonction de la température. Une nouvelle méthode desynthèse reposant sur l' utilisation du mélange EDTA-citrate a été utilisée afin d'élaborer unprécurseur, qui, traité thermiquement à 950°C, a permis d'élaborer des poudressubmicroniques de la phase BaCe03. L'activité catalytique du composé BaCe03 démarre à450°C pour atteindre la conversion totale à 675°C : dans cette gamme de température,l'efficacité catalytique de la phase BaCe03 est maximale. L'évolution de la conductivité enfonction de la température de pastilles compactées de BaCe03 a révélé l'existence d'une sériede modifications électriques fortement corrélées aux transitions structurales connues pourBaCe03 dans la littérature. À basse température (300 à 450°C), la faible conductivité de laphase orthorhombique, associée à la faible énergie d'activation, peut être liée à la migrationdes défauts extrinsèques (gaz adsorbés). Cependant, au-dessus de 500°C, la conductivité de laseconde phase orthorhombique augmente: ceci pourrait être attribué à une mobilité croissantedes atomes d'oxygène."
"Les matériaux de protection incendie sont largement utilisés pour assurer la sécurité des usagers des infrastructures. Les normes de protection incendie évoluant régulièrement, les matériaux doivent être de plus en plus performants. Ceux-ci sont généralement des mortiers constitués d’oxydes réfractaires et isolants. L’objectif de ce travail est de mettre au point un composite coupe-feu 4 h applicable par projection mais également de déterminer ses propriétés thermiques et mécaniques.Dans une première partie, cette étude reprend les différentes étapes de l’élaboration d’un matériau de protection incendie, après la présentation de la démarche qui a guidé l’élaboration de nos matériaux, nous nous sommes intéressés plus particulièrement à la composition chimique de la matrice ainsi que celle du ciment. Leurs propriétés thermiques et mécaniques ont été passées en revue.Les matières premières nécessaires à l’élaboration d’un mortier ont ensuite été sélectionnées. L’évolution, respectivement de la conductivité thermique, de la diffusivité, de la porosité, de la chaleur spécifique et des propriétés mécaniques des mortiers choisis en fonction de la nature et de la quantité de charges incorporées à la matrice a été étudiée. Une description des divers modèles analytiques et numériques permettant la représentation de la conductivité thermique et du module d’Young des matériaux a permis de développer un modèle capable de prédire le comportement thermique et mécanique des composites en fonction de la nature et de quantité de charges ajoutées.Dans une seconde partie, la cinétique de la réaction d’hydratation du plâtre afin de maîtriser les temps de prise et pour faciliter la production des projetés dans la chaîne industrielle a été étudiée. L’influence sur la cinétique d’hydratation, de la composition chimique du plâtre, de sa granulométrie et de l’ajout d’adjuvants couramment utilisés dans l’industrie plâtrière, a également été traitée.10A l’issue de cette étude, deux formulations de composites projetables ont été mises au point."
"L'objectif de cette étude est de fournir des éléments d'évaluation des futures technologies CMOS au niveau circuit. Dans ce but, des kits de conception prédictifs sont élaborés. Ces kits reposent sur la modélisation prédictive des futurs dispositifs et des interconnexions, ainsi que sur le paramétrage des outils nécessaires au déroulement d'un flot digital dans le cadre de futures technologies. Les résultats des évaluations réalisées grâce à ces kits mettent en évidence une augmentation drastique des délais d'interconnexion laissant augurer d'importants problèmes d'ajout de répéteurs pour les futurs circuits. A court terme (32nm), l'évaluation réalisée dans le cadre d'un flot digital entièrement prédictif montre que les problèmes posés par les délais d'interconnexion ne semblent pas encore jouer un rôle important pour les blocs de faible dimension. Concernant la variabilité des dispositifs, qui affecte tout particulièrement les circuits de type mémoires SRAM, une stagnation à des niveaux non acceptables est observée pour les technologies futures. Cependant, à court terme, des solutions consistant à utiliser des dispositifs faiblement dopés sont identifiées. L'intérêt d'une nouvelle mémoire SRAM, dont le principe réside dans l'utilisation de dispositifs faiblement dopés seulement pour les transistors NMOS, est également démontré."
"Les comportements mécaniques des films minces polycristallins sont encore mal compris à l'échelle sub-micronique. En particulier des hétérogénéités locales de déformation importantes sont attendues, mais elles restent difficile à quantifier expérimentalement. Les nouvelles possibilités offertes par les micro-faisceaux synchrotron de rayons X ont donc été utilisées dans ce travail pour éclairer cette problèmatique. Une réflexion de Bragg provenant d'un grain unique sub-micronique a été acquise avec une très bonne résolution dans l'espace réciproque en trois dimensions lors d'un cycle thermique. Les propriétés de cohérence du faisceau ont été utilisées pour reconstruire à trois dimensions une composante du champ de déplacement intra-grain avec une résolution d'une vingtaine de nanomètres dans les trois directions. Cette technique est basée sur des algorithmes de reconstruction de phase qui néanmoins connaissent des stagnations dans le cas des échantillons fortement déformés. Une méthodologie basée sur la connaissance de la forme du grain a donc été développée pour contourner ces difficultés. Des analyses complémentaires de diffraction X de laboratoire et de microdiffraction monochromatique ont également mis en évidence des hétérogénéités importantes de déformation entre les différents grains."
"Ce travail présente la formation et la caractérisation sous ultravide de monocouches moléculaires issu de l'auto-assemblage et de la polymérisation de molécules organiques (hexahydroxy triphenyléne-HHTP et acide diborique benzoique-BDBA) sur des surfaces monocristallines métalliques et isolantes. L'adsorption de molécules HHTP sur Ag(111) donne lieu à plusieurs réseaux moléculaires dépendants de la température. Cette étude, menée par microscopie à effet tunnel (STM), montre qu'un réseau robuste est obtenu suite à la déshydrogénation, activée thermiquement, des groupes alcool périphériques. Cela induit la formation de liaisons hydrogène entre les fonctions alcools et cétones ainsi obtenues. L'étude STM de molécules de BDBA vapo-déposées sur Ag(111) a démontrée la formation d'architectures bidimensionnelles étendues, liées de manière covalente, suite à la polymérisation des précurseurs en surface. Un résultat similaire a pu être obtenu par la copolymérisation des molécules de BDBA et d'HHTP. Ces polymères nanoporeux s'étendent en monocouche et présentent une stabilité en température exceptionnelle. Enfin, l'étude de BDBA sur le substrat isolant de KCl, menée par microscopie à force atomique en mode non contact (nc-AFM), montre un auto-assemblage étendu par des liaisons hydrogène, et met en évidence le rôle décisif de la nature chimique du substrat sur la faisabilité de la polymérisation de ces molécules en surface. L'approche développée dans ce travail, mettant en jeu des réactions chimiques en surface, constitue une voie nouvelle pour la conception de nano-architectures moléculaires originales et robustes sur surfaces."
"On constate une forte demande mondiale d' énergie propre et renouvelable en raison de la consommation rapide des combustibles fossiles non renouvelables et l'effet de serre qui en résulte. Une solution prometteuse pour produire une énergie propre et renouvelable est d'utiliser des cellules solaires pour convertir l' énergie solaire directement en électricité. Comparativement à leurs homologues inorganiques, les cellules solaires organiques (OSCs) sont maintenant intensivement étudiées en raison des avantages tels que le poids léger, la flexibilité, la compatibilité avec les procédés de fabrication à faibles coûts. Malgré ces avantages, l'efficacité de conversion (PCE) des OSCs doit encore être améliorée pour la commercialisation à grande échelle. Les cellules solaires organiques sont réalisées en pile de couches minces comprenant des électrodes, la couche de transport d' électrons, la couche de polymère actif et la couche de transport de trous. Dans cette étude, nous sommes concernés par la couche de PEDOT:PSS qui est couramment utilisée comme une couche tampon entre l'électrode anodique et la couche de polymère actif de cellules solaires organiques. Cette étude vise à intégrer différentes concentrations de nanoprismes (NPSMs) d'argent de taille sub-longueur d'onde dans du PEDOT: PSS afin de profiter de leurs propriétés optiques uniques nées de résonances de plasmons de surface localisées (LSPR) pour améliorer la collecte lumineuse et l'efficacité de génération de charge en optimisant l' absorption et la diffusion de la lumière. Nous avons constaté que les facteurs clés qui contrôlent les performances des cellules solaires plasmoniques comprennent non seulement les propriétés optiques, mais également les propriétés structurelles et électriques des couches hybrides de PEDOT:PSS comprenant des NPSMs d' Ag. D'une part, l'ajout de NPSMs d' Ag conduit ¨¤ (1) une augmentation de l'absorption optique; (2) de la diffusion de la lumière ¨¤ de grands angles ce qui pourrait conduire ¨¤ un meilleur piégeage de la lumière dans les OSCs. D'autre part, (1) la rugosité de surface est augment¨¦e en raison de la formation d'agglomérats de NPSMs d' Ag, ce qui conduit ¨¤ une meilleure efficacité de collecte de charge; (2) la résistance globale des films hybrides est également augment¨¦e en raison de l'excès de PSS introduit par les NPSMs d' Ag incomplètement purifiées, inférieur courant de court-circuit (Jsc) qui en résulte; (3) les Ag NPSMs et leurs agglomérats ¨¤ l'interface PEDOT:PSS/couche photo-active pourraient agir comme des centres de recombinaison, conduisant ¨¤ une réduction de la résistance de shunt, du Jsc et de la tension en circuit ouvert (Voc). Afin de résoudre partiellement l'inconvénient (2) et (3), en intégrant des NPSMs d¡¯Ag davantage purifiés et une petite quantité de glycérol dans le PEDOT:PSS, la résistance des couches hybrides de PEDOT:PSS-Ag-NPSMs peut ¨être réduite à une valeur comparable ou inférieure ¨¤ celles couches vierges. Les futurs progrès en chimie de surface colloïdale et l'optimisation sur le processus d'incorporation des nanoparticules seront nécessaires pour produire des cellules solaires organiques plasmoniques de meilleures performances."
"Cette thèse présente de nouvelles méthodes d'extraction de "" pistes"" dans des images gisement- temps présentées aux opérateurs d'un système sonar passif. Les pistes gisement-temps extraites sont nécessaires à la fonction trajectographie qui se trouve en aval dans la chaîne de traitement de l'information d'un tel système. Les méthodes que nous proposons se fondent sur l'analyse statistique de ce type d'image issue du traitement d'antenne. Cette analyse est l'objet du second chapitre. Puis dans une première approche, on ne considère que le cas (irréaliste) où une seule piste au plus, est présente dans l'image. Les deux extracteurs que nous construisons à partir de l'arsenal des techniques associées aux chaînes de Markov cachées (HMM), tiennent compte de l'intermittence de cette piste. Une fois la piste extraite par l'une ou l'autre méthode, on lui associe un module de trajectographie qui permet d'évaluer les performances des extracteurs et montre que leurs sorties sont exploitables. La seconde partie de la thèse se focalise sur le cas réel c'est-à-dire la présence de plusieurs pistes intermittentes et pouvant se croiser dans l'image gisement-temps. Deux extracteurs sont présentés et étudiés: l'un effectuant une extraction séquentielle c'est-à-dire extrayant piste après piste, un autre appelé extracteur parallèle, effectuant une extraction de l'ensemble des pistes simultanément. Les deux extracteurs proposés gérant de façon très insatisfaisante le croisement de pistes, on leur associe un module de trajectographie qui permet d'améliorer sensiblement celui-ci. Un bilan global du couple (extraction, trajectographie) est présenté dans la dernière partie de cette thèse."
"Le sujet de cette thèse s'inscrit dans le cadre du projet international Antares dont l'objectif est la construction d'un télescope à neutrinos situé dans un environnement marin au large de Toulon. A de fortes profondeurs, un neutrino a d'autant plus de chance de rentrer en collision avec une molécule d'eau, générant ainsi une réaction en chaîne générant un flash lumineux et une onde sonore. L'objectif de cette thèse est d'étudier cette onde sonore en vue de développer un système capable de détecter le front d'onde correspondant et d'estimer la direction originelle du neutrino Dans un premier temps, l'étude se porte sur le signal acoustique. Deux descriptions issues de la littérature et de récents travaux effectués au CPPM sont confrontées et aboutissent à une modélisation mathématique du signal et du front d'onde. Dans un second temps, plusieurs méthodes de détection sont étudiées, de la plus classique (étude du rapport de vraisemblance) à des méthodes plus récentes (filtrage adapté, classification, etc.). La comparaison expérimentale en situation semi réelle de celles-ci aboutit au choix de la méthode de détection suivante : le FASE (Filtrage Adapté Stochastique Etendu). Enfin, la position et la direction du neutrino sont estimées par un algorithme dérivé de Gauss-Newton. Cet estimateur se base sur la modélisation du déplacement du front d'onde acoustique et sur les informations temporelles de détection fournies par les hydrophones du télescope. De nombreuses configurations sont testées et les performances du système sont évaluées. Une structure d'hydrophone est proposée et une simulation dite ""globale"" finalise cette thèse. Dans celle-ci, les étapes de détection et d'estimation sont basées sur les résultats obtenus précédemment. Les bruits de mer sont des bruits réels issus de campagnes de mesure et les résultats obtenus valident les travaux de cette thèse."
"Des agrégats à base des boues de phosphate de la zone de Gantour ( Youssoufia,Maroc) sont cuits à des températures comprises entre 900 et 1200°C, et examinés par différentes techniques (DRX, MEB, analyses thermique et dilatométrique, et spectroscopie d’impédance). Les propriétés physiques des agrégats (retrait, absorption d’eau et résistance à la compression) sont mesurées et corrélées, dans certains cas, aux facteurs expérimentaux(température, temps de cuisson, teneur de l’additif), et ce en utilisant la méthodologie de la recherche expérimentale.Les agrégats amendés à l’argile smectitique sont le siège de formation de la gehlénite. Dans ce cas, la fluorapatite a partiellement résisté au traitement thermique, et a été le siège d’une ségrégation localisée. Les mesures d'impédance ont bien mis en évidence l'étape principale du frittage, lequel s’est produit par écoulement de la masse fondue. En se basant sur les valeurs de la densité, les granulats cuits à 900 et à 1100°C pouvaient bien être considérés comme des agrégats légers.Les études menées sur les agrégats des mélanges : argile kaolinitique-boue, cendre boue,et argile kaolinitique-cendre-boue ont montré la présence de la labradorite, et d’une masse fondue, qui semblait se former, entre autres, de la fusion de la fluorapatite. Par ailleurs,l'utilisation de la méthodologie de la recherche expérimentale a permis d'évaluer le poids des effets des facteurs expérimentaux sur les propriétés physiques des agrégats. Des granulats légers (1,02 < densité < 2,1 g/cm3) et résistants sont préparés avec succès dans les conditions expérimentales adoptées. Les granulats ayant le meilleur rapport résistance / densité sont ceux du mélange ternaire.Concernant le volet application, les résultats ont montré que l’utilisation des agrégats en construction est possible et que l’association d’agrégats légers au sol a permis d’activer la croissance des plants de la luzerne. D’un autre côté, l'absorption du phosphore par les racines(1,45 mg/g maximum) des plantes était meilleure dans le cas du mélange contenant de l’argile."
"Ce cours en deux parties (deuxième partie ici) introduit les deux modes dynamiques utilisés en microscopie à force atomique (AFM): mode de modulation d'amplitude (AM-AFM), plus connu sous le nom de ""Tappling"" et mode de modulation de fréquence (FM-AFM), également connu sous le nom de non-contact AFM. Les aspects pratiques et théoriques propres à chaque mode sont discutés et illustrés d'un point de vue expérimental. Concernant le non-contact AFM, nous détaillons point par point l'électronique de contrôle de l'instrument et illustrons son fonctionnement à partir des résultats d'un code numérique qui reprend très précisément une électronique de contrôle existante."
"Les méthodes de trajectographie conventionnelles par mesures d’angle supposent que la source est en mouvement rectiligne uniforme tandis que l’observateur est manœuvrant. Dans cette thèse, nous remettons en cause cette hypothèse en proposant un autre modèle de cinématique de la source : le mouvement circulaire uniforme. Nous prouvons qu’une telle trajectoire est observable à partir d’un observateur en mouvement rectiligne uniforme. Puis, nous étudions l’apport de mesures additionnelles de fréquence ou la faisabilité de la trajectographie par mesures de distances. Le cas d’une source en mouvement rectiligne uniforme et d’un observateur manœuvrant est étudié pour ce dernier type de mesures. Chaque cas donne lieu à une analyse de l’observabilité de la trajectoire de la source et à la mise au point de l’estimateur du maximum de vraisemblance. Nous montrons que ce dernier s’avère le plus souvent efficace."
"Durant ces dernières décennies, les composés de basse dimensionnalité à gap de spin ont été largement étudiés grâce notamment à l'aide qu'ils peuvent apporter dans la compréhension de la frontière entre le comportement purement quantique d'un spin isolé et le comportement classique d'un ensemble de spins. Parmi ces composés à gap de spin nous pouvons citer les chaînes alternées de spins 1/2, les composés présentant une transition spin-Peierls ou les systèmes comportant un nombre pair de spins avec un couplage d'échange antiferromagnétique (AF) entre eux. Parmi les méthodes d'étude expérimentale de ces systèmes, la Résonance de Spin Electronique (RSE) fait partie des plus puissantes parce qu'elle est très sensible aux interactions anisotropes dans ces composés. Malgré les multiples données RSE obtenues sur ces composés, l'interprétation du profil de variation thermique de la largeur de raie RSE et du facteur g reste quand même problématique à cause de l'absence de théories efficaces. Parmi les théories disponibles, la plus utilisée est celle de Kubo et Tomita qui prédit un rétrécissement par échange de la largeur de raie à température infinie. Plus récemment, Oshikawa et Affleck ont trouvé des formules de la variation thermique de la largeur de raie et du facteur g pour une chaîne homogène de spins 1/2. Une autre méthode pour interpréter les données RSE est de calculer numériquement les paramètres d'absorption du modèle qui décrit le mieux le composé étudié. Ce mémoire rapporte mes travaux de thèse dont le sujet est l'étude, par RSE, de la dynamique des spins dans les composés à base de dimères de spins 1/2, CsV2O5, VO(HPO4)*0.5H2O et KZn(H2O)(VO)2(PO4)2(H2PO4). En étudiant les courbes de résonance RSE en fonction de la température et de l'angle nous avons trouvé que ces composés, dont la susceptibilité magnétique peut être aussi bien décrite par celle d'un ensemble de dimères isolés de spins 1/2 que par celle d'une chaîne AF alternée de spins 1/2, présentent deux régimes de comportement de la largeur de raie en fonction de la température : A haute température les raies de résonance RSE sont rétrécies à cause de l'échange suivant les prédictions de Kubo et Tomita alors qu'à basse température les courbes de résonance RSE s'élargissent et changent de forme (apparition de deux raies de résonance dues à la structure fine). Avec une méthode numérique de calcul des paramètres d'absorption RSE nous montrons que ce profil de la largeur de raie correspond plus à celui présenté par une chaîne AF alternée de spins 1/2."
"Les générateurs de suites binaires aléatoires constituent la partie primordiale d'un système cryptographique. La vitesse, la qualité des suites générées, la sécurité et la consommation jouent un rôle essentiel dans le choix d'un générateur. La sécurité du système cryptographique augmente si un tel système peut être réalisé dans un seul circuit.Le travail de recherche développé consiste donc en la réalisation d'un générateur de nombres aléatoires fonctionnant en basse consommation, basse vitesse. Le circuit proposé est de type analogique et valide l'ensemble des tests NIST assurant le caractère du signal. Une réalisation sur Silicium en technologie 0,35μm a été implémentée et validée via les tests NIST développés sous Matlab. De ce travail de thèse, un certain nombre de publications ont montré la plus-value recherche des résultats."
"Les diagrammes de phases d'ordre supérieur à quatre ne peuvent pas être dessinés dans l'espace à deux dimensions d'une représentation, sans transformation numérique des coordonnées.Le mémoire présente une codification originale de la loi des phases conduisant à une classification biunivoque des domaines de variance et à une nouvelle méthode de représentation (dite ""séquentielle"") des diagrammes de phases isothermes et isobares. Le principe en est: à un point expérimental dans l'espace de N constituants correspondent ENTIER(N/2) points appelés ""séquences"" dans la représentation bidimensionnelle.Le cheminement par évaporation isotherme et isobare est également décrit en termes de réactions chimiques et de bilan de matière calculé avec la méthode des moindres carrés, non habituelle pour ce type de problème; et les règles de la géométrie appliquées à l'espace de tous les constituants.La méthode a été appliquée aux saumures marines considérées comme des systèmes thermodynamiques quaternaires."
"Dans un contexte général d’augmentation de la demande énergétique et de préoccupation croissante face au réchauffement climatique et à la limitation des ressources naturelles, l’utilisation d’énergie solaire devrait augmenter. L’avenir des différentes technologies photovoltaïques dépend évidemment de leur rendement de conversion photovoltaïque et de leur coût (ces deux paramètres peuvent être ramenés au coût par watt) mais aussi de la disponibilité des ressources. La kesterite, Cu2ZnSnS4 (CZTS), Cu2ZnSnSe4 (CZTSe) ou Cu2ZnSn(S,Se)4 (CZTSSe), composée d’éléments abondants dans la croûte terrestre, et pouvant être fabriquée en couches minces avec des procédés à bas coûts, se positionne en candidat prometteur pour la conversion d’énergie solaire à grande échelle. Dans cette thèse, l’électro-dépôt, un procédé compatible avec des exigences industrielles de production de masse à bas coût et de sécurité, est utilisé pour déposer un précurseur de cuivre, étain et zinc sur des substrats de 15 × 15 cm², de composition et épaisseur contrôlables. Ce précurseur est ensuite converti en semiconducteur par traitement thermique en présence de soufre ou de sélénium. Les couches ainsi formées de Cu-Zn-Sn-S ou Cu-Zn-Sn-Se, doivent être uniformes et présenter les propriétés appropriées (phases, composition, morphologie) pour la fabrication de cellules solaires à haut rendement. Le procédé de fabrication de la cellule solaire complète, notamment les étapes qui interviennent dans la formation de la jonction p-n (décapage chimique et dépôt de couche tampon) est également optimisé pour maximiser les rendements. A l’issue de ces optimisations, un rendement de 9.1% est obtenu pour une cellule solaire CZTSe, un nouveau record pour les cellules solaires à base de kesterite fabriquées par électro-dépôt."
"Ce travail de thèse porte sur la mise en oeuvre d'optiques focalisantes à base de multicouches tungstènesilicium réalisées par pulvérisation cathodique RF pour la longueur d'onde 0,154 nm qui correspond à la raie Kα du cuivre. Ce travail a montré la faisabilité de telles optiques avec un gradient de période allant de 1,8 nm à 2,2 nm et pouvant fonctionner avec des angles d'incidence élevés de l'ordre de 2 degrés. L'étude des spectres de réflectivité, de diffraction et, de fluorescence et de structure fine d'absorption excitées par onde stationnaire (GIXA : glancing-incidence X-ray Analysis et SWEXAFS : Standing Wave Extended X-Ray Absorption Fine Structure) a permis de caractériser la structure des couches et de la structure interfaciale des systèmes W/Si et de définir un modèle structural pour les multicouches dépendant de la quantité de tungstène déposée. Ce modèle permet d'expliquer la perte de réflectivité des multicouches à faible épaisseur par la présence d'une couche de mélange aux interfaces. Nous avons également étudié la mise en forme de ces optiques adaptées à une source de laboratoire et permettant d'obtenir un faisceau convergent. L'utilisation de la contrainte résiduelle comme outil de mise en forme a été étudiée comme alternative à une courbure mécanique."
"Les circuits de récupération d'horloge et de données sont nécessaires au bon fonctionnement de plusieurs systèmes de communication sans fil. Les travaux effectués dans le cadre de cette thèse concernent le développement de ces circuits avec d'une part la réalisation, en technologie HCMOS9 0,13 μm de STMICROELECTRONICS, de circuits CDR analogiques à 1 et 54 Mbit/s, et d'autre part, la mise en œuvre de fonctions CDR numériques programmables à bas débit. Un circuit CDR fonctionnant à plus bas débit (1 Mbit/s) a été conçu dans le cadre de la gestion d'énergie d'un récepteur ULB impulsionnel non cohérent. Ces deux structures ont été réalisées à l'aide de PLL analogiques du 3ème ordre. Un comparateur de phase adapté aux impulsions issues du détecteur d'énergie a été proposé dans cette étude. Les circuits ont ensuite été dimensionnés dans le but d'obtenir de très bonnes performances en termes de jitter et de consommation. En particulier, les performances mesurées (sous pointes) du circuit CDR à 1 Mbit/s permettent d'envisager une gestion d'énergie efficace (réduction de plus de 97% de la consommation du récepteur). Dans le cadre d'une chaîne de télémesure avion vers sol, deux circuits CDR numériques ont également été réalisés durant cette thèse. Une PLL numérique du second degré a été implémentée en vue de fournir des données et une horloge synchrone de celles-ci afin de piloter une chaîne SOQPSK entièrement numérique. Un circuit ELGS a également mis au point pour fonctionner au sein d'un récepteur PCM/FM."
"Le présent travail porte sur l'étude de nanomatériaux et couches minces élaborés à base d'oxydes de ruthénium, à finalités catalytiques et électriques. Ces matériaux sont multifonctionnels et pourraient être destinés à des applications variées aussi bien dans le domaine de la microélectronique et des microcapteurs, que dans l'industrie chimique (catalyse, conversion du méthane). L'étude développe les relations entre élaborations, microstructures et propriétés catalytiques et électriques. Les nanopoudres de RuO2 élaborées par voie sol gel présentent des propriétés catalytiques intéressantes vis-à-vis de CH4 et CO. Des affinements structuraux (méthode Rietveld) ont montré une légère modification des mailles cristallines lorsque les tailles de cristallites étaient nanométriques (10 à 20 nm). La microscopie électronique en transmission a permis de préciser les résultats obtenus par diffraction de rayons X sur les tailles de cristallites. L'efficacité catalytique a été mesurée par spectroscopie infrarouge à transformée de Fourier en fonction de la température et du temps d'exposition au flux gazeux (air-méthane ou air-monoxyde de carbone). Les conversions de CH4 et de CO en CO2 ont été observées au-dessus de 200°C pour CH4, et à partir de la température ambiante (25°C) pour CO. Un modèle semi-empirique permettant de simuler le taux de conversion a été proposé et a permis de reproduire des comportements très différents pour la conversion de CH4 ou CO en fonction du temps. Des études de couches minces à base de RuO2 et de composites RuO2-CeO2 ont été entreprises, soit par spin-coating, soit par pulvérisation cathodique. Les couches obtenues par spin-coating manifestent une certaine activité catalytique liée à leur porosité. Les couches obtenues par pulvérisation cathodique sont des couches mixtes RuO2-CeO2. Elles n'ont aucune activité catalytique notable. Elles ont un comportement électrique non linéaire fortement lié à la microstructure et à la composition. Un modèle en loi de puissance a été appliqué avec succès pour décrire l'évolution de la conductivité en fonction de la composition en RuO2. Ces couches pourraient être utilisées dans des dispositifs piézorésistifs. En parallèle à ces études, un dispositif préfigurant un multicapteur de gaz a été mis au point."
"Cette étude s’inscrit dans le cadre d’une collaboration internationale, RD53, et qui vise à fournir à la communauté scientifique un ASIC « Front-End » de lecture du futur détecteur pixels courant 2022. La technologie 65 nm choisie par la communauté scientifique devra fonctionner dans un environnement extrêmement radioactif (10 MGray) pendant cinq ans d’exploitation sans maintenance possible.Deux approches expérimentales sont décrites dans ce mémoire : 1. Des études en irradiation ont été réalisées afin d'estimer la tolérance à la dose (TID) du process 65 nm pour fixer des règles de conception qui peuvent être respectées pour les cellules numériques et analogiques implantées dans le circuit final. Des véhicules de test (PCM) ont été définis pour être irradiés à l’aide d’une source de rayons X (10 keV – 3 kW) afin d'estimer les effets de dose. Les résultats obtenus sont synthétisés dans les chapitres concernés. 2. Dans le but d'optimiser l'immunité des points mémoires aux effets des SEU, plusieurs circuits prototypes ont été conçus. Ils incluent différentes architectures en vue d’être irradiées. Plusieurs campagnes d'irradiation ont été menées en utilisant un faisceau d'ions lourds et un faisceau de protons à dessein de comparer leur comportement et d’en extraire une cross-section la plus précise possible."
"Ce papier presente trois etapes relatives a l'etude d'un nouveau calorimetre differentiel concu pour la mesure du debit de dose adsorbee dans des canaux experimentaux en reacteur nucleaire d'irradiations la conception du design des cellules calorimetriques par simulation numerique en thermique, l'etalonnage preliminaire hors reacteur des cellules calorimetriques plus compactes en montage differentiel et la qualification de ce calorimetre lors d'une campagne d'irradiations dans le reacteur MARIA (comparaison avec un calorimetre differentiel classique"
"Différentes méthodes de synthèses ont été mises au point pour contrôler la forme et la composition des nanoparticules. L’effet de la nature et la concentration des surfactants, des solvants, la température et le temps de synthèse a également été étudié. Les poudres ont été caractérisées par diffraction des rayons X et microscopie électronique à transmission, couplée à la spectroscopie d'énergie dispersive. Des propriétés catalytiques et de détection ont été évaluées respectivement en présence de faibles concentrations de CO et de NO2 dans de l’air synthétique.Des nanooctaèdres de CoxFe3-xO4 ( x=1, 1,5 et x = 1,8 ) de 15-20 nm ont été produits par synthèse hydrothermale en utilisant différents surfactants (CTAB, SDS et PVP). Des nanocubes de tailles différentes de CoFe2O4 ont été produits par synthèse solvothermique en utilisant l'oléylamine comme surfactant. La poudre de CoxFe3-xO4 avec x = 1,5 a une activité plus élevée pour la conversion du CO que les nanooctaèdres x=1, et la conversion a lieu à plus basse température dans le cas des nanocubes. Les nanocubes présentent une sensibilité inférieure de détection au NO2 à celle des nanooctaèdres, ce qui indique que les faces {111} sont plus réactives que les faces {100} dans les nanoparticules de ferrites de cobalt."
"Ce travail de thèse porte sur l’étude par la Résonance Paramagnétique Electronique (RPE) des sels à transfert de charge quasi-unidimensionnels (TMTTF)2X (X=AsF6, PF6, SbF6), matériaux modèles de chaînes de spins quantiques. Tout d’abord, nous avons examiné en onde continue et sur une large gamme de température et de fréquence, la phase d’ordre de charge déjà observée dans ces matériaux en dessous de la température TCO. Nous avons mis en évidence deux nouveaux phénomènes à T < TCO : la rotation des axes principaux du facteur g et une modification structurale liée à un dédoublement de la maille cris- tallographique. Un calcul de chimie quantique a été réalisé à l’aide de la méthode DFT confirmant nos résultats expérimentaux. Dans la seconde partie de ces travaux de thèse, nous avons présenté les résultats obtenus par RPE en onde continue et en onde pulsée sur l’étude des défauts corrélés dans les systèmes à chaînes de spins. En onde continue, nous avons détecté pour la première fois une raie RPE fine à basse température, suggérant la présence de défauts corrélés ayant les caractéristiques de solitons. Les mesures par RPE pulsée nous ont permis d’observer les premières oscillations de Rabi de solitons piégés et de déterminer leur caractère robuste. Ces derniers résultats offrent une approche alterna- tive aux qubits à base de spins pour le traitement de l’information quantique."
"Depuis les 20 dernières années, l'essor de l'IoT et du ""cloud computing"" a conditionné le besoin dedéployer massivement, et globalement, des capteurs afin d'alimenter des bases de données et améliorerla précision des algorithmes d'analyse. Pour répondre à ces demandes, de nouveaux réseaux basés surles bandes de fréquences ISM ont été déployés. Nous avons donc appréhendé de façon complète cestechnologies afin de garantir une qualité maximale pour nos produits mais aussi proposer des conseilsjustes dans un secteur ou abus de langage et promesses de performances sont monnaie courante.Cependant, le nombre grandissant d'objets émettant sous la fréquence du gigahertz lève un doutequant à l'impact sur la santé des êtres vivants. Dès lors, coupler l'aspect non invasif des VLC avecl'Internet des Objets permettrait non seulement de réduire les risques pour les êtres humains maisaussi de limiter la saturation des bandes radio.Néanmoins, les techniques d'aujourd'hui consistent principalement en la réalisation de systèmesdiffusant l'information depuis une source unique vers plusieurs récepteurs, ce qui est l'inverse du paradigmede l'IoT. Dans cette étude, nous avons donc réalisé un nouveau design basé sur les VLC qui meten place une topologie de réseau en étoile 3. Ce système, basé sur un concentrateur disposant d'une ouplusieurs caméra en guise de photo-récepteurs, est optimisé pour plus d'autonomie. Ainsi, la vitessede transmission peut être gérée dynamiquement sans être connue par les autres éléments du système."
"Deux techniques sont utilisées en parallèle sur un même composant microélectronique pour diagnostiquer son échauffement au passage de pics de courant de quelques dizaines de microsecondes. La thermographie IR hétérodyne ‘full frame’ permet d’augmenter la résolution temporelle naturelle de la caméra d’un facteur 1000 (à 10μs). L’étalonnage est réalisé sur la scène (surface en aluminium avec couche de passivation). Le banc de thermoréflectance permet, lui, d’avoir un diagnostic sur un spot de 5μm de diamètre sur la zone d’intérêt, à 0,1μs de résolution temporelle. Les signaux obtenus sur la surface passivée sont cohérents avec l’IR mais nécessitent de poursuivre la phase d’étalonnage, plus complexe que sur une surface préalablement dorée."
"Les problématiques liées à la diminution de la taille des dispositifs actuels amènent l’industrie à réfléchir à des techniques de gravure ayant des résolutions à l’échelle de l’atome. Dans ce contexte, les techniques de nanostructuration directes sont très bien adaptées et représentent un potentiel important pour un futur proche dans les laboratoires de recherches. Le projet sur lequel j’ai travaillé avait pour but de coupler dans un environnement Ultra-Vide (UHV), un Dual-Beam, composé d’un FIB (Faisceau d’Ions Focalisé) et d’un MEB (Microscope électronique à balayage) et un bâti d’épitaxie par jet moléculaire (MBE), technique ultime en termes de dépôt. Cet environnement ultra-vide répond à la nécessité de propreté absolue des substrats et constitue un moyen pertinent de rendre fonctionnels les dispositifs ainsi élaborés dans des domaines aussi variés que la micro-nanoélectronique, l’optoélectronique, le photovoltaïque, la spintronique, la plasmonique, etc. La connexion sous UHV de la nanofabrication FIB à la croissance par épitaxie par jets moléculaires représente une voie unique pour fabriquer des structures 3D en alternant des étapes de gravure et de dépôt. Parmi les différentes applications, nous avons choisi de nous focaliser sur nanostructures de silicium. Le principal challenge pour l’industrie microélectronique et pour les chercheurs qui l’accompagnent est d’être capable de réaliser une optoélectronique entièrement intégrée à base de Si. Cela nécessite de convertir les matériaux à base de Si en absorbeur/émetteur efficaces de lumière, ce qui n’est pas le cas des matériaux massifs IV-IV qui ont une bande interdite indirecte. De nombreuses voies ont été testées, sans succès à ce jour. Une des pistes les plus prometteuses pour obtenir une bande interdite directe est de combiner les effets de la fonctionnalisation chimique et du confinement quantique dans les nano-objets. Des démonstrations récentes ont mis en évidence des géométries de structures « magiques » qui ne peuvent être obtenues que par une combinaison d’étapes Top-Down (gravure) et Bottom-Up (croissance). Le problème de cette combinaison est l’introduction de contaminants et de défauts, entre les étapes, qui sont rédhibitoires pour la fabrication ultérieure de composants électroniques fiables. Le premier chapitre traitera des différentes techniques de nanostructuration afin de réaliser des structures à l’échelle nanométriques. Les différents procédés de lithographie, permettant la réalisation de motifs sont décrites. Le chapitre 2 présente le silicium et le germanium, ainsi que l’alliage SiGe. Une fois les propriétés optiques et électroniques présentées, la croissance épitaxiale de cet alliage est abordée, pour la fabrication de boites quantiques. Le chapitre 3 est focalisé sur le FIB et plus particulièrement, un outil LMAIS-FIB de lithographie ultime utilisant différents types d’ions pour réaliser des gravures nanométriques. Pour réaliser ces étapes, j’ai utilisé des faisceaux d’ions focalisés de différentes natures (Si, Ga, Ge et Au) et j’ai comparé leurs caractéristiques et les dégradations induites par la gravure. J'ai dans ce contexte développé un procédé permettant d’éliminer la majorité des défauts crées grâce à des traitements thermiques. Le chapitre 4 présente les avantages de la gravure ionique sur un substrat poreux afin d'accélérer la vitesse de gravure par rapport à un substrat plein. Grâce à la versatilité du FIB, les motifs à graver peuvent être modulables. Le couplage de cette fabrication avec la lithographie par nano-impression permet l’obtention de masters réplicables pour des applications optiques, notamment des couches anti-réflectrices. Enfin, le cinquième chapitre présente l’oxydation du silicium-germanium sur substrat SOI et notamment, l’organisation de nanocristaux de germanium enrobé dans de l’oxyde de silicium induite par une combinaison de nanogravures et de démouillage solide durant le procédé de condensation."
"Que ce soit dans la vie quotidienne des particuliers ou dans l'industrie, les objets connectés sont de plus en plus répandus. Des étiquettes électroniques aux multiples capteurs, il est question de 50 milliards d'objets connectés à l'horizon 2020. Or, ces appareils fonctionnent pour la plupart sur piles ou batteries qu'il faut bien remplacer à un moment ou un autre. Cela représente un coût financier mais aussi environnemental très élevé. Il existe des solutions d'alimentation alternatives, basées sur la récupération d'énergie et en particulier le photovoltaïque. Dans ce domaine, DRACULA TECHNOLOGIES a développé une solution alternative durable LAYER® (Light As Your Energetic Response) : il s'agit d'un procédé d'impression à jet d'encre qui permet de fabriquer des modules photovoltaïques organiques qui vont pouvoir alimenter des objets connectés basse consommation. Les conditions lumineuses auxquelles sont exposés ces objets connectés étant très diverses, Dracula Technologies travaille avec l’équipe OPTO-PV du laboratoire IM2NP sur une base de données des univers lumineux qui sert à calibrer la fabrication des modules photovoltaïques et également sur la caractérisation de ces modules."
"L’invention des LED bleues de forte intensité en 1993 a permis une révolution générale de l’éclairage pour le grand public aussi bien que pour les commerces. Ces nouveaux dispositifs, proposent un rendement énergétique bien supérieur aux technologies commerciales précédentes, ce qui explique leur déploiement massif depuis la fin des années 2000. Les matériaux semi-conducteurs composant les LED sont déjà utilisés dans le domaine de la microélectronique pour effectuer des fonctions logiques à hautes fréquences. La technologie LiFi tend à cumuler ces deux propriétés en ajoutant une fonction de transmission d’information aux points d’éclairages existants. L’information est transmise en modulant l’intensité de la lumière à haute fréquence, bien au delà des capacités distinctives de l’oeil. Cette technologie devrait subir un déploiement avec l’arrivée de l’Internet des Objets (IoT) qui apporte une grande demande de connections sans fil, incompatible avec les réseaux radiofréquences actuels. Le travail présenté ici porte sur la réception de cette modulation lumineuse. Les récepteurs usuellement utilisés sont des photodiodes mais ces dernières imposent des contraintes d’éclairage et de consommation difficilement corrélables avec les impératifs de l’IoT. Ainsi, ces travaux étudient la possibilité de réception de la modulation LiFi par des cellules et modules photovoltaïques, dont les deux principales qualités restent les grandes dimensions du détecteur permettant une omnidirectionnalité de réception et une résistance à l’ombrage ainsi que le caractère passif de la détection."
"L’objectif principal des études présentées dans cette thèse était d’étudier les propriétés photoluminescentes et photocatalytiques de la série du tungstate de bismuth Bi2WO6-BiLuWO6. Les phases polycristallines Bi2-xLuxWO6, synthétisées par voie solide et à 1000°C, ont été caractérisés par diffraction de rayons X, microscopies électroniques à balayage et en transmission, réflexion diffuse UV-visible et spectroscopie Raman. Les études structurales effectuées à partir d’affinements par la méthode Rietveld ont mis en évidence l’existence d’un domaine biphasé pour des compositions 0,1≤x≤0,3 (une phase orthorhombique et une phase monoclinique) et d’un domaine de solution solide monoclinique désordonnée pour des compositions 0,4≤x≤1. La spectroscopie Raman confirme ce désordre au niveau vibrationnel. La spectroscopie d’impédance électrique en température a révélé l’existence de deux mécanismes de conductions avec deux énergies d’activation. Les émissions photoluminescentes sous excitation UV sont conditionnées par le taux de substitution par le lutécium. Ces émissions ont été décomposées en trois composantes (G1, G2 et G3). La composante G1 serait due à des défauts spécifiques liés au bismuth. Les composantes G2 et G3 seraient liées aux transferts de charges classiques au sein des octaèdres WO6. L’efficacité photocatalytique de ces matériaux a été évaluée en utilisant la Rhodamine B comme un modèle polluant. Le tungstate de bismuth pur (BWO) a présenté la meilleure efficacité photocatalytique pour la série étudiée, du fait de la grande surface spécifique et la faible vitesse de recombinaison des porteurs de charges."
"Des films minces de LaCoO3 et LaFeO3 ont été déposés sur des substrats de Si, YSZ cubique et YSZ quadratique, par PLO. Le but était d'obtenir des films minces nanocristallins sensible à NO2. Les films minces déposés sur YSZ ont été étudiés comme couches sensibles dans des capteurs résistifs et minces déposés sur YSZ ont été étudiés comme couches sensibles dans des capteurs résistifs et électrochimiques. Les films ont été caractérisés par ORX, MES, AFM, XPS, MET-EDS. Une température de dépôt de 850°C permet d'obtenir des films minces monophasés et nanocristallins, avec un faible nombre de gouttes en surface, and sans fissures. Les films minces LaCoO3 et LaFeO3 ont respectivement des structures rhomboédrique et orthorhombique. Ils sont non-stoichiométriques, avec une formule chimique LaM0_8O3-0. Les facettes cristallographiques exposées aux gaz sont {101 }0 ={001 }c pour LaFeO3 et {2-1 O}H ={101 }c pour LaCoO3 , sur C-YSZ. Les films minces LaFeO3 ont montré une très grande sensibilité à NO2, une bonne réponse même à température ambiante, ainsi qu'une sélectivité à NH3 et CO. Les films LaCoO3 ont faiblement réagi à NO2. En tant que capteur électrochimique, LaFeO3 sur T-YSZ a montré de bonnes performances, ouvrant le champ de capteurs sur support flexible."
"Une mise au point bibliographique des propriétés thermodynamiques et structurales de la wüstite non stœchiométrique Fe1-zO et de ses «modifications» ou pseudo-phases, est faite, de 1960 à ce jour (159 références). L’attention est d’abord portée sur la complexité du diagramme de phases à l’équilibre. La transition du premier ordre W &lt;==&gt;W’est précisée sur la frontière fer/wüstite au voisinage de 1185 K. Les transitions dues à l’existence des variétés Wi à T(W) &gt; 1185 K et W’j à 1185K &lt; T(W’) &lt; T(W) (i et j =1,2,3) sont reconsidérées, de même que les déterminations structurales relatives aux défauts ponctuels et à leurs amas. Des équilibres sont envisagés, qui tentent de caractériser la stabilité des pseudo-phases. Celles-ci peuvent être interprétées en termes de transformation ou de mode de distribution des amas de défauts (percolation, surstructure) en incluant des changements dans les porteurs de charges électroniques."
"L'étude de la dynamique des parois de domaines dans les nano-systèmes ferromagnétiques est cruciale pour le développement des dispositifs de stockage de l'information basés sur le déplacement et le contrôle des parois. Ces dispositifs ont plusieurs avantages : non-volatilité, rapidité d'exécution, haute densité de stockage, et faible consommation de l'énergie. En utilisant des méthodes micromagnétiques et analytiques, nous avons constaté que l'interaction entre deux parois affectait les processus de dépiégeage sous champ magnétique, dans des nanofils en nickel à géométrie cylindrique et planaire. Nous avons mis en évidence des comportements non linéaires de la dynamique d'une paroi piégée, qui varient selon le matériau et le type de piège utilisé. Les diagrammes de phases représentant l'exposant de Lyapunov ont permis la distinction entre des zones chaotiques et périodiques, en fonction de la fréquence et de l'amplitude d'une excitation harmonique. Nous avons présenté des résultats sur la manipulation précise d'une paroi transverse sous impulsions de courant dans un nanofil planaire en nickel, structuré par une multitude de défauts artificiels. Nous avons montré que le positionnement exact de la paroi à température ambiante est possible uniquement pour des impulsions symétriques de très courte durée. Des effets inertiels pouvant s'opposer au couple de transfert de spin, ou au contraire l'amplifier ont été observés. Ces derniers résultats ouvrent une route vers le déplacement des parois dans les deux directions par des impulsions unipolaires de courant."
"Depuis les travaux d’Einstein et de De Broglie au début du XXème siècle, il est admis que la lumière peut être décrite à la fois comme une onde ou comme un ensemble de particules appelées photons. La production d’énergie solaire et la détection de la lumière reposent aujourd’hui sur l’effet photovoltaïque qui exploite la description corpusculaire. Pour explorer une autre voie de transformation de lumière en électricité, le concept d’antenne rectifiante exploite cette fois la nature ondulatoire de la lumière. En tant qu’onde électromagnétique, la lumière peut être absorbée par une antenne à l’instar d’une onde radio. Les premiers arguments théoriques sur ce principe remontent à la fin des années soixante, mais ce n’est que depuis une dizaine d’années que les tentatives de réalisation expérimentale d’antennes rectifiantes pour les fréquences du visible et du proche infrarouge voient le jour grâce aux progrès des méthodes de nanofabrication. L’objet de cette thèse est de proposer une conception innovante de nano-antennes rectifiantes qui réponde aux deux verrous technologiques principaux que sont la fabrication d’antennes à l’échelle nanométrique et la rectification du courant alternatif térahertz qui s’y établit lorsque la lumière y est absorbée. Cette proposition est accompagnée d’une étude théorique optique dont l’objectif est de fournir une compréhension fine de l’interaction lumière-matière dans les structures étudiées. La fabrication et la caractérisation de nano-antennes à partir de nanoparticules en solution colloïdales est ensuite détaillée et comparée aux modèles théoriques. Enfin, l’étude d’une diode moléculaire synthétisée sur mesure présente une solution pour combiner des propriétés mécaniques de greffage de nanoparticules et des propriétés électroniques de rectification à haute fréquence. L’approche multidisciplinaire de ce travail qui combine nanophotonique et électronique moléculaire ouvre une nouvelle perspective pour la réalisation d’un dispositif de conversion de lumière en électricité fondamentalement innovant."
"Une mise au point bibliographique des propriétés thermodynamiques et structurales de la wüstite non-stœchiométrique Fe1-zO et de ses variétés -ou pseudo-phases- en fonction de sa composition z et de la température d’équilibre est faite de 1960 à ce jour. L’attention est d’abord portée sur la complexité du diagramme de phases à l’équilibre. La transition du premier ordre W ⇆ W’ est précisée sur la frontière fer/wüstite au voisinage de 1185 K. Les transitions dues à l’existence des pseudo-phases Wi à T(W) > 1185++ K et W’j à T(W’) < 1185+ K (i et j =1,2,3) sont reconsidérées, de même que les déterminations structurales relatives aux défauts ponctuels et à leurs amas. Des équilibres sont envisagés, qui tentent de caractériser la stabilité de ces pseudo-phases. Elles peuvent être interprétées en termes de transformation ou de mode de distribution des amas de défauts (i.e., percolation, surstructure) en incluant des changements dans les porteurs de charges électroniques."
"Les Jonctions Tunnel (JT) sont des composants très importants pour les cellules solaires multi-jonction, puisqu'elles assurent la connexion électrique en série entre les différentes sous-cellules. Récemment, il a été fabriqué par EPVOM des JTs AlGaAs:C/GaAs:Te et AlGaAs:C/GaInP:Te atteignant des densités de courant pic J pic records de 10 kA/cm² sur substrat GaAs [1], en remplaçant le dopant N usuel Si par du Te et permettant ainsi d'obtenir les forts niveaux de dopages nécessaires pour augmenter la probabilité d'effet tunnel dans le composant. Nous avons travaillé sur une autre approche pour obtenir des JT de ""hautes"" performances, et ne nécessitant pas des dopages aussi forts : il s'agit de profiter des offsets de bandes de l'hétérojonction AlGaAsSb/AlGaInAs de type II pour augmenter la probabilité d'effet tunnel."
"Les jonctions tunnel sont un élément de base dans les cellules solaires multijonction (MJSC), en assurant une interconnexion électrique entre chaque sous­cellule absorbante. La minimisation des pertes électriques et optiques au travers de ces jonctions fortement dopées est ainsi une condition essentielle pour l'obtention de très hauts rendements. De plus ces cellules MJSCs fonctionnant généralement sous concentration, ces jonctions tunnel doivent être capables de conduire avec une très faible résistance de très forts courants [1]. Nous avons mené une étude théorique et expérimentale sur différentes structures de jonctions tunnel GaAs qui a permis d'identifier les mécanismes d'effet tunnel prédominants et de proposer un modèle semi­classique d'effet tunnel interbande simple et prédictif. Pour cela, nous nous sommes basés sur les résultats expérimentaux de sept échantillons de JTs GaAs de dopage N croissant, dont les performances indiquent que l'effet tunnel interbande est prédominant par rapport à l'effet tunnel assisté par les défauts. Ce résultat a été confirmé par des simulations semi­ classiques [2] et quantiques [3]. La précision des modèles semi­classiques a alors été augmentée par la considération de la non­parabolicité des bandes dans le cas de fort dopages, et la prise en compte de la non­ uniformité du champ électrique dans la structure [3] Ce modèle permet donc d'évaluer quantitativement le courant tunnel et notamment le courant pic des jonctions tunnel dans le système GaAs. En effet nous avons montré que les résultats obtenus par le modèle semi­classique étaient en accord très proche, d'une part avec les mesures expérimentales, et d'autre part avec des simulations effectuées à l'aide d'un modèle quantique développé à l'IM2NP. Ce modèle analytique, de par sa simplicité et sa précision, pourrait donc être étendu à des structures de bande plus complexes, et potentiellement pourrait être facilement intégré dans des outils plus généraux de modélisation de dérive­diffusion."
"Cet article traitede la conception d’une étiquette sans-contact UWB-RFID afin d’apporter à la technologie RFID la précision des solutions UWB pour la géolocalisation des étiquettes au sein d’un entrepôt par exemple. Elle est ainsi constituée d’un émetteur UWB et d’un récupérateur d’énergie UHF. Le premier lui permet d’envoyer des impul-sions UWB vers un récepteur UWB adéquat et le secondlui permet d’être télé-alimentée en UHF par un lecteur RFID. L’émetteur UWB et le récupérateur d’énergie UHF ont ici été implémentés dans le même circuit intégré, fabriqué en technologie CMOS 130nm,de façon à ce que la fréquence de répétition des impulsions UWB émises soit l’image de la puissance UHF reçue. Ce circuit intégré a été reporté sur un PCB sur lequel ont été gravées les deux antennes requises, à savoirl’antenne UWB et l’antenne UHF. Les résultats de mesure montrent pour cette réalisation qu’une puissance incidente d’au moins -10dBm doit être reçue par l’étiquette pour que la génération périodique des impulsions UWB débute. Enfin, une puissance incidente reçue de 5dBm permet quant à elle une communication UWB vers un récepteur adéquat à un débitde 330kbps."
"Ce travail étudie le potentiel du silicium de type n purifié par voie métallurgique pour la fabrication de cellules photovoltaïques à bas coût. Les teneurs élevées en dopants conduisent à de faibles valeurs de résistivité, ainsi qu'à une diminution de la durée de vie des porteurs de charge. La fabrication de cellules photovoltaïques a permis d'obtenir des rendements de conversion variant de 13.7% à 15.0% sur 148.6cm². Avec un procédé de fabrication amélioré, des rendements de 16.0% pourraient être obtenus. La résistivité des plaquettes a été identifiée comme facteur limitant les performances des cellules. Le co-dopage au gallium a été proposé pour augmenter la gamme de résistivité. Les cellules photovoltaïques réalisées montrent une excellente stabilité sous illumination et de faibles coefficients en température de la tension de circuit-ouvert. Ces travaux de thèse ont permis de définir le potentiel du silicium de type n purifié par voie métallurgique et de définir les spécifications nécessaires initiales au niveau de la charge à purifier pour permettre la fabrication de cellules photovoltaïques efficaces."
"Dans ce papier, nous proposons un protocole de communication asynchrone à lien radio Ultra-large bande (ULB) pour les réseaux de capteurs à faible consommation. Le choix des communications impulsionnelles ULB est justifié par leur faible consommation d'énergie pour la transmission des données, la simplicité des architectures matérielles associées qui permet un faible cout d'implémentation et leur capacité de pouvoir transmettre des données avec des débits instantanés élevés. En outre, l'utilisation d'un protocole asynchrone basé sur l'utilisation d'un récepteur de réveil à très faible consommation permet également une réduction importante de la consommation. L'étude énergétique du protocole proposé basée sur une modélisation du noeud de capteurs montre l'intérêt de l'utilisation des communications ULB et d'un protocole asynchrone à base de récepteur de réveil dans les réseaux de capteurs où une très faible consommation d'énergie est requise. Mots-Clés-Réseaux de capteurs, ULB, Récepteur de réveil, faible consommation."
"En raison de la nature discontinue des communications basées sur la radio impulsionnelle, il apparaît que plus le débit de transmission des données est élevé et plus les émetteurs et récepteurs utilisés peuvent être économes en énergie. De ce fait, la radio impulsionnelle parait bien adaptée aux applications à courte portée notamment lorsque des débits élevés sont requis. Cependant, devant le nombre d'applications nécessitant uniquement des débits de quelques octets par seconde ou moins, l'implémentation de techniques permettant la minimisation de la consommation devient alors primordiale en radio impulsionnelle. En effet, ces techniques permettent de réduire considérablement la partie statique de la consommation et ainsi, de faire tendre un système vers son efficacité énergétique maximale constatée normalement qu'en haut débit. Pour ce faire, des stratégies de conception côté émetteur ainsi que des systèmes de minimisation de la consommation côté récepteur sont proposées, puis leur intérêt est démontré sur des circuits existants."
"Cet article présente une méthode de mesure des impédances transitoires n'utilisant qu'un oscilloscope, un coupleur directionnel et un ordinateur. En plus d'obtenir des impédances comparables à celles obtenues avec un analyseur de réseaux standard, la méthode proposée permet également de visualiser l'évolution de l'impédance au cours du temps et ce, pour des puissances bien plus élevées que celles habituellement disponibles sur les analyseurs de réseaux. Appliquée à la RFID à 13,56MHz, cette méthode permet alors de mesurer les impédances en jeu lors de la communication entre un lecteur et une puce RFID, puis d'en déduire des informations telles que la consommation de la puce et les impédances de rétro-modulation, ces dernières permettant par exemple de vérifier la conformité d'une puce aux standards."
"Cet article traite d'une étude préliminaire ayant comme objectif de déterminer le matériau le plus approprié pour la réalisation des électrodes nécessaires à la mesure de l'électrocardiogramme à l'aide d'un dispositif utilisable à domicile par les patients et n'utilisant que deux électrodes. En effet, bien que la mesure de l'électrocardiogramme dans le milieu médical ne pose pas de problème particulier (environnement contrôlé, patient immobile, application d'un gel sur la peau favorisant la mesure, etc.), l'utilisation à domicile d'un tel système est plus problématique car cela nécessite tout d'abord sa miniaturisation tout en garantissant une mesure fiable même lorsque le patient est en mouvement et ce, sans qu'il n'y ait besoin d'appliquer au préalable un gel sur la peau favorisant la mesure. Pour cela, une modélisation électrique du circuit vue entre deux électrodes posées sur la peau est ici tout d'abord introduite. Puis, des critères permettant de juger de la qualité d'un électrocardiogramme et notamment de sa sensibilité au mouvement du patient sont proposés. Enfin, une série de mesures électriques et d'électrocardiogrammes sont présentées pour différents matériaux en vue d'en déduire le modèle électrique et la qualité de signal induits par chaque matériau. En plus de dessiner dès maintenant une tendance entre les différents matériaux testés, il est envisagé par la suite d'étudier la corrélation entre les paramètres du modèle et la qualité de l'électrocardiogramme et ainsi d'imaginer à terme l'intégration dans le dispositif de méthodes de compensation améliorant en temps réel la qualité de l'électrocardiogramme mesuré.."
"http://nkms.free.fr/ Polytech Marseille, Aix Marseille Université ; Domaine Universitaire Saint Jérôme, av Escadrille Normandie Niémen ; 13397 Marseille Cedex 20 RESUME : L'objectif de cette communication est double : il présente l'approche pragmatique (EAPAPA) que nous proposons (en ingénierie de l'automatique) pour notre formation d'ingénieurs à Polytech Marseille et le projet Erasmus+ nommé WESET (dans lequel cette approche est développée et appliquée) auquel Polytech Marseille contribue comme acteur responsable du module Contrôle, Commande, Optimisation et Diagnostic."
fr_abstract_s
"Dans cette étude nous avons approfondi le statut de la protection juridique du paysage dans les deux pays. Le niveau constitutionnel de protection est plus ancien et spécifique en Italie, plus récent en France et voué généralement à l’environnement. Quant à la législation ordinaire, au-delà des Alpes on retrouve une multiplicité d’instituts de protection, différents selon le bien ou le territoire considéré. Les instruments administratifs et urbanistiques sont excessifs qui assurent souvent une protection seulement formelle et engendrent une incertitude. Toutefois la protection est élevée pour certains biens ou zones, et en particulier pour les zones du littoral ou pour celles soumises à la compétence des « Architectes des Bâtiments ». Le rôle des juges administratifs est prépondérant : En Italie, en revanche, les instituts fondamentaux (la contrainte, la planification et l’autorisation) gèrent généralement les biens et les territoires protégés. La planification a été peu et mal appliquée, tandis que l’autorisation a été gérée avec une extrême légèreté par les régions. Toutefois, même après la Convention Européenne paysagère, il apparaît dans les deux Pays un véritable droit dédié au paysage. Dans les conclusions, on propose notamment pour la France un système spécifique d’autorisations, la réduction des documents urbanistiques et environnementaux, l’introduction d’une discipline plus contraignante et un aménagement spécifique. Pour l’Italie la réalisation de l’aménagement du paysage est indispensable ou mieux encore, la planification du paysage pourrait être unifiée à l’aménagement urbanistique local, on créerait ainsi un lien qui n’existe pas. En outre, on propose que l’autorisation devienne exclusivement compétence de l’Etat. L’ensemble de ces règles pour protéger le paysage pourraient s’élever au rang de « domaine du paysage » compris non comme régime propriétaire mais comme statut engageant pour les biens d’une valeurs inestimable et d’utilité sous de multiples aspects."
"Présentation de l'éditeur : ""Cet ouvrage couvre l'ensemble du programme d'histoire des idées politiques depuis l'Antiquité jusqu'à nos jours : époques antique, médiévale, moderne et contemporaine. Il ne divise pas le programme, comme cela est généralement fait, entre avant et après le XVIIIe siècle, mais propose une analyse générale de l'évolution de la philosophie politique et juridique avec ses ruptures et ses continuités. Ce manuel s'adresse aux étudiants des facultés de droit (licence 3, master 1) et des Instituts d'études politiques, ainsi qu'aux personnes qui préparent les concours administratifs. Il est constitué de fiches thématiques permettant d'appréhender rapidement et précisément les enjeux importants. Chaque fiche offre : • les repères essentiels ; • des explications précises sur les thèmes fondamentaux ; • des références bibliographiques pour approfondir une question."
"Largement entendue, la responsabilité de l’huissier de justice correspond à l’obligation, pesant sur ce dernier, d’assumer les conséquences civiles, pénales et disciplinaires de la mauvaise exécution des missions qui lui sont confiées. À cet égard, cette étude met dans un premier temps l’accent sur l’état du droit français de la responsabilité civile de ce professionnel qui a la particularité d’être à la fois un officier ministériel détenteur d’une parcelle de la puissance publique et le mandataire du créancier agissant en qualité d’agent libéral. Ainsi, sont tour à tour examinées les règles régissant l’engagement (envers son client et envers les tiers) et la mise en œuvre (détermination de la juridiction compétente et garantie offerte par l’assurance responsabilité civile professionnelle) de cette responsabilité. Dans un second temps, est étudié le régime de la responsabilité de l’État français du fait des huissiers de justice, tel qu’il se dégage de la jurisprudence de la Cour européenne des droits de l’homme."
"Après avoir délivré un commandement de payer valant saisie immobilière, le créancier poursuivant ne peut, sauf abus de saisie, voir sa responsabilité engagée à raison de ce qu’il aurait tardé à répondre, avant le jugement d’orientation autorisant la vente amiable, à une sollicitation du débiteur saisi tendant à l’autoriser à vendre amiablement le bien saisi."
"Lorsque prévention et solidarité se mêlent au service de la souveraineté nationale et de l'épanouissement du sentiment d'appartenance à l'UE. Transformer l'incertitude en risque afin de le maîtriser, il s'agit là de l'un des enseignements de la crise sanitaire de l'année 2020, planétaire et nédite, déclenchée par un coronavirus, le covid-19, que les épidémiologistes découvraient. Les périodes de confinement allaient dès lors s'enchaîner à un rythme propre selon les pays. Après un premier épisode officiel de propogation du virus au printemps suivi d'un confinement, dont le 11 mai en France était censé marquer la fin, une deuxième vague est venue porteuse de contraintes qui furent allégées le 15 décembre par notre Gouvernement. Une troisième vague pourrait survenir. Il est vrai qu'un programme de vaccination est établi qui devrait s'appliquer de manière uniforme à l'échelle de l 'Europe dès la fin décembre. Des incertitudes demeurent. L'espoir est dans la science face à un ennemi invisible qui nous apprend la prévention mot d'ordre pouvant heurter mais devenu incontournable. Prévenir c'est agir rapidement afin de donner son plein effet à l'anticipation. Les mesures adoptées par la France pour gérer la crise sanitaire sont empreintes de cette volonté. Il en est de même de la réforme de notre droit des entreprises en difficulté en cours antérieurement à la crise sanitaire et qui doit s'opérer à la lumière de la directive ""restructuration et insolvabilité"" UE 2019/1023) dont certaines des préconisations ont reçu une application anticipée. La prévention est au coeur du débat. Elle le demeure. La gestion de l'épidémie a imposé dès le départ de concilier confinement et absence de fermeture des commerces ""essentiels"". Protéger la santé de chacun d'entre nous et maintenir une activité entrepreneuriale est l'exercice délicat, et souvent périlleux, auquel les Etats furent confrontés, qui a révélé, dans leur aptitude à s'adapter, leur force et parfois quelques failles atténuées par l'entraide interne ou au sein de l'UE, voire internationale. A l'heure de la deuxième vague, la prudence des peuples, dont on observe qu'elle n'est pas comparable selon les pays, permet de nuancer ce constat. La France a soutenu et soutient son économie de manière exemplaire. De nombreuse textes ont été votés ou pris au profit des PME et des TPE. Des fonds de soutien ont été mis en place, des dispositions bancaires, fiscales, et sociales, adoptées, pour une prise en charge, qui se poursuit, des effets de la crise. Les PGE, ""Prêts Garantis par l'Etat"", en sont une illustration. Les TPE en ont été les principales bénéficiaires (90%) ; le dispositif qui devait prendre fin le 31 décembre 2020 est prorogé, un taux bas est retenu (1 à 3%) après négociations avec les banques ; une partie de ces prêts sera transformée en prêts participatifs d'une durée supérieure à 7 ans. L'Italie, l'Espagne ou encore l'Allemagne y ont eu également recours. Un plan de relance corrigé, présenté début septembre 2020 doit contribuer au rebond des entreprises, des plans sectoriels sont déjà opérationnels. La compétitivité est un enjeu majeur Les Etats oeuvrent en ce sens, dont les Etats-Unis. Au plan européen, l'assouplissement quantitatif (QE) s'est inscrit dans cette même démarche, faisant ressurgir la question de la mutualisation de la dette et de la solidarité entre Etats membres de la zone euro. Le couple franco-allemand fait avancer l'Europe. L'arrivée au pouvoir d'un nouveau président américain prônant le multilatéralisme ne devrait pas fragiliser cette entente qui prend en compte le risque que représente l'impérialisme chinois. Le plan de sauvetage européen, adopté le 21 juillet 2020 par les Vingt-Sept, permet que des subventions directes soient accordées aux pays les plus touchés par la pandémie. Le moment est historique. L'investissement stratégique est au goût du jour. En France, des entreprises sont déjà sauvées, d'autres le seront. Des faillites ne pourront être évitées. Il est nécessaire de réaliser des réformes structurelles et de mener à terme, sans retard, celles engagées. De nouvelles dispositions légales, juridiques, vont continuer à se faire jour, progressivement, parfois ponctuellement. Toutes les branches du droit sont concernées, outre le droit des entreprises en difficulté, sont visés, le droit des sociétés, ou encore le droit des contrats. Les délais de procédure sont aménagés. Il est vrai que le bouleversement fut radical. Il a fallu répondre rapidement à des exigences sanitaires qui ont prévalu sur toute autre considération. Ces exigences demeurent. Dans ce contexte, d'une exceptionnelle gravité, des prises de conscience sont faites et des changements s'opèrent en un temps record dans de nombreux domaines. L'outil numérique est largement exploité. La communication à distance a permis et permet de maintenir un lien social, éducatif. L'accès à la justice a été préservé, notamment au profit des entreprises en difficulté (en ce sens : Communiqué du 5 avril 2020 des Délégations Générales de la présidence du tribunal de commerce de Paris à la Prévention et au Traitement des Difficultés des Entreprises) et il en est encore ainsi. Le télétravail s'est développé. La protection des données doit être assurée. La capacité de production ayant parfois fait défaut, la relocalisation de certaines activités notamment dans le secteur industriel est encouragée ; la baisse des impôts de production (10 Md€), effective dès le 1er janvier prochain, le maintien de la baisse de l'impôt sur les sociétés, y contribueront. Cela favorisera sur notre sol l'émergence des ETI, éventuellement par voie de transformation de PME, qui connaissent un fort développement outre-Rhin et outre-Manche. Un nouvel ordre des relations internationales en découlera qui va déteindre sur la globalisation sans véritablement la remettre en cause. La globalisation financière est assumée, celle numérique en plein essor. Les objectifs sociétaux et environnementaux sont mis en exergue, confortant l'approche du rôle des acteurs sociétaires initiée par la loi Pacte du 22 mai 2019. Notre code civil prévoit, depuis lors, qu' ""une société peut être gérée dans son intérêt social (...)"" (art.1833 al. 2), et qu'il est possible d'en déterminer ""la raison d'être"" (art.1835). Le contrat de société, plus fréquemment, valorisera des ""valeurs nobles"" charpentant l'objet social à réaliser dans les limites convenues. La société à mission est adoptée. Les collaborateurs dans l'entreprise devront recevoir une formation pour un bon usage du digital. A défaut, une faute de gestion pourrait être retenue. L'évolution semble irrépressible, qui a déjà modifié, et continuera de modifier, durablement semble-t-il, notre façon de vivre, individuellement, et collectivement, de travailler, de nous déplacer, de consommer, gérer, ou diriger, les gouvernances sont concernées. La prudence avec le temps qui passe reste de rigueur. En France, le premier déconfinement organisé est venu, mais il a fallu dès le mois d'octobre, sinon y renoncer, à tout le moins le restreindre. Le 15 décembre, le couvre-feu est instauré. La confiance se gagne, l'effort est constant. L'état d'urgence sanitaire vient d'être prorogé jusqu'au 16 février 2021. Il était auparavant prévu qu' il devait durer jusqu'au 10 juillet inclus (v. Loi n°2020-546 du 11 mai 2020 prorogeant l'UES et complétant ses dispositions, JO 12 mai), et prendre fin de manière progressive entre le 11 juillet et le 30 octobre 2020 ; il devait être maintenu jusqu'à cette dernière date uniquement en Guyane et à Mayotte (v. Loi n°2020-856 du 9 juillet 2020 organisant la sortie de l'état d'urgence sanitaire). La prévention responsabilise. Le constat en est largement fait à l'échelon national mais aussi européen. Il faut agir en temps utile. A cet égard, le recours au procédé préventif, présent dans notre code de commerce, le démontre. Des modifications sont attendues dont certaines, particulièrement novatrices, vont conforter l'attractivité du modèle français en Europe et au-delà. Également, notre législateur s'est adapté dans des délais brefs à la situation due à la crise sanitaire en ajustant de manière temporaire le droit des entreprises en difficulté dont le droit de la prévention (v. Circ. DACS 16 juin 2020 présentant l'ord. n°2020-596 du 20 mai 2020, JO 21 mai, portant adaptation des règles relatives aux difficultés des entreprises et des exploitations agricoles aux conséquences de l'épidémie de covid-19 ; égal., ord. n°2020-341 du 27 mars 2020, JO 28 mars, portant adaptation des règles relatives aux difficultés des entreprises et des exploitations agricoles à l'urgence sanitaire et modifiant certaines dispositions de procédure pénale). La prévention du risque de défaillance d'une entreprise, d'une épidémie et plus probablement de sa propagation, ou de défaut d'un Etat, ou autre, demande des moyens d'agir. L'anticipation est la règle. Il faut tenter d'en garantir l'efficacité. Une crise peut donner l'opportunité de l'action ou la précipiter. L'histoire l'a prouvé et le prouve à nouveau. Rendue à propos d'un programme d'achats de titres publics lancé en 2015, la décision de la Cour constitutionnelle allemande de Karlsruhe du 5 mai 2020 a donné l'occasion de réaffirmer l'indépendance de la BCE et la prééminence de la CJUE (CJUE Communiqué de presse n°58/20, Luxembourg 8 mai 2020). La BCE et l'euro jouent un rôle essentiel dans la gestion de la crise sanitaire qui creuse les inégalités. L'Europe en remédiant à ces dernières se construit. Après le Mécanisme Européen de Stabilité, MES, adopté, entre autres mesures, au lendemain de la crise financière de 2008, l'emprunt commun actuel permet de franchir un pas supplémentaire dans le renforcement de l'intégration européenne et de la mutation économique."
"Prévenir afin d’éviter une liquidation judiciaire. La prévention des difficultés d’une entreprise n’a de sens que dans ce but, elle s’efface si l’issue ne peut être que l’exécution forcée, comme le veut la loi du marché. A l’heure de la mondialisation, la liberté d’entreprendre impose de pouvoir juguler, dès leur apparition, voire par anticipation, les difficultés financières, économiques, ou sociales, dont les causes pourraient être internes, ou tenir à des événements exogènes. L’amiable, le judiciaire, dénouent les conflits. Des solutions adaptées doivent être mises à la disposition des débiteurs, ou plus largement, comme le souhaite l’Union européenne. L’économie est globale, digitale, la financiarisation bouleverse les règles du jeu. Le droit, dont le droit des affaires, s’adapte. La France doit être au rendez-vous de nombreux défis avec et pour l’Europe, en soutien à sa politique internationale, multilatérale, tournée vers l''Afrique, dont la population aura doublé en 2050, ou la Chine, qui veut étendre son emprise économique avec les «nouvelles routes de la soie». La prévention est l'un de ces défis. Le droit préventif français inspire, la directive Restructuration et Insolvabilité UE/ 2019/ 1023 en atteste, et s'inspire lui-même du pragmatisme des systèmes juridiques outre-Rhin et outre-Atlantique. La France vit à l'heure d'un mouvement réformateur fort qui donne sa pleine mesure à l'exigence de prévention. Les enseignements sont immenses. La solidarité s'observe. Le couple franco-allemand se conforte. Le soutien des banques centrales est incontournable. Une mutation économique est en marche et l'Europe se construit."
"Dans une économie globale, digitale, le droit des affaires s'adapte. Il ne peut en être autrement. La France vit, à cet égard, à l'heure d'un mouvement réformateur fort. Il s'agit, à propos d'une activité entrepreneuriale donnée, pour l'efficience du droit, de permettre et de faciliter l'accès à des instruments juridiques, économiques, financiers, et concurrentiels, au sein de l'Union européenne, voire au-delà. Le climat géopolitique est mouvant. Une bipolarisation dans le monde se crée. Un droit harmonisé en Europe confortera la compétitivité des entreprises. Il n'est pas surprenant que l'actualisation des procédures de restructuration de nature préventive soit à l'ordre du jour à l'échelon national, mais aussi européen et international. La Commission européenne oeuvre en ce sens. Le droit français de la prévention est reconnu, des aménagements devront cependant lui être apportés. Plus largement, la France et l'Allemagne sont chefs de file, le Président de la République Emmanuel Macron l'a souhaité. Il est proposé aux Etats membres de conserver la spécificité de leur droit, conforme aux perspectives programmées, et, dans le même temps, de s'ouvrir aux avancées des autres systèmes juridiques dans le but de parvenir à l'établissement d'un corps de règles communes. L'intégration plus large du debt-to-equity swap, que connaît la législation allemande, et qui renvoie au loan-to-own américain, en sera une manifestation. Cela reviendra à s'éloigner davantage encore de la trajectoire initiée par l'ancienne loi n°85-98 du 25 janvier 1985 relative au redressement et à la liquidation judiciaires des entreprises, dont on a dénoncé le dogmatisme. On observe un rayonnement de la prévention. La sociologie juridique en révèle les prémices. L'OHADA (Organisation pour l'Harmonisation en Afrique du Droit des Affaires) a fait sienne la notion d'anticipation des difficultés des entreprises. Il en est de même des droits marocain et tunisien. Les outils de la prévention se façonnent au fil des réformes, ils séduisent, sont parfois critiqués et ne cessent d'évoluer. L'influence du chapter 11 est à nouveau relevée. Favoriser la restructuration de l'entreprise, généralement transnationale, avant insolvabilité, est l'objectif à atteindre, en envisageant plus largement la question de la responsabilité du dirigeant. Il s'agit d'intervenir le plus tôt possible, dès les premiers signes d'alerte, sous peine d'avoir à répondre d'une faute de gestion comme l'a déjà jugé une cour d'appel. Sauvegarde de l'entreprise et paiement des créanciers doivent se rejoindre. Un nouveau langage se fait jour. On évoque la valeur de l'entreprise, going concern, les classes de créanciers, on rappelle que les associés ne sont que des créanciers de dernier rang. La protection des investisseurs créanciers doit devenir une réalité. Le droit français le permet mais il doit franchir une étape supplémentaire. Selon les économistes, une prochaine crise systémique, qui ne sera pas bancaire, est à craindre. L'évolution législative en marche devrait permettre aux entreprises de mieux y faire face. Les entreprises américaines ont bien réagi à la crise de 2007-2008. Le cycle de conférences, dans une approche comparative et prospective Europe/EU/Afrique, s'est intéressé à l'origine de la prévention (26 octobre 2018), a pris la mesure de l'évolution et du rapprochement des systèmes juridiques de prévention (19 novembre 2018), et posé la question de la place et du rôle des créanciers et des actionnaires dans la réorganisation des actifs (26 novembre 2018). Les enjeux sont commerciaux, financiers et politiques. Des éléments de synthèse ont été dégagés."
"Décret n°2018-1219 du 24 décembre 2018 portant diverses mesures de procédure civile relatives à la reconnaissance transfrontalière des décisions en matière familiale, à la communication électronique et au rôle du ministère public en appel (JO 26 déc. 2018, texte 8)."
L’article R. 311-5 du code des procédures civiles d’exécution ne fait pas obstacle à ce qu’un créancier inscrit puisse se prévaloir de la déchéance du terme prononcée postérieurement au jugement d’orientation.
"Au visa de l’article R. 713-4 du code de la consommation, la Cour de cassation casse le jugement d’un tribunal d’instance qui écarte, d’une procédure de surendettement, la créance d’une société faute, pour cette dernière, d’avoir produit les pièces demandées par le juge, alors que cette invitation a été opérée par une lettre simple du greffe et non au moyen d’une lettre recommandée avec demande d’avis de réception."
"À la faveur de sa première interprétation du règlement n°655/2014, la CJUE clarifie la ligne de partage entre les deux cas d’ouverture de la procédure d’obtention d’une ordonnance européenne de saisie conservatoire et précise les notions de ""procédure au fond"" et de ""circonstances exceptionnelles""."
"La deuxième chambre civile de la Cour de cassation précise les conditions dans lesquelles le juge des contentieux de la protection peut prononcer la clôture de la procédure de rétablissement personnel avec liquidation judiciaire pour insuffisance d’actif, lorsque la liquidation judiciaire du patrimoine du débiteur n’a pas été prononcée."
"Il résulte de l’article L. 722-8 du code de la consommation que, pour prononcer la suspension d’une mesure d’expulsion, le juge ne doit prendre en considération que la situation du débiteur."
Il résulte de l’article L. 761-1 du code de la consommation que les causes de déchéance du bénéfice de la procédure de surendettement sont limitativement énumérées par la loi.
"L’objectif de cohérence et de sécurité juridique impose de revenir à la jurisprudence antérieure, confortée par la loi nouvelle n°2016-1691 du 9 décembre 2016 non applicable au présent litige, qui subordonne la validité de la renonciation par un État étranger à l’immunité d’exécution de ses missions diplomatiques à la double condition que cette renonciation soit expresse et spéciale. Est donc abandonnée la doctrine isolée résultant de l’arrêt de la 1re chambre civile du 13 mai 2015."
"Constitue une irrégularité de fond au sens de l’article 117 du code de procédure civile, le non-respect des règles de postulation par un avocat ayant formé une demande en nullité du commandement aux fins de saisie immobilière."
"Suggestion de réforme de l’Acte uniforme OHADA portant organisation des procédures simplifiées de recouvrement et des voies d’exécution du 10 avril 1998 au moyen d’une procédure facilitant l’exécution transnationale des décisions juridictionnelles. Il s’agirait d’uniformiser les règles régissant le contrôle de la régularité internationale desdites décisions et, singulièrement, d’élaborer une procédure de certification des décisions juridictionnelles en tant que « Titres exécutoires africains »."
La régularité de la saisine du juge du tribunal d’instance par la commission de surendettement n’est pas subordonnée à la transmission du dossier comportant l’ensemble des éléments en possession de cette dernière au titre de l’affaire considérée.
"A travers une archéologie remontant aux différentes sources, notamment, grecques, de la représentation, de la mimesis, il apparaît que son appréhension moderne est insuffisante. Le terme français de représentation n’embrasse qu’imparfaitement la polysémie que son sens politique revêt. La représentation, par l’incarnation de la multitude qu’elle constitue en peuple, donne à penser que l’électeur est simultanément auteur de l’action politique, alors même qu’il ne s’agit que d’une illusion. L’étude se propose de dépasser de façon dialectique la contradiction inhérente à la notion de démocratie représentative, qui constitue en soi un oxymore."
"Le FGTI qui, subrogé dans les droits de la victime, peut se prévaloir de l’arrêt rendu, sur intérêts civils, au profit de cette dernière et prononçant des condamnations assorties des intérêts au taux légal, est fondé à recouvrer sur le fondement de ce titre exécutoire les intérêts courus de plein droit, à compter du paiement subrogatoire, sur les indemnités qu’il a versées."
"Par deux arrêts prononcés à cinq mois d’intervalle, la Cour de cassation belge (Cass. (1re ch.) 15 janvier 2016, n°C.14.0566.F) et la Cour de justice de l’Union européenne (C.J.U.E. (3e ch.) 16 juin 2016, Pebros Servizi Srl contre Aston Martin Lagonda Ltd, aff. C-511/14) ont été amenées à interpréter la notion de « créance incontestée », au sens du règlement (CE) n°805/2004 « Titre exécutoire européen » du 21 avril 2004, dans les cas où le débiteur ne reconnait pas expressément la créance. La complexité de l’interprétation tient en partie à la place qu’il convient d’accorder au « droit de l’État membre d’origine », auquel les points b) et c) de l’article 3 du règlement font expressément référence. Or, la Cour de cassation belge et la C.J.U.E. se sont successivement prononcées sur ce point, en sens contraire."
"La motivation des décisions du Conseil constitutionnel fait l’objet, à raison, d’un nombre important de critiques. La confrontation au modèle ibéro-américain de rédaction des décisions de justice constitutionnelle permet de montrer que le juge constitutionnel français respecte l’impératif juridique de justification de ses décisions – malgré un encadrement textuel moins détaillé et un rapport différent aux précédents – mais que, par contre, le bât blesse dans le respect de l’impératif pédagogique d’explication de la décision. Néanmoins, si le style rédactionnel français des décisions constitutionnelles est critiquable, il faut reconnaître au Conseil constitutionnel le mérite de l’efficacité, notamment au regard des contraintes matérielles et temporelles qui sont les siennes."
"La globalisation – associée à la croissance de l’interconnectivité normative – invite, par son ampleur actuelle, à repenser les bases conceptuelles et catégorielles de la science du droit et à adapter les modèles classiques à ces nouvelles données. Ce besoin d’adaptation n’affecte pas toutes les branches du droit avec la même intensité. Le droit constitutionnel ne semble pas, a priori, être influencé de manière déterminante par la globalisation. Néanmoins, malgré les difficultés que rencontrent les juristes pour répondre au tournant global des sciences sociales et à saisir l’impact réel de la globalisation et de ses corollaires, il est possible d’identifier des réactions constitutionnelles à ce phénomène, qu’elles se concrétisent par la révision des textes constitutionnels ou par la jurisprudence des juges chargés de contrôler le respect de ces textes. Il convient en effet de ne pas minorer le rôle du droit constitutionnel dans le cadre de l’évolution globale du phénomène juridique, car ce sont toujours les États qui sont au centre du jeu, même si l’aire de jeu et certaines de ses règles connaissent de profondes mutations. Le présent ouvrage met en exergue la tension existante au sein de nos sociétés entre l’ancrage national de nos règles constitutionnelles et le virage de la globalité"
"La globalisation – associée à la croissance de l’interconnectivité normative – invite, par son ampleur actuelle, à repenser les bases conceptuelles et catégorielles de la science du droit et à adapter les modèles classiques à ces nouvelles données. Ce besoin d’adaptation n’affecte pas toutes les branches du droit avec la même intensité. Le droit constitutionnel ne semble pas, a priori, être influencé de manière déterminante par la globalisation. Néanmoins, malgré les difficultés que rencontrent les juristes pour répondre au tournant global des sciences sociales et à saisir l’impact réel de la globalisation et de ses corollaires, il est possible d’identifier des réactions constitutionnelles à ce phénomène, qu’elles se concrétisent par la révision des textes constitutionnels ou par la jurisprudence des juges chargés de contrôler le respect de ces textes. Il convient en effet de ne pas minorer le rôle du droit constitutionnel dans le cadre de l’évolution globale du phénomène juridique, car ce sont toujours les États qui sont au centre du jeu, même si l’aire de jeu et certaines de ses règles connaissent de profondes mutations. Le présent ouvrage met en exergue la tension existante au sein de nos sociétés entre l’ancrage national de nos règles constitutionnelles et le virage de la globalité"
"Le 1er août 2018 entrera en vigueur – dans 10 États européens, dont la France – la procédure de saisine de la Cour EDH pour avis consultatif, instituée par le protocole additionnel n°16 à la Convention européenne. Cette procédure donne un nouvel élan à la protection européenne des droits de l’homme et renforce le « dialogue des juges » en Europe."
"Com. 10 mars 2021, n° 19-19.590 (F-D) - Com. 10 mars 2021, n° 19-22.791 (F-D)"
"Com. 24 mars 2021, n° 20-13.832 (F-P+B)"
"Com. 5 mai 2021, n° 19-17.736 (FS-P)"
"Com. 16 juin 2021, n° 19-25.151 (F-D)"
"Com. 16 juin 2021, n° 19-17.186 (F-P+B)"
"Com. 7 oct. 2020, n° 19-14.755"
"Com. 23 sept. 2020, n° 19-12.542"
"Com. 7 oct. 2020, n° 19-12.996"
"Note sous Cour de cassation (com.), 17 juin 2020, n° 18-18.321 (F-D)"
"Com. 7 octobre 2020, n° 19-13.560 (F-P+B)"
"Com. 9 décembre 2020, n° 19-16.542 (F-P+B)"
décembre 2019 - décembre 2020
"Note sous Cour de cassation (com.), 17 juin 2020, n° 18-11.737 (F-P+B)"
"Com. 1er juill. 2020, n° 18-25.487 (F-P+B)"
"Com. 1er juill. 2020, n° 18-25.522 (F-P+B)"
Regard franco-allemand sur l'enchevêtrement des discours juridique et politique au prisme de la proportionnalité
"Note sous Cour de cassation (com.), 6 janvier 2021, no 19-19.600 (F-D), Société Olivier"
"L’article 18 TFUE et l’article 47 de la charte des droits fondamentaux de l’Union européenne ne s’opposent pas à une règlementation nationale habilitant les notaires, agissant dans le cadre des compétences qui leur sont dévolues dans les procédures d’exécution forcée sur le fondement d’un document faisant foi, à rendre des ordonnances d’exécution ne pouvant pas être reconnues et exécutées dans un autre État membre."
"Le+a Cour connaît régulièrement des mesures qui relèvent de la compétence législative exclusive de l'Etat s'agissant de ""la protection de la concurrence"" ou de ""la protection de l'environnement"". L'arrêt n°173 du 6 juin 2017 en est une nouvelle illustration dans le contexte sensible de la délimitation de la zone géographique des ATO(Agglomérations Territoriales Optimales) par la Région Ligurie, et plus précisément des conditions d'une telle délimitation."
"La volonté de relance de l'activité des PME en difficulté, mais viables, suppose que les acteurs de la réorganisation de l'entreprise ont les moyens juridiques et judiciaires de leur ambition. Des réformes structurelles peuvent s'avérer nécessaires. Elles viendront en soutien des procédures préventives en les rendant plus performantes et donc plus attractives. On peut considérer que le premier enjeu de la prévention est l'incitation aux réformes. L'adaptation de la gouvernance de l'entreprise ou de la réglementation au numérique, mais aussi la formation des magistrats, en font partie. Il en est d'autres. Plus directement, les enjeux de la prévention sont économiques, financiers et politiques. A l'heure du Brexit, de l'isolationnisme américain, l'application, demain, par les Etats membres, d'une procédure préventive harmonisée, confortera la zone euro en facilitant les échanges commerciaux et la circulation des capitaux. Des règles matérielles communes ne pourront que renforcer la compétitivité des entreprises transnationales. Il s'agit d'attirer les investisseurs, notamment étrangers. L' adoption prochaine de la loi PACTE (plan d'action pour la croissance et la transformation des entreprises) sera l'occasion d'apprécier les avancées réalisées dans ce domaine par la France où de nombreuses réformes sont en cours, concernant le droit des sociétés, et particulièrement la question de l'actionnariat de long terme dont la valorisation est envisagée, mais aussi le droit des sûretés en lien avec le droit des procédures collectives. Un Code européen des affaires est à l'étude. Par-delà les différences observées dans l'appréhension et le traitement de la restructuration des dettes par les Etats, un enrichissement mutuel des droits transparaît. La dimension économique du droit de la faillite est acquise ou se renforce, selon le pays concerné. L'humain et le temps sont déterminants. Ils ne cesseront de l'être."
"L'article 117 de la Constitution répartit les pouvoirs entre l'Etat et les régions de manière plus complexe qu'il n'apparaît à sa seule lecture. Le contentieux est d'ailleurs fréquent. La protection de la concurrence, de la compétence de l'Etat, demande que les contraintes du droit de l'Union européenne et du droit national soient respectées. En l'espèce, les conditions d'accès à l'activité économique de conducteur de véhicules et d'embarcations au service du public, en marge du service de ligne, sont au coeur du débat. La Cour, dans l'arrêt n°152 du 23 mai 2017, écarte le grief de méconnaissance de la Constitution par l'article 12, alinéa 1er, lettre b) de la loi de la Région Molise n°5."
"L' analyse comparative des systèmes juridiques préventifs français, marocain et de l'espace OHADA, révèle, par-delà quelques points communs, leur dissemblance qui est dans l'ordre des choses. La ""culture de la prévention "" est propre à chaque Etat. L'influence du droit francais est relevée. Ainsi, le droit marocain prévoit-il le recours à un mandataire spécial ; il offre au chef d'entreprise le bénéficie de la conciliation et depuis l'année 2017, celui de la sauvegarde. L'espace OHADA connaît le règlement préventif, mais aussi la conciliation depuis l'Acte Uniforme de 2015. En Europe, le droit allemand ignore la procédure de prévention, mais il permet un redressement libre avant insolvabilité. Le droit français, seul à accepter un état de cessation des paiements de 45 jours au plus en phase de traitement préventif, est présenté comme un modèle s'agissant de la prévention-détection et donc des mécanismes d'alerte, ou de la prévention-traitement renvoyant au mandat ad hoc et à la conciliation, laquelle, dans le dernier état législatif, est un encouragement à la déjudiciarisation ou un terrain propice à la contractualisation du traitement des difficultés des entreprises. Les illustrations à cet égard sont nombreuses, telles, à des conditions précises, la conversion de la conciliation en une sauvegarde financière accélérée (SFA) ou en une sauvegarde accélérée (SA), également, l'organisation par le conciliateur d'une cession totale ou partielle de l'entreprise devant intervenir lors de l'ouverture ultérieure d'une procédure collective, ou encore, l'invitation, par le tribunal, à l'adresse du débiteur, à recourir à la conciliation s'il ne peut prouver l'existence de difficultés insurmontables qui président à l'ouverture d'une procédure de sauvegarde. L'harmonisation des droits, mythe ou réalité ? Les deux éléments de réponse conviennent. Un droit peut en inspirer un autre, chacun conservant sa spécificité. C'est ainsi que l'on observe que l'harmonisation des droits en la matière est en cours, ou admise, voire limitée ou illusoire. L'UE a choisi cette voie alors que l'OHADA la porte en fronton. Le Maroc construit son droit dans un esprit d'ouverture. Le droit américain de la faillite est un droit fédéral."
"L’alinéa 1, in fine, de l’article 1242 du Code civil prévoit que l’on est responsable du dommage « causé […] par la fait [...] des choses que l’on a sous sa garde » (art.1384, al.1er, anc.). En présence d’un dommage consécutif au heurt d’une chose inerte, l’exigence de la causalité génératrice ne sera satisfaite que par la preuve faite par la victime de l’anormalité de la chose. A défaut de son établissement, le gardien ne saurait être responsable et l’examen des causes d’exonération est superfétatoire. Tel est le sens de la décision rendue le 13 décembre 2012 par la deuxième chambre civile de la Cour de cassation. En l’espèce, un enfant escaladant un muret pour atteindre la toiture de l’abri de la piscine, car il envisageait d'y plonger, s’est empalé sur une tige de fer utilisée comme tuteur d’un arbuste situé au pied du muret d’où il avait chuté. L’enfant décédera des suites de ses blessures. Les parents vont vainement agir, contre le propriétaire de la piscine, sur le fondement de l'ancien article 1384 alinéa 1 du code civil, l’anormalité de la chose n’ayant pas été établie. La note de jurisprudence comprend deux parties. La première partie rappelle que le fait actif causal d’une chose inerte revêt un sens précis. La causalité est strictement entendue. L’intervention matérielle de la chose est une condition sine qua non du dommage, mais il appartient, en outre, à la victime de prouver qu’elle en a été la cause génératrice et donc adéquate. En l’absence de contact avec le siège du dommage, ou, comme en l’espèce, lorsque le dommage est consécutif au heurt d’une chose inerte, il faut prouver l’anormalité de la chose à l’origine du dommage. La deuxième partie de la note traite de la preuve de l’anormalité. Selon la Cour de cassation, la démonstration doit être faite d’une position anormale de la chose, ou de son état anormal, voire, d’un vice de cette dernière. A défaut, la chose ne pourra être considérée comme ayant été « l’instrument du dommage ». Ce qui fut le cas en l’espèce."
"L’article 117 de la Constitution italienne définit les compétences législatives de l’État et des Régions de manière plus complexe que ne le laisse supposer sa seule lecture. Il a été fait état de cette complexité à l’occasion de la réforme du titre V, partie II, de la Constitution réalisée en 2001. La « protection de la concurrence » et le commerce relèvent, respectivement, de la compétence exclusive de l’État et de celle résiduelle des Régions (art.117, al.2 e.) et al.4, C.). La Cour doit régulièrement se prononcer sur le lien naturel les unissant, tenant à la nature transversale de la « protection de la concurrence ». Les juges constitutionnels invitent l’observateur à prendre la mesure du sens que revêt « la protection de la concurrence ». Le sens n’en est pas figé, pouvant aller de la prise en compte de l’objet de la mesure législative adoptée à l’appréciation de son effet sur la concurrence. Le libre jeu de la concurrence peut être bridé, ou faussé, dès lors que sont méconnues les contraintes du droit de l’Union européenne, ou celles du droit national qui en assurent la transposition. Le législateur communautaire a su s’inspirer des décisions rendues, par exemple, en retenant l’interprétation large des limitations interdites (Direct. services, 2006/123/CE, du 12 novembre 2006, art.14,15 et16). Le commerce est sous la surveillance de l'Etat. La Cour peut décider du type de commerce qui est de la compétence des Régions, mais elle peut admettre que des secteurs qui en relèvent basculent dans le domaine de la compétence de l’État. Ainsi en est-il des horaires d’ouverture des commerces. La précision était attendue, qui a permis de déduire la légitimité constitutionnelle des mesures anti-crise en vigueur (sent. n°299 de 2012). La décision, ci-analysée, en est une illustration."
"Les matières du commerce (compétence des Régions) et de « la protection de la concurrence » (compétence de l’État) sont fréquemment source de contentieux. Ainsi, en est-il à propos d’une modification, par une loi régionale, des modalités d’accès à une activité de service, et à son exercice, ayant pour effet de contrarier les libertés d’établissement et de prestation de services entre les États membres, et, par là-même, la concurrence. L’impact de la crise économique et sociale ne peut par ailleurs être négligé. Les dispositions générales de la directive européenne 2006/123/CE du 12 décembre 2006, relative aux services dans le marché intérieur, sont concernées. Il s’agit de faciliter l’exercice de la libre circulation des services. La Cour de justice s’est clairement prononcée. L’État soutient, à bon droit, qu’il n’entre pas dans la sphère de compétence des Régions de prévoir une limitation supplémentaire à la liberté de prestation de services (Région Sardaigne, arrêt n°18, 2012), ou d’exclure une procédure de « sélection entre plusieurs candidats » favorisant l’exercice de la liberté d’établissement (Région Toscane, arrêt n°291, 2012)."
"Refonte du fascicule consacré à l'Enrichissement sans cause, quasi-contrat prétorien. L'action de in rem verso est subordonnée à la réunion de conditions matérielles (enrichissement du défendeur ; appauvrissement du demandeur ; relation entre enrichissement et appauvrissement) et juridiques (absence de cause ; subsidiarité de l'action), et emporte des effets (obligation de restitution ou d'indemnisation ; point de départ des intérêts moratoires)."
"Si l'élaboration du droit de la restructuration des dettes est liée au contexte culturel, économique et politique d'un pays, il apparaît que son adéquation aux exigences des PME est un point d'ancrage commun à tous les Etats. La dernière version du projet de directive de la Commission européenne en témoigne. L' Afrique en est un exemple. La loi américaine doit évoluer en ce sens. Il peut y avoir une réorganisation des actifs sans le recours à une procédure préventive, à l'image du droit allemand. L' inverse se vérifie plus largement. Appréhender la prévention implique d'en connaître l'origine et les moyens ; la capacité de coordination des intérêts en présence qui s'opposent, sans être fondamentalement divergents, en est un. La possibilité de priver du pouvoir de nuisance les récalcitrants, créanciers et/ou actionnaires, est un acquis ou un objectif à atteindre ; la conclusion d'un accord amiable ou l'adoption d'un plan de sauvegarde en dépendent. C'est lâ l'un des aspects majeurs de l'histoire de la prévention et de l'analyse tirée de son dernier état évolutif, dans une approche comparative et prospective Europe/EU/Afrique. L' entreprise est l'objet de toutes les attentions, elle doit le demeurer. C'est en acceptant de l'oublier en apparence qu'elle sera d'autant mieux protégée. Il est utile que le regard se porte sur les acteurs externes que sont les créanciers, bien souvent titulaires de sûretés, qui aspirent à devenir plus pleinement des partenaires de l'activité économique en soutenant le financement d'un plan. La prise de conscience est faite, ou devra l'être. L' adaptation de la loi est en marche."
"Le colloque du 22 avril 2015, organisé sous la direction scientifique de Anne-Marie Romani, lors des 9es Journées Scientifiques de l'Université de Toulon, a donné lieu à un ouvrage collectif : "" La banque dans tous ses (É)états –Intermédiation et croissance- Regards croisés France, Belgique, Italie, Maroc, Sénégal"". L'ouvrage appréhende l'intermédiation bancaire (de bilan et de marché), dans une approche comparative et prospective, Europe- Afrique, en dehors, et dans le cadre du droit des entreprises en difficulté, y compris dans l'espace OHADA. En réaction à la crise financière de 2008, dite des subprimes (prêts à risques), qui a révélé les limites de la finance, un nouveau modèle économique se construit. Le constat avait pu en être fait, dans les années 1980, lors du passage d’une économie d’endettement à une économie de marché. La crise financière s’est répandue dans le monde entier, elle a vite atteint l’Europe ; l’Afrique n’a pas été concernée, mais le « risque Afrique » existe. L'ouvrage repose sur la prise en compte du contrôle prudentiel -global, européen et national-, et de ses implications, notamment sur l’offre de crédit qui est en mutation progressive ; la liquidité est moindre. Nous vivons à l’heure de Bâle III (2010). Il s’agit dans une économie mondialisée de rendre le système bancaire international résilient et d’éliminer l’effet domino d’une faillite. Des ratios de solvabilité s’imposent aux banques de manière échelonnée jusqu’en 2019. Des moyens sont utilisés afin d’assurer la stabilité de la zone euro et l’intégration du secteur financier européen, ainsi en est-il avec la construction de l’Union bancaire. Notre législation s’est mise en conformité avec les directives européennes en matière économique et financière. De nombreux textes pourraient être cités, tels que l’ordonnance du 20 février 2014 qui achève la transposition de la directive CRD 4 dont certaines dispositions avaient été partiellement anticipées par la loi du 26 juillet 2013 de séparation et de régulation des activités bancaires (la directive CRD 4 et le règlement européen CRR sont la déclinaison européenne des accords internationaux Bâle III). Le Maroc accepte Bâle III, son application en Afrique subsaharienne nécessitera des adaptations. Les résultats des tests de résistance et de qualité de bilan, communiqués le 26 octobre 2014 par la BCE, ont démontré que le système bancaire européen est solide. Il s’agit de relancer l’offre de crédit. Les banques réagissent de manière variable selon les pays. Le taux de bancarisation doit être pris en considération. Plus généralement, la réglementation du secteur peut contrarier l’efficacité du programme d’assouplissement quantitatif (QE) lancé par la BCE. On observe un « retour en grâce » de la titrisation de créances ou de crédits, étant rappelé que l’utilisation qui a été faite de la technique financière est à l’origine de la crise. Il ne s’agirait pas d’externaliser le risque, mais de permettre le financement de programmes d’infrastructures. Le Maroc et l’Union Economique et Monétaire de l’Ouest africain (UEMOA) s’y intéressent. L’Europe également. Les sources de financement non bancaires se développent. La banque demeure un acteur incontournable qui s'impose ou s'adapte au service du financement de l'économie réelle."
"A l'origine de la prévention, il y a l'homme et le besoin de confidentialité. Le chef d'entreprise, le représentant d'une personne morale, confrontés à des difficultés notamment financières, ont toujours souhaité négocier avec leurs principaux créanciers, afin d'éviter un dépôt de bilan, et cela en marge d'un tribunal. Il en est de même aujourd'hui. Des cellules informelles de prévention facilitent l'accès à l'information. La sociologie juridique renvoie à un environnement même lointain qui contribue à la connaissance du droit, il en est ainsi de la construction du droit de la prévention. L'aspect volontariste dans le recours à la prévention et dans le choix d'une procédure d'anticipation, contractuelle, ou judiciaire, demeure. L'évolution législative a permis que prévention et procédure se rejoignent grâce au pouvoir de concilier du juge, depuis la loi du 1er mars 1984, pour un bon usage des négociations contribuant à la conclusion d'un accord amiable. Les atermoiements, les avancées et les critiques sont à l'image de la complexité de la matière. Les réformes à l'étude tendent à la modernisation de l'outil juridique attendue par les investisseurs. Le droit français de la faillite est un atout sur la scène internationale. La loi Africaine s'en inspire dans le respect de la culture des Etats. La Commission européenne encourage l'adoption par les Etats membres de règles matérielles communes. La prévention de demain devrait amplifier le phénomène de déjudiciarisation du traitement des difficultés des entreprises. La conciliation joue un rôle central, appelé à se renforcer. La tradition romano germanique s'oppose au pragmatisme, apprécié, du droit anglo-saxon ou du droit fédéral américain."
"Il s'agit des propos introductifs à l'ouvrage collectif tiré du colloque qui s'est tenu le 22 avril 2015 à l'occasion des 9es Journées Scientifiques de l'Université de Toulon : ""La banque dans tous ses (E)états - Intermédiation et croissance- Regards croisés France, Belgique, Italie, Maroc, Sénégal"". La Banque est-elle un acteur financier incontournable ? Il est précisé que l’ouvrage prend la mesure de cette interrogation, dans un contexte favorable en Europe aux investisseurs -euro faible, baisse du prix de l’énergie, assouplissement quantitatif (QE) de la BCE -, sur fond de régulations financières et bancaires Bâle III. La place et le rôle des banques sont appréhendés dans une optique comparative des législations des cinq pays concernés, voire, plus largement, et sous un angle prospectif. Alors que la structure des banques est à l’étude au niveau européen, l’Afrique doit relever le défi de la bancarisation dont le taux progresse mais demeure faible avec des disparités selon les régions. Le raisonnement est articulé en trois temps. Dans une première partie, après une présentation de l’encadrement du crédit, il est rendu compte de la spécificité des systèmes bancaires, de leurs adaptations, de l’impact de l’environnement économique et social dans l’émergence bénéfique de circuits financiers non bancaires. La finance participative, crowdfunding, s’ouvre à l’Afrique. Une deuxième partie concerne la question du refinancement, via la relance de la titrisation des créances et des crédits. La dernière partie de l’ouvrage est consacrée au recouvrement des créances bancaires, hors, et dans le cadre du droit des entreprises en difficulté, y compris dans l’espace OHADA. L’intermédiation des banques est régulée ; elle est également encouragée par la relance de la titrisation ; elle peut parfois être préservée dans le contexte, amiable ou judiciaire, du règlement des conflits entre la banque et le débiteur. Les banquiers centraux, les politiques de relance des États et les banques sont les moteurs de la croissance. L’ouvrage apporte des éléments de synthèse et ouvre la réflexion autour de l’évolution, dans une économie mondialisée, des droits français, belge, italien et, s’agissant des États africains, pour l’essentiel, des droits marocain et sénégalais, en quête du meilleur business model. Les banques sont des entreprises. Le transfert du crédit aux entreprises et aux ménages est l’objectif à atteindre en créant ou en améliorant les conditions de sa réalisation au service des investissements et de la croissance. Le banquier, dont le métier évolue, s’adapte."
"L'endettement est une source de financement de l'activité économique. Le crédit aux entreprises en difficulté doit être encouragé et, pour ce faire, les créanciers, investisseurs potentiels, doivent être protégés. A cet égard, le droit fédéral américain invite à la réflexion en raison des dispositions qu'il comporte et qui sont autant de pistes à suivre pour atteindre cet objectif. La Commission européenne demande aux Etats membres d'adopter une procédure de restructuration préventive unique qui saura valoriser l''approche économique du droit des procédures collectives. La France fait figure de modèle, compte tenu des moyens de prévention qu'elle offre, lesquels, après certains ajustements, sauront demain répondre plus efficacement à l'exigence de pragmatisme qui prévaut outre-Atlantique ou encore outre-Rhin. Il est vrai que le code de commerce, depuis la loi Macron du 6 août 2015, s'est enrichi d'un article L.631-19-2 qui permet la reprise interne grâce à la dilution forcée ou à la cession forcée des droits sociaux. Certes, ce réaménagement des droits entre actionnaires et créanciers ne vaut que dans le contexte d'un plan de redressement et sous réserve du respect de conditions contraignantes qui en limitent l'application. Le message délivré est néanmoins sans ambiguïté, d'autant plus qu'il est l'expression d'une volonté législative arrivée à maturation. En effet, une mesure comparable, auorisant l'expropriation des actionnaires, prévue dans l'avant-projet de réforme, qui donnera lieu à l'ordonnance du 12 mars 2014, n'avait finalement pas été retenue dans le texte définitif. Le raisonnement doit être conduit en des termes différents s'agissant du droit tunisien, ou du droit OHADA, où la protection du débiteur l'emporte sur celle des créanciers, dans le but d'assurer la sauvegarde de l'emploi. L'évolution attendue sera riche d'enseignement."
"L’ordonnance n°2016-131 du 10 février 2016, portant réforme du droit des contrats, du régime général et de la preuve des obligations (J0 11 février, texte n°26), reconnaît l’enrichissement sans cause – quasi-contrat prétorien- devenu l’enrichissement injustifié (C. civ. art.1303 à 1303-4). Y recourir doit permettre de compenser le transfert de valeur d’un patrimoine à l’autre par l’octroi d’une indemnité à l’appauvri à la charge de l’enrichi (V. Rapport au président de la République relatif à l’ordonnance précitée, texte n°25). Le concept de cause est délaissé. Dans la partie « Généralités » du fascicule, il est observé que l’enrichissement injustifié prend place dans le nouveau droit des quasi-contrats. La prudence des juges dans la construction du quasi-contrat se retrouve sous la plume du législateur. La sécurité juridique doit être préservée. La subsidiarité de l’enrichissement injustifié se vérifie par rapport aux quasi-contrats que sont la répétition de l’indu et à la gestion d’affaires, le caractère subsidiaire de l’action étant par ailleurs consacré. La réforme s’inscrit dans un ligne jurisprudentielle éprouvée ; elle semble parfois être en retrait (l’évolution jurisprudentielle n’est pas nécessairement reprise, ainsi les causes de l’enrichissement ne sont -elles pas explicitées dans l’article 1303-1), et innove sur certains points (la faute de l’appauvri, qui ne saurait désormais priver ce dernier du droit d’agir, permet au juge de modérer ou de supprimer l’indemnité – la dette de valeur est consacrée – la mauvaise foi de l’enrichi est sanctionnée). Deux Chapitres structurent la rubrique. Le premier est consacré à l’examen des conditions d’ordre matériel ou économique et d’ordre juridique dont la réunion garantit le succès de l’action. Le Chapitre II traite de l’indemnisation de l’appauvri. L’indemnité doit être déterminée et évaluée."
"L’impôt sur le revenu, qui frappe le revenu annuel net global d’un foyer fiscal, quelle que soit la source de ce revenu, selon des modalités prenant en considération la situation propre de ce foyer fiscal, n’est pas une dette professionnelle, mais personnelle."
"L’étude du constitutionnalisme dans l’ordre juridique de la Communauté Économique des États de l’Afrique de l’Ouest (CEDEAO), sous le prisme de la protection des droits fondamentaux, paraît particulièrement intéressante tant l’organisation Ouest-africaine a connu une profonde mutation. Au départ économique, la CEDEAO a transcendé sa dimension initiale pour atteindre la supranationalité, seul moyen à l’efficacité avérée qui lui permettra à la fois de se saisir de son ambition communautaire et de ne pas manquer le rendez-vous de la mondialisation. En témoigne la constitutionnalisation sans cesse grandissante de l’ordre juridique communautaire par une méthode prétorienne de protection des droits fondamentaux qui a permis à la Cour de justice de la CEDEAO d’asseoir son autonomie. Pour autant, le constitutionnalisme ne semble pas pénétrer définitivement l’ordre juridique Ouest-africain qui n’est qu’à son stade embryonnaire. Mais, devant les exigences d’un renouveau démocratique africain, il a fallu se tourner résolument vers la création d’un environnement juridique et politique propice à la réalisation du projet d’intégration africaine. Pour mieux définir la conviction communautaire et consacrer définitivement le renouveau du régionalisme, les États membres ont dû abandonner leur ambition théorique irraisonnée, calqué sur le développementalisme, pour garantir au processus d’intégration, les éléments indispensables à la construction de son « identité », notamment son « identité constitutionnelle ». En ce sens, l’évolution normative de la CEDEAO, d’abord initiée par le traité révisé, ensuite par le Protocole sur la démocratie et la bonne gouvernance et enfin le Protocole d’Accra relatif à la Cour de justice, a permis de déterminer le cadre constitutionnel de la Communauté. Ce sont ces évolutions fondatrices de l’ordre juridique communautaire qui ont permis à la fois la juridicisation des droits fondamentaux et l’affirmation d’un constitutionnalisme Ouest-africain. Ces principes de convergence constitutionnelle permettent ainsi de répondre au défi politique et sécuritaire, clef de voûte de la construction d’un espace public communautaire : l’espace CEDEAO."
"La création d’un divorce par consentement mutuel par acte sous signature privée contresigné par avocats, déposé au rang des minutes d’un notaire – plus communément dénommé le « divorce sans juge » – constitue l’un des importants apports de la loi n°2016-1547 du 18 novembre 2016 de modernisation de la justice du 21e siècle. Or, cette loi méconnait les conséquences internationales de ce divorce, ce qui, en droit positif, engendre une situation très inconfortable non seulement pour les personnes souhaitant divorcer, mais également pour les praticiens du droit – avocats et notaires – qu’elles vont solliciter. La situation devrait prochainement quelque peu s’améliorer, notamment à la faveur d’une évolution du droit de l’Union européenne matérialisée par une refonte du règlement « Bruxelles II bis ». Ainsi, la « relation » entre le divorce par consentement mutuel extrajudiciaire et le droit international privé, aujourd’hui contrariée, pourrait s’apaiser dans les années à venir."
"Le premier président de la cour d’appel peut ordonner le sursis à l’exécution de toutes les décisions du juge de l’exécution, à l’exception de celles qui, dans les rapports entre créanciers et débiteurs, statuent sur des demandes dépourvues d’effet suspensif à moins qu’elles n’ordonnent la mainlevée d’une mesure."
"Com. 20 octobre 2021, no 20-17.765 (P-B)"
"Com. 29 septembre 2021, no 20-10.436 (P+B)"
"Présentation de l'éditeur : ""L'année 2020 a marqué le quarantième anniversaire de ce conflit assez méconnu opposant l'Irak à l'Iran et ayant eu d'importantes répercussions en Europe, surtout en France. Cette dernière soutenant l'Irak discrètement et militairement depuis le milieu des années 1970, l'Iran, pour se défendre et faire infléchir la position française, organisera des actes de représailles sur les intérêts français, aussi bien à l'étranger que sur son sol. Entre 1985 et 1986, une dizaine d'attentats seront commis sur le territoire français. Ces actes de terreur obligeront les pouvoirs publics à renforcer l'arsenal législatif dans la lutte contre le terrorisme. Ce conflit bouleversera également la scène politique française par la révélation de scandales politiques, financiers et diplomatiques au travers des affaires Luchaire et Gordj"
"Dès l’instant où l’obligation assortie d’une astreinte a été exécutée, fût-ce par un tiers, l’astreinte ne peut plus donner lieu à liquidation pour la période de temps postérieure à cette exécution, sauf si le créancier justifie d’un intérêt légitime à ce qu’elle soit exécutée par le débiteur lui-même."
"Dans un arrêt du 20 mai 2021, la Cour de cassation rappelle l’interdiction faite au JEX de remettre en cause un titre exécutoire constitué par une décision de justice ainsi que les conditions de mise à exécution d’un tel titre, parmi lesquelles figure la présentation d’une expédition revêtue de la formule exécutoire."
"Quel autre thème que celui des droits sociaux fondamentaux pouvait permettre les réflexions croisées de constitutionnalistes, de comparatistes et d'européanistes, à l'heure où les débats suscités par les processus nationaux de ratification du projet de Constitution pour l'Europe, et plus spécifiquement par la vocation de celui-ci à promouvoir une « Europe sociale », tenaient le devant de la scène ? Partant de la notion de « valeurs constitutionnelles communes » associée dans le projet de traité aux droits sociaux fondamentaux, le présent ouvrage collectif entend dresser un panorama, sinon exhaustif, du moins très large du traitement réservé à ces droits dans les ordres juridiques des États européens. Ainsi, dans un premier temps, est-il procédé à un état des lieux de la protection constitutionnelle des droits sociaux fondamentaux dans les « États de la vieille Europe » (Allemagne, France,Italie, Espagne), ainsi que dans les nouveaux États membres. Leur situation est mise en parallèle avec celle des pays de common law où la protection constitutionnelle de ces droits rencontre toujours des résistances, dont il convient de cerner les raisons comme les implications. Une analyse de la protection des droits sociaux fondamentaux est effectuée, dans un second temps, en droit européen, communautaire et non communautaire. Il est souvent affirmé que les derniers traités européens ont procédé à leur consécration et que la Cour de justice des Communautés européennes est l'instigatrice de cette protection. Qu'en est-il en réalité ? Les droits sociaux peuvent-ils prétendre à la fondamentalité dans une intégration économique telle que l'Union européenne ?Au terme de cet ouvrage, dont le but est d'identifier les interactions entre les niveaux et les mécanismes pluriels de protection des droits sociaux fondamentaux ainsi positionnés entre droits nationaux et droit européen, la réalité de cette notion de « valeurs constitutionnelles communes » peut ainsi recevoir un nouvel éclairage."
"La définition du droit des étrangers dépend aujourd’hui à la fois des droits interne, européen et communautaire. L’interpénétration croissante de ces différentes sources juridiques rend ainsi difficilement intelligible le droit des étrangers qui voit, au surplus, sa technicité se complexifier à outrance. Cet écheveau normatif place alors les juges nationaux, européen et communautaire dans une position délicate, puisque ces juges sont amenés, par l’interprétation des divers textes qui réglementent la situation juridique des étrangers et qui garantissent leurs droits face à la puissance publique, à en démêler eux-mêmes les fils en recherchant, en permanence, l’équilibre difficile entre la logique de l’ordre public et la logique des droits. C’est précisément pour contribuer à mesurer l’impact de cette imbrication des sources et des interactions des jurisprudences sur l’évolution du droit des étrangers que le présent ouvrage a été conçu. Constitutionnalistes, administrativistes, européanistes et communautaristes de l’UMR 6201 du CNRS se sont ainsi attelés à cette difficile tâche en croisant leurs regards sur cet étranger à la fois sujet du droit et sujet de droits. L’approche pluridisciplinaire, alliée à la méthode comparatiste et à la vision historique, permet ainsi de sortir quelque peu des sentiers battus en offrant un éclairage qui, sans être exhaustif, permet de comprendre les lignes de force des évolutions actuelles de la situation juridique des étrangers."
"Dans un important arrêt du 10 juillet 2020, sans dissiper toutes les incertitudes, l’Assemblée plénière de la Cour de cassation apporte d’utiles précisions sur la nature et les conséquences juridiques d’une mesure de gel des avoirs d’un établissement bancaire adoptée par le Conseil de sécurité de l’ONU – en réaction à la poursuite du programme nucléaire iranien – et transposée en droit de l’Union européenne. Cet arrêt donne l’occasion à la Cour de cassation de souligner l’importance du critère d’extériorité dans la notion de force majeure et d’appréhender la délicate problématique de l’exercice des procédures civiles d’exécution sur des avoirs gelés."
"La légalité de l’emploi de la force en droit international est commandée par le principe de l’article 2§4 de la Charte des Nations Unies et les exceptions qu’il admet. Les Etats utilisent toutefois de plus en plus souvent l’argument de la légitimité pour justifier leurs interventions militaires. Cela apparaît notamment si l’on considère le cas de la France, qu’elle intervienne sur autorisation du Conseil de sécurité ou avec le consentement d’un Etat"
"En 1963, Charles de Gaulle et Konrad Adenauer avaient promis, aux dirigeants turcs de l’époque, de soutenir la candidature de la Turquie à la Communauté économique européenne. Renouvelée régulièrement, cette promesse, n’a cessé d’interférer dans le débat sur la construction de l’Europe. En effet, l’adhésion de la Turquie à l’Union ne semble pas pouvoir être envisagée sans une profonde redéfinition du projet européen. Aux yeux des opposants à cette adhésion, ce pays est totalement étranger à la civilisation européenne. Accepter sa présence au sein des institutions européennes reviendrait à renoncer à l’identité culturelle de l’Union. Or, le thème de « l’identité culturelle » est précisément l’un des arguments avancés par les souverainistes pour remettre en cause la construction de l’Europe. Ce serait leur faciliter la tâche que d’accepter un pays musulman comme membre de l’Union. Le fait que la constitution turque consacre solennellement le caractère laïque de l’Etat n’a guère tempéré les critiques ni calmé les esprits. L’Union européenne devait rester une organisation chrétienne, il en allait de sa survie. Le christianisme devenait l’élément essentiel de l’identité européenne et, partant, l’une des conditions pour une éventuelle adhésion. Certains demandèrent même que les « origines chrétienne de l’Europe » soient expressément rappelées par les traités constitutifs . Depuis 2006, le processus d’adhésion de la Turquie, connaît un net ralentissement. De part et d’autre, l’enthousiasme a laissé place aux crispations et à la défiance. La dérive autoritaire du président Erdogan et les ambiguïtés de sa politique étrangère sont vivement critiquées par les Européens. L’opposition à l’entrée de la Turquie dans l’Union se renforce et provoque, en retour, une forte réaction d’hostilité d’Ankara envers l’Union dénoncée comme un « club chrétien ». L’antagonisme séculaire entre l’Orient musulman et l’Occident chrétien est, de nouveau, au cœur du débat européen."
"Dans un arrêt du 15 avril 2021, la deuxième chambre civile de la Cour de cassation se prononce sur la recevabilité de l’appel contre un jugement d’orientation ordonnant la vente forcée d’un immeuble, dans la circonstance où une seconde déclaration d’appel a été formée pour appeler à la cause des créanciers inscrits omis dans la première déclaration d’appel."
"Texte remanié de : Th. doct. : Droit : Aix-Marseille : 2003. Titre de soutenance : L' influence des idées des Lumières françaises sur les juristes et publicistes lombards au XVIIIe siècle, 1740-1790"
"Textes issus de colloques annuels organisés par l'Université de Rome-La Sapienza, 2003-2006. Présentation de l'éditeur : ""La Révolution romaine commence en 1846 avec la fermentation politique qui suivit les première mesures du pape Pie IX, et prend fin en juin 1849 avec la prise de Rome par l'armée envoyée au secours du pape par le futur Napoléon III, encore Président de la IIe République française. Cet ouvrage s'articule autour de trois problématiques : la République romaine et ses relations avec la France et le droit romain, les relations entre la République romaine et les républicains français, la question des relations entre la République romaine de 1849, la France et l'Église."
"La deuxième chambre civile de la Cour de cassation se prononce sur les modalités d’application du délai d’attente de deux mois qui suit la délivrance du commandement d’avoir à quitter les lieux, prévu par l’article L. 412-1 du code des procédures civiles d’exécution, lorsque l’expulsion porte sur un lieu habité par la personne expulsée ou par tout occupant de son chef."
"En cas d’inexécution par le débiteur des mesures recommandées homologuées, le créancier ne recouvre le droit de pratiquer des mesures d’exécution que dans le cas où il est mis fin au plan soit par une décision du juge statuant en matière de surendettement soit par l’effet d’une clause résolutoire prévue par ces mesures ou par l’ordonnance les homologuant."
"En cas d’impossibilité pour une juridiction de se procurer l’adresse du défendeur, le règlement (CE) n°805/2004 du Parlement européen et du Conseil du 21 avril 2004 ne permet pas de certifier en tant que titre exécutoire européen une décision judiciaire relative à une créance, rendue à la suite d’une audience à laquelle n’ont comparu ni le défendeur ni le tuteur désigné pour les besoins de la procédure."
"Il résulte de l’article L. 331-1 du code des procédures civiles d’exécution que le créancier chirographaire, qui n’est pas une partie à la procédure de distribution, n’a pas qualité à contester le projet de distribution du prix de vente."
"La loi n°2019-222 du 23 mars 2019 de programmation 2018-2022 et de réforme pour la justice comporte plusieurs dispositions disparates relatives aux procédures civiles d’exécution, dont les dates d’entrée en vigueur sont échelonnées dans le temps. En ce domaine, elle opère notamment un important transfert de compétence juridictionnelle au profit du juge de l’exécution concernant la saisie des rémunérations et offre de nouvelles illustrations des tendances plus générales de dématérialisation et de déjudiciarisation partielle des procédures."
"Le jugement d’adjudication ne statuant sur aucune contestation, n’est susceptible d’aucun recours sauf excès de pouvoir."
"(CJUE 27 juin 2019, aff. C-518/18, RD c/ SC, D. 2019. 1399) - Observations sous Cour de justice de l'Union européenne, 27 juin 2019, n° C-518/18"
"L’autorité compétente d’un État membre peut valablement refuser l’exécution d’une demande de recouvrement portant sur une créance afférente à une sanction pécuniaire infligée dans un autre État membre, au motif que la décision ordonnant cette sanction n’a pas été préalablement notifiée à l’intéressé, en application de la directive 2010/24/UE."
"(CJUE 10 juill. 2019, aff. C-722/17, Norbert Reitbauer et a. c/ Enrico Casamassima, D. 2019. 1455) - Observations sous Cour de justice de l'Union européenne, 10 juillet 2019, n° C-722/17"
"À l’occasion de sa 22e session diplomatique, la Conférence de La Haye de droit international privé a adopté – le 2 juillet 2019 – la Convention sur la reconnaissance et l’exécution des jugements étrangers en matière civile ou commerciale. Bien que le mécanisme mis en place apparaisse assez complexe, cette Convention est de nature à offrir une plus grande prévisibilité à la circulation transfrontière des jugements et des transactions judiciaires dans un contexte mondial ou européen post-Brexit. La concernant, deux traits caractéristiques peuvent être mis en lumière, à savoir : les importantes limites de son domaine d’application et le caractère semi-uniforme de la procédure de reconnaissance ou d’exécution qu’elle régit."
"L’article 1er in fine du protocole n°7 sur les privilèges et immunités de l’Union européenne doit être interprété en ce sens que l’autorisation préalable de la CJUE n’est pas nécessaire lorsqu’un tiers engage une procédure de saisie-arrêt d’une créance auprès d’un organisme relevant d’un État membre et ayant une dette correspondante envers le débiteur du tiers, bénéficiaire de fonds octroyés aux fins de l’exécution de projets cofinancés par le Fonds social européen."
Chapitre concernant l'exécution des décisions de justice dans les pays membres du Conseil de l'Europe (32 p.)
"Le jugement d’orientation prononcé en matière de saisie immobilière a l’autorité de la chose jugée quant à l’existence et au montant de la créance du créancier poursuivant, même si ces éléments n’ont pas été contestés devant le juge de l’exécution. Par ailleurs, l’instance engagée par la saisine de ce juge ne s’éteint pas avec ce jugement, mais avec l’ordonnance d’homologation du projet de répartition du prix de vente de l’immeuble."
"En l’absence de disposition imposant un mode de preuve spécifique, la preuve de l’expédition d’une lettre recommandée avec demande d’avis de réception, au moyen de laquelle la contestation de la saisie-attribution est dénoncée à l’huissier de justice instrumentaire, ne résulte pas exclusivement de la production d’un récépissé délivré à l’expéditeur par les services postaux."
"La Conférence de La Haye de droit international privé (HCCH) est une organisation, créée il y a 125 ans, sous l’égide de laquelle sont adoptés des conventions et protocoles internationaux. On en dénombre 40, à ce jour. Le dernier instrument en date est l’importante convention du 2 juillet 2019 sur la reconnaissance et l’exécution des jugements étrangers en matière civile ou commerciale. Le présent recueil permet de réunir l’ensemble de ces textes, en les enrichissant de très nombreuses références bibliographiques et de plus de 600 références jurisprudentielles provenant de juridictions belges, françaises, luxembourgeoises et suisses. Sont également incluses des références faites aux arrêts de la Cour de justice de l’Union européenne et de la Cour européenne des droits de l’homme, lesquelles peuvent être amenées à intégrer les conventions de La Haye dans leur raisonnement. Enfin, ce recueil contient des informations sur les Parties contractantes aux différentes conventions et sur les éventuelles réserves par elles formulées ainsi que des précisions sur les Autorités centrales désignées pour la bonne application de ces instruments."
"Le Fonds Européen de défense (FEDef) laisse une place prédominante pour les Etats membres de l’UE. Le triangle de Weimar permet d’offrir une photographie des grandes tendances au sein de l’UE en la matière. Le FEDef s’avère être bien reçu au sein des trois pays démontrant sa capacité à répondre aux enjeux malgré la persistance de divergences liées aux intérêts nationaux. En outre, en tant que Fonds destiné à l’économie de défense, le Fonds doit faire face aux différences économiques entre les Etats. Il apparait néanmoins que ces difficultés n’ont pas empêché l’élaboration de certains projets en communs."
"Cette chronique est organisée autour de plusieurs thématiques : Consommateurs, Éducation, Formation professionnelle, Jeunesse, Sport. La période étudiée concerne les sessions 2016 à 2019."
"Présentation de l'éditeur : « La Méditerranée, ce sont des routes », aimait à dire Lucien Febvre. Manière d’acter que, pour cette « Mer au milieu des terres » à la confluence entre trois continents, l’élément liquide n’a rien d’un obstacle aux communications et tout d’un Pont. Ce sont les conditions et les modalités de l’échange dans cet espace à la fois zone de convergence et de frictions qu’examinent les contributions ici réunies en suivant le commerce, belliqueux ou pacifique, des hommes, des esprits, des marchandises sur la longue durée d’une histoire partagée. Une histoire faite de flux, qu’atteste en diachronie la vitalité de circuits d’échanges éprouvés au fondement de la thalassocratie, mais aussi de reflux. Quand l’actualité récente des mobilités indique que les biens sont mieux accueillis que les personnes, remettant en question l’héritage du Mare nostrum."
"CJUE 9 sept. 2015, aff. C-4/14, Bohez c/ Wiertz (Mme), D. 2015. 1846 ; ibid. 2016. 1045, obs. H. Gaudemet-Tallon et F. Jault-Seseke ; AJ fam. 2016. 46, obs. A. Cassagnes"
"(CJUE, 3e ch., 4 juin 2020, aff. C-41/19, FX c/ GZ, D. actu. 18 juin 2020, obs. G. Payan ; D. 2020. 1827, note D. Foussard ; AJ fam. 2020. 530, obs. A. Boiché)"
"L’irrégularité affectant l’acte dépourvu du sceau du notaire ne relève pas des défauts de forme que l’article 1318, devenu 1370, du code civil sanctionne par la perte du caractère authentique et partant, exécutoire, de cet acte, lesquels s’entendent de l’inobservation des formalités requises pour l’authentification par l’article 41 du décret du 26 novembre 1971."
"Il résulte de l’article R. 512-1 du code des procédures civiles d’exécution que si les conditions prévues pour pratiquer une mesure conservatoire ne sont pas réunies, le juge peut en ordonner la mainlevée à tout moment."
"La CJUE se prononce dans le sens d’une restriction de l’office de la juridiction sollicitée pour délivrer un certificat au titre de l’article 53 du règlement Bruxelles I bis. Ainsi, cette juridiction ne peut pas vérifier d’office le respect des règles de compétence applicables en matière de contrats conclus par des consommateurs, définies dans la section 4 du chapitre II dudit règlement."
"Le langage en tant que Tiers organisant la représentation du Monde est au centre des questionnements relatifs au sens et à la pensée. Les Sciences de l'information dans le cadre d'une approche sémiotique sont susceptibles d'apporter un regard pertinent sur des questions généralement réservées aux sciences du langage. Il s'agit de mettre en perspective les dimensions historiques, anthropologiques, culturelles, politiques et même ésotériques de ce médium en considérant que celles-ci ne sont pas neutres dans la production du sens. L'hébreu, langue ressuscitée constitue notre terrain d'étude."
"Présentation de l'éditeur : ""Cet ouvrage couvre l'ensemble du programme d'histoire des idées politiques depuis l'Antiquité jusqu'à nos jours : époques antique, médiévale, moderne et contemporaine. Il ne divise pas le programme, comme cela est généralement fait, entre avant et après le XVIIIe siècle, mais propose une analyse générale de l'évolution de la philosophie politique et juridique avec ses ruptures et ses continuités. Ce manuel s'adresse aux étudiants des facultés de droit (licence 3, master 1) et des Instituts d'études politiques, ainsi qu'aux personnes qui préparent les concours administratifs. Il est constitué de fiches thématiques permettant d'appréhender rapidement et précisément les enjeux importants. Chaque fiche offre : • les repères essentiels ; • des explications précises sur les thèmes fondamentaux ; • des références bibliographiques pour approfondir une question."
"Présentation de l'éditeur : ""Cet ouvrage couvre l'ensemble du programme d'histoire des idées politiques depuis l'Antiquité jusqu'à nos jours : époques antique, médiévale, moderne et contemporaine. Il ne divise pas le programme, comme cela est généralement fait, entre avant et après le XVIIIe siècle, mais propose une analyse générale de l'évolution de la philosophie politique et juridique avec ses ruptures et ses continuités. Ce manuel s'adresse aux étudiants des facultés de droit (licence 3, master 1) et des Instituts d'études politiques, ainsi qu'aux personnes qui préparent les concours administratifs. Il est constitué de fiches thématiques permettant d'appréhender rapidement et précisément les enjeux importants. Chaque fiche offre : • les repères essentiels ; • des explications précises sur les thèmes fondamentaux ; • des références bibliographiques pour approfondir une question."
"C’est dans l’exercice de son pouvoir souverain d’appréciation de la valeur et de la portée des éléments de fait et de preuve produits qu’une cour d’appel, qui n’est pas tenue de suivre les parties dans le détail de leur argumentation, a pu déduire que le débiteur s’est heurté à des difficultés d’exécution, tenant au comportement du créancier, constituant une cause étrangère au sens de l’article L. 131-4, al. 3, du code des procédures civiles d’exécution."
"Dans un arrêt du 7 juillet 2021, la 1re chambre civile de la Cour de cassation casse – au visa des articles 30, paragraphe 1, de la convention de Vienne sur les relations diplomatiques du 18 avril 1961 et L. 111-1-2 du c. pr. exéc. – un arrêt d’appel ayant ordonné la vente forcée de l’immeuble constituant le lieu de résidence de l’ambassadeur de la République démocratique du Congo."
"Dans un arrêt du 1er juillet 2021, la deuxième chambre civile de la Cour de cassation se prononce notamment sur les règles applicables en matière de prescription d’une action en liquidation d’une astreinte, en clarifiant la nature juridique d’une condamnation assortie d’une telle mesure."
"La Constitution italienne de 1947 ne reconnaît pas l'environnement dans son catalogue de droits fondamentaux. Malgré cette absence l'environnement est aujourd'hui constitutionnellement garanti en tant que valeur constitutionnelle primaire et absolue.Cette évolution est le fruit de la politique jurisprudentielle de la Cour constitutionnelle italienne. Cette étude se propose d'apporter une réflexion sur la prise en compte de l'environnement au niveau constitutionnel, par l'évocation de nombreuses décisions rendues par la Cour en ce domaine. Elle s'efforce de mettre l'accent sur la difficulté de cette reconnaissance qui réside essentiellement dans la nature même de l'environnement. En effet, la propension de l'environnement à interférer dans toutes les branches du droit pose le problème de son appréhension en tant que véritable discipline juridique autonome. L'analyse de la jurisprudence de la Cour constitutionnelle montre que c'est par le jeu d'une interprétation évolutive, particulièrement audacieuse, que le juge est parvenu à intégrer la question environnementale au sein de l'ordonnancement italien. ""Prix de Thèse Mention"" du Centre Français de Droit comparé (2003-2004)"
"Un organisme public qui poursuit, par la voie d’une action récursoire, le recouvrement de sommes versées à titre d’aliments à un créancier d’aliments, dans les droits duquel il est subrogé à l’égard du débiteur d’aliments, est fondé à se prévaloir de la compétence de la juridiction du lieu de la résidence habituelle dudit créancier, prévue à l’article 3, sous b), du règlement n° 4/2009 (CE) du Conseil, du 18 décembre 2008, relatif à la compétence, la loi applicable, la reconnaissance et l’exécution des décisions et la coopération en matière d’obligations alimentaires."
"Dans un arrêt du 4 février 2021, la deuxième chambre civile de la Cour de cassation revient utilement sur l’étendue des obligations incombant aux tiers entre les mains desquels est pratiquée une saisie conservatoire ainsi que sur les sanctions encourues en cas de manquement et sur les causes exonératoires pouvant, le cas échéant, être invoquées."
"Pour l'entrée en vigueur de la question prioritaire de constitutionnalité, le Conseil constitutionnel a adopté un règlement de procédure. Celui-ci fixe des règles de transparence, en prévoyant notamment une audience publique. Toutefois, lu en creux, ce texte de quatorze articles démontre la volonté du Conseil de conserver des marges de souplesse."
"A l’heure où le retour de djihadistes français fait débat et alors que la première évaluation de la loi « SILT » (sécurité intérieure et lutte contre le terrorisme) du 30 octobre 2017 offre un bilan contrasté, l’identification et l’évaluation des réponses juridiques structurelles dans la lutte contre le terrorisme demeurent essentielles. Si ces réponses supposent la mise en place d’un arsenal répressif dissuasif, ce dernier – jugé parfois contre-productif tant il est vrai que les terroristes rêvent de mourir en martyr – ne peut en aucun cas suffire. L’anticipation, la prévention, l’éducation, la déradicalisation sont autant de pistes de réflexion qu’il convient d’explorer pour tenter, à long terme, de combattre cette forme particulière de criminalité qui n’est plus systématiquement organisée."
"Compagnoni, premier titulaire historique d’une chaire de droit constitutionnel à la fin du XVIIIe siècle, s’inspira grandement de Rousseau pour écrire ses Elementi di diritto costituzionale democratico. Contrairement aux critiques qui lui ont été adressées, il n’était pas un vulgaire copiste, tant il s’efforça d’adapter, en juriste, l’œuvre du philosophe genevois. L’ouvrage précurseur de l’Italien commence par décrire et justifier les droits de l’Homme, dans le sillage du Discours sur l’origine et les fondements de l’inégalité parmi les hommes. Il distingue cependant les droits parfaits des droits imparfaits, les droits de sécurité et de secours, en ce que ces derniers, à l’inverse des premiers ne puissent justifier de l’usage de la force car mettant l’homme en relation avec ses pairs. Or, c’est en dernière instance de la possibilité d’exercer ces droits que naît le pacte social. De la distinction originale que Rousseau fait entre formes de l’Etat et forme de Gouvernement, entendue comme exercice du pouvoir exécutif, Compagnoni s’inspire pour ériger, aux côtés des démocratie, aristocratie, monarchie, une nouvelle forme, mixte, affectant l’organisation du pouvoir législatif, la démocratie représentative, opérant une distinction entre énonciation, opérée par les représentants, et confection de la loi, détenue elle par le seul souverain. C’est pourtant un dévoiement de l’œuvre rousseauiste, tant l’inaliénabilité de la souveraineté en est le cœur."
"S’interroger sur l’influence de la doctrine sur la jurisprudence du Conseil constitutionnel pourrait sembler une entreprise empreinte de narcissisme. Après tout, quelle importance cela peut-il avoir que les arguments de la communauté scientifique aient un impact sur les décisions du Conseil constitutionnel, à partir du moment où cet organe promeut efficacement les droits et libertés du citoyen et garantit le respect des règles constitutionnelles ? Quel intérêt y a-t-il à déceler des traces d’une influence doctrinale sur la jurisprudence du Conseil constitutionnel si ce n’est celui de satisfaire un besoin, tout à fait compréhensible de reconnaissance ?"
Le législateur italien souhaite imposer les utilisateurs de cigarettes électroniques mais son texte a été censuré par la Cour constitutionnelle.
"Présentant une version enrichie de la Journée décentralisée de l’AFDC organisée à Toulon par le CDPC Jean-Claude Escarras et l’ILF-GERJC, en novembre 2016, cet ouvrage explore l’impact réel de la « révolution numérique » qui modifie, chaque jour un peu plus, nos modes de vie. Cette révolution technologique a une incidence sur la vie démocratique et sur le fonctionnement de nos institutions. Certains assurent déjà qu’internet garantit un meilleur pluralisme de l’information ou expliquent que les blogs, forums et autres réseaux sociaux sont les nouveaux lieux des débats de société. D’autres mettent en avant que ces mêmes réseaux sociaux offrent les moyens aux citoyens d’exercer une sorte de contre-pouvoir. L’Estonie, ou plus récemment la France, ont donné la possibilité aux citoyens de participer à l’écriture de leur Constitution nationale ou au processus législatif via internet. Internet semble ainsi offrir de nouveaux outils à la démocratie. Cependant, cette démocratie connectée n’est-elle pas qu’une illusion ? Les représentants sont-ils vraiment plus accessibles ? Les citoyens se sentent-ils réellement plus impliqués ? Peut-on réellement espérer un renouvellement de la vie démocratique grâce au numérique ? Des risques de dérives semblent poindre. Dès lors, peut-on les éviter ou du moins les anticiper pour mieux les contenir ? L’ensemble de ces questions est abordé tout au long de la journée d’étude dont cet ouvrage est issu."
"Cette chronique est organisée autour de plusieurs thématiques : Consommateurs, Éducation, Formation professionnelle, Jeunesse, Sport. La période étudiée concerne les sessions 2016 à 2019 de la VIIIème législature du Parlement européen."
"Cette communication est relative à « l’impensé constitutionnel », voire « l’impensable » sécessionniste, vocable en soi péjoratif. Structurellement, la défense de la Constitution, qu’elle soit assurée par ses interprètes doctrinaux ou juridictionnels, ne peut se résoudre, de par la logique-même du système clos de la norme interne fondamentale, à penser l’hypothèse-même de la sécession. Une exception justifie la règle, le précédent d’une célèbre jurisprudence canadienne relative à la question québécoise, dont les potentialités sont peu explorées en Europe. L’appréhension juridique ne peut évidemment être exclusive d’un questionnement politique, tant la question peut apparaître explosive. Elle pose néanmoins la question fondamentale du primat du politique sur le juridique."
"Rendues par le Conseil constitutionnel à une semaine d’intervalle, les décisions n° 2015-500 QPC et n° 2015-503 QPC invitent à revenir sur les différentes réponses que peut livrer le Conseil constitutionnel lorsque l’objet d’une question prioritaire de constitutionnalité (QPC) n’est pas une disposition législative « brute », mais l’interprétation consolidée d’une telle disposition et qu’il estime cette interprétation contraire à la Constitution."
"Compagnoni, premier titulaire historique d’une chaire de droit constitutionnel à la fin du XVIIIe siècle, s’inspira grandement de Rousseau pour écrire ses Elementi di diritto costituzionale democratico. Contrairement aux critiques qui lui ont été adressées, il n’était pas un vulgaire copiste, tant il s’efforça d’adapter, en juriste, l’œuvre du philosophe genevois. L’ouvrage précurseur de l’Italien commence par décrire et justifier les droits de l’Homme, dans le sillage du Discours sur l’origine et les fondements de l’inégalité parmi les hommes. Il distingue cependant les droits parfaits des droits imparfaits, les droits de sécurité et de secours, en ce que ces derniers, à l’inverse des premiers ne puissent justifier de l’usage de la force car mettant l’homme en relation avec ses pairs. Or, c’est en dernière instance de la possibilité d’exercer ces droits que naît le pacte social. De la distinction originale que Rousseau fait entre formes de l’Etat et forme de Gouvernement, entendue comme exercice du pouvoir exécutif, Compagnoni s’inspire pour ériger, aux côtés des démocratie, aristocratie, monarchie, une nouvelle forme, mixte, affectant l’organisation du pouvoir législatif, la démocratie représentative, opérant une distinction entre énonciation, opérée par les représentants, et confection de la loi, détenue elle par le seul souverain. C’est pourtant un dévoiement de l’œuvre rousseauiste, tant l’inaliénabilité de la souveraineté en est le cœur."
"Le 3 décembre dernier le collège de la Grande Chambre a accepté la demande d’avis consultatif des juges de l’assemblée plénière de la Cour de cassation portant sur le « statut » du parent d’intention d’un enfant conçu dans le cadre d’une convention de mère porteuse. Cette demande d’avis est doublement intéressante, d’une part, parce qu’elle conduit à une réflexion interactive entre la Cour de cassation et la Cour européenne des droits de l’homme sur ce sujet sensible qu’est le « statut » du parent d’intention, et plus précisément celui de la mère d’intention, mais aussi en ce qu’elle inaugure la transformation de l’articulation des systèmes juridictionnels."
"Les décisions prises par la Cour constitutionnelle en 2016, en matière fiscale, ne sont guère nombreuses (six au total) mais elles portent toutes sur des questions déjà traitées plusieurs fois, sous un angle ou un autre. Si leur intérêt individuel demeure limité, aussi bien pour les problématiques soulevées que pour les réponses données par la Cour, elles s’inscrivent néanmoins dans la continuité de la jurisprudence constitutionnelle. Une jurisprudence qui préserve la notion de prélèvement fiscal et qui préserve la compétence du législateur étatique."
"L’Italie subit l’une des pires crises bancaires qu’un Etat de l’Union européenne ait pu connaître depuis 2008. Pour tout dire, l’Italie n’est jamais vraiment sortie du marasme économique provoqué par l’effondrement des banques américaines et le scandale des subprimes. Cette situation produit aujourd’hui des conséquences aussi bien politiques qu’institutionnelles."
Non renseigné
"Les écosystèmes aquatiques subissent des dommages, qu’ils soient dus à leur artificialisation ou à la pollution mais, dans tous les cas, ces dommages sont le fait de l’homme. Le droit reconnaît la notion d’écosystème qu’il tente de protéger, sur le plan interne, européen ou international, par la voie répressive ou par des procédures préventives. Cela n’empêche pourtant pas la détérioration des milieux aquatiques avec, pour résultat, moins de faune, de flore et des conditions défavorables à leur développement. On peut alors s’interroger sur l’efficacité du droit positif. Est-il suffisant ou conviendrait-il de le réformer ? Le regard croisé de plusieurs disciplines juridiques, sur le thème de la protection des écosystèmes aquatiques, permet de mieux comprendre les enjeux et les difficultés pour le droit de préserver un environnement, d’autant plus fragile qu’il demeure en partie caché par la profondeur de l’eau."
"La trajectoire politique de Matteo Salvini, retracée depuis ses origines, prit un essor considérable notamment en raison d'une utilisation accrue de l'outil numérique qui a contribué à sa popularité et à la montée de son mouvement, la Ligue. Cette étude essaie de confronter les instruments traditionnels de communication politique aux nouveaux outils nés avec l'apparition de l'outil numérique dont le chef de la Ligue a incontestablement tiré bénéfice"
"Le droit communautaire accorde sa confiance au marché et au jeu de la concurrence pour parvenir à la satisfaction de l’intérêt général. L’expression « service public » n’apparaît que dans de rares articles des traités. Il en a été longtemps différemment en France. La tradition centralisatrice de notre pays expliquait le développement des services publics nationaux, l’héritage révolutionnaire justifiait les références à l’égal accès de tous à certaines prestations. La rationalisation de la gestion des services d’intérêt général devait améliorer leur performance. Au cours des années 70, l’opposition des conceptions communautaire et française a été mise en exergue avec le retour en force du libéralisme en Europe, la privatisation de nombreux secteurs, la dérégulation. Ces phénomènes semblaient remettre en cause le régime protecteur des services publics. On prit alors conscience du danger et le service public fut défendu par le gouvernement, le Conseil d’Etat, les parlementaires... L’objet de cette étude est de comprendre l’influence du droit communautaire de la concurrence sur la conception française du service public et de montrer qu’il s’agit en réalité d’une interaction. D’abord, l’analyse permet de décrire l’évolution qui s’est produite sous l’effet de la dynamique engendrée par la réalisation du marché commun. Le concept de service public a été attaqué frontalement ; cette contribution montre comment il s’est adapté aux exigences des traités communautaires. Les dispositions communautaires allaient en effet à l’encontre du droit français de la concurrence qui excluait certaines activités de puissance publique de son champ d’application et légitimait certaines pratiques anticoncurrentielles. Ensuite, il a paru important d’expliquer comment, au début des années 90, les juridictions communautaires ont progressivement pris en compte les spécificités françaises et d’analyser les arrêts qui ont permis d’infléchir la jurisprudence de Luxembourg dans un sens plus favorable au service public. Ce mouvement a été confirmé par le traité d’Amsterdam. Il convenait donc de justifier les solutions novatrices relatives à l’application du droit communautaire de la concurrence, qui sont apparues avec le temps. Cette contribution insiste sur le double mouvement qui s’est enclenché : évolution du régime juridique des services publics français entraînée par les exigences du droit communautaire, évolution de l’approche communautaire des services publics liée à la résistance de la conception française du service public. Ainsi, plutôt que de parler d’opposition, il est aujourd’hui davantage fait référence à un mouvement de réconciliation entre l’Europe et les services publics."
"Les îles italiennes, à rebours de la revendication sécessionniste évoquée précédemment, ont pu, au lendemain de la Libération, négocier avec l’Etat central un statut spécifique leur assurant une très large autonomie marquant leur spécificité. La Sicile en est l’exemple paradigmatique. Pourtant, les velléités indépendantistes étaient prégnantes au lendemain de la Seconde Guerre. Le pragmatisme des responsables politiques îliens, partagé avec celui des dirigeants nationaux, a su créer, à l’instar des autonomies espagnoles, une voie médiane, pacificatrice, non exclusive cependant de poussées nationalistes à venir."
"Cet écrit est relatif à « l’impensé constitutionnel », voire « l’impensable » sécessionniste, vocable en soi péjoratif. Structurellement, la défense de la Constitution, qu’elle soit assurée par ses interprètes doctrinaux ou juridictionnels, ne peut se résoudre, de par la logique-même du système clos de la norme interne fondamentale, à penser l’hypothèse-même de la sécession. Une exception justifie la règle, le précédent d’une célèbre jurisprudence canadienne relative à la question québécoise, dont les potentialités sont peu explorées en Europe. L’appréhension juridique ne peut évidemment être exclusive d’un questionnement politique, tant la question peut apparaître explosive. Elle pose néanmoins la question fondamentale du primat du politique sur le juridique."
"L’ouvrage analyse les interventions, dans les contentieux constitutionnels incidents français et italien, des tiers porteurs d’intérêts collectifs, dont l’intérêt à intervenir est lié aux effets généraux de la décision. Comme précisé par le sous-titre, l’analyse du sujet s’inscrit dans le cadre d’une « étude sur l’élargissement du débat contradictoire dans un contrôle de constitutionnalité concret et objectif ». Mon analyse se fonde en effet sur le postulat de départ, illustré dans l’introduction, selon lequel les deux systèmes étudiés constituent des mécanismes de contrôle de constitutionnalité concrets et objectifs, ayant comme but principal la protection des droits et libertés constitutionnellement garantis, dans une perspective concrète et générale. C’est à la lumière de cette qualification que les interventions des tiers porteurs d’intérêts collectifs ont été analysées en tant qu’instrument procédural essentiel et caractéristique d’un tel système. L’objectif de l’étude était de comprendre, d’une part, si ces interventions sont compatibles avec le modèle incident de justice constitutionnelle en place et, d’autre part, comment elles devraient être encadrées pour assurer la juste conciliation entre les avantages et les inconvénients qu’elles peuvent produire. En effet, ces interventions peuvent offrir une contribution précieuse à la réalisation des objectifs du contentieux constitutionnel. Néanmoins, elles exposent également les juges constitutionnels à des risques, liés à l’encombrement de leurs prétoires et aux pressions que leur activité pourrait subir, au détriment de l’efficacité et de la légitimité du contrôle de constitutionnalité. Le caractère à double tranchant de cet instrument procédural doit donc être pris en compte dans les réflexions sur son encadrement juridique. Si l’objet de la recherche porte sur un aspect procédural, dont la portée est apparemment limitée, l’étude montre que les choix concernant le fondement et l’encadrement des interventions des tiers porteurs d’intérêts collectifs exercent une influence importante sur la configuration du système de justice constitutionnelle et sur sa capacité à atteindre ses objectifs. La thèse défendue se fonde ainsi sur l’exigence de perfectionner les régimes du débat contradictoire des deux mécanismes de contrôle incident, pour mieux répondre à leur vocation de systèmes portant sur la protection concrète et générale des droits. L’analyse comparée a mis en évidence des différences marquées entre les deux systèmes de contrôle. Ces différences ne découlent pas seulement des approches divergentes adoptées en matière d’interventions, mais aussi des différents caractères du contrôle - de la prise en compte des faits à la technique de motivation pour n’en citer que quelques-uns - qui déterminent l’incidence effective des interventions dans la procédure et dans la décision. Malgré ces différences, qui ne permettent guère l’identification d’un modèle unique en matière de recevabilité et de prise en compte des interventions, des solutions partiellement convergentes ont néanmoins pu être mises en avant. Ces solutions ont pour but de valoriser, dans les deux systèmes, les interventions des tiers porteurs d’intérêts collectifs, en limitant les inconvénients qu’elles peuvent entraîner. Les réflexions conclusives menées à l’issue de notre étude conduisent à confirmer l’intuition initiale, selon laquelle un système de contrôle de constitutionnalité visant à la protection concrète et objective des droits et libertés devrait se doter d’un débat contradictoire adapté à cette caractéristique. La participation des formations sociales engagées dans la protection des droits et libertés concernées par la question de constitutionnalité, dès lors, peut offrir une contribution précieuse au contentieux constitutionnel."
"Le référendum faussement qualifié d’initiative partagée institué par la révision constitutionnelle de 2008 n’a eu pour but que d’établir une réforme en trompe l’œil : faire accroire à une intervention d’une initiative populaire minoritaire quand ce sont en réalité des partis de gouvernement, certes oppositionnels, qui ont la maîtrise de l’initiative de cette institution, alors même, pourtant, que le seuil de signatures citoyennes requises apparaît en pratique inatteignable. La revendication d’un réel référendum d’initiative citoyenne, à l’instar d’autres pays européens, a vu, dans le sillage du mouvement des « gilets jaunes », bousculer sainement ce schéma, sans pour autant que le pouvoir en place ne satisfasse, ne serait-ce qu’à titre expérimental au niveau local, une alternative citoyenne potentiellement fructueuse."
"L'ouvrage ""Les systèmes de welfare à l'épreuve des nouvelles dynamiques migratoires"" (ES, Naples, 2018) est le fruit d’un projet de recherche PHC-Galilée réalisé par le Centre de Droit et de Politique Comparés-J.C. Escarras de l'Université de Toulon et le Département de Sciences Juridiques de l’Université d'Udine (Italie). L'ouvrage, réunissant des contributions en langue française et italienne, analyse, sous l’angle spécifique de la garantie des droits fondamentaux, la question de la protection sociale des étrangers dans les systèmes juridiques européens (en particulier dans les systèmes français et italien) face à la crise économique et migratoire."
"Le régionalisme italien se traduit par un transfert des compétences de l’État au profit des régions en matière d’organisation des services de santé. Les dépenses dans ce domaine, déjà lourdes à supporter, ont augmenté sous l’effet de la crise économique, incitant le législateur puis le constituant à restreindre l’autonomie de gestion des régions et à accroître le pouvoir de coordination de l’État."
"Plaidoyer en faveur de l’article 49 alinéa 3 « Les publicistes le savent bien, les instruments du droit politique sont multi-fonctionnels : peu importe ce pour quoi ils ont été conçus, ils serviront pour tout ce à quoi ils peuvent être utiles ». Guy CARCASSONNE Il est de bon ton de critiquer l’article 49, alinéa 3 de la Constitution. Abrégé sous la formule aussi pratique qu’inexacte de « 49-3 », on dit de son utilisation qu’elle est « une brutalité », « un déni de démocratie ». Pour certains, « un passage en force », pour d’autres, « un aveu de faiblesse ». À son sujet, il n’est fait l’économie d’aucune raillerie, au risque, parfois, de verser dans la caricature. Néanmoins, il faut reconnaître que l’on serait tenté de concéder, à ce 49-3 bashing, une part de vérité. Il s’agit, tout de même, de considérer qu’un projet de loi est devenu une loi sans l’aval du Parlement. Voilà qu’on apprend qu’une loi n’est pas toujours votée. Elle peut, simplement, être considérée comme adoptée. Cela soulève quelques interrogations sur la séparation des pouvoirs et sur l’efficacité des contre-pouvoirs. Mais est-ce pour autant qu’il faille le supprimer ? Notre contribution consistera à plaider en faveur de son maintien. Nous estimons qu’un cantonnement, de plus, voire, sa suppression, s’écarterait de la logique institutionnelle voulue en 1958. Le doyen Georges Vedel voyait dans la Ve République, trois traits majeurs : l’élection du président au suffrage universel, le Conseil constitutionnel et l’article 49 alinéa 3 de la Constitution. Il est clair que, pour nous, cette disposition est, bien, « l’acmé du parlementarisme rationalisé ». Nous montrerons que toute la vulgate qui consiste à le considérer comme antidémocratique est infondée. Par cela seul qu’il s’agit d’un procédé voulu et adopté par le constituant souverain, il est démocratique. De plus, avant de conduire à considérer une loi comme adoptée, l’article 49 alinéa 3 enclenche automatiquement la mécanique de la motion de censure, donc, la responsabilité du gouvernement devant le Parlement. Illustration parfaite de ce que la séparation des pouvoirs est bien sauve. Que les députés ne s’en saisissent pas, ou qu’elle ait peu de chance d’aboutir n’enlève rien au mérite qu’elle a d’exister. En réalité, les craintes que suscite l’article 49 alinéa 3 s’intègrent dans un contexte problématique plus large : faut-il conserver les dispositifs du parlementarisme rationalisé lors même qu’il existe un fait majoritaire ? Cela ne crée-t-il pas un déséquilibre des pouvoirs ? L’article 49 alinéa 3 fait partie de ces dispositifs. Il a été pensé pour compenser l’absence de majorité. Il n’est pas une innovation de la Constitution de 1958, mais issue des derniers balbutiements de la IVe République agonisante. L’existence d’une majorité parlementaire ne saurait justifier la suppression de l’article 49 alinéa 3. D’abord, le fait majoritaire n’est qu’un fait, rien ne garantit qu’il survivra toujours. On a même pu voir, sous François Hollande, un fait majoritaire frondeur. Sans oublier, ensuite, qu’en dehors des majorités rétives, il a pu, notamment, servir contre une opposition qui, abusant de ses maigres droits, cherche l’enlisement des débats. L’article 49 alinéa 3 permet de surmonter les blocages : le gouvernement peut montrer son activisme, l’opposition, son désaccord, et la majorité, ses réticences. Nous plaidons pour que cette arme de dissuasion, démocratique et efficace, soit maintenue."
"Après plusieurs années de discussions et au bout de huit votes de confiance (trois à la Chambre des députés et cinq au Sénat), le Parlement italien a enfin adopté, le 26 octobre 2017, une loi modifiant le système électoral de la Chambre des députés et du Sénat : la loi n o 165, promulguée le 3 novembre 2017."
"La constitution des comores, expliquée et commentée article par article. La Constitution du 23 décembre 2001 est la Loi fondamentale du nouvel ensemble comorien post-séparatisme proclamé par les accords de Fomboni et baptisé : Union des Comores. Rafsandjani MOHAMED analyse ici cette Constitution, en vigueur depuis maintenant plus de 15 ans, en expliquant article par article."
"Depuis 2020, les personnes âgées sont appelées à être particulièrement vigilantes compte tenu des risques qu’elles encourent face au coronavirus. Il est capital de venir en aide aux séniors, personnes vulnérables, lorsque le confinement est nécessaire et même quand ces derniers sont autorisés à se déplacer. En effet, il convient de limiter les risques auxquels ils sont particulièrement exposés. Des mesures de soutien aux séniors, destinées à prévenir ou compenser les difficultés qui résultent de la mise en place de barrières sanitaires destinées à les protéger, peuvent être adoptées sur le fondement des dispositions des traités de l’UE, afin d’organiser une forme certaine de solidarité. Il convient pour les États de se saisir du problème, criant aujourd’hui, et d’élaborer des mécanismes destinés à garantir à nos aînés des conditions de vie satisfaisantes et respectueuses de leurs droits en tant que personnes."
"(CJUE, 3e ch., 17 sept. 2020, aff. C-540/19, WV c/ Landkreis Harburg ; D. actu. 9 oct. 2020, obs. F. Mélin ; D. 2020. 1798 ; Europe 2020. Comm. 371, obs. L. Idot)"
"L’Opération EUNAVFOR Sophia conduite par l’Union européenne, dans le cadre de la PSDC contre les trafiquants d’êtres humains en Méditerranée a pour but d’interrompre le business model des trafiquants. Elle se déroule pour l’instant dans les eaux internationales au large de la Libye, elle a reçu l’appui politique du Conseil de sécurité des Nations unies. Une intervention dans les eaux territoriales et a fortiori sur le territoire terrestre exigerait cependant une autorisation du Conseil, éventuellement de la Libye, ce qui est pour l’instant très incertain. La pertinence même de l’opération est aussi discutée car elle risque de n’avoir pour effet que de pousser les trafiquants à changer leurs itinéraires."
"Ne constitue pas un titre exécutoire, un jugement rendu par le juge du tribunal d’instance, à l’occasion de la procédure de saisie des rémunérations, qui n’a pas pour objet de constater une créance liquide et exigible, mais, à défaut de conciliation, de vérifier le montant de la créance en principal, intérêts et frais et, s’il y a lieu, de trancher les contestations soulevées par le débiteur."
"Il résulte des articles L. 723-3 et R. 723-7 du code de la consommation que lorsque la créance dont la vérification est demandée n’est pas contestée en son principe, le juge ne peut pas l’écarter au motif que le créancier ne produit pas les pièces justificatives sans inviter préalablement celui-ci à les produire."
"Dans le cadre de la procédure de saisie-appréhension, une fois revêtue de la formule exécutoire, l’ordonnance portant injonction de délivrer ou de restituer, qui produit tous les effets d’un jugement contradictoire en dernier ressort, n’est pas susceptible de rétractation, mais peut faire l’objet d’un pourvoi en cassation pour contester la régularité de la délivrance de la formule exécutoire."
"L’impact des délais de paiement sur le recouvrement forcé des créances d’argent semble, à titre principal, prendre les traits d’une suspension de l’exigibilité de la créance litigieuse et, à titre accessoire, peut se traduire par une limitation du montant des intérêts ainsi que, par extension, des sommes auxquelles le créancier peut prétendre."
"Lorsque la décision de recevabilité d’une demande de traitement de la situation financière du débiteur intervient après que la vente forcée d’un bien immobilier lui appartenant a été ordonnée par un jugement d’orientation, exécutoire de plein droit nonobstant appel, le report de la date d’adjudication ne peut résulter que d’une décision du juge chargé de la saisie immobilière, saisi à cette fin par la commission de surendettement des particuliers, pour causes graves et dûment justifiées."
"Lorsque le délai d’un mois pour former une contestation relative à une saisie-attribution expire normalement un samedi, un dimanche ou un jour férié ou chômé, il est prorogé jusqu’au premier jour ouvrable suivant."
"Etude du règlement (CE) n°861/2007 du Parlement européen et du Conseil du 11 juillet 2007 instituant une procédure européenne de règlement des petits litiges (JOUE n° L 199, 31 juillet 2007, p. 1)"
"En matière de saisie immobilière, lorsque la décision de recevabilité de la commission de surendettement intervient avant que le jugement d’orientation ne soit rendu, le juge de l’exécution, saisi d’une demande de constatation de la suspension de la procédure, n’a pas, à cette occasion, à procéder aux vérifications relatives à la créance ni à en fixer le montant."
"En cas d’impossibilité pour une juridiction de se procurer l’adresse du défendeur, le règlement (CE) n°805/2004 du Parlement européen et du Conseil du 21 avril 2004 ne permet pas de certifier en tant que titre exécutoire européen une décision judiciaire relative à une créance, rendue à la suite d’une audience à laquelle n’ont comparu ni le défendeur ni le tuteur désigné pour les besoins de la procédure."
janvier 2013 - décembre 2013
janvier 2015 - décembre 2015
janvier 2012 - décembre 2012
janvier 2014 - décembre 2014
janvier 2017 - décembre 2017
janvier 2011 - décembre 2011
"Com. 28 févr. 2018, n° 16-24.507 (F-D) ; Com. 28 févr. 2018, n° 16-26.735 (F-D)"
"Com. 28 févr. 2018, n° 16-19.718 et 16-21.337 (F-D) ; Com. 28 févr. 2018, n° 16-21.338 (F-D)"
"Com. 4 juill. 2018, n° 17-15.038 (F-P+B)"
"Com. 20 juin 2018, n° 16-27.693 (F-P+B)"
"Com. 3 oct. 2018, n° 17-10.557, F-P+B+I"
"Com. 7 nov. 2018, n° 17-18.449 (F-D) ; Com. 7 nov. 2018, n° 17-20.478 (F-D)"
"Les mesures conservatoires autorisées par le juge français ne contrarient pas l’injonction Mareva ordonnée par le juge étranger. En l’absence d’identité d’objet, l’autorité de la chose jugée des décisions chypriotes, exécutoires en France, ne s’oppose pas à d’autres mesures conservatoires portant sur les biens détenus en France par les sociétés débitrices."
"À peine de déchéance du bénéfice de sa sûreté, tout créancier inscrit doit déclarer sa créance, peu important que son exigibilité soit suspendue en conséquence d’une réclamation présentée dans les conditions prévues par l’article L. 277 du livre des procédures fiscales."
"Le montant de la mise à prix fixé par le créancier poursuivant dans le cahier des conditions de vente ne peut être modifié qu’à la demande du débiteur dans les conditions prévues à l’article L. 322-6, al. 2, du code des procédures civiles d’exécution."
"L’irrégularité affectant l’acte de dénonciation d’une saisie conservatoire de créances, rédigé en méconnaissance de l’article R. 523-3 1° du code des procédures civiles d’exécution, constitue un vice de forme qui ne peut entraîner la nullité de l’acte qu’à charge pour celui qui l’invoque de prouver le grief que lui cause cette irrégularité."
"Analyse des dispositions de la loi n°2019-222 du 23 mars 2019 de programmation 2018-2022 et de réforme pour la justice et du décret n°2019-1333 du 11 décembre 2019 réformant la procédure civile, ayant trait à l’obligation faite aux parties de tenter de parvenir à la résolution amiable de leur différend, avant de pouvoir – le cas échéant – le porter à la connaissance de la juridiction compétente afin qu’il soit tranché."
"Les contestations et demandes incidentes soulevées après l’audience d’orientation ne sont recevables que si elles portent sur des actes de la procédure de saisie immobilière postérieurs à cette audience ou si, nées de circonstances postérieures à celle-ci, elles sont de nature à interdire la poursuite de la saisie."
"La cassation du jugement d’adjudication n’entraîne pas l’annulation par voie de conséquence du jugement subséquent d’adjudication sur réitération des enchères, ce second jugement n’étant pas la suite, l’application ou l’exécution du premier et ne s’y rattachant pas par un lien de dépendance nécessaire."
"Les resolutions et les débats récents devant le Conseil de sécurité des Nations unies permettent de souligner les grands axes de la politique étrangère de la Russie. Elle défend les principes des relations internationales comme la souveraineté de l’Etat et l’interdiction du recours à la force. Quand deux principes s’opposent, elle essaye de les concilier en fonction de ses intérêts. Elle défend également le statut du Conseil de sécurité au sein de l’Organisation des Nations Unies face aux organes subsidiaires et aux juridictions pénales interantionales. Cela permet de protéger sa position internationale."
"(CJUE 19 déc. 2019, aff. jtes C-453/18 et C-494/18, Bondora AS c/ Carlos V.C., XY ; D. actu. 30 janv. 2020, obs. F. Mélin ; D. 2020. 23 ; Rev. prat. rec. 2020. 17, chron. A. Raynouard ; Europe 2019. Comm. 87, obs. L. Idot)"
"Une décision judiciaire prononcée sans que le débiteur ait été informé de l’adresse de la juridiction à laquelle il convient d’adresser la réponse, devant laquelle comparaître ou, le cas échéant, auprès de laquelle un recours peut être formé contre cette décision, ne peut être certifiée, en tant que titre exécutoire européen, conformément aux dispositions du règlement (CE) n°805/2004 du 21 avril 2004."
"Les nouveaux règlements européens adoptés le 24 juin 2016 instituent, dans les domaines des régimes matrimoniaux et des effets patrimoniaux des partenariats enregistrés, une « procédure de déclaration constatant la force exécutoire ». Replacée dans le contexte de l’action de l’Union européenne menée depuis l’entrée en vigueur du traité d’Amsterdam en matière de circulation transfrontière des titres exécutoires, cette solution n’apparaît guère innovante. Bien au contraire, cette procédure se révèle être de facture classique. À ce premier trait caractéristique, vient s’ajouter un second : envisagée à la lumière de son articulation avec les droits nationaux, la procédure de déclaration constatant la force exécutoire s’analyse en une procédure « semi-uniforme »."
"Par un arrêt du 4 juin 2020, la Cour de justice se prononce en faveur de la compétence internationale des juridictions de l’État membre d’exécution pour connaitre d’une action en opposition à exécution introduite par le débiteur d’une créance d’aliments."
"Le Fonds de garantie des victimes d’actes de terrorisme et d’autres infractions (FGTI) est subrogé dans les droits de la victime pour obtenir des personnes responsables du dommage causé par l’infraction le remboursement de l’indemnité ou de la provision versée par lui, dans la limite des réparations à la charge desdites personnes, et peut exercer ses droits par toutes voies utiles, dont la réalisation d’une saisie-attribution sur la part disponible du compte nominatif d’un détenu."
"Les articles 9 à 11 s’inscrivent dans le chapitre II des règlements (UE) n°2016/1103 « Régimes matrimoniaux » et (UE) n°2016/1104 « Effets patrimoniaux des partenariats enregistrés » du 24 juin 2016. Ils ont respectivement trait à une disposition originale prévoyant un déclinatoire de compétence justifié par des raisons tenant au fond du droit, à un chef de compétence reposant sur la situation d’un immeuble dans un État membre ainsi qu’à l’institution d’un for de nécessité (ou forum necessitatis). Les articles 42 à 52 sont quant à eux insérés dans le chapitre IV desdits règlements et sont relatifs à la procédure de déclaration constatant la force exécutoire d’une décision de justice étrangère. Pour l’essentiel, leur rédaction est inspirée de la procédure d’exequatur allégé régie dans le règlement (CE) n°44/2001 du 22 décembre 2000 dit « Bruxelles I » et déclinée, ensuite, dans le règlement (UE) n°650/2012 du 4 juillet 2012 dit « Successions transfrontières »."
"Cour EDH, 13 mars 2018, C. M. contre Belgique, req. 67957/12"
"L'arrivée au pouvoir de Vladimir Poutine marque un tournant politique pour la Russie. Ce pays, qui a connu une perte de puissance sur la scène internationale pendant la décennie 1990, affiche désormais une volonté de retrouver sa place historique de grande nation. Face à elle se trouve l'OTAN, créée jadis pour s'opposer à l'URSS. Après la guerre froide, cette organisation de défense s'est étendue dans l'ancien espace soviétique avec l’adhésion des États d'Europe centrale et orientale. Une telle situation, combinée à la stratégie russe de retrouver sa puissance, notamment dans son étranger proche, fait que les tensions reviennent. Toutefois, il semble erroné de considérer l'OTAN comme un bloc unique. En effet, en partant du fait que les décisions de l'Alliance sont prises à l'unanimité, il apparaît pertinent d'analyser la politique entre l'OTAN et la Russie à travers les relations bilatérales des membres de l’organisation. La combinaison de celles-ci sur la scène internationale amène soit un rapprochement basé sur des intérêts communs soit une confrontation reposant sur des divergences. Cette analyse permet de mettre en avant les dynamiques politiques, économiques et sécuritaires aboutissant à un équilibre des puissances à la fois européen et global."
"La juridiction qui prononce le jugement d’orientation n’est pas tenue d’actualiser d’office le montant en principal, frais, intérêts et autres accessoires de la créance réclamée dans le commandement valant saisie immobilière. Par ailleurs, l’annulation d’un tel jugement d’orientation ayant ordonné l’adjudication entraîne de plein droit l’anéantissement, par perte de fondement juridique, du jugement d’adjudication."
"Etude du règlement (CE) n°1896/2006 du 12 décembre 2006 instituant une procédure européenne d'injonction de payer (JOUE n° L. 399, 30 décembre 2006, p. 1)"
"jugement par lequel le juge du tribunal d’instance a statué sur le recours formé contre la décision d’orientation de la commission de surendettement est rendu en dernier ressort, de sorte que l’appel se heurte à une irrecevabilité manifeste qui n’est pas susceptible d’être couverte."
"Il appartient au juge saisi d’une demande de liquidation d’une astreinte de s’assurer, au besoin d’office, que l’astreinte a commencé à courir et de déterminer son point de départ."
"Analyse de la jurisprudence de la Cour de cassation relative à l’accueil en France des Mareva injunctions – ou Freezing injunctions – (Civ. 1re, 30 juin 2004, Stolzemberg, n°01-03.248 et n°01-15.452), à la lumière de l’évolution de la législation de l’Union européenne et, singulièrement, de l’entrée en application du Règlement (UE) n°1215/2012 « Bruxelles I bis »."
"Par un arrêt du 4 juin 2020, la Cour de justice se prononce en faveur de la compétence internationale des juridictions de l’État membre d’exécution pour connaitre d’une action en opposition à exécution introduite par le débiteur d’une créance d’aliments."
"Ne peut être frappé de pourvoi en cassation indépendamment d’un jugement sur le fond, le jugement rendu en dernier ressort par le juge du tribunal d’instance déclarant irrecevable une demande de vérification de certaines créances en application de l’article L. 723-3 du code de la consommation."
Il résulte des dispositions de l’article L. 321-5 du code des procédures civiles d’exécution que le débiteur qui a consenti une promesse de vente postérieurement à la délivrance d’un commandement de payer valant saisie immobilière n’est pas fondé à se prévaloir des effets de l’indisponibilité du bien prévue à l’article L. 321-2 du même code.
"Dans l’exercice de son pouvoir souverain d’appréciation, le juge du tribunal d’instance a pu déduire l’absence de bonne foi d’une débitrice dont les actes délictueux sont directement à l’origine de la totalité de son endettement."
"L’article R. 311-5 du code des procédures civiles d’exécution, qui interdit toute contestation ou demande incidente formée après l’audience d’orientation à moins qu’elle ne porte sur des actes de procédure postérieurs à celle-ci, est exclusif de l’application de l’article 566 du code de procédure civile régissant l’effet dévolutif de l’appel."
"L’article 24, points 1 et 5, du règlement (UE) no 1215/2012 du Parlement européen et du Conseil, du 12 décembre 2012, concernant la compétence judiciaire, la reconnaissance et l’exécution des décisions en matière civile et commerciale, doit être interprété en ce sens que l’action d’un créancier en contestation de l’état de distribution du produit d’une adjudication judiciaire d’un immeuble, tendant, d’une part, à la constatation de l’extinction par compensation d’une créance concurrente, et, d’autre part, à l’inopposabilité de la sûreté réelle garantissant l’exécution de cette dernière créance, ne relève pas de la compétence exclusive des juridictions de l’État membre où l’immeuble est situé ou des juridictions du lieu d’exécution forcée."
"Le lancement par les Etats –Unis, le 13 avril 2017, de la BU-43/B Massive Ordnance Air Blast (MOAB) sur une position souterraine de l’Etat islamique en Afghanistan soulève des interrogations aussi bien techniques que stratégiques et juridiques. La MOAB est en effet la bombe non nucléaire la plus puissante dont disposent les Etats-Unis. Portant 8480 kilos d’explosifs, elle est guidée et larguée par avion. Outre l’explosion, elle provoque une surpression dans un rayon de 150 m ce qui en fait également une armes thermobarique. Elle fait partie d’un ensemble de bombes anti-bunker à la disposition de l’armée des Etats-Unis. L’emploi de la MOAB semble avoir eu dans un premier temps une justification tactique. C’était selon les militaires américains, « la bonne arme pour la bonne cible ». Mais dans un second temps, elle est apparue comme manifestant la volonté des Etats-Unis d’éradiquer l’Etat islamique comme l’avait annoncé le président Trump. Enfin, on ne peut ignorer que l’emploi d’une telle arme, même si elle ne relève pas de la dissuasion comme les armes nucléaires, constitue également un avertissement pouvant être destiné à l’Iran comme à la Corée du Nord. Du point de vue juridique enfin, la MOAB n’est pas une arme interdite par le droit international humanitaire pour autant que son emploi respecte les principes généraux de ce droit et notamment la proportionnalité entre l’avantage militaire direct attendu et les éventuels dommages collatéraux causés aux civils. Du fait de ses effets de surpression, son emploi se trouve donc limité. Par ailleurs, comme toute arme, elle est destinée à engager la force, et donc soumise aux règles d’engagement (ROE), dans ce cas édictées en conformité avec le droit international, dans ce cas, par les autorités américaines. La frappe du 13 avril confirme une évolution significative des ROE aux Etats-Unis, le pouvoir politique choississannt désormais de laisser une très large marge de maoeuvre aux militaires sur le terrrain."
"Une juridiction, au sens du règlement (CE) n° 1896/2006 du 12 décembre 2006, saisie dans le cadre d’une procédure d’Injonction de payer européenne, peut demander au créancier des informations complémentaires relatives aux clauses du contrat de prêt invoquées à l’appui de la créance en question, afin d’effectuer un contrôle d’office du caractère éventuellement abusif de ces clauses."
"(CJUE 4 sept. 2019, aff. C-347/18, Alessandro Salvoni c/ Anna Maria Fiermonte ; D. actu. 14 oct. 2019, obs. F. Mélin ; D. 2019. 1656 ; Procédures 2019. Comm. 285, obs. C. Nourissat ; Europe 2019. Comm. 471, obs. L. Idot)"
La seule qualité d’associé d’une société civile immobilière (SCI) ne suffit pas à exclure une personne du champ d’application des dispositions du code de la consommation relatives au surendettement des particuliers.
"Com. 11 mars 2020, n° 18-22.960 et 18-22.962"
"Com. 26 févr. 2020, n° 18-18.680"
"Com. 5 févr. 2020, n° 18-21.754"
"Caractérise une situation de surendettement l’impossibilité manifeste pour une personne physique de bonne foi de faire face à l’engagement qu’elle a donné de cautionner la dette d’une société, qu’elle en soit ou non la dirigeante."
"Seuls peuvent être déchus du bénéfice de leur sûreté les créanciers inscrits ayant été préalablement sommés de déclarer leur créance. Par ailleurs, le relevé de forclusion ne concerne que les créanciers n’ayant pas déclaré leur créance dans le délai de deux mois suivant la sommation qui leur a été adressée à cette fin."
"Lorsqu’il applique la procédure européenne de règlement des petits litiges instituée par le règlement (CE) n°861/2007 du Parlement européen et du Conseil du 11 juillet 2007, le juge est tenu de faire observer et d’observer lui-même le principe de la contradiction."
"La Cour européenne des droits de l’homme condamne la France pour violation de l’article 6, paragraphe 1er, de la Convention européenne des droits de l’homme, en raison de la motivation jugée insuffisante du rejet d’une demande de question préjudicielle à destination de la Cour de justice de l’Union européenne."
"L’opposabilité au codébiteur et à la caution solidaires de la substitution de la prescription, ayant pu se produire, en l’état du droit antérieur à la loi du 17 juin 2008, à la suite d’une décision d’admission des créances au passif du débiteur principal, ne peut avoir eu pour effet de soumettre l’action en paiement du créancier contre le codébiteur et la caution solidaires au délai d’exécution des titres exécutoires."
"Lit-Verlag, 2016, xxv + 319 pages"
"Hart Publishing, 2013, xxii + 386 p"
"Université Panthéon-Sorbonne (Paris 1) 2017, dir. Th. Revet, 593 p. + IX p. (dactyl.), LGDJ Bibl. de droit privé à paraître"
"Cette esquisse dévoile le sabordage du PCI au début des années 1990, que rien, pourtant, ne semblait présager. Ce parti, hégémonique à gauche, au contraire de son homologue français, pendant plus de 45 années, ne participa pourtant jamais au pouvoir national, exclu par le rapprochement de la Démocratie chrétienne et du PSI. Il fut cependant souvent présenté comme le Parti communiste occidental le plus ouvert. A telle aune que son premier responsable, Enrico Berlinguer, proposa, sous l’influence de facteurs tant endogènes (les années de plomb durant la décennie 1970) qu’exogènes (le coup d’Etat au Chili en 1973) et d’un renouvellement idéologique (l’Eurocommunisme), un compromis historique à la DC, lui permettant d’entrevoir l’ouverture des palais officiels, en s’appuyant sur son aile gauche, menée par Aldo Moro. Son assassinat en 1978 mit fin rapidement à l’expérience de soutien sans participation de gouvernements dirigés par la DC. Par ailleurs, la dispute du leadership à gauche entre le PSI et le PCI exacerba les tensions entre les deux frères ennemis à travers la cristallisation sur la question, voulue par Bettino Craxi, secrétaire général du Parti socialiste et Président du Conseil, de la désindexation partielle de l’échelle mobile des salaires. Malgré la victoire aux élections européennes du Parti communiste, après la mort tragique de Berlinguer, le référendum initié par le PCI signa sa défaite sur cette question emblématique. Le Parti, quoique gardant un poids électoral conséquent, fit l’objet d’un tournant impulsé par son nouveau secrétaire général, Occhetto, désireux d’imprimer un réformisme que souhaitait depuis longtemps les « miglioristes » de l’aile droite du PCI. Cette transformation, désirée par quelques leaders communistes, fit l’objet d’âpres débats, mais le suivisme de la base accepta la mue sociale-démocrate du mouvement fondé par Gramsci, abandonnant par là même toute une culture politique, au profit d’une conversion au « réalisme » économique, favorisé par les révolutions libérales anglo-saxonnes, qui est le prélude à ce que nous analysons comme l’ensevelissement de l’idée même de la gauche."
"La création d’un corps européen de garde-frontières et de garde- côtes par le règlement UE du 14 septembre 2016 constitue une innovation intéressante mais le projet peut sembler inachevé. En effet, apparu à la suite de la crise des réfugiés de 2015, il ne tient pas assez compte de l’unité du milieu marin qui ne connaît pas de véritables frontières et de sa spécificité. Plutôt qu’un corps de garde-côtes, c’est une fonction garde-côtes qui doit être envisagée au niveau européen afin de mettre en œuvre une stratégie de sauvegarde maritime face aux menaces qui touche le milieu marin et à celles qui en proviennent. Cette construction bénéficie déjà des mécanismes de coopération entre institutions européennes et Etats membres. L’intervention nécessaire des marines militaires rend toutefois la coopération complexe et une réflexion plus générale sur l’articulation entre la Politique de Sécurité et de Défense Commune et la politique de l’union apparaît aujourd’hui indispensable."
"L’article 201, paragraphe 1, sous a), de la directive 2009/138/CE du Parlement européen et du Conseil, du 25 novembre 2009, sur l’accès aux activités de l’assurance et de la réassurance et leur exercice (solvabilité II), doit être interprété en ce sens que la notion de « procédure judiciaire » visée à cette disposition inclut une procédure de médiation judiciaire ou extrajudiciaire dans laquelle une juridiction est impliquée ou susceptible de l’être, que ce soit lors de l’engagement de cette procédure ou après la clôture de celle-ci. Il s’ensuit, pour l’assuré, une garantie de la liberté de choix de son avocat lors d’une procédure de médiation."
"(CJUE, 3e ch., 14 mai 2020, aff. C-667/18, Orde van Vlaamse Balies, Ordre des barreaux francophones et germanophone c/ Ministerraad, D. actu. 5 juin 2020, obs. C. Collin ; D. avocats 2020. 329 et les obs.)"
"Le projet de loi de programmation pour la justice 2018-2022 comporte des dispositions relatives aux procédures civiles d’exécution. Alors que celles ayant trait à la procédure de saisie immobilière initialement prévues ont été retirées, demeurent celles concernant la saisie des rémunérations. Il y est question de confier la gestion des fonds issus de cette saisie à la Caisse des dépôts et consignations."
Rédaction des 44 fiches suivantes : Le droit européen à l’exécution ; Le juge de l’exécution (organisation – compétence) ; Le juge de l’exécution (procédure) ; Procédures civiles d’exécution (Ministère public) ; Huissier de justice ; Procédures civiles d’exécution (créancier poursuivant) ; Procédures civiles d’exécution (débiteur poursuivi) ; Procédures civiles d’exécution (tiers aux poursuites) ; Procédures civiles d’exécution (conditions des poursuites) ; Procédures civiles d’exécution (recherche des informations) ; Biens saisissables ; Les titres exécutoires ; Procédures civiles d’exécution (effets des poursuites) ; Opérations d’exécution (règles générales) ; Opérations d’exécution (pénétration forcée dans un local privé) ; Opérations d’exécution (concours de la force publique) ; Astreinte ; Mesures conservatoires (conditions et mise en œuvre communes) ; Saisie conservatoire sur les biens meubles corporels ; Saisie conservatoire des créances ; Sûretés judiciaires ; Ordonnance européenne de saisie conservatoire ; Saisies des créances de somme d’argent (présentation générale) ; Saisie-attribution (droit commun) ; Saisie-attribution des créances à exécution successive ; Saisie-attribution des comptes ouverts auprès d’établissements habilités par la loi à tenir des comptes de dépôt ; Saisie des rémunérations du travail ; Procédure de paiement direct des pensions alimentaires ; Saisie entre les mains d’un comptable public ; Saisies-ventes des biens meubles incorporels ; Saisie-vente (conditions) ; Saisie-vente (procédure – opérations) ; Saisie-vente (procédure – vente) ; Saisie-vente (procédure – incidents) ; Saisies de véhicules terrestres à moteur ; Saisie-appréhension ; Saisie immobilière (dispositions générales) ; Saisie immobilière (conditions) ; Saisie immobilière (procédure – la saisie de l’immeuble) ; Saisie immobilière (procédure – opérations préparatoires et orientation) ; Saisie immobilière (procédure – vente de l’immeuble) ; Saisie immobilière (procédure – incidents) ; Saisie immobilière (distribution) ; Expulsion.
"L’article 3, § 1 du règlement (CE) n°861/2007 du Parlement européen et du Conseil du 11 juillet 2007 instituant une procédure européenne de règlement des petits litiges doit être interprété en ce sens que la notion de « parties », utilisée dans la définition des litiges transfrontaliers, vise seulement les parties requérante et défenderesse au principal, et non les parties « intervenantes »."
"Simplification des procédures de recouvrement forcé mises en œuvre par les comptables publics – opérée par la loi n°2017-1775 du 28 décembre 2017 de finances rectificative pour 2017 (spéc. art. 73) – au moyen d’une triple réforme : création d’une « saisie administrative à tiers détenteur » se substituant aux procédures préexistantes, harmonisation des règles applicables au contentieux du recouvrement des créances publiques et dématérialisation de la notification des saisies adressées aux établissements bancaires."
"En matière de recouvrement des amendes, le juge de l’exécution ne connaissant, en application combinée des articles 530-2 du code de procédure pénale et 9 du décret n°64-1333 du 22 décembre 1964, que de la régularité en la forme de l’acte de poursuite, il ne peut pas apprécier le respect de l’obligation faite, par l’article R. 49-6 du code de procédure pénale, au comptable public d’envoyer au contrevenant un avis l’invitant à s’acquitter du montant de l’amende forfaitaire majorée."
Analyse de l’article 55 du règlement (UE) n°1215/2012 du 12 décembre 2012 dit « Bruxelles I bis » relatif à la circulation transfrontière des décisions ordonnant une astreinte. Cette analyse s’inscrit dans un projet international intitulé « The Edward Elgar Research Handbook » donnant lieu à la publication d’un ouvrage collectif à la rédaction duquel ont participé des universitaires de plusieurs pays européens.
"Six mois après que le président Trump ait pris ses fonctions à la Maison Blanche, peut-on constater une évolution de la politique étrangère américaine à travers les votes et les débats au Conseil de sécurité ? Il est sans doute trop tôt pour se prononcer car la mise en place de l’administration et en particulier du secrétariat d’Etat se révèle difficile. En effet, le Secrétaire d’Etat ne donne pas une véritable impulsion à la politique étrangère et se borne plutôt à l’administration du secrétariat alors que le rôle direct de l’entourage du président est considérable. Avec la nomination de Mme N.Haley comme représentant permanent des Etats Unis au Conseil de sécurité, quelques tendances semblent néanmoins se dessiner mais elles ne sont pas très originales. Entre le 20 janvier et le 30 juin 2017, le Conseil de sécurité a adopté 28 résolutions à l’unanimité et parfois sans véritable débat. Seuls deux projets de résolution, présentés entre autres par les Etats-Unis, n’ont pas été adoptés du fait du vote contre de la Russie. Ils concernaient tous les deux la situation en Syrie sur laquelle les positions des occidentaux et de la Russie sont pour l’instant inconciliables. Par contre, l’unanimité a été obtenue sur la lutte contre le terrorisme, sur les opérations de paix que les Etats-Unis souhaitent beaucoup plus efficaces mais également sur la question nucléaire posée par la Corée du Nord, même si Washington souhaite un engagement beaucoup plus important de la Chine. Alors que le président Trump mais également Mme Haley ont envisagé à plusieurs reprises la possibilité pour les Etats-Unis d’agir unilatéralement avec des moyens militaires, il semble cependant que le choix entre unilatéralisme et multilatéralisme mais également diplomatie et emploi de la force ne soit pas tranché."
"Le juge de l’exécution n’est pas compétent pour connaitre de la demande de nullité de l’acte de signification d’une injonction de payer européenne, déclarée entre-temps exécutoire par la juridiction de l’État membre d’origine à défaut d’opposition formée par le défendeur dans les conditions prévues par le règlement (CE) n°1896/2006."
La sanction de la péremption prévue par les articles R. 321-20 et R. 321-21 du code des procédures civiles d’exécution s’applique à l’ordonnance du juge-commissaire ordonnant la vente de l’immeuble d’un débiteur en liquidation judiciaire par adjudication judiciaire. Le juge de l’exécution est compétent pour statuer sur une demande de prorogation des effets d’une telle ordonnance.
Une demande de dommages-intérêts fondée sur l’exécution dommageable d’un avis à tiers détenteur entre dans la compétence du juge de l’exécution. Elle ne relève pas des dispositions de l’article L. 281 du LPF et n’est donc pas soumise à la demande préalable auprès de l’administration fiscale prévue à l’article R. 281-1 de ce code.
"Les présidents des régions italiennes sont des acteurs privilégiés de la vie politique transalpine. Ils le sont devenus avec la réforme constitutionnelle n° 1 de 1999 et l’introduction du suffrage universel direct pour leur élection (article 122 C.) mais également avec la réforme du titre V de la seconde partie de la Constitution en 2001. Le renforcement des compétences des régions mais également, de manière générale, de leur autonomie a donné naissance à ceux que l’on appelle dorénavant les « Gouverneurs » en référence à leurs homologues américains. Les portraits présentés ici montrent combien cette fonction, très influente au niveau régional, l’est également au niveau national ; tant pour le lien entre les différents niveaux d’administration que pour la carrière politique des governatori…"
"Le contentieux de l’impayé occupe une place importante dans les rapports personnels de l’individu, civils ou commerciaux. Son omniprésence devant l’ensemble des juridictions, est de nature à perturber non pas seulement la santé de l’économie, mais également celle des mécanismes juridiques. D’emblée, l’analyse du contentieux de l’impayé en France et au Maroc nous renvoie à observer une décrue et un accroissement respectifs, de ce type de contentieux devant les juridictions compétentes ces dernières années. En effet, ce type de contentieux ne cesse de s’accroitre devant l’ensemble des juridictions Marocaine, de plus en plus encombrées par ce contentieux et qui subissent avec résignation un phénomène d’impayé explosif. Cette hausse des affaires liées à l’impayé n’est pas un phénomène de circonstances, et n’est pas lié à la crise économique actuelle. Elle résulte d’un accroissement sans précèdent de l’impayé, et un mode de recouvrement qui s’appuie inconditionnellement sur la justice étatique. En France, La situation est bien différente. En effet, la baisse du contentieux de l’impayé devant les juridictions civiles et commerciales, ne résulte point d’un hasard, ou de causes purement accidentelles. Elle résulte vraisemblablement d’une relation synergique d’un ensemble de facteurs. Il s’agit d’une conjonction de plusieurs dispositions législatives, de procédés et de techniques qui ont contribué à cette baisse. Par ailleurs, L’exploration de l’évolution positive ou négative du contentieux de l’impayé ne constitue pas uniquement un moyen idoine pour appréhender la genèse et le fondement de l’impayé, elle constitue également un indicateur fiable de l’approche systémique adoptée en matière de recouvrement de créances dans les deux pays."
"Une fois de plus, le résultat des élections italiennes des 24 et 25 février dernier a montré un visage que l’on connaît déjà trop de l’Italie et de la politique italienne. Pourtant, peu d’éléments dans ces résultats peuvent surprendre l’observateur assidu de la vie politique transalpine. L’organisation anticipée de ces élections était déjà un signe avant-coureur du malaise qui régnait au sein des institutions. Pour rappel, le 22 décembre dernier, le Parlement italien était dissout après l’adoption du budget pour l’année 2013 et Mario Monti démissionnait de ses fonctions. Le président du Conseil avait été contraint à une telle décision par la « fronde » des parlementaires du PdL initiée début décembre : boycott de votes de confiance au gouvernement ou encore abstention sur le vote du budget..."
"Au-delà même de l’intitulé de ce travail, qui mérite d’être relativisé s’agissant du cas français, l’objectif de cette étude était d’étudier, dans une perspective globale, les rapports entretenus par le Conseil constitutionnel avec les droits étrangers. Il s’inscrivait ainsi dans une perspective nouvelle d’application de certaines analyses élaborées dans le cadre du travail doctoral. Au regard de certains spécificités du contentieux constitutionnel français, tenant notamment au style rédactionnel des décisions, l’approche retenue ne s’est pas limitée à l’étude de la jurisprudence mais a intégré également le discours des juges, leurs carrières (ont-ils étudié à l’étranger, maîtrisent-ils des langues étrangères, etc.) ainsi que leur rapport avec la doctrine, notamment en ce qui concerne la question spécifique du recours aux solutions juridiques étrangères. Il convenait également d’aborder les spécificités du système juridique français, comme le contexte d’adoption de la Constitution et la tradition juridique (en intégrant également les techniques de raisonnement du Conseil, son rapport général à la jurisprudence internationale, etc.) afin d’évaluer le degré de propension à la comparaison du Conseil. L’évaluation de cette sensibilité aux droits étrangers devait également tenir compte des moyens mis à disposition des membres du Conseil et a permis de montrer que certaines influences implicites ainsi que l’importation de certaines techniques décisionnelles pouvaient être identifiées."
"Des questions de la vie quotidienne sont susceptibles d’amener le juge à se prononcer sur la nature d’un élément a priori « simple » du quotidien. Ce fut notamment le cas de la tomate. La Cour suprême des Etats-Unis a été confrontée à cette épineuse question il y a déjà plus de cent ans. Dans une décision Nix v. Hedden , les juges de la Cour suprême ont dû contrôler la conformité à la Constitution d’une loi douanière du 3 mars 1883 qui mettait en place une taxe de 10 % sur l’ensemble des légumes importés aux Etats-Unis. De facto, elle ne s’appliquait pas aux fruits. Le receveur des douanes de l’Etat de New York a estimé que la tomate était un légume, ce qui permettait une augmentation notable des recettes. Cette décision, qui a constitué le point de départ de l’étude, a permis de s’interroger sur l’autorité des dictionnaires dans l’interprétation mais également sur le rapport du juge avec la langue vernaculaire, notamment par la prise en compte de la perception sociale de l’objet du litige. En effet, dans l’opinion majoritaire de la Cour, Justice Horace Gray, après avoir constaté que la tomate était bien un fruit (en se fondant notamment sur la définition du Webster), concluait toutefois que puisqu’elle était consommée essentiellement en plat de résistance, devait être considérée comme un légume par la Cour. Cette décision montre bien les difficultés à interpréter le « réel » et les limites de la taxinomie dans le cadre de l’interprétation."
"L’Amérique latine constitue, sous bien des aspects, un véritable laboratoire constitutionnel. Ce constat se vérifie également en matière de développement de la justice constitutionnelle. Après le droit de chaque citoyen à présenter des réclamations contre les violations des droits fondamentaux dans le décret constitutionnel pour la liberté de l’Amérique mexicaine de 1814 ou le droit de contester les actes règlementaires ou législatifs des pouvoirs publics dans la Constitution de Yucatán de 1841, tous deux inspirés par les écrits de Sieyes et l’idée de défense politique de la constitution, c’est le modèle du judicial review des Etats-Unis qui a influencé les premières configurations de justice constitutionnelle sur le continent. Ces dernières décennies, c’est davantage le modèle dit « européen », fondé notamment sur un contrôle concentré et abstrait qui a connu une influence croissante. Aujourd’hui, l’Amérique latine connaît des juridictions constitutionnelles indépendantes du pouvoir judiciaire, d’autres qui y sont soumises, des chambres constitutionnelles au sein de cours suprêmes et des juridictions ordinaires compétentes pour exercer un contrôle de constitutionnalité. Ce melting-pot mimétique a permis l’émergence de systèmes mixtes mais également de procédures (amparo, habeas corpus, habeas data, etc.) et d’institutions spécifiques (Tribunal qualificateur des élections). Cette autonomisation des modalités de la justice constitutionnelle a conduit, notamment par l’intermédiaire du perfectionnement de la protection des droits fondamentaux, à l’apparition d’un modèle qualifié aujourd’hui de « néoconstitutionnalisme »."
"Là se situe le cœur du débat italien : faut-il reconnaître dans l’objecteur de conscience fiscale une motivation déterminée par sa conscience personnelle et, dans la suite logique, un statut légal d’objecteur de conscience ? Mais le verbe ne commande pas la forme, autrement dit le fait de se baptiser « objecteur de conscience fiscale » ne fait pas automatiquement du dénommé un objecteur de conscience au regard du droit positif."
"L’objet de cette étude est de mettre en avant le débat sur la délinquance des mineurs ainsi que le système judiciaire français et marocain mis en place afin de contrecarrer cette délinquance.La délinquance poursuivie par les forces de l’ordre et sanctionnée par la justice est caractérisée par le droit pénal. Lorsque le droit connait des changements, le champ de la délinquance expérimente des oscillations et par voie de conséquence, l’enregistrement des comportements délictueux aussi. Or, la croissance de la délinquance, et particulièrement celle des mineurs, s’analyse en fonction de son environnement juridique. Dans cette mouvance et bien que la délinquance des mineurs évolue dans les mêmes proportions et au même degré que celle des majeurs et bien qu’elle soit sanctionnée plus sévèrement, elle demande une attention particulière précisément, parce qu’il s’agit de mineurs.Dès lors, le rôle de la justice des mineurs ne doit pas se limiter uniquement à la répression. Cette dernière doit se donner les moyens de les comprendre pour être capable d’agir sur ce qui les a motivés et empêcher toute récidive. Sa mission doit avoir également un rôle « éducatif » et « préventif ».Sanction et éducation sont devenues ainsi deux dimensions indissociables dans le traitement de la délinquance des mineurs. Et c’est dans cette optique que le législateur français et marocain essaye de construire une politique de traitement de la délinquance tout en respectant la personnalité juridique fragile du mineur."
Proclamation internationale de la Charte du Droit du Vivant In support of and in partnership with the United Nations Harmony with Nature Programme
"La loi n°2015-177 du 16 février 2015, modifiant le Code civil, a défini les animaux comme des « êtres vivants doués de sensibilité », marquant une évolution du droit français. Ne bénéficiant ni de la personnalité juridique, ni d’un régime juridique nouveau, les animaux demeurent, sous réserve des lois qui les protègent, soumis au régime des biens. L’objectif du colloque du 29 mars 2018 est double : -Démontrer que l’animal de compagnie est apte à recevoir la personnalité juridique. -Proposer, de manière concomitante, la reconnaissance de la catégorie de personne (physique) non humaine et la création d’un régime propre. Mots clés : Personnalité juridique, Animal de compagnie, Catégorie juridique, Personne (physique) non humaine, Doctrine, Statut de l’animal de compagnie, Régime juridique, Objet ou Sujet de droit, Proposition législative"
"Sous l’Ancien Régime, il est classique de comparer le pouvoir du père au pouvoir du roi, le roi endossant alors une figure particulière à l’égard de sa propre famille (son frère biologique doit, par exemple, être considéré comme son fils symbolique). Ainsi se trouve tracé le lien entre la famille et l’Etat. Dans cette lignée, on a coutume de dire (et les développements contemporains sur la formation de l’Etat relayent cette opinion) que la famille est la première cellule de l’Etat. Les Politiques eux-mêmes insistaient pour faire remonter la monarchie à la cellule familiale. Cependant, une évolution a lieu au début du XVIIème siècle, écartée par l’historiographie, ce que cette contribution a vocation à démontrer. Une lente et minutieuse enquête lexicographique a dû être menée. Elle était certes compliquée dans la mesure où les réflexions des juristes de Louis XIII et de Richelieu sont très dispersées dans l’ensemble de l’œuvre. Pour autant, elle ne fut pas moins riche de résultats. Il ressort que les juristes n’envisagent plus la famille comme le premier maillon de la chaîne. Or, si la famille est désormais absente, il n’y a que trois possibilités à envisager : ou bien l’Etat est un tout formé par une masse indéfinie, ou bien l’Etat est la somme d’agrégats individuels, ou bien il est la combinaison de plusieurs corps. C’est manifestement la troisième hypothèse qui est retenue, faisant le trait d’union entre l’Etat bodinien fondé sur la famille et le Léviathan de Hobbes qui englobe des individus sujets. Ce ne sont donc plus des familles mais des individus regroupés dans des corps qui composent l’entité étatique."
"Schéma présenté à l’occasion du premier volet de la trilogie des colloques et ouvrages sur la personnalité juridique de l’animal, programme de recherche français initié au sein de l’Université de Toulon. Voir également : scheme 2.0, scheme 3.0, “Evolution of the concept of the legal personality – Proposal emerged from the trilogy on the legal personality of animal”"
"Schéma présenté à l’occasion du deuxième volet de la trilogie des colloques et ouvrages sur la personnalité juridique de l’animal, programme de recherche français initié au sein de l’Université de Toulon. Voir également : schéma 1.0, schéma 3.0, et « Evolution de la notion de personnalité juridique – Proposition issue de la trilogie sur la personnalité juridique de l’animal »"
"Le droit serait-il schizophrène ? Comment peut-on être une chose et son contraire ? Comment est-il possible d’être à la fois objet et sujet ? Ce sont les questions qui se posent lorsqu’on analyse la manière dont le droit français appréhende l’animal. La loi n°2015-177 du 16 février 2015 définit les animaux comme des « êtres vivants doués de sensibilité » tout en les soumettant, « sous réserve des lois qui les protègent » au régime des biens. En d’autres termes, juridiquement, l’animal, pourtant reconnu comme vivant et sensible, est traité d’une certaine manière comme un simple meuble. C’est là une contradiction du droit des plus surprenantes : l’animal est à la fois sujet et objet. Bien évidemment, ce statu quo est intenable et il conviendra d’y remédier dans les meilleurs délais. Le remède le plus pertinent consiste à reconnaître la personnalité juridique aux animaux. Ce faisant, ils basculeraient dans la catégorie des personnes en tant que personnes physiques non-humaines et ils bénéficieraient d’un régime spécifique. La summa divisio héritée de notre droit romain distinguant les personnes et les biens serait préservée et le droit des animaux trouverait sa cohérence. Ce mémoire vise à fonder cette proposition juridique en montrant pourquoi elle est non seulement possible mais aussi souhaitable. Les données scientifiques, les recherches historiques et la dynamique internationale : tout converge pour offrir aux animaux ce statut de personne physique non-humaine."
"Doctrine en faveur de la personnalité juridique de l'animal, défendue par Caroline Regad et Cédric Riot, enseignants-chercheurs, Faculté de droit, Université de Toulon."
"Remise à Madame Valérie GOMEZ-BASSAC, Députée de la 6e circonscription du Var, d’une proposition de rédaction d’un texte de loi en faveur de la reconnaissance de la personnalité juridique des animaux de compagnie. Ce document est confidentiel."
"Premier ouvrage de droit français consacré explicitement au thème de la personnalité juridique de l’animal, ce livre pose les bases d’une personne physique non-humaine. Avec La personnalité juridique de l’animal, une nouvelle page du droit des animaux est désormais écrite. En effet, face à l’incohérence d’un droit qui range toujours les animaux, pourtant « êtres vivants doués de sensibilité », parmi les choses, les auteurs suggèrent de refondre la catégorie des personnes. Dans cette perspective, les contributions rassemblées visent, dans un premier temps, à fonder et, dans un second temps, à proposer la création d’une nouvelle catégorie de personne dotée d’un régime spécifique. Actuellement, le droit différencie les personnes physiques et les personnes morales. Il est envisagé d’intégrer les animaux aux personnes physiques, en distinguant les personnes humaines et les personnes non-humaines. Ce premier volume traite de ceux qui sont les plus proches de l’homme, c’est-à-dire les animaux de compagnie. Les autres animaux seront abordés dans les prochains volets."
"Schéma révélé à l’occasion du troisième volet de la trilogie des colloques et ouvrages sur la personnalité juridique de l’animal, programme de recherche français initié au sein de l’Université de Toulon, mettant en évidence toute la dimension de la doctrine. Voir également : schéma 1.0, schéma 2.0, et « Evolution de la notion de personnalité juridique – Proposition issue de la trilogie sur la personnalité juridique de l’animal »"
"Schéma révélé à l’occasion du troisième volet de la trilogie des colloques et ouvrages sur la personnalité juridique de l’animal, programme de recherche français initié au sein de l’Université de Toulon, mettant en évidence toute la dimension de la doctrine. Voir également : Scheme 1.0, scheme 2.0, “Evolution of the concept of the legal personality – Proposal emerged from the trilogy on the legal personality of animal”"
Intervention sur l'esprit de la Déclaration de Toulon lors du premier congrès virtuel en droit des animaux qui s'est tenu en Argentine du 25 au 29 mai 2020. Congrès cité en référence au point 44 du rapport de l'ONU Harmony with Nature du 28 juillet 2020.
Intervention sur l'esprit de la Déclaration de Toulon lors du premier congrès virtuel en droit des animaux qui s'est tenu en Argentine du 25 au 29 mai 2020. Congrès cité en référence au point 44 du rapport de l'ONU Harmony with Nature du 28 juillet 2020.
"Souhaitant mieux comprendre la création des Ecoles de droit, de manière systématique, tous les actes normatifs relatifs aux Facultés de droit du XIXème siècle ont été consultés. En effet, ces normes comprennent un certain nombre d’indications, justifiant ce qu’il était « utile » de faire ou pas. Ainsi, dans un premier temps (1804-1870), une méthode « utile » est orientée vers un praticien de l’ordre judiciaire puis, dans un second temps (1870-1905), elle sera destinée à un juriste, en quelque sorte, « tout terrain ». Si certaines questions perdurent en dépit des années (la dictée en cours, le niveau des études, la critique sur la sévérité ou non des examens…), on peut observer un réel changement de dynamique notamment par le biais du droit public. Alors que l’enseignement reposait sur des connaissances élémentaires nécessaires au futur praticien en lien avec la méthode exégétique, progressivement, la méthode dogmatique l’emporte dans la perspective d’un enseignement plus généraliste."
"La loi n°2015-177 du 16 février 2015 a défini les animaux comme des « êtres vivants doués de sensibilité », tout en les maintenant, sous réserve des lois qui les protègent, dans le régime des biens. Face à cette incohérence, les auteurs suggèrent de refondre la catégorie des personnes, au sens juridique, pour y intégrer l’animal. Le droit différencie les personnes physiques et les personnes morales. L’ouvrage propose d’intégrer les animaux dans la catégorie des personnes physiques, en distinguant précisément les personnes humaines et les personnes non-humaines. Cette proposition doctrinale permettrait de doter les animaux, forts d’une personnalité juridique nouvelle, d’un statut cohérent et efficace. La première édition concernait les animaux de compagnie. Il s’agit désormais de s’intéresser aux animaux liés à un fonds lato sensu, c’est-à-dire les animaux de rente, de divertissement et d’expérimentation. Les animaux sauvages seront envisagés dans le prochain volet qui viendra conclure la trilogie sur la personnalité juridique de l’animal."
"La recherche est menée sous l’angle des idées politiques alimentées par des considérations pratiques. Il a fallu brasser sur trois siècles l’ensemble des ouvrages parus sur la question et sélectionner ceux qui seraient pertinents pour la démonstration. Le thème étant riche, tant du point de vue des sources que du point de vue de la bibliographie, la recherche a dû être restreinte aux seuls propos abordant la question de la sécurité juridique. Certes, la sécurité juridique renvoie à l’aspiration à un ordre juridique non aléatoire et non susceptible de changer à tout moment mais il faut aussi que la situation présente soit perçue comme viable, ce qui est rarement le cas en matière d’imposition. La volonté de changement est animée par le sentiment d’impôts trop lourds, trop complexes, trop diversifiés mais, ce faisant, on s’expose à l’insécurité juridique. Et voilà en germe le paradoxe : changer et prendre le risque de l’incertitude ou ne pas changer et s’enliser. Partant, cet article propose de tracer la genèse de cette dynamique paradoxale, encore présente de nos jours, qui lie l’appel au changement et la crainte de l’insécurité juridique. Il s’agit donc de voir comment et pourquoi s’est formé de manière irréversible ce climat anxiogène inhérent à ce paradoxe de l’imposition d’autant que, en définitive, il n’est pas sûr que beaucoup de choses aient vraiment changé."
"La loi n°2015-177 du 16 février 2015 a défini les animaux comme des « êtres vivants doués de sensibilité », tout en les maintenant, sous réserve des lois qui les protègent, dans le régime des biens. Face à cette incohérence, nous suggérons de refondre la catégorie des personnes, au sens juridique, pour y intégrer l’animal. Le droit différencie les personnes physiques et les personnes morales. Nous proposons d’intégrer les animaux dans la catégorie des personnes physiques, en distinguant précisément les personnes humaines et les personnes non-humaines. Cette proposition doctrinale permettrait de doter les animaux, forts d’une personnalité juridique nouvelle, d’un statut cohérent et efficace. La première édition concernait les animaux de compagnie. Il s’agit désormais de s’intéresser aux animaux liés à un fonds lato sensu, c’est-à-dire les animaux de rente, de divertissement et d’expérimentation. Les animaux sauvages seront envisagés dans le prochain volet."
"La limite peut être entendue dans deux sens dont les implications sont différentes, voire opposées. Elle peut être considérée comme un horizon indépassable, un mur infranchissable qui borne très distinctement des domaines d'étude et des champs d'action. Dans une seconde acception, la limite est, au contraire, la ligne qui peut être franchie et par extension, la limite devient amovible : elle peut être déplacée, même légèrement, en fonction des événements. Le thème de ce colloque nous invite non pas à étudier « la limite du droit », comme le suggérerait le premier sens du mot limite. Cette idée n'aurait pas de sens, sauf à considérer un système pur de droit, isolé de tous les autres paramètres qui peuvent graviter autour de lui. Nous sommes, à l'inverse, amenés à nous rendre « Aux limites du droit », dans cette zone d'interférence, de superposition, d'interpénétration non seulement des disciplines théoriques mais aussi des circonstances ponctuelles dont l'enchevêtrement donne à la pratique de la science juridique toute son ampleur. Les limites du droit font donc référence à un lieu de communication et de négociation où la recherche d'un équilibre spontané est autant permanente qu'impossible à pérenniser. En effet, les divers facteurs qui entrent dans les limites du droit sont si riches et pourtant si mouvants qu'il paraît judicieux de tenter, tant que faire se peut, de cerner leur essence. D'autant que les limites sont souvent révélatrices de la nature profonde du droit."
"Partant de la célèbre définition de Bodin selon laquelle « République est droit gouvernement de plusieurs ménages et de ce qui leur est commun, avec puissance souveraine »,on se rappelle que la souveraineté était un élément constitutif de l’Etat. Mais, eu égard au débat contemporain sur la souveraineté, on peut se demander quelles ont été les grandes étapes de l’évolution doctrinale. Formulé autrement, la souveraineté est-elle un critère sine qua non qui préside à la définition de l’Etat ? Un panorama qui veut s’étendre sur plusieurs siècles exige la prise en compte des sources de préférence en provenance de « grands » auteurs : Bodin, le Bret, Hobbes, Rousseau ou encore Carré de Malberg, Jellinek, Kelsen… Ces travaux ont conduit à la conclusion suivante : si dans une analyse traditionnelle, la souveraineté est un véritable élément organique constitutif de l’Etat, dans une approche contemporaine, elle est une notion ébranlée qu’il faut réinventer. Historiquement, la souveraineté a pu être considérée comme le « cœur » de l’Etat, voire son « âme » mais, au début du XXème siècle, elle a subi diverses attaques si bien qu’actuellement, elle est peut-être devenue une notion à renouveler, avec d’éventuels effets sur l’Etat lui-même."
"Actuellement, et dans la plupart des Etats, les animaux restent généralement soumis, avec certaines réserves, au régime des choses. A l’évidence, cette manière de les appréhender juridiquement est un échec puisque la sixième extinction massive a déjà débuté. A l’inverse, en considérant les animaux comme des personnes juridiques, une nouvelle relation plus équilibrée pourrait s’établir avec la Nature. Cette perspective vise à extraire de la catégorie des choses les animaux non-humains dont les intérêts seraient désormais pris en compte. Cette proposition d’attribuer une personnalité juridique aux animaux se fonde sur des arguments de droit, d’anthropologique, d’histoire et d’autres sciences. Ces dernières prouvent que les animaux sont des êtres vivants, sensibles et intelligents. Plus encore, ils possèdent les substrats neurologiques de la conscience, comme l’affirme la Déclaration de Cambridge. Le droit doit en tirer les conséquences et c’est en ce sens que l’alinéa premier de la Déclaration de Toulon dispose que « les animaux doivent être considérés de manière universelle comme des personnes, et non des choses ». Puisqu’il existe des personnes morales (Etats, associations, sociétés…), c’est-à-dire des entités abstraites jouissant de la personnalité juridique, on voit mal pourquoi les animaux demeurent dépourvus d’une telle personnalité. Forts d’une personnalité juridique nouvelle, les animaux basculeraient dans la catégorie des personnes et plus précisément des personnes physiques, avec, d’un côté, les personnes humaines et, de l’autre, les personnes non-humaines, chacun ayant un régime spécifique. Il ne s’agit pas d’accorder des droits équivalents aux uns et aux autres mais de faire entrer les animaux, avec le masque de la personne, dans le grand théâtre du droit. Tout en préservant la summa divisio, héritée du droit romain, entre les personnes et les choses, la création d’une personnalité non-humaine permettrait alors de proposer une solution novatrice et prometteuse pour contrer la menace écologique et l’extinction des espèces. A cet égard, la Charte du Droit du Vivant proclamée le 26 mai 2021 en lien avec le programme Harmony with Nature de l’ONU insiste sur l’évolution nécessaire d’un droit « sur » le vivant à un droit « du » vivant."
"Mesurer l’absolu : voilà toute la force de la pensée imprimée par les juristes de Louis XIII et de Richelieu à la monarchie, eux qui contribuent de manière significative à la construction de l’État par le droit. Les successeurs des Légistes et des Politiques précisent le fondement, le critère et l’étendue de l’absolu qu’ils se doivent désormais de mesurer. La souveraineté est au cœur du processus : elle est déployée sous la forme d’une véritable arme de combat dans une sorte de théologie politique exposée par les juristes. Conscients que dans le cadre de la monarchie mesurée, il faut poser des repères, les juristes du pouvoir fort utilisent le droit de manière instrumentale pour contribuer à l’affermissement du roi et de l’État. Si, a priori, définir, c’est contenir, les juristes se réservent une possibilité d’extension continue des droits, pour peu que la thèse qu’ils défendent l’exige. Pour autant, l’histoire du règne de Louis XIII est bien celle du passage des droits du roi à un droit de l’État."
"Droit des Animaux : Toulon répond à Cambridge Issue de la trilogie des colloques organisés au sein de l’Université de Toulon sur la personnalité juridique de l’animal, la Déclaration de Toulon est conçue comme une réponse par des universitaires juristes à la Déclaration de Cambridge, prononcée le 7 juillet 2012. Cette dernière, proclamée à la fin d’une conférence par des scientifiques de divers horizons, dont Stephen Hawking, affirme que les animaux non-humains possèdent les substrats neurologiques de la conscience. Elle constitue une évolution fondamentale dans la considération des animaux. La Déclaration de Toulon se présente comme la réponse juridique qu'il manquait à la Déclaration de Cambridge. Un événement à ne pas rater, Toulon répond à Cambridge vendredi 29 mars 2019."
Schéma issu de la trilogie sur la personnalité juridique de l'animal illustrant les différentes manières pour le droit d'appréhender le vivant.
"A partir de sources notamment législatives et jurisprudentielles, l'objectif est de montrer que le système juridique parfois se grippe dans les effets qu’il peut produire. Contrairement aux méthodes classiques qui déterminent des classifications de normes en fonction du thème ou des organes qui les créent, cette recherche prend une autre dimension. En effet, le corpus des normes analysées a pour critère les effets éventuellement absurdes desdites normes."
Intervention sur l'esprit de la Déclaration de Toulon lors du premier congrès virtuel en droit des animaux qui s'est tenu en Argentine du 25 au 29 mai 2020. Congrès cité en référence au point 44 du rapport de l'ONU Harmony with Nature du 28 juillet 2020.
"ATTENTION - SCHEMA DE TRANSITION Schéma de transition précédant le dernier chapitre de la trilogie sur la personnalité juridique de l’animal. Voir les autres données de recherche : schémas 1.0, 2.0, 3.0 et l'Evolution de la notion de personnalité juridique - Proposition issue de la trilogie sur la personnalité juridique de l'animal."
"Schéma synthétique reprenant l’évolution de la notion de personnalité juridique telle qu’elle est proposée par la trilogie sur la personnalité juridique de l’animal, programme de recherche français initié au sein de l’Université de Toulon. Voir également : scheme 1.0, scheme 2.0, scheme 3.0"
"Schéma synthétique reprenant l’évolution de la notion de personnalité juridique telle qu’elle est proposée par la trilogie sur la personnalité juridique de l’animal, programme de recherche français initié au sein de l’Université de Toulon. Voir également : schéma 1.0, schéma 2.0, schéma 3.0"
Schéma issu de la trilogie sur la personnalité juridique de l'animal illustrant les différentes manières pour le droit d'appréhender le vivant.
"La loi n°2015-177 du 16 février 2015 a défini les animaux comme des « êtres vivants doués de sensibilité », tout en les maintenant, sous réserve des lois qui les protègent, dans le régime des biens. Nous proposons de refondre la catégorie des personnes afin d’y intégrer l’animal, facilitant la création d’un régime juridique cohérent et efficace. Aux côtés des personnes humaines, la catégorie des personnes non-humaines serait créée. Cette perspective est à nos yeux d’autant plus nécessaire que le droit français doit se mettre en conformité avec l’article 9 de la Déclaration Universelle des Droits de l’Animal du 15 octobre 1978 qui affirme que « La personnalité juridique de l'animal et ses droits doivent être reconnus par la loi. » Le législateur apporterait ainsi les réponses pratiques et théoriques aux nombreuses incohérences de notre droit sur la question des animaux. Cette première édition concerne les animaux de compagnie. Nous élargirons ces réflexions aux autres animaux (d’élevage, etc.) dans les prochains volets."
"Si le Code civil définit les animaux comme des êtres vivants, ceux-ci restent pourtant majoritairement soumis au régime des choses. Cette contradiction pourrait être résolue par la reconnaissance de leur personnalité juridique. Les conférenciers expliqueront pourquoi il s’agit du seul moyen pour le droit des animaux de trouver sa cohérence, et comment y parvenir."
"Par l'édit de Marly, Louis XIV légitime ses bâtards, le duc de Maine et le comte de Toulouse, violant ainsi le principe d'indisponibilité inhérent au statut de la Couronne de France. L'affaire fit grand bruit en son temps, opposant les partisans des princes légitimes aux proches des princes légitimés. Cependant, après la mort du Roi-Soleil, l'édit de 1717 révoque les dispositions de l'édit de Marly « puisque les lois fondamentales de notre royaume nous mettent dans une heureuse impuissance (…) de disposer de notre Couronne même…». On assiste en réalité à une forme de contrôle de constitutionnalité avant l'heure. Pour le prouver, il faut, d'une part, analyser les conditions d'un tel contrôle : l'existence d'une Constitution et la présence d'un gardien. D'autre part, il convient de s'assurer des effets en droit. Ce travail a conduit à s’interroger notamment sur l’organe et sur les modalités du contrôle."
"Schéma présenté à l’occasion du premier volet de la trilogie des colloques et ouvrages sur la personnalité juridique de l’animal, programme de recherche français initié au sein de l’Université de Toulon. Voir également : schéma 2.0, schéma 3.0, et « Evolution de la notion de personnalité juridique – Proposition issue de la trilogie sur la personnalité juridique de l’animal »"
"Considérant que le cyber-juriste est un professionnel du droit (ou en passe de le devenir) ; qu'il tient à s'informer sans contrainte (de temps, de support…) ; qu'il s'intéresse à l'actualité d'autant plus s’il est lui-même concerné ; qu'il doit évoluer dans une sphère virtuelle dont les contours bougent souvent ; qu'il s'exprime et souhaite partager ses réflexions et ses interrogations et qu'il recherche l'interactivité, il en résulte que le blog juridique qui attire (doit) comporte(r) ces éléments. Face à un monde en perpétuelle mutation notamment avec l’arrivée d’Internet, la dématérialisation et la rapidité des communications, la figure du juriste change d’aspect. Les blogs juridiques en sont l’illustration, notamment dans la construction et la diffusion du droit, ce qui, par ricochet, pose la question du statut de la doctrine."
"L’article se propose de montrer que l’anthropocentrisme juridique est actuellement ébranlé par les avancées du zoocentrisme et du biocentrisme. Les cas de personnification juridique d’animaux ou de droits accordés à la Nature se multiplient à travers le monde et ne peuvent plus être ignorés. Nouveau paradigme, nouvelle ère ? Seul l’avenir pourra le confirmer, mais nous n’en sommes peut-être qu’au frémissement d’un mouvement qui va prendre de l’ampleur et qui va s’intensifier."
"La « justice politique » est la combinaison de deux sphères, celle de la justice et celle de la politique, ce qui est d’autant plus étrange que si la première relève théoriquement de l’impartialité, la seconde peut être taxée d’une véritable partialité. Cette tension ne peut être résolue qu’en intégrant le concept de prudence. La « justice politique » est donc la fusion de toutes les dimensions de la justice et de la politique ; elles se manifestent, se révèlent et se subliment l’une par l’autre. Or, la prudence irrigue l’une et l’autre, ramenant à l’humain ces deux sphères. Dans ces conditions, il était intéressant de se demander quel est le rôle que joue la prudence dans la version fusionnée de cette « justice politique » de la première moitié du Grand Siècle. A partir des imprimés et des manuscrits des juristes de Louis XIII et de Richelieu, il ressort que la prudence joue sans cesse un rôle de légitimation. Ainsi, la « justice politique » dans les moyens mobilise de manière permanente la prudence et la « justice politique » dans la fin fait un appel déterminant à ladite prudence. C’est bien ce que ce papier a tenté de mettre en exergue. Il est à préciser que le terme de prudence, dans les écrits des juristes, se rattache d’avantage aux conseillers du roi dans l’exercice d’une justice de type politique qu’à la théorie générale du pouvoir fort."
"En recueillant les textes normatifs et en les conférant avec les réalisations de la période (par exemple le Code Savary, la naissance de la Chambre de commerce, le Bureau de commerce…), l’objectif est de savoir comment l’entité étatique se structure, notamment pour réguler le commerce. Il ressort de cette recherche que le XVIIème siècle est marqué par des réactions de méfiance, a fortiori, parce que l’Etat moderne est en train de se dessiner : il faut à la fois construire mais aussi se protéger. Le XVIIIème siècle s’inscrit dans ce prolongement avec des hésitations. Elles sont, d’une part, de type structurel en ce qui concerne la forme juridico-politique à donner aux relations commerciales. D’autre part, elles sont de type conjoncturel sur la réponse à proposer à la pression des impératifs du commerce international : l’Etat doit-il camper dans une attitude protectionniste ou, au contraire, libéraliser les échanges ? En conséquence, le commerce international peut avoir un impact direct sur la structure étatique, mais l’inverse est possible, quoique plus difficilement réalisable puisque les moyens dont disposent l’Etat régulateur impliquent forcément une action en interne. Il y a là un problème de logiques contradictoires, de tensions inévitables : les impératifs de l’échange viennent se heurter aux exigences de l’autoprotection."
"« Faire sentir, c’est déjà agir », disait Richelieu. Les juristes de Louis XIII et de Richelieu avaient bien conscience de ce ressort du pouvoir. Faire sentir et en tout cas faire croire, voilà en substance le pouvoir car cela permet de pouvoir c’est-à-dire, au sens large, avoir la capacité formelle et matérielle de faire, de ne pas faire ou de faire faire quelque chose. Appliqué au système qui émerge progressivement, c’est donner non seulement une autorité morale à une entité gouvernante mais aussi les moyens de son action, les outils, notamment juridiques, qui seront le prolongement de sa puissance. Les écrits des juristes de Louis XIII et de Richelieu, mis en lumière avec une bibliographie récente issue des facultés de droit et de lettres, ont été les matières premières de ce travail. La question est la suivante : quelles sont les stratégies de légitimation du pouvoir mises en œuvre par les juristes de Louis XIII et de Richelieu ? De manière générale, les juristes s’attellent à créer un imaginaire propre au pouvoir, à proposer une théologie politique, à trier les informations, à donner une analyse clé en main, à marteler des idées, à vouloir influer sur la pratique, à s’assurer de la légitimité du pouvoir, à garantir la bonne réception de celui-ci. De ce point de vue, la symbolique est forte et l’attention des juristes se porte, entre autres, sur les manières d’exploiter au mieux tous les événements exceptionnels qui rythment la vie d’une monarchie : du sacre aux funérailles en passant par les mariages royaux, l’entrée des villes ou des lits de justice. Conscients de l’intérêt de bien réfléchir sur les formes, les juristes de Louis XIII et de Richelieu se sont appliqués à mettre en œuvre ce principe dans la rédaction de leurs œuvres."
"L’objectif est de considérer la mise en scène du droit dans la fiction, peu importe le type de fiction retenue. L’idée est de s’avancer sur un terrain assez peu exploré : celui du manga. Le choix s’est porté sur celui de Fairy Tail qui connaît à l’heure actuelle un grand succès, notamment au Japon. Alors que le droit n’est pas explicitement le thème des plus de 420 chapitres publiés, je souhaitais essayer de (re)construire le système juridique du monde qui y est décrit. Or, la manière d’appréhender le droit des Japonais ne correspond guère à celle d’un Occidental. Il a fallu en tenir compte en décidant d’incorporer, en quelque sorte, de la fiction à la fiction : en faisant « comme si » la société décrite s’inscrivait dans le modèle occidental. Partant, les règles de droit public et de droit privé sont relevées de manière systématique pour le corpus et sont intégrées à l’intérieur d’un système juridique forgé pour l’occasion."
"Dès lors que le chercheur va puiser à la source, il découvre quatre critères de définition de la raison d’Etat : la contravention à la loi, l’objectif de primauté de l’intérêt public, la possibilité assumée d’écraser le particulier, le tout entouré de l’aura du secret. Le secret et l’intérêt de l’Etat sont les critères les plus importants. Les moyens sont donc proportionnés à la fin dans l’objectif de la conservation du corps. La raison d’Etat présente à la fois un versant négatif et positif. Le côté négatif de la raison d’Etat se loge dans la violence assumée. Le côté positif consiste en la réalisation d’un nouvel ordre rationnel entre les hommes, l’ordre rationnel du Léviathan. Avec la raison d’Etat, l’adage « la fin justifie les moyens » est mis à l’honneur. Quitte à écraser le particulier, la raison d’Etat s’active officiellement pour préserver l’intérêt général. Sa force est également sa faiblesse. Elle est basée sur le secret et s’articule sur deux principe : autonomie de la décision, hétéronomie des conditions de sa mise en œuvre. Autrement formulé : opacité de la source, transparence dans la mise en scène. Il a donc fallu se replonger au cœur des grands procès politiques de l’époque : celui du maréchal d’Ancre, celui des Marillac ou encore celui de De Thou. Or, tout bien considéré, le secret présente deux facettes : il peut être à visée inoffensive dès lors qu’il relève de l’omission ou bien il peut être à visée offensive, voire agressive dès lors qu’il travestie une situation, ce que le maquillage juridico-politique ne manque pas de (tenter de) masquer. Ce faisant, il faut se demander comment est utilisé le secret dans le cadre de la raison d’Etat. Si le secret est nécessaire à la raison d’Etat parce qu’il offre le salut à un vaste ensemble, il est également parfois perverti, ouvrant ainsi les affres de la raison d’Etat. De la raison d’Etat comme moyen théorique d’utiliser le secret en vue de la conservation de l’ensemble (secret inoffensif a priori de l’omission), on passe donc à la raison d’Etat pragmatique puis aux dérives pratiques qu’elle peut engendrer (secret agressif de manipulation). Il s’agit donc de considérer la genèse de la raison d’Etat sachant que les deux aspects précédemment relevés du secret peuvent se combiner."
"Schéma présenté à l’occasion du deuxième volet de la trilogie des colloques et ouvrages sur la personnalité juridique de l’animal, programme de recherche français initié au sein de l’Université de Toulon. Voir également : scheme 1.0, scheme 3.0, “Evolution of the concept of the legal personality – Proposal emerged from the trilogy on the legal personality of animal”"
Illustration du buisson de la vie à l'origine du droit du vivant proposé par la trilogie des colloques et ouvrages universitaires sur la personnalité juridique de l'animal.
Schéma issu de la trilogie sur la personnalité juridique de l'animal illustrant les différentes manières pour le droit d'appréhender le vivant.
Illustration du buisson de la vie à l'origine du droit du vivant proposé par la trilogie des colloques et ouvrages universitaires sur la personnalité juridique de l'animal.
Intervention sur l'esprit de la Déclaration de Toulon lors du premier congrès virtuel en droit des animaux qui s'est tenu en Argentine du 25 au 29 mai 2020. Congrès cité en référence au point 44 du rapport de l'ONU Harmony with Nature du 28 juillet 2020.
"Dès lors que le chercheur va puiser à la source, il découvre quatre critères de définition de la raison d’Etat : la contravention à la loi, l’objectif de primauté de l’intérêt public, la possibilité assumée d’écraser le particulier, le tout entouré de l’aura du secret. Le secret et l’intérêt de l’Etat sont les critères les plus importants. Les moyens sont donc proportionnés à la fin dans l’objectif de la conservation du corps. La raison d’Etat présente à la fois un versant négatif et positif. Le côté négatif de la raison d’Etat se loge dans la violence assumée. Le côté positif consiste en la réalisation d’un nouvel ordre rationnel entre les hommes, l’ordre rationnel du Léviathan. Avec la raison d’Etat, l’adage « la fin justifie les moyens » est mis à l’honneur. Quitte à écraser le particulier, la raison d’Etat s’active officiellement pour préserver l’intérêt général. Sa force est également sa faiblesse. Elle est basée sur le secret et s’articule sur deux principe : autonomie de la décision, hétéronomie des conditions de sa mise en œuvre. Autrement formulé : opacité de la source, transparence dans la mise en scène. Il a donc fallu se replonger au cœur des grands procès politiques de l’époque : celui du maréchal d’Ancre, celui des Marillac ou encore celui de De Thou. Or, tout bien considéré, le secret présente deux facettes : il peut être à visée inoffensive dès lors qu’il relève de l’omission ou bien il peut être à visée offensive, voire agressive dès lors qu’il travestie une situation, ce que le maquillage juridico-politique ne manque pas de (tenter de) masquer. Ce faisant, il faut se demander comment est utilisé le secret dans le cadre de la raison d’Etat. Si le secret est nécessaire à la raison d’Etat parce qu’il offre le salut à un vaste ensemble, il est également parfois perverti, ouvrant ainsi les affres de la raison d’Etat. De la raison d’Etat comme moyen théorique d’utiliser le secret en vue de la conservation de l’ensemble (secret inoffensif a priori de l’omission), on passe donc à la raison d’Etat pragmatique puis aux dérives pratiques qu’elle peut engendrer (secret agressif de manipulation). Il s’agit donc de considérer la genèse de la raison d’Etat sachant que les deux aspects précédemment relevés du secret peuvent se combiner."
Illustration du buisson de la vie à l'origine du droit du vivant proposé par la trilogie des colloques et ouvrages universitaires sur la personnalité juridique de l'animal.
"Dossier de presse en partenariat avec Le Monde. 8e journée des sciences sociales, Sociétés en danger – Menaces, peurs, perceptions, savoirs, réactions, résiliences, Fondation des Sciences Sociales, Sciences Po Paris. https://fondation-sciences-sociales.org/wp-content/uploads/2020/11/Dossier-de-presse-Sociétes-en-danger-2.pdf"
La seule qualité de membre d’un groupement agricole d’exploitation en commun (GAEC) ne suffit pas à exclure une personne du champ d’application des dispositions du code de la consommation relatives au surendettement des particuliers.
"Référendum et démocratie sont au cœur de tant d'écrits... comment, à nouveau "" informer et aller plus loin "" ? Gustavo Zagrebelsky et Massimo Luciani sont allés "" plus loin "" et nous ont livré leurs réflexions. Ce nouveau numéro des Cahiers du C.D.P.C. est l'occasion de présenter une traduction inédite de La difficile democrazia, un essai publié en 2010, par le Professeur et Président émérite de la Cour constitutionnelle italienne. En rapport direct avec cette même démocratie, le Professeur Massimo Luciani introduit une réflexion théorique sur l'institution référendaire et sur le rapport entre démocratie représentative et institutions de participation populaire. Parce que la démocratie, comme le référendum, nécessite un apprentissage et donc du temps, c'est dans cette perspective que les jeunes chercheurs du C.D.P.C. ont placé leurs travaux lors des troisièmes Doctoriades qui se sont déroulées en 2011, dans le cadre des Journées de la Jeune Recherche de l'École doctorale "" Civilisations et sociétés euro méditerranéennes et comparées "" de l'Université du Sud Toulon-Var. Une fois de plus, ce numéro 10 des Cahiers, est fidèle au souhait du fondateur du C.D.P.C., en proposant une information utile et "" en allant plus loin ""."
"Le mythe de l’unité de « La Gauche » est historiquement invalidé tant les périodes éphémères de rapprochement de ses composantes contrastent avec l’affrontement fratricide qui les caractérisent le plus fréquemment. L’observation contemporaine des gauches italienne et française en fournit une parfaite illustration. Les options libérales de Renzi et de Hollande sont contestées non seulement par d’autres mouvements situés à leur gauche, mais aussi dans leur propre formation politique. Cette lutte entre deux acceptions antithétiques « des gauches » résulte d’une querelle qui s’est historiquement cristallisée en 1848, quand deux conceptions de la République se faisaient face. 1848 est certainement la date de naissance des gauches en ce qu’elle fait pour la première fois émerger la question sociale. Pour les révolutionnaires, la République se devait d’être « sociale », pour les modérés, elle ne devait être, pour l’heure qu’Institution. Cette dissymétrie entre ces deux conceptions antithétiques de la République est la matrice de la division structurelle des gauches."
"Régulièrement, certains groupes de pression français réclament la création d’une taxe comportementale dite « Robin des Bois », portant sur les transactions financières. L’idée est tout aussi régulièrement rejetée par les responsables politiques, craignant d’alourdir un peu plus la situation économique fragile de la France. En Italie, le Gouvernement a accepté de tenter l’expérience, validée par le législateur."
"Dans le sillage de la Révolution française et de l’Empire naquit, dans la partie septentrionale de la péninsule, l’idée de l’unité italienne, arrimée, comme dans l’Hexagone, à la sécularisation partielle des biens de l’Eglise. L’anticléricalisme révolutionnaire et plus tardivement « risorgimental » constitue un autre point de jonction entre les deux pays latins. En France, les bases de la conservation et de la préservation du patrimoine furent jetées dès la révolution et l’Empire, puis concrétisées sous la IIIe République. L’Italie connut, elle, des chemins différents, puisque ce furent les Etats pontificaux qui, au début du XIXe siècle légiférèrent en la matière. Néanmoins, la logique anticléricale et libérale des acteurs du Risorgimento explique le désintéressement du Royaume nouvellement à l’égard du patrimoine cultuel italien. C’est paradoxalement sous la période fasciste que furent prises, à l’échelle nationale, les mesures les plus importantes. L’accumulation de ces différentes scansions, fruit de l’Histoire tourmentée de la péninsule, explique une réglementation complexe, tranchant avec la relative simplicité de la législation française. La succession des différentes histoires italiennes, celle des Etats pontificaux, du Royaume des Deux-Siciles, des campagnes révolutionnaires de Bonaparte et des régimes qu’il installa, du Risorgimento, du fascisme puis de la République brossent, par strates, une législation aussi raffinée que complexe que cette étude entend exposer."
"Cet article a pour objet de déterminer la figure actuelle du juge au sein des systèmes de justice pénale français et anglais. L’étude qui consiste à présenter et à identifier, dans ses grandes lignes, les missions et rôles des juges français et anglais, insiste sur l’évolution de leurs fonctions respectives. A partir d’exemples précis, l’auteur met en exergue l’atténuation des divergences originelles - fondées sur la traditionnelle et désormais désuète dichotomie accusatoire/inquisitoire - en ce qui concerne les attributions des juges français et anglais."
"L’analyse des procédures de reconnaissance préalable de culpabilité en droit français et en droit anglais peut, de prime abord, sembler poser un certain nombre de difficultés. En effet les modèles inquisitoire et accusatoire de justice pénale, de par leurs spécificités, ne paraissent se prêter que difficilement à une telle étude comparée. Cependant cette affirmation n’est plus à l’heure actuelle, en raison notamment de la construction européenne, entièrement exacte. Les modèles de justice pénale dans la plupart des Etats de droit ne répondent plus à la dichotomie initialement posée et tendent progressivement à converger vers un modèle commun au sein de procédures pénales mixtes fondées essentiellement sur les principes du contradictoire et du procès équitable. C’est dans ce contexte qu’il convient de se pencher sur la notion de reconnaissance préalable de culpabilité. Celle-ci tend à s’affirmer au gré des réformes législatives, devenant ainsi un véritable outil de politique criminelle visant à davantage d’efficacité de la procédure pénale, par une plus grande célérité dans le traitement des affaires pénales. Toutefois, en France comme en Angleterre, le recours croissant aux procédures de reconnaissance préalable de culpabilité nécessite une modification structurelle et organisationnelle de la justice pénale. En conséquence et s’agissant de l’évolution globale des systèmes de justice pénale, une tendance générale se profile érigeant l’autorité des poursuites au rang de pivot central du processus judiciaire. Des garanties doivent donc être offertes afin de conserver une procédure pénale d’équilibre, à la fois efficace et légitime."
"En a-t-on fini avec la sexualité ? Tel aurait pu être le voeu principal du colloque qui les 13, 14 et 15 novembre 2008 a fait l'objet de trente communications caractérisées par leur variété disciplinaire (sciences, histoire, droit, théologie, sociologie, littérature, cinéma...) et la diversité d'origine des intervenants (France, Maroc, Espagne, Italie...). En effet, la sexualité, aujourd'hui réprimée dans certains pays ou tolérée dans d'autres, pourrait-elle être simplement perçue dans une indifférence libératrice ? C'est aux conférenciers qu'il est revenu d'y répondre au cours de journées qui se sont articulées autour de deux volets : * Sexualités et moralités, * Sexualités et libertés. Ce colloque organisé par le Doyen Pierre Sanz de Alba et placé sous l'égide de l'Ecole doctorale de l'USTV "" Sociétés et civilisations euro-méditerranéennes et comparées "" a rencontré un large succès. La qualité des contributions, l'intérêt qu'a suscité le thème, les débats fructueux auxquels il a donné lieu ont conduit naturellement le Doyen Pierre Sanz de Alba à prendre l'initiative, en 2009, de la publication des Actes, dans cet ouvrage dirigé aujourd'hui par Maryse Baudrez et Thierry Di Manno, directeurs du CDPC JEANCLAUDE ESCARRAS, UMR-CNRS 7318. Cette publication marque une nouvelle étape dans la valorisation de travaux pluridisciplinaires menés sur les civilisations et les sociétés euroméditerranéennes et comparées par des unités de recherche de l'Université du Sud Toulon-Var en Droit , Economie, Gestion, Lettres, Sciences de l'Information et de la Communication."
"La réforme de la saisine du Conseil constitutionnel a fait l'objet de nombreuses contributions. Néanmoins, les chercheurs du CDPC JEAN-CLAUDE ESCARRAS ne pouvaient manquer de consacrer, dans les Cahiers du CDPC aussi, quelques pages à cette réforme tant elle est largement inspirée de la question incidente italienne La communicabilité entre les systèmes juridiques français et italien a donc progressé. Cette communicabilité démontrée par le fondateur du CDPC qui appelait de ses vœux l'introduction du contrôle a posteriori de constitutionnalité des lois en France... Ce numéro 9 des Cahiers du CDPC met aussi à l'honneur, dans sa seconde partie, les contributions des jeunes maîtres de conférences, docteurs et doctorants, français et italiens, qui ont uni leurs réflexions pour apporter des éléments de réponse à la question : "" Existe-t-il un patrimoine euro-méditerranéen ? ""."
"Cette critique du Gouvernement Monti (novembre 2011 – avril 2013), exclusivement composé de techniciens, qui préluda à la grande coalition gauche/droite, repose sur le postulat selon lequel un gouvernement apolitique illustre le dernier acte de la démission du politique, après le triomphe de la théorie de la représentation et la consécration du parlementarisme rationalisé. C’est éloigner plus encore et, peut-être, définitivement, les citoyens de la décision politique, puisque l’expertise se substitue au choix, l’unilatéralité de la décision à son alternative possible et c’est, de surcroît, favoriser très probablement la montée des forces extrêmes."
"La question soulevée était de savoir si était compatible avec le principe de libre prestation des services une loi d’un État membre de l’Union interdisant sur le territoire national la retransmission télévisée de manifestations sportives qui se déroulent dans d’autres États membres, lorsque ces retransmissions montrent des panneaux publicitaires vantant des produits dont le premier État interdit la promotion à la télévision. Le problème en l’espèce concernait la promotion de boissons alcooliques et portait sur la légitimité de la loi française, dite loi Évin, au regard du droit communautaire. La question de la compatibilité de cette législation avec le traité CE était soulevée depuis longtemps et n’était toujours pas résolue. On se souvient que les dispositions françaises avaient conduit Antenne 2 à ne pas diffuser, en 1995 déjà, la rencontre de rugby du Tournoi des cinq nations Irlande contre France. Pourtant, l’internationalisation croissante de la publicité, l’engouement pour les manifestations sportives et leur part croissante dans les programmes télévisuels imposaient de trouver une solution. La Cour a estimé que la question posée était irrecevable en la forme. Toutefois, cet article suggère des éléments de réponse. Sur le fond, ils peuvent certes apparaître comme provocateurs. Toutefois, d’un point de vue juridique, le but est d’articuler principes communautaires et dispositions nationales et de rappeler que les règles internes, restrictives de la liberté de prestation des services, doivent respecter les critères de nécessité et de proportionnalité posés par la juridiction de Luxembourg. De plus, si les traités communautaires admettent l’existence de règles nationales plus strictes, ces dernières ne peuvent a priori s’appliquer qu’à l’égard des seuls diffuseurs nationaux (principe de la discrimination à rebours). Le 13 juillet 2004, dans le cadre d’une affaire qui opposait la même société Bacardi-Martini à TF1, une solution différente a été rendue par la juridiction communautaire qui a jugé la loi Évin compatible avec le droit communautaire (affaire C-249/02). La CJCE a retenu la légitimité de la restriction à la libre prestation des services, en affirmant purement et simplement l’influence sur la santé de la publicité pour les boissons alcooliques, sans renvoyer à des critères scientifiques et en inversant indubitablement la charge de la preuve. Se pose alors la question de savoir si l’on peut accepter que de telles mesures de protection de la santé publique dépendent des autorités nationales et soient aussi diverses qu’inégales."
"La directive communautaire n° 2008/115/CE, dite directive « retour », met en place des normes communes aux pays membres de l’UE, afin de mener une politique plus protectrice des ressortissants de pays tiers en situation irrégulière devant être éloignés du territoire. La directive privilégie le rapatriement volontaire, par rapport au rapatriement forcé, et vise à garantir, en tout état de cause, le respect des droits fondamentaux de la personne se trouvant en situation irrégulière sur le territoire de l’un des pays membres de l’UE et en attente d’en être éloignée. Sept ans après l’adoption de la directive, cinq ans après l’expiration du délai de transposition et au vu de l’actualité toujours aussi pressante en matière d’immigration, il est pertinent de dresser un premier bilan sur la transposition de cet instrument européen dans trois pays qui se trouvent en première ligne face aux phénomènes migratoires dans le bassin méditerranéen : la France, l’Espagne et l’Italie.L’ouvrage, qui réunit les contributions d’éminents spécialistes des trois pays concernés, analyse les différentes législations espagnole, française et italienne dans une optique comparative et selon une approche critique, afin de comprendre si ces législations ne sont pas plutôt restées en deçà des possibilités offertes par la directive « retour » en prévoyant le strict minimum en matière de droits et libertés. Il s’agit de comprendre également et surtout si la transposition de la directive européenne ne s’est pas transformée en trahison, notamment quant à la question des délais de rétention administrative des étrangers en attente d’être éloignés, en matière de mesures alternatives à la rétention et en matière de garanties procédurales devant entourer l’éloignement de l’étranger en situation irrégulière."
"Le nombre conséquent d’attributions dévolues à la Cour constitutionnelle, fruit de la volonté des Pères de la République d’établir, en réaction au fascisme, un Etat de droit irréprochable, justifia une attention soutenue quant à la qualité du recrutement des membres la constituant. Entre les exigences de la droite, soucieuse d’établir la nature juridictionnelle de la Consulta, et celles de la gauche, au contraire désireuse d’affirmer sa nature exclusivement politique, la solution retenue, toute compromissoire, établit un recrutement mixte, à la fois technique et politique des quinze juges composant la Cour. L’intervention des juridictions suprêmes, élisant en leur sein cinq de ses membres, compte parmi les désignations techniques, tant la compétition électorale est généralement immune de positionnement idéologique. Les cinq juges nommés par le Président de la République est plus difficilement classable et divise la doctrine, partagée entre le rôle arbitral (ce qui rangerait ces désignations dans la catégorie technique) et le rôle politique du chef de l’Etat. Indubitablement, les juges élus par les parlementaires le sont selon des considérations politiques. Mais la majorité renforcée qui est exigée, les trois cinquièmes des élus nationaux, ont obligé majorité et opposition à transiger entre elles. Cependant, ce système a pu conduire à une paralysie institutionnelle justifiant des velléités de réforme, accueillies fraîchement par la doctrine."
Droit comparé-Justice constitutionnelle-Euroméditerranée
"Le présent ouvrage constitue l'aboutissement du colloque intitulé «Le droit des étrangers en situation irrégulière après la transposition de la directive ""retour""», qui s'est tenu à la Faculté de droit de Toulon le 23 mai 2014, sous l'égide du Centre de droit et de politique comparés Jean-Claude Escarras."
"La légitimité de la Cour constitutionnelle résulte en partie de la pédagogie dont elle fait preuve, non pas tant auprès de la « communauté des clercs », c'est-à-dire des juristes, mais surtout vis-à-vis de l’ensemble des citoyens, au-delà du strict syllogisme employé pour démontrer la solution à laquelle elle parvient. L’obligation de motivation qui en découle se doit donc d’allier raison pure et phronesis, soit démonstration technique et prudence (au sens aristotélicien) ou sagesse en l’absence de laquelle les purs éléments juridiques ne suffiraient pas en regard du peuple. La motivation apparaît d’autant plus importante que l’alternative brutale constitutionnalité / non conformité est largement atténuée par l’introduction de décisions interprétatives et que, par ailleurs, la Cour s’autorise depuis 1988 un contrôle sur les lois constitutionnelles. L’exemple des décisions rendues en 1991 et 1993 sur les questions référendaires relatives aux lois électorales des assemblées parlementaires fournit une illustration saisissante de la pédagogie dont a dû faire preuve la Consulta, dans un contexte politique fort tendu. Tout d’abord conspuée, la Cour, une fois les tensions apaisées, sut se faire entendre et comprendre. Mais les remous qui entourèrent la décision n° 47 de 1991 semblent, entre autres exemples, justifier l’introduction des opinions dissidentes, pourtant toujours différée par la classe politique, malgré les souhaits exprimés par la doctrine italienne."
"Le nombre conséquent d’attributions dévolues à la Cour constitutionnelle, fruit de la volonté des Pères de la République d’établir, en réaction au fascisme, un Etat de droit irréprochable, justifia une attention soutenue quant à la qualité du recrutement des membres la constituant. Entre les exigences de la droite, soucieuse d’établir la nature juridictionnelle de la Consulta, et celles de la gauche, au contraire désireuse d’affirmer sa nature exclusivement politique, une solution de compromis a établi un recrutement mixte, à la fois technique et politique des quinze juges composant la Cour. L’intervention des juridictions suprêmes, élisant en leur sein cinq de ses membres, compte parmi les désignations techniques, tant la compétition électorale est généralement immune de positionnement idéologique. En revanche, la nomination par le président de la République de cinq juges nommés est plus difficilement classable et divise la doctrine, partagée entre le rôle arbitral (ce qui rangerait ces désignations dans la catégorie technique) et le rôle politique du chef de l’Etat. Indubitablement, les juges élus par les parlementaires le sont selon des considérations politiques. Mais la majorité renforcée qui est exigée, les trois cinquièmes des élus nationaux, ont obligé majorité et opposition à transiger entre elles. Cependant, ce système a pu conduire à une paralysie institutionnelle justifiant des velléités de réforme, accueillies fraîchement par la doctrine."
"L'animal, un homme comme les autres ?"" a été le thème du Colloque pluridisciplinaire initié par le Doyen Pierre Sanz de Alba et qui s'est tenu à Toulon les 18 et 19 novembre 2010. La qualité des contributions, l'intérêt qu'a suscité le thème, les débats fructueux auxquels il a donné lieu ont conduit naturellement les directeurs du CDPC JEAN-CLAUDE ESCARRAS et de BABEL, unités de recherche de l'université du Sud Toulon-Var, à publier les quinze contributions axées sur l'animal-objet, miroir de l'homme et l'animal-sujet, le paradoxe de l'homme."
"Liberté d’expression, neutralité et impartialité sont parfois difficiles à concilier. La question soulevée était de savoir si les obligations de neutralité et d’impartialité étaient inhérentes à la liberté d’information, de nature constitutionnelle en Italie. Des dispositions législatives qui prévoient que les émissions télévisées de nature politique doivent refléter de façon impartiale les diverses forces politiques du pays ne privent-elles pas un émetteur de programmes de son identité politique en portant atteinte à sa liberté d’expression ? La réponse de la Cour constitutionnelle italienne était particulièrement attendue, compte tenu du contexte politique en Italie. La Cour a posé ici des garde-fous aux modalités d’exploitation d’une chaîne de télévision par un leader politique. La sentence du 20 novembre 2002 s’attachera à limiter l’attribution des fréquences hertziennes à une même personne physique ou morale, et notamment aux formations politiques."
"Cette sentence de la Cour constitutionnelle italienne est relative à la réglementation de l’octroi des fréquences télévisuelles. Une loi de 1997 avait instauré un régime provisoire qui imposait aux chaînes hertziennes disposant de fréquences excédentaires de les transférer sur le satellite ou le câble. Mediaset, qui contrôlait les chaînes 4, 5 et 6, devait ainsi libérer certaines fréquences au profit de la chaîne Europa 7 autorisée à émettre mais qui n’avait jamais disposé de fréquences pour le faire. La loi de 1997 ne prévoyait toutefois aucun terme à ce régime provisoire. La date de transfert sur le réseau satellitaire ou câblé des chaînes hertziennes en surnombre n’était pas fixé. Le régime temporaire prévu par la loi de 1997 était inconstitutionnel. La sentence explique les effets négatifs engendrés par cette absence de date sur la libre concurrence et sur le pluralisme des sources d’information, principes fondamentaux protégés par la Constitution. La position dominante de Mediaset a perduré jusqu’en 2002, date de la sentence dans laquelle la Cour a fixé au 31 décembre 2003 le terme du régime provisoire. Cette décision est l’occasion d’analyser les liens entre le pouvoir exécutif et le pouvoir législatif, ce dénouement tardif ayant sans doute une explication politique. Le vide juridique laissé par la loi de 1997 profitait incontestablement aux chaînes du groupe Mediaset, propriété de M. Berlusconi dont les chaînes, une fois sur les réseaux de distribution alternatifs, auraient perdu une partie substantielle de leur audience, les programmes diffusés par câble et satellite n’étant que très partiellement introduits dans les foyers. Le 2 avril 2003, les députés italiens ont néanmoins approuvé un projet de réforme limitant à deux le nombre de chaînes de télévision pouvant être détenues par un groupe privé, quel que soit le mode de diffusion des programmes utilisé. Ce vote a constitué un revers pour le président du Conseil qui, à travers la tutelle financière de son gouvernement, contrôlait aussi la RAI, service public italien."
"A l’occasion d’un portrait critique de Silvio Berlusconi, dont on retrace l’accession au pouvoir, favorisé par un contexte historique favorable, après la disparition des deux grands partis ayant façonné l’Histoire italienne depuis la Libération jusqu’aux années 1990, la Démocratie chrétienne et le Parti communiste italien, alors même que sa réussite professionnelle est fortement sujette à caution, on s’est attaché à décrire dans cet article l’instrumentalisation personnelle qu’il fit du pouvoir pendant les sept années où il fit président du Conseil. Cette « illusion démocratique » illustre une nouvelle appréhension du politique, favorisée par la transformation néolibérale de l’Etat, où la dérégulation généralisée de l’économie s’accompagne paradoxalement d’une personnalisation et d’une scénarisation effrénée du pouvoir."
"Protection des droits de l’homme, promotion de la culture, respect d’un marché intérieur sans frontières et de la concurrence sont autant de questions impliquées dans le droit européen des émissions de télévision. Faire une synthèse des règles auxquelles est soumis le secteur de la télévision en Europe, tel est le premier objectif de cette étude. Identifier les imperfections du système européen de radiodiffusion télévisuelle permet dans un deuxième temps de proposer des mesures pour mieux harmoniser l’espace audiovisuel. Les difficultés venaient d’abord des textes fondateurs de cet espace, d’une part la convention européenne des droits de l’homme et la convention « Télévision transfrontière » du Conseil de l’Europe du 15 mars 1989, d’autre part les traités communautaires et la directive « Télévision sans frontières » du 3 octobre de la même année. Leur nature juridique différente et leur approche complémentaire du secteur de la télédiffusion (axée sur la protection des informations et des idées pour les premiers, sur la promotion d’un marché commun des services audiovisuels pour les seconds) demandaient une analyse préalable de leurs implications. De plus, au début des années 90, le droit européen des émissions de télévision traversait une période de mutations provoquées par un fort courant de déréglementation, conséquence de la multiplication des réseaux privés, et par le développement de techniques nouvelles (diffusions par satellite et câble, émissions haute définition). Malgré les problèmes posés par la multiplicité et la variété des secteurs concernés (droit des concentrations, quotas de diffusion, droits d’auteur et droits voisins…), il était indispensable de dégager des propositions susceptibles de structurer l’espace européen, d’autant plus que la définition de règles harmonisées face à l’hégémonie des programmes nord-américains était urgente."
"La première directive « Télévision sans frontières » du 3 octobre 1989 a constitué une étape essentielle dans la construction d’une Europe audiovisuelle. Elle a permis la mise en place d’un espace culturel commun au sein duquel les idées radiodiffusées pouvaient circuler librement. Mais elle a également contribué à parfaire la Communauté économique car les questions relatives à la télévision comportent incontestablement un aspect financier. La renégociation de la directive, prévue à son article 26, devait permettre d’adapter le texte à l’évolution du domaine de la radiodiffusion télévisuelle. La question soulevée dans cette étude est de savoir quelles sont les modifications apportées par la nouvelle directive adoptée le 30 juin 1997. Sont-elles notables ou accessoires ? Il convenait de cerner les lacunes de la directive de 1989, d’identifier les améliorations et d’apprécier leur portée. Le texte initial semblait constituer une étape essentielle dans la construction d’une Europe audiovisuelle. Le texte révisé est-il plus clair afin de limiter la survenance de contentieux de plus en plus nombreux ? Est-il davantage détaillé, prévoyant des règles plus explicites de mise en œuvre ? Une telle appréciation nécessitait de prendre en compte le développement et les progrès des technologies de communication audiovisuelle. Il convenait en effet de démontrer que la nouvelle directive ne se contentait pas d’améliorer la substance du texte mais régissait de nouvelles techniques de transmission des programmes, apparues au cours de la décennie écoulée. Le bilan dressé ne devait toutefois pas passer sous silence les lacunes du texte révisé. Avec pour objectif de parfaire les règles adoptées, il a semblé utile de recenser les difficultés non résolues par la directive de 1997, dans la perspective d’une autre renégociation."
"L'introduction de la question prioritaire de constitutionnalité a confronté le Conseil constitutionnel à une question attendue mais délicate : celle de la prise en compte, dans le contrôle de la disposition législative renvoyée, de sa portée effective, c'est-à-dire des interprétations que cette disposition a déjà pu recevoir au fil de ses applications concrètes. Le Conseil a finalement admis la prise en compte de celle-ci en affirmant qu'« en posant une question prioritaire de constitutionnalité, tout justiciable a le droit de contester la constitutionnalité de la portée effective qu'une interprétation jurisprudentielle constante confère à cette disposition ». Après un an d'intégration du droit vivant dans la jurisprudence constitutionnelle, soit une quinzaine de décisions utiles, l'on tracera ici les grandes lignes de l'orientation observée."
"Face à la problématique commune de la lenteur excessive des justices pénales modernes, les procédures de guilty plea encouragées en droit européen et en droits internes, se développent progressivement au sein des systèmes pénaux français et anglais. La mise en évidence, dans les deux systèmes, d’un contexte procédural favorable à l’institution des procédures de guilty plea permet de comprendre les raisons de la formation de ces procédures en France et Angleterre. En effet, l’existence préalable de procédures pénales sommaires ayant favorisé l’expansion de ces formes de justice dite négociée, révèle que l’ensemble de ces procédures contribuent indéniablement à l’accélération du traitement des infractions pénales. Cependant, elles doivent présenter des garanties juridiques afin de préserver l’effectivité des justices pénales modernes en maintenant une procédure pénale d’équilibre, à la fois efficace et légitime. Dans ces conditions seulement, ces procédures constituent donc un outil pertinent et performant au sein des politiques pénales managériales française et anglaise et participent à l’efficacité des justices pénales modernes."
"Le trust est une institution traditionnelle des États de common law, où il est pratiqué depuis plusieurs siècles. Il en va autrement en Italie, État de civil law. Poussée par la mondialisation à acclimater des procédés de gestion commerciale ou patrimoniale étrangers, l’Italie a accepté le principe du trust sur son territoire mais c’est essentiellement par le biais du droit fiscal. Les revenus et le patrimoine du trust sont assujettis à des impôts italiens, selon une démarche des plus réalistes qui passe outre les incohérences existant entre les particularismes du trust et le système juridique italien."
"En Italie, l’extradition d’une personne ne peut avoir lieu qu’accompagnée de « garanties » et « assurances » selon lesquelles les faits ne seront pas punis de la peine de mort par l’État demandeur. La Cour constitutionnelle italienne, dans cette sentence, apporte une réponse à la question de savoir si ce système des « garanties » et « assurances » (prévu par le code de procédure pénale et les lois de ratification et d’exécution des principaux traités d’extradition) constituait une solution acceptable compte tenu de la Constitution qui prohibe de manière absolue la peine capitale. Cette sentence permet d’aborder le problème des rapports entre les différentes sources juridiques en matière d’extradition : interfèrent directement normes constitutionnelles, internationales, lois de ratification et d’exécution des traités ainsi que législations ordinaires. La Haute juridiction a refusé l’extradition lorsque la peine de mort n’était pas abolie par l’État requérant. La vie nécessite une protection qui est incompatible avec une appréciation au cas par cas, par le ministre des Grâces et de la Justice, de la fiabilité des « garanties » et « assurances » accordées. La sentence italienne marque un revirement de la position de la Cour. Il était intéressant de comparer la solution italienne avec celle qui se dégage en France. Le Conseil d’État fonde encore sa jurisprudence sur le principe des « assurances suffisantes ». C’est dire que la juridiction française demande au gouvernement des précisions pour arrêter son appréciation ; de nombreuses décisions le confirment et les motivations retenues éclairent la position du Conseil d’État. L’analyse menée ici révèle également que le système des « assurances suffisantes » est en vigueur dans toute l’Europe : Allemagne, Autriche, Danemark, Pays-Bas, Royaume-Uni, Suisse... La Cour européenne des droits de l’homme s’en est aussi inspirée. La portée de la sentence italienne contribue donc à asseoir l’indépendance des juridictions nationales par rapport au pouvoir politique dans le domaine des relations internationales ; elle ouvre une voie susceptible d’être suivie par d’autres pays."
"Le spending review et la compétitivité économique sont la base d'un vent de réformes sans précédent en Italie. Parmi les multiples projets, il en est un qui fait « grincer des dents » presque toute la classe politique italienne. L'Italie compte aujourd'hui 110 provinces. Ce nombre que l'on reconnaît volontiers comme excessif et financièrement très coûteux, devrait, en principe, drastiquement diminuer. Il est vrai qu'il existe des disparités importantes entre ces mêmes entités."
"Communément considéré comme l’« enfant prodige » de la politique italienne, Enrico Letta est devenu, le 24 avril 2013, le plus jeune président du Conseil des ministres italien. Âgé de 46 ans, il incarne depuis plusieurs années maintenant une sorte de « relève » de la classe politique. Pour preuve, son ascension dans les arcanes du pouvoir est fulgurante..."
"Le dimanche 31 mai 2015 se sont tenues des élections au sein de sept régions (Campanie, Ligurie, Marches, Ombrie, Pouilles, Toscane et Vénétie) ainsi que des scrutins municipaux dans près de mille communes (le second tour s’étant déroulé quinze jours plus tard, le 14 juin). Très attendues, elles ont été largement commentées par la presse qui n’a pas manqué d’opérer une lecture de ces résultats au niveau national pour faire un point sur la confiance et la popularité du Gouvernement Renzi mais aussi plus largement du Pd."
"La consultation publique sur les réformes constitutionnelles organisée par le Gouvernement Letta est la consultation qui, en Europe, a recueilli la plus large participation au cours de l’année 2013. Cette consultation s’est déroulée via internet entre le 8 juillet et le 8 octobre 2013. Avec un peu plus de 200.000 questionnaires validés par l’Istat, elle démontre bien l’intérêt pour le changement mais également la préoccupation des Italiens à l’égard de la situation politique du pays. Nous vous proposons ici quelques chiffres de synthèse communiqués le 12 novembre 2013 lors de la remise du rapport final sur cette consultation..."
"Le Mouvement 5 étoiles (pour eau, transports, développement, réseaux et environnement) du comique et polémiste italien, Beppe Grillo fait une apparition remarquée et inattendue lors des dernières élections municipales partielles. Ce mouvement qui se veut « une association libre de citoyens » plutôt qu’un parti politique est issu du rapprochement de nombreux forums citoyens et d’associations civiques locales (Meetup) constitués sous le nom des « Amis de Beppe Grillo »..."
"Ces dernières années, le marché du jeu vidéo s’est grandement développé. Il touche des populations larges tant en terme d’âge, qu’en terme de nationalité. Ainsi, ce secteur constitue un poids non négligeable dans l’économie, puisque le chiffre d'affaires mondial issu de la vente de jeux vidéo devrait dépasser les 38 milliards d’euros en 2010. En France, bien que ce secteur génère un chiffre d’affaires de 4 milliards d’euros, avec de grands noms tels que GAMELOFT, ATARI, UNIVERSAL ou UBISOFT, il n’est pas offert aux intervenants du secteur de régime juridique clair ou univoque pour la création etl’exploitation de ce type d’oeuvre. Cette absence est à l’origine d’une perte de compétitivité de la France dans un marchémondialisé, où certains pays comme les Etat Unis offrent une sécurité propice à son développement. Elle entraine la fuite de projet à l’étranger et freine l’investissement. L’analyse amène donc à la proposition d’un régime propre appliqué aux jeux vidéo en matière de droit d’auteur. Certains jeux vidéo rassemblent au sein d’univers virtuels des dizaines de milliersd’utilisateurs, qui interagissent alors qu’ils se situent matériellement dans divers pays. Cette ouverture de l’espace de jeu, au départ individuel, à une communauté de joueurs, a profondément modifié la nature de ces jeux. A ce titre, ils soulèvent des questions relatives à leur régulation. Face au constat d’une autorégulation des univers virtuels, considérée comme despotique à l’aune des rapports déséquilibrés institués par les éditeurs et défaillante dans les rapports entre utilisateurs, il est envisagé de créer un cadre juridique spécifique aux univers virtuels."
"En politique, les Italiens, mais ils ne sont pas les seuls, sont friands de nouveaux visages. Comme si le renouveau de la politique transalpine passait, ce qui est sans doute en partie vrai, par le renouveau de sa classe politique. Après l’« enfant prodige », Enrico Letta, aujourd’hui président du Conseil, la gauche et l’opinion publique italienne « s’entichent » d’un trentenaire alliant un solide parcours politique à sa jeunesse. Il semble incarner en tout point la « relève » que les Italiens attendent et appellent de leurs vœux. Né le 11 janvier 1975 à Florence, Matteo Renzi est marié depuis 1999 et père de trois enfants. Il grandit dans la petite commune de Rignano sull’Arno où son père, Tiziano Renzi, proche de la Démocratie Chrétienne, est conseiller municipal..."
"Sujet à trop d’indéterminations en France, le concept de NCP a nécessité une analyse comparative fondée sur l’expérience italienne afin d’en identifier les contours et, par la suite, d’étudier sa place dans le droit. Il en ressort que les NCP renvoient à une multiplicité de catégories normatives de caractère axiologique et téléologique, servant de stratégie politique et structurant le système juridique. Par ailleurs, l’introduction de la forme programmatique dans les Constitutions modernes mêle l’indétermination de la norme dans son énonciation moderne au droit dans sa déclinaison traditionnelle. Les programmes ont ainsi du mal à trouver leur place dans le droit. Les NCP semblent a priori être privées d’effet juridique ; leur formulation imprécise ne mettrait en place que de simples objectifs. Toutefois, leur développement dans l’espace juridique paraît suffisamment important pour qu’il ne soit pas assimilé à une malfaçon du droit qui entacherait sa pureté. En effet, le concept NCP renvoie à de véritables « normes » constitutionnelles programmatiques dont la normativité reste particulière, signe non pas d’une « crise » du droit mais de son « évolution »."
"L’eau en tant que ressource naturelle en mouvement permanent est difficilement saisissable par le droit. La pénurie de cette ressource demeure un problème majeur dans certaines régions du sud de l’Europe et plus particulièrement en Espagne, en France et en Italie. Devant ce défi, une protection juridique renforcée s'impose par nécessité .Le droit est-il en mesure d’assurer à cette ressource une telle protection à travers un statut spécifique qui tienne compte de son caractère vital et rare? Ce travail s’inscrit principalement dans une perspective de droit comparé entre les systèmes juridiques français, italien et espagnol, étudiés à la lumière du droit de l’Union européenne ayant favorisé une standardisation de la protection de l’eau, même si elle demeure insuffisante. La domanialisation et la patrimonialisation de l’eau, ainsi que la reconnaissance progressive d’un droit à l’eau potable et à l’assainissement sont les solutions proposées par les droits publics étudiés. La question de l’effectivité et de l’efficacité de la protection publique de l’eau est au cœur de l’étude et invite à s’interroger sur les acteurs de la mise en œuvre de la protection de la ressource en eau ainsi que sur un partage des compétences souvent complexe. Dans cette perspective, le rôle du juge administratif et des juges des eaux publiques en matière d’application effective des normes protectrices de la ressource en eau est fondamental. Ainsi, la question de l’effectivité de la protection s’est imposée, car elle découle tant de son caractère préventif que du caractère répressif des sanctions appliquées en cas de dommages causés à la ressource en eau."
"La question de l’effectivité de la sanction pénale fait régulièrement débat. La sanction pénale serait, bien souvent, partiellement effective. Il faut dire que selon une acception classiquement répandue, l’effectivité de la sanction pénale correspondrait au taux d’application de la sanction prononcée et au rapport de conformité entre la sanction prononcée et la sanction exécutée. Tout écart entre ces deux pôles serait la manifestation d’une situation d’ineffectivité. Cette approche de l’effectivité est réductrice, elle ne permet pas d’appréhender cette notion dans sa globalité.Considérant comme effectif « ce qui produit un effet », l’étude de l’effectivité de la sanction pénale ne peut se limiter à une simple vérification de la correspondance entre la sanction prononcée et la sanction exécutée, elle s’étend à l’appréciation des effets produits par la sanction. L’effectivité, qui est un état, ne peut se confondre avec l’exécution qui correspond à l’ensemble du processus permettant d’y parvenir. Au cœur de la recherche de production des effets de la sanction, l’exécution est alors source de l’effectivité recherchée. Selon toute probabilité, la sanction exécutée produira des effets. Pourtant, sauf à vider de son sens la notion d’effectivité, il n’est pas possible de considérer que tous les effets que la sanction est susceptible de produire relèvent de son effectivité. Seuls les effets conformes à la finalité qui lui est assignée intègrent cette notion. Tout en distinguant l’effectivité de l’efficacité, l’effectivité de la sanction s’appréciera à l’aune des effets qui contribuent au maintien de la paix sociale."
"Le juge constitutionnel est au cœur de la problématique de la conciliation de l'ordre et de la liberté. Cette problématique est bouleversée par le terrorisme, qui frappe l'État dans son essence et provoque des réactions normatives plus restrictives pour les libertés. L'analyse comparée du contrôle de constitutionnalité de la législation antiterroriste révèle l'exercice d'un contrôle sur la mesure d'une législation construite comme un droit parallèle."
"L’absence de loi ou une lacune dans la loi peuvent empêcher l’exercice légitime d’un droit constitutionnellement garanti. Pour remédier à cette situation, certains systèmes constitutionnels ont mis en place un type de contrôle de constitutionnalité qui - différemment des techniques plus connues dont l’objet est la norme juridique - porte sur l’abstention du législateur à produire la norme juridique nécessaire à la concrétisation d’un droit prévu par le texte de la Constitution. Une omission normative inconstitutionnelle, qu’elle soit législative ou non, est de toute évidence indésirable dans un système juridique. La problématique est de trouver des mécanismes procéduraux efficaces pour la contrer sans pour autant troubler l’équilibre entre les Pouvoirs de l’État. Cette problématique se situe dans le domaine de l’articulation entre les Pouvoirs classiques de l’État pour la garantie des droits. Dans notre étude ladite articulation présuppose l’existence de rapports horizontaux (non-hiérarchiques) entre le Législatif et le Judiciaire en vue de concrétiser des droits subjectifs prévus expressément dans le texte de la Constitution. Ainsi, nous proposons une analyse du contrôle constitutionnel des omissions législatives inconstitutionnelles, utilisant comme paramètre d’investigation celui prévu par l’actuelle Constitution brésilienne promulguée en 1988. On cherche à savoir si la théorie et la praxis du contentieux constitutionnel sont en adéquation (et sous quelles conditions elles le sont) avec le modèle de démocratie adopté dans l’État en question. La Constitution brésilienne présente certaines des caractéristiques des modèles de démocratie continue et participative sans porter expressément ni l’une ni l’autre de ces nomenclatures dans son texte constitutionnel. Cette analyse prétend principalement avoir un objectif théorique et prospectif en travaillant avec des concepts qui sont valides (ou peuvent l’être) au-delà de l’ordonnancement qui les a créés. Le mécanisme de contrôle juridictionnel des omissions législatives est relativement rare dans les ordres juridiques. C’est pourquoi on utilisera la démarche procédurale du contentieux constitutionnel brésilien comme source d’argumentation d’arrière-plan du jeu conceptuel de paramètres et paradigmes de comparaison à propos de l’objet principal. Les hypothèses à vérifier sont les suivantes : si, et dans quelles conditions, le contrôle juridictionnel des omissions législatives inconstitutionnelles est en adéquation avec le modèle de démocratie continue et de la théorie de la séparation des Pouvoirs classiques de l’État. Si, et dans quelle mesure, il peut apporter du renouveau à ces théories et se prêter à la concrétisation du texte constitutionnel en ce qui concerne les matières du vivant. On propose d’analyser ces aspects à travers l’étude du contrôle juridictionnel des omissions législatives inconstitutionnels en tant que voie pour une démocratie continue (I) et en tant qu’instrument de contre-pouvoir (II)."
"Ces cinq dernières années, les difficultés qui résultent des notions de laïcité et de neutralité sont réapparues avec les affaires Baby Loup et CPAM, lesquelles ont donné lieu à deux arrêts rendus par la Cour de cassation en date du 19 mars 2013 qui ont eu une véritable incidence sur ces deux principes. La Chambre sociale y apporte en effet un nouvel élément de clarification permettant de déterminer leur champ d’application en présence d’organismes de droit privé : les organismes de droit privé exerçant une mission de service public se verront appliquer ces notions à l’inverse des organismes de droit privé ayant une activité d’intérêt général. Cependant, la Haute Cour ne prend pas le soin de définir les éléments contenus dans la distinction, ce qui peut porter à confusion. C’est ainsi que le Conseil d’État, saisi le 20 septembre 2013 par le Défenseur des droits alors en exercice, Dominique Baudis, a rendu une étude le 19 décembre 2013 destinée à y apporter des précisions. Il en résulte que pour déterminer le champ d’application des principes de neutralité et de laïcité, il convient de distinguer l’activité d’intérêt général de la mission de service public, expressions non définies par la Cour de cassation et nécessitant des éclaircissements (Partie I). Une fois ces difficultés écartées, cette distinction permettra de déterminer leur éventuelle application et, le cas échéant, leurs modalités (Partie II)."
"La stratégie d’anticipation procédurale en matière civile permet d’effectuer un choix approprié entre les différentes règles existantes en fonction du but poursuivi. L’anticipation du procès et la stratégie au cœur de l’action dessinent en creux le champ de la liberté individuelle laissée au justiciable dans le procès civil. En anticipant la survenance du litige ou en envisageant les modalités de résolution de ce dernier, la technique contractuelle fait de l’évitement du recours juridictionnel une stratégie d’anticipation. Une fois le litige né, le choix d’agir en justice suppose l’évaluation des chances de succès de l’action par rapport au résultat escompté. Seront parfois préférés les modes amiables de règlement des différends, voire le recours à un juge privé en la personne de l’arbitre. Mais si l’action est diligentée, le justiciable devra nécessairement soulever un certain nombre de questions nécessaires à l’élaboration de la stratégie qu’il retiendra pour son affaire. Pour réduire l’aléa judiciaire, plusieurs paramètres doivent être pris en compte tels que l’évolution du droit, de la jurisprudence, la réaction de l’adversaire ainsi que l’office du juge. L’efficacité de la stratégie d’anticipation varie selon le degré de prévisibilité de ces différents éléments qui forment l’objet de cette étude."
"En devenant juge constitutionnel, Marta Cartabia cumule les… singularités : elle n’est que la troisième femme à siéger à la Consulta (après Fernanda Contri et Maria Rita Saulle qu’elle remplace) ; à 48 ans, elle devient la plus jeune membre de la Cour siégeant actuellement et même un des plus jeunes juges à occuper une telle fonction. Pourtant, le parcours du Professeur Marta Cartabia, nommée le 2 septembre 2011 par le Président de la République Giorgio Napolitano, met en évidence que ces singularités ne sauraient aucunement l’emporter sur le bien-fondé objectif de sa nomination. Depuis longtemps intéressée par le droit européen, elle est diplômée de l’Université de Milan, en 1987, pour un travail intitulé « Existe-t-il un droit constitutionnel européen ? » et dirigé par Valerio Onida (qui deviendra Président de la Consulta en septembre 2004)...."
"Certaines fonctions exercées par la juridiction de l'Union Européenne semblent particulièrement révélatrices de l'existence d'une justice constitutionnelle. Ainsi, la Cour de justice, gardienne et interprète des textes fondateurs de son ordre juridique (Partie 1), juge de la répartition des compétences entre institutions et États membres ou entre les institutions elles-mêmes (Partie 2) et garante des droits fondamentaux inhérents à l'Union (Partie 3), s'apparente à une Cour constitutionnelle."
"La crise économique de 2008 a secoué l’économie mondiale en laissant des séquelles dans divers secteurs, notamment dans celui de la banque dont le mode de fonctionnement et le rôle d’intermédiation ont montré leurs limites. La pratique de l’intérêt et la mise en œuvre du mécanisme de la spéculation ont été à l’origine de ces crises contemporaines. Face à cet événement des économistes, des politiques, voire même des banquiers, se sont levés pour trouver une issue à cette turbulence financière. Les premiers rapports et travaux rendus ont été presque tous orientés vers une problématique commune, les uns proposent une finance alternative à la finance conventionnelle et d’autres plaident pour une finance plus éthique. Les solutions suggérées recoupent les principes de la finance islamique. Cette dernière fonctionne en conformité avec les règles de la loi islamique : l’interdiction du riba, de la spéculation, ou encore l’application du principe de partage des profits et des pertes, alternative au riba dans le système islamique. Si du point de vue économique l’intégration de l’industrie islamique dans le système financier français ne pose pas de souci, tel ne semble pas être le cas au regard des règles juridiques qui gouvernent l’organisation et le fonctionnement des banques en France et celles qui régissent le droit des contrats. C’est pour cette raison, qu’il est nécessaire de voir si la banque islamique remplit toutes les conditions requises pour être érigée au rang d’une banque au sens des dispositions du Code monétaire et financier, autrement dit l’intérêt est-il une condition nécessaire pour une opération de crédit, permettant par la même occasion de qualifier une institution de banque ? Ces interrogations ne se limitent pas seulement sur le plan organisationnel, elles s’étendent aussi au fonctionnement de cette banque, car leur régime juridique et la qualification des produits utilisés dans ces banques islamiques méritent un examen judicieux afin d’en déduire le droit applicable en cas de contentieux. Notre thèse se propose d’apporter des solutions à ces interrogations ou, à tout le moins, d’essayer de trouver des voies pour faciliter l’accueil et l’intégration de ces banques en France."
"L’homme n’habite plus vraiment, il s’abrite. Habiter exige une « part de monde » propre à l’individu afin que se réalise l’évènement de demeurer. Le droit au logement rend compte d’une exigence démocratique qu’il convient de prendre au sérieux au sein des sociétés française et italienne. Elle se manifeste inévitablement comme l’expression d’une certaine ouverture du droit positif au présent, le droit ne pouvant rester aveugle à ce qui l’entoure. L’individu questionne constamment ses ressources lorsque ces dernières ne suffisent pas ou plus à lui assurer une existence digne et se tourne le cas échéant vers la solidarité nationale afin d’y remédier. Il convient dès lors de s’interroger sur la consistance normative du droit au logement en droit comparé au regard de la convergence de ses sources internationales et nationales. Cet enrichissement donne corps à une mise en œuvre selon le principe de subsidiarité opérée par les acteurs les plus proches de la misère sociale à qui il revient en définitive de traduire la signification du droit au logement."
"Élément clé de sa nomination, Matteo Renzi devait incarner le renouveau de la politique italienne, retour et analyse d’un « pari » commencé en février dernier."
"Les présidents des régions italiennes sont des acteurs privilégiés de la vie politique transalpine. Ils le sont devenus avec la réforme constitutionnelle n° 1 de 1999 et l’introduction du suffrage universel direct pour leur élection (article 122 C.) mais également avec la réforme du titre V de la seconde partie de la Constitution en 2001. Le renforcement des compétences des régions mais également, de manière générale, de leur autonomie a donné naissance à ceux que l’on appelle dorénavant les « Gouverneurs » en référence à leurs homologues américains. Les portraits présentés ici montrent combien cette fonction, très influente au niveau régional, l’est également au niveau national ; tant pour le lien entre les différents niveaux d’administration que pour la carrière politique des governatori…"
"Il ne manquait plus qu’elle pour que le tableau soit réellement complet… C’est désormais chose faite et de quelle manière ! Le 4 décembre 2013, la Cour constitutionnelle italienne s’est invitée dans le large débat qui agite l’Italie depuis quelques mois avec le nouveau mouvement de réforme constitutionnelle en publiant un communiqué lapidaire (voir traduction ci-après) indiquant sa décision d’invalider la constitutionnalité de la loi électorale n° 270 de 2005, plus connue sous le sobriquet (peu flatteur) de Loi Porcellum."
"Le 31 janvier, après 4 tours de scrutin, le Parlement italien a élu Sergio Mattarella pour succéder à Georgio Napolitano. Figure aussi importante que discrète de la politique italienne, il s'agit d'un homme capable d'être l'""instrument de la réconciliation des Italiens avec l'Etat"". Le nouveau président et les conditions de son élection sont au coeur de tous les commentaires et de toutes les attentions."
"Il existe, sans nul doute, un patrimoine euro-méditerranéen de l’eau. Cependant, plus largement encore, la gestion de cette ressource de plus en plus rare est un véritable enjeu géopolitique . Depuis longtemps, il s’agit d’un problème auquel les États sont confrontés ; cette ressource est même un patrimoine à protéger. La multiplication des usages de l’eau et les difficultés de gestion de la ressource ont engendré des problèmes de partage entre les États. Toutefois, alors que l’eau ne connaît pas de frontière, et que nombreuses sont les situations où le territoire sur lequel elle se trouve est transfrontière, le droit international ne semble s’être préoccupé que tardivement de l’enjeu qu’elle constitue en tant que ressource vitale pour les populations et même plus largement en tant que patrimoine de l’humanité. Traditionnellement, l’eau douce était uniquement considérée au regard de la navigation et les autres utilisations de cette ressource n’étaient pas prises en compte dans l’élaboration des textes de droit international. La primauté de la navigation fut consacrée par la Résolution sur les régulations internationales relatives à l’utilisation des cours d’eau internationaux, adoptée par l’Institut de Droit International, en 1911, à Madrid."
"Les mineurs non accompagnés sont de plus en plus nombreux à demander l’asile dans l’Union européenne. Pour obtenir le statut de réfugié, ces mineurs doivent, comme leurs ainés, démontrer qu’ils sont confrontés à des risques d’atteintes graves en cas de retour dans leur pays d’origine. Lorsque le jeune est sans papiers, il doit en outre démontrer qu’il est mineur. Or les techniques employées par les autorités nationales sont loin d’être sans lacunes et défauts. En Europe, un représentant légal doit être nommé pour accompagner le mineur, particulièrement vulnérable, dans les différentes démarches qu’il doit accomplir auprès des autorités nationales. Le représentant du mineur doit garantir l’intérêt supérieur de l’enfant. Des normes communes découlent de directives européennes en ce qui concerne les attributions et les missions des représentants des mineurs. Cependant, la marge de manœuvre laissée aux États explique que les compétences varient ces derniers. Ainsi la diversité des dispositions nationales met en jeu l’efficacité du système. En outre, l’effectivité des règles adoptées est imparfaite, la protection des mineurs souffrant ainsi de nombreuses lacunes."
"Dans le cadre du Master 2 « Droits Fondamentaux » spécialité « Migrations et Droits des étrangers », j’ai effectué mon stage au sein de l’association GISTI du 2 mai au 29 juillet 2016. Le GISTI est une association indépendante à but non lucratif. Depuis ses débuts, l’association se place sur deux terrains d’action : l’information et le soutien des immigrés. Cependant, contrairement à beaucoup d’autres organisations, le GISTI va utiliser le droit pour faire évoluer leur situation. Au fil des années, le GISTI a acquis une certaine notoriété, notamment par le fait qu’il a toujours cherché à prendre part au débat public. Ceci explique l’importance qu’il a aux yeux de nombreuses organisations publiques ou privées, de la presse mais aussi auprès de l’administration. Le GISTI étant une association reconnue parmi ses pairs, nous pouvons nous demander quel est vraiment son rôle en matière de droit des étrangers. Nous présenterons alors l’association dans son ensemble (Chapitre 1) avant de voir plus précisément le rôle d’un stagiaire au sein du GISTI (Chapitre 2)."
"La charte de kouroukan-fouga ou charte du mandén est un ensemble de règles juridiques proclamée en 1236 par l’empereur du mandén (Sondjada kèta, 1190-1255) à kouroukan-fouga (plaine située à Kâaba à la frontière entre le Mali et la Guinée-Conakry). Elle fut la constitution de l’empire mandingue (appelé aussi empire du Mali). Classée patrimoine culturel immatériel de l’humanité par l’Unesco en 2009, cette charte au-delà de ce statut culturel, est une véritable valeur juridique dont les pays mandingues directement concernés devraient s’inspirer."
"Le gestation pour autrui est qualifiée de convention c'est à dire d'accord entre les parties (la mère qui porte l'enfant et les commanditaires) et qui est destiné à produire des conséquences juridiques (la filiation de l'enfant). Ainsi, le droit français estime que ce contrat est nul puisque sa cause et son objet sont illicites. La nullité de cette convention protège l'ordre social mais aussi l'individu. En effet, l'importance de la filiation transcrite à l'état civil d'une personne n'est plus à démontrer : « alors que l'état civil n'était naguère envisagé que comme moyen de preuve de l'identité civile, il occupe désormais un rôle reconnu dans la constitution de l'identité psychologique ». Le droit français ne veut donc pas transcrire à l'état civil d'une personne n'importe quelle filiation. En l'état de l'interdiction actuelle, la question qui se pose est celle du sort de l'enfant né d'une telle convention puisque malgré l'interdiction, elle reste pratiquée à l'étranger. L'enfant se trouve donc élevé en France et vit chez les personnes se définissant eux-mêmes comme ses parents. Or, quel lien juridique est accordé à ces personnes pour élever et prendre soin de l'enfant ? Peuvent-ils se voir établir un lien de filiation avec cet enfant par les modes d'établissement de la filiation prévu en droit français c'est à dire par l'effet de la loi, par la reconnaissance, par la possession d'état ou l'adoption ? Comment notre droit français accueille-t-il ces conventions pratiquées légalement à l'étranger ? Le droit français peut-il, sans anéantir la prohibition d'ordre public, accepter de faire produire des effets juridiques en France aux conventions de GPA pratiquées dans d'autres pays ? Nous verrons, dans un premier temps, les raisons qui ont conduit le droit français à prohiber les conventions de gestations pour autrui et à empêcher ceux qui contournent la loi en se rendant à l'étranger d'obtenir les effets juridiques souhaités (partie I). Néanmoins, depuis ces dernières années, la reconnaissance en droit français des GPA pratiquées à l'étranger semble actée, puisque sous l'impulsion de la Cour européenne des droits de l'homme, notre pays accepte les conséquences juridiques qui découlent de ces conventions (partie II)."
"C’est par cette expression (Il nemico allo specchio) que le 24 septembre 2014, Ferruccio de Bortoli (photo ci-dessous), le directeur du Corriere della Sera signait un éditorial qui prend aujourd’hui tout son sens et qui constituait sans doute la prémisse d’une rupture entre l’opinion publique et la « méthode Renzi ». Il n’empêche que l’exercice du « bilan » est, par définition, toujours périlleux puisque les bilans sont souvent, sinon toujours, « contrastés ». À défaut d’en établir un véritable, il est au moins possible de s’attarder sur les éléments les plus marquants du Gouvernement Renzi, qu’il s’agisse du fond ou de la forme."
"Dès sa prise de fonction le 1er janvier 2019, le nouveau président brésilien met en place un profond changement des directives de la politique publique en matière d’environnement, adoptant des pratiques amplement dénoncées par les forums internationaux. « On protège trop de forêts, et ce au détriment du développement de l’agriculture et de l’élevage, […] or la protection environnementale ne produit pas de PIB », déclare le président dans son discours au Forum de Davos en janvier 20191-2. Sur le plan juridique, on remarque que cet agenda gouvernemental aligné sur le développement économique du secteur privé porte atteinte aux directives protectrices de l’environnement énoncées dans l’article 225 de la Constitution brésilienne dès 19883. Sur le plan infraconstitutionnel, bien que le système législatif de prévention des risques environnementaux se soit structuré au cours des dernières décennies, ces dysfonctionnements dans la sphère de l’Exécutif fédéral entraînent une insécurité juridique, ce qui n’est pas sans conséquence sur le système juridictionnel. Lorsqu’on constate une atteinte à l’environnement, le droit procédural par exemple ne semble pas à même de faire face à sa protection effective. C’est déjà le cas en temps de normalité législative car les procédures sont souvent lentes, mais actuellement le décalage de la réponse judiciaire s’accentue par rapport à l’immédiateté des effets produits par cette nouvelle politique4 et l’absence de mesures procédurales spécifiques de protection. En effet, la protection de l’environnement en tant qu’objet du procès judiciaire demeure largement dépendante de la manière dont les juges appliquent les normes procédurales générales prévues dans le Code de procédure civile. C’est pourquoi, dans ce contexte d’urgence environnementale, les mesures exécutoires atypiques créées par l’article 139, alinéa IV du nouveau Code de procédure civile de 2015 constituent une piste sérieuse qu’il convient d’étudier afin de savoir si, et comment, elles peuvent s’appliquer à la protection de l’environnement, ce qui serait en définitive rendre effectif le texte de l’article 225 de la Constitution décidé par le pouvoir constituant originel de 1988. La mesure exécutoire atypique est un concept ouvert, ce qui entraîne des difficultés dans sa mise en application. Si les mesures typiques d’exécution sont indissociablement liées à la nature de l’obligation de droit matériel subjacente, cela n’est pas le cas pour les mesures atypiques. Sur ce point, le législateur est omis, et le Code de procédure ne les énumère pas, ses contours sont donc construits au cas par cas. La portée de ces mesures exécutoires atypiques a vocation à s’amplifier en raison des principes de protection de l’environnement. Les limites de ses pouvoirs sont majoritairement régulées par le principe de proportionnalité qui oriente la solution des conflits entre droits fondamentaux. C'est pourquoi, l’opportunité de l’application des mesures atypiques ne doit pas être évaluée seulement sur la base des principes spécifiques à la procédure d’exécution tels que la patrimonialité et les moyens moins contraignants pour le débiteur. On pense que l’application de mesures atypiques d’exécution dans les cas concrets visant la protection des droits diffus et collectifs corrélés à la protection de l’environnement est une manière d’ouvrir un autre point de vue sur le rôle du juge d’exécution, celui où ses décisions se font perméables aux principes contemporains qui régissent le droit environnemental, et dans lequel les mesures exécutoires atypiques trouveraient une interprétation conforme aux principes constitutionnels. Il apparaît ainsi que les mesures atypiques semblent avoir le pouvoir d’inverser la logique traditionnelle de réparation du dommage environnemental, en accordant au procès judiciaire un rôle renforcé dans la concrétisation des principes de précaution et de prévention. Pour ce faire, il est nécessaire d’une part que la procédure d’exécution d’une décision de justice s’autorise le recours à des principes de spectre dissuasif plus large que ceux appliqués au système procédural, ce qui pourrait justifier l’acceptation de mesures atypiques aujourd’hui refusées, et d’autre part il faut que son application ne soit pas subsidiaire à l’obligation principale malgré son caractère instrumental, afin que le juge ne soit pas obligé d’épuiser les mesures typiques avant de déterminer une mesure atypique. Ainsi, bien que prévues par le nouveau Code de procédure civile comme un pouvoir élargi du juge en vue de la quête d’effectivité des décisions de justice, les mesures exécutoires atypiques sont souvent refusées par la jurisprudence (I). Une effectivité potentiellement plus large émerge cependant de la construction d’un cadre théorique justifiant juridiquement son application aux décisions de justice dont l’objet est la protection de l’environnement."
"Était-il bien raisonnable de s'interroger, six ans seulement après l'introduction de la QPC, sur la possibilité de franchir une étape supplémentaire, par la mise en place d'un contrôle de la constitu-tionnalité des décisions de justice ? La QPC ayant atteint « sa vitesse de croisière » et de nombreuses décisions ayant été rendues dans le cadre de ce nouveau mécanisme, à quoi bon s'interroger sur de possibles évolutions ? Le système de protection déjà en place ne suffit-il pas à garantir convenable-ment les droits et les libertés du citoyen, dans un État de droit comme la France ? Au regard de l'introduction récente et du succès de la QPC, ces questionnements pouvaient paraître surprenants ; l'adoption d'une procédure de contrôle a posteriori de la constitutionnalité des lois, sous la forme d'une question de constitutionnalité, a été considérée comme un réel progrès pour l'État de droit français. La QPC permet de remettre en cause la constitutionnalité d'une loi entrée en vigueur dont la non-conformité à la Constitution peut se révéler bien des années après son adoption, notamment lors de ses applications concrètes. Elle participe ainsi progressivement à une « démarginalisation » de la Constitution comme norme juridique de contrôle, elle rapproche celle-ci des justiciables et démocratise, de ce fait, l'accès au Conseil constitutionnel. Toutefois, en dépit du progrès que représente la QPC et de la réussite qu'atteste le nombre de saisine réalisé depuis 2010, on pouvait se demander si l'objectif affiché par la réforme telle que pensée en 2008 avait bel et bien été atteint. Si l'un des intérêts de la QPC était en effet de lutter contre la concurrence des normes internationales en matière de protection des droits fondamentaux, en par-ticulier celle de la CEDH, l'objectif premier de la réforme était d'ouvrir une nouvelle voie de recours aux justiciables, pour assurer la protection de leurs droits constitutionnels. Or, bien qu'il s'explique notamment par le souci de ne pas engorger le Conseil constitutionnel, le filtrage opéré par les juri-dictions administratives et judiciaires paraît sévère. En outre, l'appréciation du caractère sérieux de la question posée par les juridictions suprêmes n'est pas toujours très compréhensible et peut poser des difficultés, notamment lorsqu'il s'agit de questions qui ont pour objet l'interprétation jurispru-dentielle de la loi donnée par les cours suprêmes elles-mêmes. Le système de contrôle de constitu-tionnalité organisé en France recèle donc encore des lacunes, il est nécessairement perfectible et ces défauts, ajoutés aux difficultés pratiques pour atteindre le Conseil constitutionnel, semblent contri-buer à un certain délaissement de la QPC par les justiciables ordinaires."
"Cette thèse porte sur le procédé de passerelle entre la conciliation et la sauvegarde, dans une approche comparative droit français, droit OHADA. Le procédé de passerelle permet au chef d’entreprise d’élaborer un plan de redressement dans le cadre de la conciliation, avant de le faire adopter lors d’une sauvegarde accélérée. La présente thèse se subdivise en deux parties. Dans la première partie, un diagnostic est posé pour comprendre les raisons de l’adoption de ce mécanisme de traitement des difficultés des entreprises en droit français, et celles qui pourraient ou non justifier sa reconnaissance en droit OHADA. Il ressort de cette partie que, dans les droits français et OHADA, la rigidité du régime de la cessation des paiements ainsi que l’unanimité obligatoire de l’accord de conciliation constituent un handicap au redressement des entreprises, car le principe d’unanimité donne un droit de véto à chaque créancier. Le procédé de passerelle permet de passer outre l’opposition des créanciers minoritaires qui utilisent ce droit de véto pour faire adopter le projet de redressement du chef d’entreprise par vote majoritaire sur le terrain judiciaire. Dans la deuxième partie, une étude prospective est menée ; le procédé de plan pré-négocié joue un rôle à la fois préventif et curatif : il permet, d’une part, la libre négociation, entre un débiteur et ses créanciers, d’un plan conventionnel de restructuration et, d’autre part, l’anticipation de l’intervention du tribunal pour optimiser l’actif et maîtriser le passif. Le procédé de passerelle est adopté en droit français ; la présente thèse propose une étude de son régime juridique. Il n’est pas reconnu pour le moment en droit OHADA ; son applicabilité y est analysée. Pour l’amélioration de la prévention et du traitement des difficultés des entreprises dans les droits français et OHADA, des pistes sont proposées. Il en est ainsi notamment, d’un côté, de l’adoption des comités de créanciers dans la procédure de conciliation française et, de l’autre, de la reconnaissance du mécanisme de plan pré-négocié, de la réforme du régime de la cessation des paiements et de la spécialisation des tribunaux connaissant des contentieux régis par l’AUPC en droit OHADA. La présente étude, en plus d’apporter une contribution scientifique à l’étude des nouvelles procédures de sauvegarde accélérée en droit français, vise à lancer le débat sur l’opportunité de l’adoption du procédé de passerelle en droit OHADA. L’organisation de la cession pré-arrangée dans le cadre du mandat ad hoc en droit français et l’opportunité de l’adoption d’une telle cession en droit OHADA sont des problématiques qui pourraient faire l’objet d’une recherche."
"La législation irakienne définit le contrat comme étant l’union d'une offre faite par la partie contractante avec l'acceptation d'une autre partie et ce de manière à établir les effets dans l'objet du contrat. Ainsi, la place occupée par le contrat de vente en droit irakien est importante. Lorsque les parties relèvent d’ordres juridiques différents, leurs rapports sont régis par le droit international privé, qui détermine le tribunal apte à trancher le litige. Cette thèse de doctorat vise alors à vérifier la capacité à appliquer les règles de conflit de compétence internationale en droit irakien sur des contrats « virtuels » ou dématérialisés. Comme nous le verrons, dans ce domaine, « virtuel » ne veut pas pour autant dire que ce contrat n’est pas réel, comme le spécifie très clairement la loi irakienne. Il reste rattaché au territoire. Le problème est que le droit irakien, en ignorant les notions de frontière et de territorialité, ne reconnaît pas sa propre « immatérialité ». Cette réalité dans les textes et la pratique implique que les opérations qui se produisent sur Internet ne sont pas prises en compte par les règles de conflits de compétence internationale. C’est la raison pour laquelle nous avons souhaité vérifier et comprendre la capacité et l’effectivité des règles de conflit de compétence internationale dans le cadre de litiges sur Internet. Ce faisant, nous espérons mettre en lumière les règles les plus appropriées, qui correspondent le mieux à la nature du contrat virtuel, à savoir son immatérialité. Pour ce faire, nous entreverrons quelques développements sur les litiges de l’Internet. Ainsi nous disposons de deux domaines de recherche : un premier au niveau de la législation nationale, comme le droit français et le droit américain ; un deuxième au niveau des conventions internationales, comme les conventions des Nations-Unies en 2005, la convention de la Haye en 2005, la convention de Bruxelles en 1968 ou encore les règlements de Bruxelles 2000 et 2012."
"L’homme fort de la nouvelle extrême-droite italienne, Matteo Salvini, a su créer, à travers un improbable attelage gouvernemental, une dynamique politique fondée moins sur sa position institutionnelle de vice-président du Conseil et Ministre de l’Intérieur que sur le développement d’un sentiment xénophobe lui permettant de doubler l’étiage de son mouvement. Son activisme politique exacerbé, au mépris de toutes les règles de droit public, interne ou international, constitue le symptôme douloureux d’un pays bouleversé par la crise migratoire."
"Rapport de stage du 25 avril au 24 juin 2016 au sein du cabinet de Me Yeliz Sefolar-Benanmar rédigé dans le cadre du Master 2 Droits Fondamentaux, parcours Personnes et procès (Université de Toulon). L’objectif de ce stage était de me permettre de comprendre le fonctionnement d’un cabinet d’avocats, les tâches qui y sont effectuées, d’appréhender les aspects de la profession d’avocat mais également et surtout de bénéficier d’expérience en matière de mécanismes juridictionnels pour la protection des droits fondamentaux. Par ailleurs, c’est l’opportunité de mettre en pratique mes connaissances acquises pendant ces cinq années de droit afin d’en tirer une large expérience à travers des missions confiées. Ce rapport a donc pour objectif de présenter dans une première partie le cabinet d’avocat, dans une deuxième partie le carnet de bord, dans une troisième partie un cas relatif à une ordonnance de quitter le territoire français et enfin les annexes."
"Après de nombreux mois de réflexion et de discussion, le processus engagé en fin d’année dernière est aujourd’hui entériné, le décret-loi no 191 du 4 décembre 2015 (Dispositions urgentes pour la cession à des tiers des entreprises du groupe ILVA) a été converti en loi par la loi no 13 du 1er février 2016."
"De la volonté des parties de s’accorder sur les éléments essentiels d’un contrat, naît tout un processus contractuel qui se traduit par la création d’obligations, éléments susceptibles d’être à l’origine d’un désaccord. Dès lors qu’un juge est saisi d’un litige, les opérations d’interprétation et de qualification du contrat litigieux auxquelles il se consacre se définissent par une double fonction. Dans un premier temps, l’interprétation du contenu du contrat permet au juge de repérer les éléments de fait qui ont été déterminants de la volonté des parties de contracter. Dans un second temps, une fois déterminés, ces éléments qui sont porteurs du sens du contrat, vont permettre au juge d’apporter, une solution au désaccord qui oppose les parties. Or, la solution ne trouvera son efficacité que si le juge applique aux éléments de fait qu’il a identifiés le droit approprié ; il faut pour cela qualifier le fait au sens où la qualification, consiste à déterminer la catégorie dans laquelle s’inscrit le contrat, afin de lui appliquer le régime juridique qui lui correspond. Elle est en cela le préalable à l’application d’une règle juridique. Opération intellectuelle, la qualification fait ainsi office de charnière entre les deux fonctions attachée à l’opération d’interprétation que sont l’interprétation des données de fait et la solution apportée par le juge sur le contenu contractuel litigieux."
"[début du texte] Le 4 décembre prochain marquera un moment important de la vie politique italienne, quel que soit d'ailleurs le résultat de la consultation référendaire. En cas de réponse négative, les semaines qui suivront seront déterminantes pour l'avenir politique de Matteo Renzi et peut-être même du Parti démocrate. ."
"Le constat réalisé par Mathieu Disant, concernant la jurisprudence relative au filtrage des questions prioritaires de constitutionnalité (QPC), confirme la tendance du Conseil d’État et de la Cour de cassation à réaliser, même lorsqu’est en cause leur propre interprétation de la loi, un « contrôle de constitutionnalité » de cette interprétation, à l’instar de ce qu’elles font lorsqu’elles doivent filtrer une question qui porte uniquement sur une disposition législative. Or, en analysant cette tendance dans l’optique d’une amélioration de la protection des droits et libertés du justiciable, il est possible d’identifier des avantages, mais aussi certains risques qui nous conduisent à émettre des réserves sur le mécanisme actuel de la QPC."
L'essai analyse la loi italienne n° 110 du 2017 (introduction d'un crime de torture dans le code pénal)
"La transparence de la vie politique est un «cheval de bataille » du gouvernement Renzi mais aussi des deux gouvernements qui l’ont précédé, à savoir le gouvernement Monti et le gouvernement Letta."
"Au 1er juillet 2019, 12.060.577 dossiers de santé dématérialisés ont été activés en Italie. [...] La dématérialisation des dossiers médicaux en Italie est un projet initié en 2009. Cette année-là, l’Italie s’engage vers la Sanità Digitale. [...] Le Gouvernement adoptait le décret n.178 du 29 septembre 2015, intitulé Règlement relatif au Fascicule sanitaire électronique, qui reprenait en substance les recommandations de l’Autorité de Garantie afin de mettre en place le dossier médical. Le décret du 25 octobre 2018 concernant les modalités techniques et les services télématiques donnait l’entière liberté aux Régions et Provinces autonomes de mettre en place le système informatique de documentation médicale."
"Le 19 juillet 2018, les députés et sénateurs, réunis en séance commune, ont élu le nouveau juge constitutionnel. L’alliance Mouvement 5 Étoiles et Ligue du Nord a réussi un double exploit : s’accorder sur le choix d’un juge et procéder à sa désignation dans un bref délai."
"Le référendum faussement qualifié d’initiative partagée institué par la révision constitutionnelle de 2008 n’a eu pour but que d’établir une réforme en trompe l’œil : faire accroire à une intervention d’une initiative populaire minoritaire quand ce sont en réalité des partis de gouvernement, certes oppositionnels, qui ont la maîtrise de l’initiative de cette institution, alors même, pourtant, que le seuil de signatures citoyennes requises apparaît en pratique inatteignable. La revendication d’un réel référendum d’initiative citoyenne, à l’instar d’autres pays européens, a vu, dans le sillage du mouvement des « gilets jaunes », bousculer sainement ce schéma, sans pour autant que le pouvoir en place ne satisfasse, ne serait-ce qu’à titre expérimental au niveau local, une alternative citoyenne potentiellement fructueuse."
"Depuis l'entrée en vigueur de loin° 94-475 du 10 janvier 1994, le législateur français s'est inscrit dans un processus de protection, au demeurant intéressé, du dirigeant caution dans l'optique de favoriser le redressement du débiteur en difficulté. Ce processus qui a atteint son point culminant lors de la réforme de 2005 a eu une influence sur le droit des procédures collectives applicables dans l'espace OHADA non sans heurter l'équilibre de l'institution du cautionnement dans sa globalité. Depuis la réforme de l'AUC du 10 septembre 2015, le droit OHADA adopte le même régime de traitement de la caution du débiteur en difficulté que le législateur français. Celui-ci consiste à favoriser le sort de la caution en instrumentalisant sa situation tant que l'espoir de sauver le débiteur en difficulté subsiste réellement. Cela se traduit notamment par une application ciblée de la règle de l'accessoire dans différentes étapes de la procédure selon un fil conducteur presque identiquement défini pat' chaque législateur, pourtant dans un environnement juridique et social différent. L'impact de ce paradoxe sur la protection efficiente de la caution se fait ressentir dans l'application des mesures de discipline collective à la caution d'une part, et l'exercice des recours de celle-ci d'autre part."
"Certaines des nouvelles institutions élues en droit français des procédures collectives s’appellent procédure de sauvegarde financière accélérée ( loi n°2010- 1249 du 22 décembre 2010 de régulation bancaire et financière) et procédure de sauvegarde accélérée de droit commun (ordonnance n°2014-326 du 14 mars 2014 portant reforme de la prévention des difficultés des entreprises et des procédures collectives). Ce sont des branches de la sauvegarde classique même si elles gardent certaines particularités et forment les passerelles entre les procédures de conciliation et de sauvegarde classique. Sur le plan européen, les choses ont changé car le règlement n°1346/2000 régissant la matière a été révisé et le nouveau texte adopté. Il entrera en vigueur le 26 juin prochain. Si au regard du règlement de 2000, l’éligibilité de ces nouvelles sauvegardes souffre toujours de divergence au sein de la doctrine, quant à celui de 2015, elle ne laisse place à aucune suspicion."
"Entre la reconnaissance du préjudice écologique par la Cour de cassation française, dans l’arrêt Erika du 25 septembre 2012 et la loi du 8 août 2016 établissant les voies de recours juridictionnels, le droit de l’environnement a connu en France une évolution fulgurante. La loi de 2016 notamment s’avère très généreuse puisqu’elle donne à l’Etat, aux collectivités territoriales et même aux associations agrées la faculté d’engager des actions en réparation du préjudice écologique. Le droit italien a connu les mêmes mutations dans les années quatre-vingt, soit trente ans avant la France. Pourtant, alors qu’il apparaissait précurseur au regard du droit français, il a soudainement bifurqué dans un sens plus restrictif, rognant sur le droit aux recours des régions et des collectivités locales. Ce changement de capte surprend de prime abord mais, en l’examinant de plus près, on constate la logique du raisonnement suivi par le législateur, qui conduit à s’interroger sur l’éventualité d’un avenir similaire du droit français."
"Par la loi n°2010-1249 du 22 décembre 2010 de régulation bancaire et financière et l’ordonnance n°2014-326 du 12 mars 2014 portant reforme de la prévention des difficultés des entreprises et des procédures collectives, le législateur français instituait deux nouvelles procédures en droit des entreprises en difficulté. Il s’agit, respectivement, de la sauvegarde financière accélérée (SFA) et de la procédure de sauvegarde accélérée de droit commun (PSA). Inspirées des célèbres prepackaged plans (plans pré-arrangés) du chapitre 11 du code fédéral Américain de la faillite, ces procédures constituent, en l’état actuel de la législation française, les deux passerelles entre les procédures de conciliation et de sauvegarde classique. Elle sont soumises aux règles régissant la sauvegarde de droit commun sous réserve des dérogations prévues à cet effet. La question de leur admissibilité à l’égard du règlement européen n°1346/2000 de l’insolvabilité continue à faire l’objet d’une grande divergence."
"Le contentieux contractuel éclate lorsque l'une des parties n'exécute plus ou pas ses obligations. La préoccupation essentielle en matière contractuelle, consiste à lutter contre une rupture abusive et arbitraire du contrat. La particularité du droit civil français en matière de rupture du contrat, est la place très importante qu'occupe le juge judiciaire. L'assurance de la protection des intérêts des parties et la garantie d'une sanction juste et équitable, reposent en effet, sur les épaules du juge. Il est par conséquent inconcevable que le juge soit écarté du jeu de la rupture contractuelle. L'orientation traditionnelle a toujours fait du juge un pilier dans le domaine de la séparation contractuelle, mais cette tendance commence quelque peu à se dissiper. Le juge doit toujours jouer un rôle très important en droit des contrats en général et dans sa rupture en particulier. Une protection efficace des parties donnerait à l'intervention du juge une utilité plus importante. Il est primordial, que le juge soit un acteur, en matière de rupture contractuelle, dont le rôle serait la garantie du respect des lois et la protection complète des parties ; le cas contraire, pourrait réduire alors la portée du code civil actuel à un simple catalogue de lois régissant l'anéantissement du contrat."
"La crise de 2008 a produit une insécurité caractérisée par le recul d'abord des transactions financières puis des activités économiques en general. Cette crise d'ampleur mondiale et dont la France ne s'est toujours pas relevée, prend sa source dans le secteur financier, à l'image de la crise issue du krach boursier de 1929."
"L’Italia è tornata … « L’Italie est de retour » affirmait Matteo Renzi, le 22 février dernier, à l’occasion des deux ans de son accession à la présidence du Conseil. Ce n’est sans doute pas le contenu de ce numéro 8 de La Lettre d’Italie qui démentira cette affirmation tant l’actualité juridique et politique est riche et variée."
"Les étrangers non-résidents représentent une catégorie juridique très particulière de contribuables. Deux conditions sont nécessairement réunies pour taxer ces opérateurs économiques : un critère de territorialité qui tient à la localisation de leur obligation fiscale, il s'agit du domicile fiscal ; et puis un critère de nationalité. La définition de ce contribuable est négative car il s'agit ainsi d'une personne qui n'a pas la nationalité française, et qui n'a pas son domicile fiscal en France. Le législateur ne fait que très rarement référence explicitement aux étrangers non-résidents dans le Code général des impôts, le principe étant l'assimilation avec les non-résidents français. Néanmoins, lorsqu'il y fait directement référence, c'est notamment pour le discriminer par rapport aux nationaux. Il en résulte une différence de traitement fiscal le plus souvent contraire aux normes communautaires et internationales lesquelles agissent efficacement contre toute dérive protectionniste du législateur français. En effet, le principe d'égalité et son corollaire le principe de non-discrimination constituent des principes fondamentaux du droit communautaire et international, lesquels se chargent de neutraliser les dispositifs fiscaux nationaux qui font application du critère de nationalité dans la taxation des étrangers non-résidents en France. En définitive, la rupture du principe d'assimilation envers les nationaux non-résidents crée des contentieux juridiques qui forcent l'Etat français à s'aligner sur la législation externe et qui tendent par conséquent à aboutir à la fin des discriminations."
"Qu'est-ce qui caractérise alors la conflictualité Ouest-africaine ? Quel est le rôle que la CEDEAO joue dans la stabilisation des États membres ? La CEDEAO est-elle devenue l'instrument de pacification et de consolidation pour les États ? Autant de questions sur lesquelles nous allons essayer de répondre à travers une analyse théorique des relations internationale et particulièrement des questions de guerre et de paix. Notre travail se délimite dans le triptyque espace-temps-institution. En effet, nos investigations couvrent la période allant de l 990 à 2009. L'intérêt de ce choix procède d'un constat depuis un certain temps de l'engagement et de l'efficacité de la CEDEAO dans le règlement des conflits en Afrique de l'Ouest. Ensuite, pour mieux connaître le rôle et l'objectif de la communauté pour la paix et la sécurité dans son espace. Cela se justifie par ses interventions dans la recherche de solutions à des nombreux conflits en Afrique de l'Ouest telles qu'au Liberia, en Sierra Leone, en Guinée ­Bissau et en Côte d'Ivoire. Ces derniers constituent aussi notre champ d'application. La démarche d'élaboration de ce document constituera à faire une étude du contexte de la conflictualité Ouest africaine d'une part, et une étude des techniques de règlement des conflits en Afrique de l'Ouest d'autre part. Ainsi, les concepts théoriques des relations internationales seront nos outils d'analyse."
"Les aidants qui travaillent tout en prenant en charge un ainé dépendant devraient être en mesure d’aménager leur rythme professionnel. Pourtant, le cadre juridique en vigueur dans l’Union est inexistant. La Commission a proposé des dispositions cohérentes avec les acquis de l’Union, dans le respect des principes de subsidiarité et de proportionnalité. Les aidants verront leurs droits en matière d’emploi protégés et toute mesure éventuelle de licenciement prohibée. L’objectif général est de garantir la mise en œuvre du principe de l’égalité entre hommes et femmes en ce qui concerne leurs chances sur le marché du travail. L’absence de congé adéquat pour s’occuper de proches âgés accentue l’inégalité du partage des responsabilités familiales entre les sexes."
"Le 9 juin 2017, le Conseil des ministres, sur proposition de Giuliano Poletti, ministre du Travail et des Politiques sociales, a approuvé l’examen préliminaire d’un décret législatif portant application de la loi n° 33 du 15 mars 2017 (loi sur la lutte contre la pauvreté, la réorganisation des prestations sociales et le renforcement des systèmes d’interventions et de des services sociaux). Ce décret-législatif (no 147 du 15 septembre 2017, Dispositions relatives à l’introduction d’une mesure nationale de lutte contre la pauvreté) a finalement été publié le 13 octobre dernier (GU, no 240)."
"La cassation d’un arrêt exécuté ne peut donner lieu qu’à restitution, peu important qu’il ait été rendu en matière de référé."
I. La vie de la Cour ; II. L'actualité juridictionnelle de la Cour.
"L’article 75, paragraphe 2, sous a), du règlement (CE) no 4/2009 du Conseil, du 18 décembre 2008, « Aliments », doit être interprété en ce sens qu’il s’applique seulement aux décisions rendues par les juridictions nationales dans des États qui étaient déjà membres de l’Union européenne à la date de l’adoption de ces décisions. De même, ce règlement doit être interprété en ce sens qu’aucune de ses dispositions ne permet que des décisions en matière d’obligations alimentaires, rendues dans un État avant l’adhésion de celui-ci à l’Union européenne et avant la date d’application dudit règlement, soient reconnues et exécutées, après l’adhésion de cet État à l’Union, dans un autre État membre."
"(CJUE, 6e ch., 1er juill. 2021, aff. C-301/20, UE, HC c/ Vorarlberger Landes- und Hypothekenbank AG, D. actu. 21 juill. 2021, obs. A. Panet ; D. 2021. 1286 ; AJ fam. 2021. 503, obs. A. Guichard)"
"L’article 70, paragraphe 3, du règlement (UE) no 650/2012 sur les successions transfrontières, doit être interprété en ce sens qu’une copie certifiée conforme du certificat successoral européen, portant la mention « durée illimitée », est valable pour une durée de six mois à partir de la date de sa délivrance et produit ses effets, au sens de l’article 69 de ce règlement, si elle était valable lors de sa présentation initiale à l’autorité compétente. De plus, l’article 65, paragraphe 1, dudit règlement no 650/2012, lu en combinaison avec l’article 69, paragraphe 3, de ce règlement, doit être interprété en ce sens que le certificat successoral européen produit des effets à l’égard de toutes les personnes qui y sont nommément citées, même si elles n’en ont pas demandé elles-mêmes la délivrance."
"L’adoption de l’ordonnance n°2020-1400 du 18 novembre 2020 « portant adaptation des règles applicables aux juridictions de l’ordre judiciaire statuant en matière non pénale et aux copropriétés » et celle du décret n°2020-1405 datant du même jour, ont pour finalité d’assurer la continuité de l’activité juridictionnelle durant la période couverte par l’état d’urgence sanitaire, au moyen d’une réorganisation du fonctionnement des juridictions. Pour nombre d’entre elles, les solutions dérogatoires retenues en réaction à la COVID-19 et à ses conséquences ne sont pas totalement inconnues du droit judiciaire français. L’innovation porte moins sur la nature des règles mises en place, que sur l’intensité de la dérogation apportée par rapport au droit commun de la procédure civile. Trois axes semblent avoir guidés le pouvoir règlementaire, à savoir : adapter la formation de jugement (dispositif de transfert de compétences juridictionnelles et limitation du nombre de magistrats devant siéger), aménager les audiences (accès limité, dématérialisation ou suppression) et simplifier les échanges entre les parties et avec les juridictions (prédominance des échanges « par tout moyen »)."
"JOUE L 343, p. 10"
Ord. n° 2011-1540 du 16 nov. 2011 portant transposition de la directive 2008/52/CE du Parlement européen et du Conseil du 21 mai 2008 sur certains aspects de la médiation en matière civile et commerciale
"CJUE, 26 avril 2012, C-92/12"
"JOUE L 201, p. 107"
"(Décision n° 1093/2012/UE du Parlement européen et du Conseil du 21 novembre 2012 relative à l'Année européenne des citoyens (2013) ; Rapport de la Commission au Parlement européen, au Conseil et au Comité économique et social européen établi en application de l'article 25 TFUE concernant les progrès réalisés sur la voie de l'exercice effectif de la citoyenneté de l'Union pendant la période 2007-2010, Com (2010) 603 final)"
"Com. 22 janv. 2020, n° 18-19.526 (FS-P+B)"
janvier 2019 - février 2020
"Com. 5 déc. 2018, n° 17-15.973 (F-P+B+I)"
"Com. 10 octobre 2018, n° 17-18.547 (F-P+B)"
"Com. 17 octobre 2018, n° 17-14.986 (F-P+B)"
"Com. 29 mai 2019, n° 16-26.989 (F-P+B) ; Com. 13 juin 2019, n° 17-24.587 (F-P+B)"
"Com. 29 mai 2019, n° 18-14.911 (F-P+B)"
"Cour de cassation (com.), 13 février 2019, n° 17-18.049 (FS-P+B+I), Sté Mergermarket Limited c/ Sté Consolis"
"Com. 23 oct. 2019, n° 17-25.656 (FS-P+B) et n° 18-16.515, FS-P+B"
"Com. 9 oct. 2019, n° 18-17.730"
"Com. 9 oct. 2019, n° 18-17.563, FS-P+B+I"
"Soc. 5 juin 2019, n° 18-12.189, inédit"
janvier 2018 - décembre 2018
"(CJUE, 3e ch., 15 avr. 2021, aff. C-729/19, TKF c/ Department of Justice for Northern Ireland, Procédures 2021. Comm. 167, obs. C. Nourissat ; Europe 2021. Comm. 238, obs. L. Idot)"
"Analyse de l’ensemble des arrêts prononcés – au jour de la publication – par la CJUE, à l’égard du règlement (CE) n°1393/2007 du 13 novembre 2007 relatif à la transmission des actes judiciaires et extrajudiciaires aux fins de signification. La portée de ces arrêts est étudiée à l’aune du nouveau règlement UE n°2020/1784 dont l’entrée en application est fixée le 1er juillet 2022."
I. La vie de la Cour ; II. L'actualité juridictionnelle de la Cour.
I. La vie de la Cour ; II. L'actualité juridictionnelle de la Cour.
"L'objectif de cet article est, à partir d'une réflexion relative aux phénomènes de globalisation et de globalisation du droit, de montrer comment le droit comparé apparaît comme une méthode privilégiée d'interprétation pour les juges dans ce nouveau contexte. La principale conséquence réside dans le développement du phénomène de circulation des solutions juridiques entre les juridictions constitutionnelles par l'utilisation de la méthode comparative. La question du droit de vote des détenus a servi d'exemple pour illustrer ce phénomène d’interaction croisée."
"La création d'un divorce par consentement mutuel par acte sous signature privée contresigné par avocats, déposé au rang des minutes d'un notaire constitue l'un des importants apports de la loi n°2016-1547 du 18 novembre 2016 de modernisation de la justice du 21 e siècle. Le caractère extrajudiciaire de ce divorce n'engendre pas seulement des conséquences au niveau interne. Ce trait caractéristique n'est pas neutre lorsqu'il est envisagé dans une perspective internationale."
"Il y a violence économique lorsqu’un déséquilibre dans les droits et obligations des parties à un contrat est la résultante de l’exploitation par l’une d’elles de l’état de faiblesse (économique) de l’autre, au moment de la formation de la convention. Enjeu croissant de justice contractuelle, le phénomène trouve de plus en plus fréquemment réponse dans les législations modernes. Dans l’espace géographique couvert par l’OHADA, il demeure du ressort des droits internes, renforcés par des législations d’origine communautaire (UEMOA et CEMAC notamment) dans certains domaines spécifiques. Les deux projets de textes de l’OHADA destinés à l’uniformisation du droit commun des obligations abordent certes la question, mais suivant deux approches différentes : l’un l’assimile au vice de violence tandis que l’autre l’associe à la lésion. Le présent article s’intéresse ainsi au régime juridique de la violence économique au sein de la zone de compétence de l’OHADA. Il commence par sonder ce régime qui, de lege lata, n’est pas harmonisé et se révèle lacunaire. Ce constat, associé à d’autres considérations d’ordre juridique et conjoncturel, permet de déterminer, de lege ferenda, les fondements d’un régime juridique uniforme optimisé."
"Dans le prolongement de la Commission bicamérale pour les réformes constitutionnelles , le constituant italien, à vingt-trois mois d’intervalle, s’est consacré à la question de l’organisation régionale. Il vient en effet de procéder coup sur coup à deux révisions portant modifications du titre V de la Constitution italienne de 1947. Ces deux révisions sont fondamentales bien qu’elles ne changent pas l’orientation de la forme de l’État italien car le constituant italien confirme en 2001 le choix fait en 1947 d’une Italie régionale et non fédérale (I). Cependant, les Régions se voient concéder davantage d’autonomie, accentuant, par là même, la décentralisation de l’État (II)."
"Les contributions réunies ici s'intéressent à ce que pourraient être les conditions d'un multiculturalisme réussi, dont deux dimensions sont ici privilégiées : celle de conflits religieux qui n'ont cessé d'être instrumentalisés au service de causes moins saintes que profanes ; celle des modalités d'intégration dans un espace que les institutions internationales ne sont manifestement pas parvenues à unifier, laissant aux organisations privées la charge de reprendre à leur compte cette responsabilité sociale dont les sociétés civiles portent la demande pressante et qui paraît bien détenir la clé de la soutenabilité d'un avenir commun."
La présente contribution revient sur deux textes fondateurs du droit du vivant : la Déclaration sur la personnalité juridique de l’animal dite Déclaration de Toulon de 2019 ainsi que la Charte du Droit du Vivant de 2021 proclamée en partenariat avec le programme Harmony with Nature de l’ONU. Ces textes mondialement mobilisés contribuent à enrichir et à diffuser la jurisprudence de la Terre sur tous les continents.
La présente contribution revient sur deux textes fondateurs du droit du vivant : la Déclaration sur la personnalité juridique de l’animal dite Déclaration de Toulon de 2019 ainsi que la Charte du Droit du Vivant de 2021 proclamée en partenariat avec le programme Harmony with Nature de l’ONU. Ces textes mondialement mobilisés contribuent à enrichir et à diffuser la jurisprudence de la Terre sur tous les continents.
fr_abstract_s
"Les « boues rouges » sont des déchets obtenus à l'issue de la production d'alumine à partir de bauxite. Dans le cadre du respect de la convention de Barcelone pour la protection de la mer Méditerranée les quantités de boues rouges rejetées en mer au large de Cassis ont été progressivement réduits depuis 1990 et ont pris fin le 31 décembre 2015, seul le résidu liquide étant à présent évacué en mer. Le résidu solide de bauxite modifié est aujourd'hui estoqué à Mange-Garri. Ce projet s'intéresse à l'utilisation du résidu de bauxite modifié (RBM), à faible phytotoxicité, dans le « traitement » d'anciens sites industriels contaminés tels que les scories du site de l'Escalette à Marseille. Au niveau psychossocial, cette a réutilisation des boues rouges pourrait alors être localement perçue soit comme une double solution, soit, compte tenu de l'attachement local-entre autres dynamiques représentationnelles-comme l'addition d'un deuxième problème à un environnement déjà pollué. La mise en oeuvre d'un tel dispositif d'inertage pourrait être freinée par la résistance des communautés locales ou de professionnels de l'environnement sur la base de leur attribution de risques à ces dispositifs. La perception des risques en général peut être soit atténuée-par manque d'information, ou par des mécanismes de protection identitaire ; soit amplifié-lors que ces mêmes mécanismes agissent pour exagérer la perception d'un risque qui serait autrement perçu comme négligeable (Kasperson et al., 2003). Cette étude s'intéresse aux variables associés à l'atténuation ou à l'amplification des risques associés à ce projet par la population de Marseille, plus directement concernée par le projet de traitement des sites avec le RBM. A cette fin, un questionnaire a été appliqué à 207 participants, entre habitants de différents arrondissements marseillais, ainsi qu'à des professionnels de l'environnement. Les principaux résultats ainsi que leur contribution à la compréhension des enjeux locaux associés à la question des boues rouges seront présentés."
"L'arsenic est un métalloïde se rencontrant naturellement sous forme de trace dans de nombreux sols. Les activités anthropiques (agriculture, extraction et exploitation de minerais principalement) ont conduit à son accumulation dans l'environnement. L'abaissement à 10 µg/L de la limite de qualité pour l'arsenic dans l'eau de consommation pose la question de l'efficacité des traitements existants. De nouvelles techniques plus performantes d'élimination de l'arsenic sont donc de plus en plus nécessaires. L'objectif de ce travail est de développer de nouvelles méthodes d'analyses de l'arsenic, fiables et utilisables sur le terrain, ainsi que des méthodes simples d'élimination de l'arsenic, de mise en œuvre facile et applicables à de petites unités de traitement comme celles rencontrées en zones à habitat dispersé (débit < 10 m3/h). L'adsorption de As(III) et As(V), méthode répondant aux critères précédemment définis, a été étudiée. Tout d'abord des supports classiques ont été considérés : des (oxy)hydroxydes de fer; puis des supports innovants : des argiles pontées dérivées d'une montmorillonite. Celle-ci a été modifiée par différents polycations (fer, titane et aluminium) de façon à créer des sites favorables à l'adsorption. L'adsorption a été réalisée selon différentes conditions, et dans des milieux plus ou moins complexes. Il s'avère que les (oxy)hydroxydes de fer fixent plus d'arsenic que les argiles pontées, tant sous la forme As(III) que As(V). Néanmoins, l'étude de la désorption a montré que l'argile pontée au fer était le seul support régénérable quasiment à 100%. Connaissant les différences de comportement selon la nature des espèces de l'arsenic inorganique As(III) et As(V), l'oxydation de As(III) par différents oxydants usuels a été l'objet d'une partie de l'étude. Les oxydants testés sont H2O2, NaOCl, FeCl3, KMnO4 et MnO2(s), couramment employés dans les traitements. De façon à quantifier la capacité oxydante de ces réactifs, une méthode colorimétrique a été développée. Celle-ci, facilement transposable sur le terrain, peut être appliquée aux eaux peu chargées en phosphate avec une limite de quantification de 20 µg As/L. Il s'avère que les oxydants les plus facilement utilisables dans une unité de potabilisation sont KMnO4 et FeCl3. A la suite de cette étude, un support à base d'une résine de polystyrène recouverte d'oxyde de manganèse a été synthétisé. Ce solide combine des propriétés d'oxydation et d'adsorption simultanées. Les capacités d'adsorption de ce solide vis-à-vis de As(V) et de As(III) sont remarquables et supérieures à une majorité des adsorbants étudiés récemment. La dernière partie a consisté en l'étude de la faisabilité des procédés mis au point sur un milieu plus proche des conditions naturelles. Pour cela, une eau artificielle représentative des eaux de type granitique, habituellement concernées par la pollution arséniée, a été préparée à partir de la compilation des compositions d'eaux souterraines destinées à la production d'eau potable. Ainsi, les concentrations en ions majeurs communes à ces eaux ont pu être déterminées. Cette eau artificielle a ensuite été utilisée après dopage en As(III) et As(V) dans diverses expériences d'oxydation et d'adsorption de façon à appréhender les mécanismes mis en jeu dans le milieu naturel. Il apparaît que les ions majeurs ont peu d'influence sur ces procédés, démontrant leur applicabilité au sein d'une filière de traitement."
"L'étude de l'écologie des bactéries, du phytoplancton et du zooplancton est d'intérêt majeur puisque ces organismes constituent la base du réseau trophique. En milieux côtiers, ces communautés planctoniques sont soumises à des apports anthropiques susceptibles de modifier leur écologie. C'est dans cette problématique que se situe ce travail de thèse. Un suivi annuel (avec un pas d'échantillonnage bimensuel) ainsi que nycthéméral, des bactéries, du phytoplancton et du zooplancton a été effectué dans deux écosystèmes différemment influencés par les apports anthropiques, la Petite Rade et la Grande Rade de Toulon (France, Méditerranée occidentale). Durant le cycle annuel, les concentrations de cuivre, plomb et cadmium ont également été dosées dans l'eau, la matière particulaire en suspension, les bactéries, le phytoplancton et le zooplancton. Il en a résulté que l'écologie planctonique était principalement influencée par les facteurs météorologiques, les deux rades démontrant une écologie différente du fait de leur géomorphologie. La Petite Rade a notamment mis en évidence une densité planctonique supérieure et une diversité inférieure à celles de la Grande Rade. Concernant les métaux, l'étude in situ a permis de mettre en évidence des concentrations métalliques plus importantes dans la Petite Rade que dans la Grande Rade, quel que soit le compartiment étudié. Il a été également montré que certains facteurs tels que la densité, la composition taxonomique et le milieu dans lequel évoluent les communautés planctoniques, avaient une influence sur l'évolution des concentrations métalliques dans les communautés planctoniques. Les bactéries et le phytoplancton ont démontré des capacités d'accumulation des métaux, en particulier pour le cuivre et le plomb, très importantes. En revanche, le zooplancton semble constituer une rupture dans la bioaccumulation des métaux dans le réseau trophique. Enfin, le rôle prépondérant de la matière particulaire en suspension dans le piégeage des métaux a été confirmé, démontrant de ce fait la nécessité de travailler sur des échantillons planctoniques purs pour avoir une bonne estimation des concentrations métalliques dans les différents compartiments planctoniques"
"La société ALTEO produit 300 000 tonnes par an de résidus de bauxite, la Bauxaline (résidus lavés et séchés au filtre-presse). Elle a engagé un vaste programme d’actions afin de développer différentes voies de valorisation/réutilisation de ce déchet, dont le projet de faisabilité Baux Geste [1]. Deux sites pilotes constitués par d'anciens résidus miniers émettant des drainages miniers fortement acides (pH ≈ 2.6) ont été sélectionnés, chacun de ces sites possèdent des caractéristiques particulières et différentes. Le premier site contient du plomb et du zinc lixiviables aux teneurs de 1.4 et 24 mg/kg, respectivement, mais pas d’arsenic lixiviable (Figure 1A). Ce sol est faiblement phytotoxique. Le second site est fortement phytotoxique et contient de l'arsenic et du plomb lixiviables aux teneurs de 5.2 et 22.9 mg/kg, respectivement, ainsi qu'une teneur élevée en cuivre lixiviable de 559.4 mg/kg (Figure 1B). Un traitement de ces sols a été réalisé à 7 doses de Résidu de Bauxite Modifié (RBM) avec un témoin non traité, puis les quantités de métaux lixiviables ont été mesurées après quatre semaines de réaction. Des essais de croissance des plantes sur ces sols traités ont également été menés en parallèle et sont encore en cours. Dans le cas des 2 sites, un apport croissant de dose de RBM a conduit à une augmentation du pH et à un fort abattement des éléments lixiviables observés des échantillons traités (Figure 1). De même, une réduction importante voire une suppression de la phytotoxicité a été observée dans les essais de végétalisation. Cette approche mixte en cours de démonstration permettrait de supprimer durablement les éléments traces lixiviables, à la fois par leur stabilisation chimique dans les sols par le RBM et par leur stabilisation physique par la phytostabilisation."
"Les écosystèmes marins constituent une cible majeure des changements globaux qui affectent de façon pérenne notre planète.Nous nous sommes intéressés à deux de ces changements susceptibles de menacer le milieu marin : le réchauffement climatique par le biais de l’étude de l’effet du stress thermique sur l’expression de ho-1 et la pollution via l’étude de l’impact des métaux lourds à effets perturbateurs endocriniens (le cadmium et le plomb) sur l’axe hypothalamo-hypophyso-gonado-hépatique (HHGH) chez le loup (Dicentrarchus labrax L.).Nos résultats montrent une réponse importante de ho-1 aux stress thermique et chimique dans le foie. Nous avons également démontré une forte accumulation hépatique du Cd, et à un degré moindre du Pb, accompagnée d’une sur-expression du gène mt codant pour les métallothionéines. Des variations dans l’expression de gènes clés le long de l’axe HHGH (arom b, fshß, arom a…) ont été observées après intoxication par les deux métaux sans pour autant induire des effets physiologiques observables.La question se pose de savoir si la synergie des stress physico-chimiques impacte la dynamique et l’état sanitaire des populations marines."
"L'objectif de ce travail est l’étude du phénomène de la réabsorption dans les plasmas et de son influence sur les paramètres des raies spectrales et sur les probabilités de transitions. Nous avons établi des formules théoriques des facteurs correctifs de l’auto-absorption pour les paramètres des raies, élargies par effets Stark et Doppler, dans le cas d’un plasma homogène et à l’équilibre thermodynamique local (ETL). A l’aide d’un algorithme développé sous Matlab, nous avons ajusté des spectres expérimentaux des raies de Xe II et de Si II émises par un plasma créé par un laser impulsionnel et obtenu les paramètres des raies corrigés de la réabsorption. Nous proposons une méthode pour déterminer les rapports des probabilités de transitions, tenant compte de l’effet d’auto-absorption. Cette méthode a été appliquée aux raies du Xe II et nous avons constaté une auto-absorption relativement faible. Elle a été ensuite appliquée aux raies des multiplets 2, 7.26, 8, 9 du Si II, réabsorbées : les rapports des probabilités de transitions obtenus sont proches des valeurs expérimentales d'autres auteurs pour les multiplets 8 et 9 et proches des valeurs théoriques pour les multiplets 2 et 7.26. Lorsque les raies spectrales sont fortement auto-absorbées, elles présentent un creux au sommet dû aux variations de température et de densité du centre à la périphérie du plasma. Nous proposons un modèle de plasma en deux régions : l’émission est prépondérante au centre, et l’absorption à la périphérie. Nous avons établi un algorithme d’ajustement de raies auto-inversées, que nous avons appliqué aux raies de résonance centrées sur 394.40 nm et 396.15 nm de l’Al I, émises par un plasma créé par laser sur une cible solide d’aluminium dans l’air. Les raies ainsi corrigées ont un rapport des intensités en très bon accord avec la valeur théorique."
"Un mélange de composants fluorescents est caractérisé par sa Matrice d' Excitation- Emission de Fluorescence (MEEF). Il est souvent utile de déterminer les caractéristiques spectrales et les concentrations relatives de chaque constituant du mélange à partir de ce type de données Dans un premier temps, nous décrirons le phénomène de fluorescence et les techniques de mesure relatives. Nous insisterons alors sur le modèle trilinéaire de fluorescence.Nous ferons ensuite le point sur les méthodes d'analyses multilinéaires utilisées dans le cas de mélanges ou de composés organiques, en particulier l'algorithme de décomposition trilinéaire. PARAFAC. L'application de ces méthodes est restreinte aux solutions peu concentrées. Dans le cas contraire le chevauchement des différents spectres crée des effets d'écran qui ne sont alors pas pris en compte. Nous illustrerons sur des données réelles, les limites intrinsèques de PARAFAC pour l'analyse de telles solutions.La technique usuelle de correction de l'effet d'écran est basée sur un vieux modèle, que nous redémontrerons, et nécessite la mesure de l'absorbance des solutions. Ceci est source d'erreurs, et parfois impossible. Une des principales avancées de cette thèse concernera donc la mise en oeuvre d'un protocole de correction simple des effets d'écrans, ne nécessitant pas la mesure de l'absorbance. Nous proposerons également d'autres approches purement numériques. Notre propos sera illustré sur des mélanges réalisés en laboratoire, et des échantillons de matière organique très concentrés. Enfin, nous donnerons deux exemples d'application des différentes techniques exposées au suivi de la matière organique ."
"mots-clés : MON, métaux traces, proton, DPASV, PROSECE Résumé : L'analyse du comportement d'acides fulviques et humiques standards (Suwannee River et Laurentian River), vis-à-vis des métaux traces (Cd, Pb) et du proton, a été réalisée à partir de différentes titrations potentiométriques et voltammétriques, par additions de solutions standards en mode logarithmique. Les données expérimentales ainsi obtenues ont été traitées à l'aide du logiciel PROSECE, en définissant un modèle basé sur un « chimio-type » constitué de propriétés discrètes analysables des MON étudiées. Ce « chimio-type », constitué d'un set de trois types de « quasi-particules » (Sposito, 1981), a permis de modéliser les sites acides (6 « quasi-particules » de type I), les sites complexants échangeables (4 de type II) ainsi que les sites complexants spécifiques (2 de type III) présents sur la MON. Les paramètres de complexation optimisés (i.e. concentrations, constantes de complexation et d'acidité de ces « quasi-particules ») permettent de prévoir le comportement de la MON vis-à-vis des métaux traces, et les phénomènes de compétition métal/métal et métal/proton. Ces paramètres, quasi-thermodynamiques, sont facilement intégrables dans un modèle de transport de contaminants tels que le modèle SiAM3D-MOCO (IFREMER). Abstract: To analyse the properties of humic and fulvic acid standards (Suwannee River and Lauretian River) concerning their relation towards trace metals (Cd, Pb) and protons, several potentiometric and voltametric titrations have been carried out. The experimental results were achieved by logarithmic addition of standard solutions prior to mathematical treatment by new calculation software that was developed in our laboratory. PROSECE (Programme d'Optimisation et de SpEciation Chimique dans l'Environnement) defines a discreet model based on a so-called ""chimio-type"", representing the characteristics of the analysed natural organic matter (NOM). This ""chimio-type"" consists of three different types of ""quasi-particles"" (Sposito, 1981), which allows to model the acid sites (6 ""quasi-particles"" of type I), convertible complexing sites (4 of type II) and specific complexing sites (2 of type III) inside the NOM. Optimised complexing parameters (e.g. concentration, complexation-and acid-constants of these ""quasi-particles"") allow predicting the properties of NOM related to trace metals and to the phenomenon of metal/metal and metal/proton competition. These ""quasi-dynamic' parameters can be integrated easily in a contaminants transport model like SiAM3D-MOCO (IFREMER)."
"L'objet de cette étude est de mettre au point une méthode d'analyse et de modélisation des propriétés de complexation et d'acidité de la matière organique naturelle (MON) la plus rigoureuse possible, en représentant ses interactions par des paramètres fiables, et en s'affranchissant de la structure exacte des composés organiques la constituant. La MON a été définie à partir d'un modèle chimique constitué de quasi-particules reproduisant les interactions non-spécifiques de la MON vis-à-vis des cations, et les interactions spécifiques à un seul métal. Le logiciel PROSECE, spécifiquement développé à cet effet, permet de calculer la spéciation chimique et d'optimiser un grand nombre de paramètres de complexation à partir de la modélisation de données expérimentales. Comparé à d'autres traitements de données, PROSECE s'est montré nettement plus approprié, d'autant plus lorsqu'il est couplé à une technique d'ajouts logarithmiques. Les interactions MON-métaux traces ont été analysées par DPASV, sur la plus large fenêtre analytique possible. Un stand de mesure a été spécialement élaboré afin d'automatiser les titrations logarithmiques. Les interactions MON-proton ont été analysées par titration acido-basique. Une caractérisation simultanée des interactions entre une MON standard, l'acide fulvique Suwannee River, le cadmium, le plomb et le proton a été réalisée. Les propriétés d'acidité d'échantillons prélevés sur La Seine au cours d'une année ont été étudiées et reliées à différents facteurs biogéochimiques. Une concentration minimale de 0.04 meq en sites acides a été définie comme limite en dessous de laquelle l'optimisation des propriétés d'acidité d'une MON devient incorrecte. PROSECE a été également utilisé pour modéliser des expériences de quenching de fluorescence, rendant plus précise leur analyse. L'analyse des propriétés de MON marines a été initiée. Du fait de la complexité du milieu, la caractérisation des propriétés de complexation est plus difficile, nécessitant quelques adaptations analytiques."
"’impact de plusieurs éléments traces métalliques (ETMs) (plomb, zinc, cuivre ou cadmium) sur la croissance de deux espèces phytoplanctoniques marines, la diatomée Skeletonema costatum et le dinoflagellé toxique Alexandrium catenella, a été étudié à partir de cultures réalisées en conditions physiologiques et en conditions contaminantes. Les résultats obtenus ont révélé des perturbations de croissance chez S. costatum et chez A. catenella, lorsque ces espèces étaient exposées à de fortes concentrations métalliques. Les effets létaux se manifestaient par une forte mortalité cellulaire, accompagnée, chez A. catenella, par la conversion d’un certain nombre de cellules végétatives en kystes de résistance. Puis, l’exsudation de la matière organique dissoute (MOD) par S. costatum et par A. catenella en réponse aux stress métalliques, a été caractérisée. L’exsudation du carbone organique dissous (COD) variait en fonction de la phase de croissance, et était associée au niveau de contamination métallique, et à la nature du métal testé. Les valeurs maximales de COD exsudé par cellule étaient mesurées en réponse à la contamination par le cuivre 16 µM pendant la phase d’adaptation, et en réponse à la contamination par le cadmium 200 µM pendant la phase exponentielle de la croissance. Plus spécifiquement, la matière organique dissoute fluorescente (FDOM) exsudée par S. costatum et par A. catenella a été analysée par spectroscopie de fluorescence 3D associée à l’algorithme PARAFAC. Les traitements PARAFAC révélaient quatre composantes, lesquelles étaient attribuées à deux contributions principales, l’une liée à l’activité biologique de l’espèce, l’autre liée à la décomposition de la matière organique. Les composantes C1 et C2 étaient combinées à des pics tryptophaniques et à des substances humiques, tandis que les composantes C3 et C4 étaient associées à la production de matière organique marine. Par ailleurs, le glucose et le galactose étaient prédominants parmi les aldoses constituant les polysaccharides exsudés dans la MOD. De plus, des modifications protéomiques étaient observées dans les protéomes d’A. catenella, en réponse aux divers stress métalliques. Les protéines de stress exprimées par A. catenella étaient mises en évidence par comparaison des profils d’expression protéiques (PEPs) obtenus par électrophorèse bidimensionnelle (électrophorèse 2D), en conditions physiologiques et en conditions contaminantes. Les protéines de stress étaient impliquées dans de nombreuses catégories fonctionnelles : réponse au stress oxydatif (superoxyde dismutase, sous – unités du protéasome), photosynthèse (ribulose 1,5-bisphosphate carboxylase, complexe péridinine chlorophylle – protéine, ferrédoxine-NADP réductase), métabolisme des carbohydrates (triosephosphate isomérase, ribose 5-phosphate isomérase, malate déshydrogénase, photorespiration et métabolisme du phosphore (phosphoglycolate phosphatase), métabolisme énergétique (ATP – synthase), signal cellulaire (calmoduline), activité chaperonne (HSP 70, HSP 90) et bioluminescence d’A. catenella (luciférine – binding protéine). La sur – expression de la phosphoglycolate phosphatase (PGP) et celle de l’ATP – synthase, observées en réponse à la contamination par le plomb, pourraient participer à une stratégie de défense mise en place par A. catenella afin (i) de se protéger du stress oxydatif lié à la contamination métallique, la PGP étant impliquée dans la dissipation de l’excès d’énergie (ii) de produire davantage de réserves énergétiques (ATP) et ainsi de répondre à des besoins accrus en raison du stress métallique, dans le but de s’y adapter."
"Des résultats récents ont montré que les podzols équatoriaux stockent d’importantes quantités de carbone dans leurs horizons Bh profonds. Cette constatation amène deux questions principales : (1) comment et à quel rythme se sont formés ces sols (2) dans quelle mesure le changement climatique pourrait induire une production par ces sols de carbone atmosphérique susceptible d’impacter le système climatique mondial.Dans ce contexte, nous avons réalisé un modèle qui permet de contraindre les flux de carbone à la fois par les stocks observés et leur âge 14C. En situation suffisamment simplifiée, nous avons établi une relation formelle entre l’évolution des stocks et l’âge 14C de celui-ci. Appliqué aux podzols amazoniens, notre modèle a apporté des résultats nouveaux et inattendus. Il a permis de montrer que ce sont les horizons de surface des aires podzolisées les plus hydromorphes qui sont les plus gros contributeurs de MOD transférée vers le réseau hydrographique et la mer. On observe que la formation des Bh n’est possible qu’en envisageant deux compartiments, rapide et lent. Une estimation basse de leur temps de formation permet de différencier des podzols relativement jeunes (temps de formation de l’ordre de 15 103 - 25 103 ans), développés sur des sédiments Holocènes relativement récents, et des podzols âgés (temps de formation de l’ordre de 180 103 - 290 103 ans), développés sur des sédiments plus anciens. Le taux d’accumulation du carbone dans les podzols étudiés varie de 0,54 à 3,17 gC m-2 an-1, ce qui correspond à une séquestration de carbone de l’ordre de 3 1011 gC an-1, faibles à l’échelle annuelle, mais significative aux échelles géologiques.Les expérimentations de percolation en colonne nous ont permis de montrer la réactivité du Bh et la présence, malgré des rapports C/N très élevés (63 en moyenne), d’une activité bactérienne significative qui modifie la nature de la MOD qui le traverse. Cette dernière a la capacité de transporter Al et Fe sous forme de complexes organo-métalliques, complexes susceptibles de migrer à travers des matériaux très kaolinitiques. Ces résultats participent à la compréhension des transferts de MOD d’origine pédologique dans les nappes profondes.Dans l’hypothèse de l’apparition d’un climat à saisons contrastées, nous avons pu montrer qu’une durée sans pluie de 90 jours après disparition de la nappe perchée ne permettrait pas d’atteindre le point d’entrée d’air par assèchement des horizons superficiels. Néanmoins, dans l’hypothèse d’une entrée d’air, l’extrapolation des taux de minéralisation mesurés expérimentalement en conditions oxiques aboutit à une production de C atmosphérique de l’ordre de 2,0 1014 g de CO2 par an, ce qui peut impliquer une rétroaction positive du système climatique mondial."
"Les sols jouent un rôle important dans la plupart des activités qui se produisent sur terre, parmi lesquelles sa participation dans les grands cycles biogéochimiques n'est pas des moindres. En effet, la matière organique des sols (MOS) joue un rôle clé dans la durabilité environnementale, car elle est liée au cycle du carbone et des nutriments et ceci est un élément primordial pour les études liées au changement climatique mondial et pour les études agronomiques. L'un des principaux constituants de la MOS sont les substances humiques (AH), acides fulviques (AF) et humine (HU). L'étude des propriétés optiques de la matière organique est un outil important pour la compréhension structurale et moléculaire des fractions humiques. Ce travail porte sur la genèse de Spodosols Amazoniens par l'évaluation des caractéristiques structurelles de leur matière organique et de leurs propriétés de complexation métallique en utilisant diverses techniques spectroscopiques. Les résultats ont montré de grandes accumulations de carbone en profondeur et que la matière organique dans ces spodosols amazoniens est constituée en quatre groupes : -plus récalcitrant, humifié et ancien ; -labile et jeune ; -récalcitrant, peu humifié et ancien ; humifié et jeune. De toute évidence, le travail a montré que le processus d'humification n'a aucune relation directe avec la datation de la matière organique et que les facteurs tels que la texture, la présence d'eau et les micro-organismes ont influencé les processus de formation et d'humification de cette matière organique. Enfin, la fraction des AF du sol semble avoir une contribution provenant des eaux souterraines, sa structure chimique variant peu au sein d'un profil et ayant une interaction sélective avec métaux présents dans ces sols. La fraction des AH, cependant, s'est révélée moins sélective, associé divers types de métaux, tels que K, Fe, Mg, Zn et Al. Par contre, la structure chimique des AH semble dépendre du profil de sol. En raison de la diversité des métaux que les AH peuvent complexer, ils devraient être les principaux responsables de la fertilité du sol. Les résultats ont montré que les trois fractions humiques (AF, AH et HU) sont fortement impliquées dans le processus de podzolisation des sols et que les AF ont un rôle prédominant dans le transport d'Al tandis que les AH sont les protagonistes du transport du Fe."
"L’étude réalisée porte sur l’évaluation du niveau de contamination par les éléments traces métalliques (ETM) des sédiments de la rade de Toulon, une rade semi-fermée soumise à un fort impact anthropique. Le prélèvement de carottes d’interface en 52 points répartis sur l’intégralité de la Rade a permis d’établir une cartographie précise des caractéristiques sédimentaires et des teneurs en métaux/métalloïdes. Les résultats obtenus sur les sédiments de surface ont montré l’état de contamination significatif de la rade (en particulier en Cu, Hg, Pb, et Zn), notamment dans les zones les plus enclavées de la petite rade, où les teneurs peuvent dépasser de plusieurs ordres de grandeur les limites définies par la législation en vue d’opérations de dragage. La distribution de la contamination observée a clairement indiqué un export de la petite vers la grande rade (normalement moins exposée), probablement gouverné par des processus hydrodynamiques responsables de la remise en suspension du sédiment contaminé. Les profils sédimentaires de carottes d’interface prélevées dans des zones de contamination contrastée ont révélé la présence systématique de pics de contamination dans les 20 premiers cm. Compte tenu des taux de sédimentation déterminés, ceci démontrerait que la rade a été soumise à un épisode de multi-contaminations majeur, probablement lié aux conséquences de la 2nde guerre mondiale. L’analyse des eaux interstitielles et surnageantes (paramètres physico-chimiques, majeurs, traceurs diagénétiques et ETM) de ces carottes d’interface a permis d’étudier la mobilité des ETM dans le sédiment. Les profils obtenus apparaissent essentiellement contrôlés par des mécanismes diagénétiques et démontrent le rôle exercé par les principales phases porteuses présentes dans le sédiment (oxy-hydroxydes de Fe et de Mn, sulfures) sur la mobilité des ETM. La modélisation de ces profils a permis d’évaluer les flux diffusifs à l’interface eau sédiment, afin de déterminer l’action du sédiment, en tant que puits ou source de contamination pour la colonne d’eau. Les flux diffusifs sortant calculés apparaissent relativement faibles en comparaison des teneurs totales mesurées dans le sédiment, démontrant que la majorité des ETM est fortement immobilisée dans le sédiment.Enfin, ce travail a été complété par des expériences de remise en suspension en laboratoire et sur le terrain, visant à simuler différents scénarios possibles (tempête,trafic maritime, dragage). Dans les conditions étudiées, si pour certains ETM la remobilisation en solution est faible (ex. As, Hg), elle peut au contraire être très significative pour d’autres (ex. Cd, Cu, Pb) conduisant à une contamination non négligeable de la colonne d’eau."
"De la surrection des Pyrénées au creusement des galeries de la grotte du Mas d’Azil, des topographes aux géoarchéologues, l’histoire géologique de cette exceptionnelle cavité n’aura plus de secrets pour vous. À partir des vestiges laissés par l’activité de la rivière ou l’étude minutieuse des cartographes, ce carnet décrit les différentes phases de formation de la grotte et l’intact des changements climatiques sur l’occupation humaine du lieu."
"La grotte du Mas d’Azil est un phénomène géologique imposant par ses dimensions et la complexité de son réseau ; elle est aussi un haut-lieu de la Préhistoire. Sous le porche sud, en rive gauche de la rivière, une terrasse recèle encore plusieurs niveaux d’occupation de la fin de la dernière période glaciaire jusqu’à l’âge du Bronze intercalés avec des limons d’inondation. Le réseau karstique qui se développe en rive droite, quant à lui, était réputé vidé de son contenu archéologique. En complément d’une série d’interventions d’archéologie préventive, un programme de recherche destiné à faire un état des lieux cartographique et archéologique de la cavité est en cours depuis 2013. L’étude des vestiges de coupes stratigraphiques en place permet de proposer une nouvelle histoire de la grotte en lien avec les occupations humaines, notamment pendant la dernière glaciation. En effet, des sédiments d’origine fluviatile couvrent les niveaux aurignaciens sur plusieurs mètres d’épaisseur. Les populations humaines ne réinvestissent la grotte qu’à partir du Solutréen / Badegoulien, puis massivement durant le Magdalénien, dont plusieurs niveaux d’occupation résiduels ont été retrouvés. L’importante dynamique d’aggradation sédimentaire au cours de la dernière période glaciaire permet de discuter la question de l’accès à la cavité ou à certaines de ses galeries tout au long de cette période et de faire le lien entre conditions climatiques, dynamiques sédimentaires et occupations préhistoriques."
"Notre travail a consisté en un suivi mensuel de l'évolution des concentrations du phosphore inorganique, organique hydrolysable, oxydable dans la petite rade et la grande rade de Toulon. L'activité des phosphatases responsables des conversions du phosphore organique en phosphore inorganique a été mesurée en parallèle. Les concentrations en phosphore inorganique sont toujours très basses (70-80 nM) en particulier en période estivale. Elles fluctuent beaucoup d'avril à septembre et sont plus stables pendant le reste de l'année. La fraction hydrolysable, représente en moyenne 17 à 22 % du phosphore total dont 4 % correspondent à la fraction hydrolysable par la phosphatase. Ces concentrations évoluent de la même manière que celle des formes inorganiques. L'activité phosphatasique dissoute ne représente 10 à 35% en moyenne de l'activité totale. Elle inclut des composantes à forte et à faible affinités, l'activité à faible affinité étant toujours plus élevée que l'activité à forte affinité. Dans les deux sites, les activités à faible affinité présentent des maxima qui coïncident avec ceux des abondances phytoplanctoniques. L'activité particulaire est responsable de près de 60 % de l'activité totale. Elle comporte également des composantes à forte et faible affinités. L'activité à forte affinité serait externe et son pH optimal est proche de celui de l'eau de mer, alors que l'activité à faible affinité serait interne et son pH optimal est supérieur à 9. L'activité particulaire à forte affinité est particulièrement élevée pour la plus petite classe de taille (0,45-1 µm) alors que l'activité à faible affinité est plus élevée pour la plus grande classe (>90µm). Cette activité intracellulaire est plusieurs centaines de fois plus élevés chez les larves de cirripèdes que chez les autres espèces. L'activité phosphatasique à forte affinité est toujours basse quand les concentrations en orthophosphate sont élevées. En revanche, elle augmente souvent quand ces concentrations diminuent. Lorsque ce n'est pas le cas, les concentrations en phosphore organique hydrolysable étaient élevées. L'ensemble de ces données permet de mieux comprendre le mode de régulation de l'activité phosphatasique par les composés phosphorés et de conclure que la mesure de sa composante à forte affinité peut être considérée comme le meilleur indicateur du stress phosphoré dans la rade de Toulon."
"La stéatose hépatique non-alcoolique (NAFLD) est l’affection hépatique chronique la plus fréquente à l’heure actuelle dans le monde industrialisé car fortement liée au développement du syndrome métabolique et des dyslipidémies associées. À ce jour, les mécanismes de la pathologie restent mal définis et les moyens thérapeutiques disponibles ont une efficacité modérée. Des études épidémiologiques ont rapporté un effet bénéfique de la consommation de thé pour lutter contre les désordres hépatiques et les facteurs de risque cardiovasculaires tel que la dyslipidémie. Cependant, les mécanismes par lesquels le thé atténue la stéatose hépatique et la dyslipidémie restent méconnus. Par conséquent, l’objectif de ce travail de thèse était d’évaluer les effets et les mécanismes d’action d’un mélange de thés vert, oolong et Pu-erh, le thé Hao Ling, sur la NAFLD et la dyslipidémie, à travers deux approches, l’une sur modèle cellulaire et l’autre sur modèle animal.Les résultats ont permis de mettre en évidence que le thé Hao Ling permettait de diminuer la lipogenèse hépatique in vitro et in vivo et ainsi d’atténuer la stéatose induite par un régime hyperlipidique et riche en saccharose chez le rat Wistar. Nous avons observé que ce thé permettait d’améliorer le profil lipidique sanguin en augmentant le taux de HDL plasmatique. Nous avons également pu mettre en évidence que le thé possédait des propriétés antioxydantes et hépato-protectrices permettant de lutter contre un inducteur de stress oxydant in vitro et de diminuer la peroxydation lipidique in vivo. Enfin, nous avons démontré que le stress oxydant ""per se"" entraînait une accumulation de lipides intracellulaires sur hépatocytes isolés et que le thé, grâce à ses propriétés antioxydantes, prévenait ce phénomène. Le thé Hao Ling constitue une bonne approche nutritionnelle dans la prévention de la NAFLD et dans le maintien du rapport LDL-Cholestérol/HDLCholestérol."
"Ce livre propose la réédition augmentée de notes de bas de pages et d'une bibliographie de l'ouvrage que le mathématicien Paul Appell écrivit en 1925 pour rendre hommage à son ami d'enfance Henri Poincaré. Dans cette biographie à la fois intimiste et scientifique, il met ainsi en lumière des anecdotes méconnues sur la vie de Poincaré comme par exemple l'origine de son zéro au baccalauréat en mathématiques. Il analyse ensuite d'une manière très accessible son oeuvre scientifique (comprenant ses travaux en mathématiques, en mécanique, en astronomie, en physique mais également en philosophie) avec le regard avisé de celui qui resta tout au long de sa carrière dans le sillage de « l'illustre géomètre ». Il rappelle enfin les différents engagements de Poincaré en matière d'enseignement notamment mais aussi son rôle dans l'affaire Dreyfus."
"Ce livre présente un portrait inédit du mathématicien français Henri Poincaré à partir de ce qu'en disaient les journaux de son temps. Un choix abondant de coupures de presse permet en effet une approche originale du personnage : on y découvre les faits les plus marquants de sa carrière mais aussi son rôle dans l'espace public, tant pour ses multiples compétences scientifiques et techniques que pour ses éclairages philosophiques. Doublement académicien, auteur d'ouvrages largement diffusés, son aura dépassa le seul cercle des érudits pour toucher le grand public dans les domaines les plus variés, société savante et presse généraliste ayant fait de lui une sorte de référent dans la plupart des champs de la connaissance et au-delà. Des anecdotes les plus insolites aux publications méconnues, en passant par les diverses polémiques dans lesquelles on l'entraîna souvent malgré lui, les journaux nous dévoilent un Poincaré inattendu, qui se prêta au jeu de cette dialectique entre espace savant et espace public, assumant ainsi de façon originale une forme de ""vulgarisation scientifique"" comme un rôle d'éclaireur."
"Le recyclage des déchets organiques est une préoccupation environnementaleimportante tant du point de vue énergétique que du point de vue du réchauffement climatique.Le compostage permet la transformation d'un déchet en produit, le compost, mais permetaussi d'augmenter le stockage du carbone dans les sols. Parmi les déchets, les boues de stationd'épuration sont un type de déchet urbain organique problématique et grandissant. La teneurélevée en matières organiques des boues peut ainsi être valorisée grâce au co-compostageavec des déchets verts. En effet, de nombreuses modifications chimiques se produisent aucours du processus de maturation d’un compost transformant les molécules simples ensubstances humiques bénéfiques aux sols. Cependant, il n'existe pas encore, à ce jour, detechniques rapides et universelles permettant de prédire la stabilité et la maturité du compost.Il est, en effet, nécessaire, pour suivre l'évolution d'un compost, de mesurer un nombreimportant de paramètres biologiques (test de respirométrie, test de germination), physiques(pH, température,…) et chimiques (potentiel d'oxydo-réduction, capacité d’échange de cations(CEC),…). Ceci n'est pas économiquement viable et rarement fait sur les plates-formes decompostage.L’objectif de cette étude a donc été d’identifier quel(s) paramètre(s) pourrai(en)t êtreutilisé(s) pour suivre aisément, sur une plate-forme de compostage, l’évolution du compostsur le terrain. Pour cela, nous avons suivi un co-compostage de boues de station d’épurationau cours du processus de maturation dans des composteurs de particuliers (sur place et enserre) et dans l’andain (sur place) afin d’observer l’effet de la taille mais aussi l’effet desconditions climatiques sur le processus de compostage. Une méthode d’extraction simple etpeu coûteuse de la matière organique extractible à l’eau (WEOM) utilisant un percolateur àeau à température ambiante est proposée. Ces extraits à l'eau ont été caractérisés par différentsparamètres, tels que le carbone organique dissous (CODWEOM) et l’azote total (NTWEOM) oules acides gras volatils (AGVWEOM) mais aussi par différentes techniques spectrométriquesd'absorption UV-Visible et de fluorescence. Pour l’absorption, des indices nous informent surl'aromaticité (SUVA254 WEOM), la taille moléculaire (E2/E3 WEOM), le poids moléculaire (E4/E6WEOM) et sur les substitutions des composés organiques (EET/EBZ WEOM) de la matièreorganique. En ce qui concerne la fluorescence, les indices d'humification HLIF, Kalbitz WEOM,Milori WEOM, Ra,c WEOM et Zsolnay WEOM ont été utilisés pour étudier les propriétés de laWEOM. De plus, à partir des matrices d'excitation-émission de fluorescence, les composantsCP/PARAFAC ont été déterminés. Au total, ce sont 23 paramètres qui ont été suivis au coursdu compostage.Les résultats permettent de discriminer les indices les plus pertinents pouvant êtreutilisés sur le terrain. Ces indices sont le SUVA254 WEOM, ainsi que les indices de Kalbitz WEOMet de Milori WEOM. La spectrométrie est une technique pertinente pour le suivi du compost surle terrain. Il n'a pas été décelé d'influence de la météorologie pour les composteursparticuliers, mais bien un effet de taille pour certains paramètres."
"Le but de ce travail est l'étude des apports (nutriments, métaux, matière organique) d'une petite rivière côtière à la Méditerranée. Dans ce contexte, cette étude vise à déterminer les apports durant le fonctionnement de base de la rivière et lors des crues. Dans un second temps, la répartition des polluants métalliques entre la phase dissoute et particulaire a été abordée. Enfin, la caractérisation de la matière organique et de son rôle sur leur biodisponibilité a été évaluée. Cette étude a montré que les paramètres physico-chimiques présentent une variation journalière, perturbée lors des pluies, qui provoquent une chute des valeurs mesurées - pH, conductivité, potentiel rédox, température, taux de O2 -. En ce qui concerne les ions majeurs, les pluies provoquent une baisse des concentrations sauf pour le potassium qui voit ses concentrations augmenter lors des crues. Au printemps et surtout en été, les flux de nutriments couplés au faible débit, causent une eutrophisation des eaux. Concernant les métaux, leur principale source est située en zone urbaine ou commerciale et ils présentent une forte affinité pour la fraction particulaire. Lors des pluies, les concentrations métalliques mesurées présentent de fortes augmentations dues notamment au lessivage de surface. Globalement, pour les polluants métalliques, les concentrations en régime de base sont faibles et traduisent une eau de bonne qualité, alors que leurs augmentations pendant les crues deviennent problématiques pour l'environnement. L'étude de l'influence de l'Eygoutier a été étendue à la rade de Toulon dans le cadre d'un programme national de recherche ECOTDYN. En régime de crue l'impact de l'Eygoutier s'est fait ressentir sur 85 mètres de l'embouchure et durant 8 d'heures. Sachant que la rade de Toulon est un milieu semi-fermé et donc fragile, ces apports peuvent constituer un risque pour l'environnement."
"Le développement croissant de l’urbanisme et des activités anthropiques fait peser une pression de plus en plus importante sur les zones côtières, touchant particulièrement les régions méditerranéennes. Les processus liés au changement climatique et aux activités humaines impactent en profondeur les écosystèmes marins et tout particulièrement les systèmes planctoniques qui présentent une sensibilité accrue aux perturbations du milieu. Toutefois, les effets de ces processus agissant de concert à long terme sur la structure et le fonctionnement des écosystèmes côtiers sont encore mal compris. Ainsi, les séries temporelles planctoniques représentent des outils précieux pour la compréhension des processus impactant le milieu marin. Les séries temporelles du laboratoire EBMA-PROTEE de Toulon font partie des séries les plus anciennes de Méditerranée et présentent l’intérêt de combiner l’impact des processus naturels et anthropiques sur les communautés phyto- et zooplanctoniques. C’est dans ce contexte que ce travail a pour objectif de préciser le rôle de ces facteurs sur la dynamique et la composition phyto- et zooplanctonique en Méditerranée nord-occidentale (Toulon, France). Une première partie a ainsi consisté à évaluer le rôle des mélanges verticaux se produisant au sein du bassin Ligure, comme moteur de la dynamique printanière du plancton au sein de la Rade de Toulon. Cette étude a permis d’identifier des variations significatives du régime des précipitations associées à des modifications de la salinité au sein du site de la Grande Rade de Toulon (« LaB ») sur la période 2005-2013. Variations retrouvées également sur 2 autres sites côtiers du bassin Ligure (« SOFCOM » and « Point B »). Conformément à l’hypothèse énoncée, les années conditionnées par un automne et un hiver secs ont présenté en moyenne une production phytoplanctonique printanière supérieure au sein des 3 sites, comparées aux années les plus humides. Toutefois, la présence de différences entre les sites a souligné l’impact important de mécanismes locaux sur le conditionnement de la dynamique phytoplanctonique. L’impact des changements de régime des précipitations sur la composition micro-phytoplanctonique et méso-zooplanctonique a été étudié au cours d’une seconde partie. Les résultats ont suggéré un changement profond des communautés phytoplanctoniques lié à l’augmentation des précipitations, impactant également la structure des communautés zooplanctoniques. Ainsi, ces résultats semblent indiquer que les précipitations interviennent au sein de deux processus, impactant différemment les communautés planctoniques : d’une part, une absence de précipitations participerait à la mise en place de mélanges hivernaux et à la fertilisation des eaux de surfaces au sein de bassin Ligure profitant à la rade de Toulon; d’autre part, les changements dans la composition phytoplanctonique ont évoqué la mise en place d’un processus d’eutrophisation durant les périodes pluvieuses, alimenté par les apports des rivières locales. En parallèle de cet aspect temporel, le suivi de la Petite et Grande Rade de Toulon caractérisés par un degré différent de la pression anthropique a permis, au cours d’une troisième partie, de tester le potentiel des compartiments phyto- et zooplanctoniques en tant qu’indicateur de l’augmentation de la pression anthropique. Cette étude a identifié une sensibilité accrue de la composition du zooplancton par rapport au phytoplancton. Enfin, des indicateurs de pression anthropique des zones côtières, basés sur les familles de copépodes, ont été proposés."
"Une grande variabilité de déchets contenants des nanomatériaux sont largement produits partout dans le monde. En fin de vie, la lixiviation de ces déchets entreposés en décharge, conduit à l’émission de contaminants (organiques et métalliques) dont les modes de transport diffèrent largement selon leurs interactions avec les ligands du milieu traversé. L’union européenne souhaite promouvoir à long terme la réduction progressive des rejets de contaminants dans l'environnement et le Ministère de l’Ecologie projette de mettre en place des normes spécifiques limitant les émissions de nanoparticules. Or, à ce jour, peu de données sont disponibles sur les quantités de contaminants potentiellement émis par les déchets et sur leur capacité de transport. En particulier, les contaminants présents sous forme nanoparticulaires, facilement bioassimilables, ont un comportement dynamique mal identifié et donc peu prévisible.La complexité et la variabilité dans la composition des déchets et des nanomatériaux impliquent l'étude d'un large panel d'entre eux ; ainsi, notre travail a porté sur la quantification de l'émission des nanoparticules au cours de leur vieillissement à partir de différents déchets (résidus de boues rouges, boues de station d’épuration, sédiments marins et Mâchefers d'Incinérations d’Ordures Ménagères MIOMs) ainsi que leur transport à travers un milieu poreux.Nous avons identifié les éléments métalliques fortement présents sous forme colloïdale et leur devenir après 1 an et demi de vieillissement. Les tests de transport des nanoparticules, menés par des expériences de percolation en colonne, ont montré que le transport de nanoparticules métalliques a été facilité dans certains cas, dans d’autres, plus classiquement ralenti. Par microscopie Electronique à Balayage, nous avons montré que de nombreux métaux étaient couplés avec des oxydes d'aluminium et de fer ainsi que des substances organiques naturelles de type humiques."
"Ce travail a porté sur l’étude de l’évolution des communautés zooplanctoniques à partir de séries temporelles de relevés effectués de 1995 à 2010 dans deux écosystèmes côtiers couplés, la Petite Rade de Toulon (PR) et Grande Rade de Toulon (GR) (Méditerranée Nord Occidentale, France) en relation avec les facteurs climatiques, les paramètres physiques et chimiques de l'eau et avec le phytoplancton. Le pas d’échantillonnage des relevés de zooplancton et du phytoplancton était d’un mois en moyenne. La maille du filet utilisé était de 90 µm, afin de cibler le mésozooplancton. La PR a différé de la GR dans son fonctionnement écologique, car elle est semi-fermée, mais aussi parce que l'activité anthropique y était beaucoup plus importante. Nos résultats ont montré que, de 1995 à 2010 dans les deux baies, l'abondance du zooplancton a sensiblement augmenté, surtout dans la PR. Il a été également établi, en utilisant différents outils statistiques, que la plus grande partie des espèces de zooplancton évolue de manière coordonnée chaque année, mais d’une manière différente d’une année à l’autre. C’est ce que nous avons appelé la signature annuelle, qui était très marquée dans la PR. Plusieurs paramètres environnementaux comme la température, l’oxygène, la salinité et l’ensoleillement, qui ont été simultanément enregistrés, expliquent cette signature annuelle. Il a été montré en effet qu’ils influencent très sensiblement la population de zooplancton, de manière instantanée ou avec un effet retardé. Les interactions responsables de cette évolution sont fort complexes, mais il a été aussi établi que ces facteurs sont plus forts lorsqu’ils agissaient de manière coordonnée. La répartition du zooplancton en groupes taxonomiques a montré que la diversité a augmenté jusqu’en 2005, puis a diminué légèrement, tout en restant à des niveaux plus élevés qu’en 1995. L’étude détaillée de la diversité, avec une classification des indices eux-mêmes, a fait l’objet du dernier chapitre. Enfin, nous émettons l’hypothèse que la diminution des stocks de poissons au cours des dernières décennies dans toute la région a entraîné une diminution des taux de prédation sur les communautés zooplanctoniques, ce qui peut expliquer l’augmentation de peuplements zooplanctoniques au cours de ces dernières années. Cet accroissement de l’abondance du zooplancton a pu entraîner à son tour une diminution de la biomasse du phytoplancton. Cette diminution a été parallèlement observée par notre équipe. Ceci suggère un système de contrôle top-down du réseau trophique."
"L’étude du comportement des éléments métalliques est primordiale compte tenu leur effet souvent toxique dansde nombreux écosystèmes. Ces derniers lorsqu’ils interagissent avec la Matiere Organique (MO), peuvent formerdes complexes plus ou moins stables. Ainsi, la MO joue un rôle important dans leur spéciation chimique et leurtransport. Dans ce travail, l’analyse de cette complexation est réalisée par Quenching de Fluorescence (QF).Cette technique permet de modéliser la fixation des sites de complexation à l’aide d’une constante thermodynamiquedéterminée à partir d’un modèle 1 : 1. Le quenching de fluorescence a été mesuré par spectroscopie defluorescence en mode stationnaire et en mode résolue en temps. Les mesures de fluorescence en mode stationnairefournissent des Matrices d’Excitation et d’Émission de Fluorescence (MEEFs). L’extraction des différents composantsde ces MEEFs est effectué par séparation de sources : la décomposition multilinéaire CP/PARAFAC,qui permet de caractériser spectralement les composants. Les mesures par Spectroscopie Laser Résolue en Temps(SLRT) permettent une caractérisation spectrale et temporelle des composants fluorescents. L’étude des lois dedécroissance de la fluorescence induite par impulsion laser nanoseconde en l’occurrence a permis de déterminerle type d’interaction entre la MO et les quencheurs. Pour se faire, un algorithme de déconvolution temporellea été appliqué à chaque décroissance de fluorescence mesurée. L’interprétation des données temporelles a étéaccomplie en utilisant le graphique de Stern–Volmer. Les résultats des interactions du cuivre, de l’europiumet de l’uranium avec les Acides Humiques (AH) et les Acides Fulviques (AF) montrent des décroissances defluorescences importantes et des constantes de stabilité entre 2,04 et 4,52. Le cuivre a permis de valider notremodèle d’étude et l’interaction de l’europium et l’uranium avec les AH et AF étudiés a révélé des constantesde stabilité en général en bonne corrélation avec la littérature. Les résultats de la SLRT ont souvent révélé desdécroissances bi–exponentielles et des temps de vie entre 0,40 et 14 ns et montrent que les interactions étudiéesont principalement engendrer un quenching statique et donc la formation d’un complexe moléculaire à l’étatfondamental. Cette étude a donc permis par caractérisation spectrale et temporelle, de déterminer l’interactionde la matière organique avec les métaux plus ou moins toxiques."
"Une allégation de santé dans l’UE nécessite des preuves cliniques de l’efficacité et de la sécuritéd’une supplémentation nutrititionnelle. Les probiotiques, en particulier les bactéries lactiques,rentrent dans ce cadre règlementaire dans lequel l’EFSA indique que les preuves obtenues chez despatients avec des troubles fonctionnels intestinaux sont transposables chez une population de sujetssains. Le protocole LAPIBSS est un essai clinique de haute qualité méthodologique évaluantl’efficacité de 2 souches de Lactobacillus acidophilus à diminuer la sévérité des symptômes dusyndrome de l’intestin irritable. Les résultats confirment la sécurité d’emploi des souches utiliséesmais ne montrent pas une diminution significative des symptômes comparée au placebo après 8semaines. L’effet global du traitement est statistiquement significatif sur le score de flatulence. Uneffet placebo et l’hétérogénéité importante de la sévérité des symptômes à l’inclusion pourraientexpliquer nos résultats. Une meilleure compréhension des effets physiologiques des probiotiqueschez l’homme pourrait améliorer le rationnel de leur utilisation en recherche clinique."
"Les écosystèmes côtiers méditerranéens sont soumis à de multiples sources de contamination (activités agricoles, industrielles et touristiques) impactant à des degrés divers les compartiments environnementaux (colonne d'eau, sédiments, biota, ...). En particulier, des études précédentes ont démontré que les sédiments de la Rade de Toulon présentent un niveau de multi-contamination très important, qui pourrait avoir des conséquences non-négligeables sur l'écosystème côtier, ce qui nécessite la réalisation de différentes études complémentaires visant à mieux comprendre les processus régissant le transfert potentiel des polluants vers le milieu environnant. Dans ce contexte, les objectifs de cette thèse consistent à étudier les processus contrôlant la dynamique des polluants inorganiques dans la colonne sédimentaire et à l'interface eau/sédiment et à évaluer leur remobilisation potentielle vers la colonne d'eau. Trois modes principaux de dispersion peuvent conduire à une remobilisation des contaminants: (1) la diagénèse précoce mobilisant des éléments nutritifs et des éléments traces métalliques et métalloïdes, (2) la remobilisation des contaminants lors de remises en suspension sédimentaire, et (3) le flux diffusif à l'interface eau/sédiment en lien avec la mobilité des contaminants dans la colonne sédimentaire. Pour atteindre ses objectifs, ce projet de thèse combine des approches in-situ (carottage d'interface, extraction de l'eau interstitielle, traitement/analyse des eaux interstitielles et des fractions solides, …), des expériences en laboratoire (suivi cinétique de la remobilisation des contaminants lors des mélanges eau/sédiment, extractions séquentielles…), des outils de modélisation (calcul de la spéciation chimique, simulation de la partition dissous/particulaire, simulation des profils sédimentaires, réactivité de la matière organique, …) ainsi que le traçage des sources de contamination et de leur transfert entre différents compartiments environnements à l'aide des isotopes stables du Pb. Ces travaux ont été réalisés en étroite collaboration avec le LASEM et avec la contribution de différents partenaires (EPOC/Université de Bordeaux, Ruđer Bošković Institut/Croatie, CAM/Université de Lausanne/Suisse, IFREMER, IRSN, …)."
"Face à la limitation de la modélisation paramétrique, nous avons proposé dans cette thèse une procédure standard pour combiner les données reçues a partir de Réseaux de capteurs sans fils (WSN) pour modéliser a l'aide de Réseaux de Neurones Artificiels (ANN). Des expériences sur la modélisation thermique ont permis de démontrer que la combinaison de WSN et d'ANN est capable de produire des modèles thermiques précis. Une nouvelle méthode de formation ""Multi-Pattern Cross Training"" (MPCT) a également été introduite dans ce travail. Cette méthode permet de fusionner les informations provenant de différentes sources de données d'entraînements indépendants (patterns) en un seul modèle ANN. D'autres expériences ont montré que les modèles formés par la méthode MPCT fournissent une meilleure performance de généralisation et que les erreurs de prévision sont réduites. De plus, le modèle de réseau neuronal basé sur la méthode MPCT a montré des avantages importants dans le multi-variable Model Prédictive Control (MPC). Les simulations numériques indiquent que le MPC basé sur le MPCT a surpassé le MPC multi-modèles au niveau de l'efficacité du contrôle."
"L’objectif principal de ce travail de thèse est d’évaluer l'impact des activités anthropiques sur le fonctionnement des systèmes aquatiques. L’impact des rejets urbains de Fès sur le Sebou, l'une des plus grandes rivières du Maroc, a été choisi pour cette étude. Les eaux usées domestiques et industrielles de la ville de Fès (~1M hab), véhiculées par son affluent l’oued Fès, sont rejetées dans le Sebou quasiment sans traitement. Deux sites du Sebou, en amont et en aval des rejets de la ville de Fès, et un site situé sur l’oued Fès ont été étudiés. Une campagne de prélèvement de carottes de sédiments et onze campagnes mensuelles de prélèvement d’eau et de matières en suspension (MES) ont été effectuées dans le but de quantifier les apports en nutriments, éléments traces métalliques (ETM) et carbone organique afin d’étudier la dynamique de ces polluants. Les résultats obtenus ont mis en évidence une augmentation des concentrations de presque tous les éléments étudiés en aval de la ville de Fès aussi bien dans la colonne d’eau que dans les sédiments. L’étude des carottes sédimentaires a révélé une contamination modérée par les butylétains totaux (ΣBT) avec la prédominance du monobutyétain dans les trois sites d’étude et sur toutes les profondeurs. Les sédiments du Sebou, en amont de la ville de Fès se caractérisent par les teneurs les plus faibles en métaux alors que ceux de l’oued Fès présentent une forte pollution polymétallique, accentuée dans les sédiments de surface, ce qui reflète clairement la signature d'apports anthropiques récents résultant des rejets non traités de la ville de Fès. L’augmentation consécutive des teneurs des métaux dans les sédiments du Sebou en aval de la confluence Fès-Sebou traduit l'influence significative des particules polluées de l’oued Fès. Dans la colonne d’eau, les concentrations mesurées dans le Sebou en amont de la confluence Fès-Sebou sont proches des rivières naturelles, à l’exception de Cl-, Cr, Na+ et NO3- dont les concentrations traduisent des pollutions agricoles et/ou domestiques. En revanche, la signature anthropique des rejets de la ville de Fès apparait évidente dans les eaux de l’oued Fès qui montrent des concentrations très élevées en ETM dissous et particulaires. La majorité des ETM provenant des apports anthropiques subissent des changements importants de leur coefficient de distribution Kd et présentent un comportement non conservatif dans le mélange entre les eaux du Sebou et celles de l’oued Fès. Ces résultats ont été confirmés par le modèle WHAM qui permet de prédire correctement le fractionnement dissous/particulaire, du Cu, Pb et Zn. La spéciation chimique de ces éléments ainsi que leur répartition dissous/particulaire apparaissent significativement influencées par les conditions particulières (anoxie, forte contamination, teneurs importantes en matière organique) des eaux de l’oued Fès. Ces conditions sont aussi à l’origine du comportement non conservatif de la majorité des éléments lors d'un mélange Sebou/Fès, observé aussi bien dans la colonne d’eau que dans les sédiments."
"La gestion au long terme des sédiments de dragage contaminés soulève le problème du devenir des éléments potentiellement toxiques contenus dans ces matrices. Les paramètres physicochimiques influencent la spéciation et la distribution des contaminants sur les différentes phases porteuses organiques ou minérales, ainsi lors de la gestion à terre des sédiments la modification de facteurs tels que l’aération, les cycles d’humectation/séchage et l’activité bactérienne va influencer les paramètres physico-chimiques et donc la spéciation des contaminants. Afin de préciser les mécanismes responsables de la mobilité des éléments potentiellement toxiques et d’estimer l’acceptabilité environnementale des sédiments de dragage en scénario de valorisation (p. ex. butte paysagère, remblai ou sous couche routière), l’étude a été axée sur trois principales étapes :I établir la caractérisation totale des sédiments (granulométrie, minéralogie, teneur en eau, composition de la phase solide, composition de l’eau interstitielle) et évaluer selon des procédures normaliséesl’influence de facteurs (pH, L/S, température…) sur la lixiviation des éléments et sur les mécanismes géochimiques mpliqués ; II développer un jeu de paramètres d’entrée pour le code géochimique ORCHESTRA selon des procédures normalisées (quantification des phases porteuses les plus réactives : argiles, carbonates, oxy-hydroxydes de fer ou d’aluminium et matière organique - acide fulviques et humiques) ; III modéliser et prédire les courbes de solubilité des éléments décrites lors des tests normalisés issus de l’étape (i) par l’intermédiaire du jeu de paramètres d’entrée défini dans l’étape II. Les tests de lixiviation et la réalisation de modèles sont des approches complémentaires, indispensables pour appréhender et préciser les mécanismes contrôlant la mobilité et la rétention des éléments. Les modélisations des tests de lixiviation dynamique en colonne sont très sensibles aux variations des paramètres d’entrée, c’est pourquoi les modèles pour les éléments majeurs doivent être le plus adéquats possible. En général, les prédictions pour Al, Ca, Cl, Fe, H2CO3, Mg, Si, SO4, Cu, Cr, MoO4 2- , Pb et Zn ont été proches des données expérimentales, ce qui a indiqué que les processus majoritaires contrôlant la solubilité des éléments ont été pris en compte. Par contre, les prédictions pour Ni et As n’ont pas été satisfaisantes, montrant que certains processus de rétention restent encore inconnus et qu’ils ne sont pas pris en compte par la base de données MINETEQ2A. Pour mieux décrire le comportement d'As, il semblerait intéressant d’intégrer, dans le module NiCA-Donan, la complexation potentielle d'As par la MON."
"L’érosion des stocks naturels de Paracentrotus lividus, oursins comestibles de Méditerranée, malgré les mesures actuelles réglementant les captures, soulève des inquiétudes sur la pérennité de son exploitation et sur l'équilibre des écosystèmes. En 2009, à la demande des pêcheurs professionnels, des gestionnaires de la ressource et de la communauté d’agglomération Toulon Provence Méditerranée (TPM), l'Institut Océanographique Paul Ricard et l'Équipe de Biologie Moléculaire Marine du laboratoire PROTEE (Université de Toulon), ont engagé un travail de recherche visant à mieux comprendre l'influence des processus naturels et anthropiques qui façonnent les populations naturelles, dans le but d'améliorer les stratégies de soutien aux populations, notamment par le réensemencement de juvéniles d'oursins.L'objectif du travail de thèse a été de (i) caractériser l'état des stocks sauvages de l'aire toulonnaise et de leurs dynamiques par la mise en place du suivi démographique bisannuel à long terme de huit stations références, (ii) de développer les méthodes de production de juvéniles en écloserie et (iii) d'évaluer l'efficacité du repeuplement et son impact sur les populations naturelles. Compte-tenu du cycle de vie bentho-pélagique de P. lividus, nous avons inscrit cette problématique dans une échelle régionale.Les résultats obtenus permettent de dresser un premier état des lieux de la structuration et des fluctuations d'abondances des peuplements de P. lividus sur l'aire toulonnaise et démontrent la fragilité des stocks naturels vis à vis des captures. La maîtrise complète du cycle de vie en aquaculture a été obtenue, notamment en optimisant les conditions d’élevage au stade critique de la métamorphose. Enfin, les repeuplements expérimentaux, réalisés à l'échelle pilote, ont présenté un succès variable selon les sites mais potentiellement important, sans aucun effet sur la structuration et la diversité génétique des stocks naturels. Le travail engagé et les premiers résultats sont discutés dans le contexte de la conservation et de l'exploitation durable de cette ressource naturelle. Ils constituent en outre un préalable indispensable aux réflexions sur l’ajustement ou la mise en place de nouvelles actions de gestion des populations et de la ressource."
"Les matrices d’excitation et d’émission de fluorescence (MEEF) sont utilisées pour caractériser la matière organique naturelle (MON). Afin de mieux exploiter ces informations, un algorithme trilinéaire, PARAFAC, est employé. Après l’élimination des diffusions Rayleigh et Raman et la correction de l’effet d’écran, cette méthode permet de séparer les composants spectraux présents dans les MEEF.Ce travail présente deux études : la qualification et la quantification de la MON selon son origine environnementale et le calcul des constantes de complexation de la MON et du cuivre sous forme ionique.Les composants spectraux et leurs intensités relatives sont calculés par PARAFAC à partir 1146 échantillons regroupés suivant les missions, leur type de milieu, ou le niveau de salinité. Pour étudier ces composants, une nouvelle représentation spectrale est proposée afin de mettre en évidence leur variabilité spectrale. Les résultats montrent que le regroupement d’échantillons d’origine diverse conserve le recouvrement spectral global et les intensités relatives. Sur l’ensemble du domaine spectral, les zones correspondant aux substances humiques sont peu variables, comparées à la zone protéinique.La complexation des métaux par la MON est analysée par une technique combinant quatre outils : l’ajout logarithmique d’ions métalliques, la mesure de MEEF, la méthode PARAFAC et l’algorithme PROSECE. La mesure du quenching de fluorescence ne se limite pas seulement à la modélisation d’une intensité de fluorescence mais à celle de l’intensité relative de chaque composant PARAFAC surpassant ainsi les méthodes utilisées jusqu’à présent. Finalement, l’application de cette technique originale permet de quantifier les propriétés de complexation de la MON à l'aide d'un modèle de complexation utilisant 2 sites de complexation par composant en utilisant la totalité du signal de fluorescence."
"Ce mémoire a pour objectif d'étudier la stabilité de systèmes dynamiques chaotiques à partir de la structure géométrique de leurs attracteurs dont une partie s'appuie sur une variété appelée variété lente. Dans ce but, une nouvelle approche basée sur certains aspects du formalisme de la Mécanique du Point et de la Géométrie Différentielle a été développée et a conduit à une interprétation géométrique et cinématique de l'évolution des courbes trajectoires, intégrales de ces systèmes dynamiques au voisinage de la variété lente. L'utilisation du formalisme de la Mécanique du Point a permis, grâce à l'emploi des vecteurs, vitesse et accélération instantanées attachées à un point courant de la courbe trajectoire, de discriminer le domaine lent du domaine rapide et de situer la position de la variété lente à l'intérieur de l'espace des phases. Certaines notions de Géométrie Différentielle, comme la courbure, la torsion et le plan osculateur, ont fourni une équation analytique de la variété lente indépendante des vecteurs propres lents du système linéaire tangent, donc définie sur un plus grand domaine de l'espace des phases. La variété lente a alors été envisagée comme le lieu des points où la courbure des courbes trajectoires, intégrales de ces systèmes dynamiques, est minimum (en dimension deux ce minimum devient égal à zéro). Le signe de la torsion a permis, de caractériser son attractivité et, de discriminer la partie attractive de la partie répulsive de la variété lente et de statuer sur la stabilité de ces courbes trajectoires. Ainsi, la présence dans l'espace des phases d'une variété lente attractive qui contraint les courbes trajectoires, intégrales du système dynamique à visiter son voisinage permet d'étudier la structure de l'attracteur. Cette approche basée sur certains aspects du formalisme de la Mécanique du Point et de la Géométrie Différentielle et qui s'est accompagnée de l'élaboration de programmes numériques a permis de constituer un nouvel outil d'investigation des systèmes dynamiques chaotiques. Son application à des modèles de référence comme celui de B. Van der Pol, de L.O. Chua ou d'E.N. Lorenz a permis d'obtenir plus directement et avec précision l'équation analytique de leur variété lente. De plus, une étude détaillée des modèles de type prédateur-proie comme celui de Rosenzweig-MacArthur ou d'Hastings-Powell, a conduit d'une part à la détermination de leur variété lente et d'autre part à la conception d'un nouveau modèle de type prédateur-proie à trois espèces appelé Volterra-Gause dont l'attracteur chaotique a la forme d'un escargot (chaotic snail shell)."
"Cet ouvrage retrace de façon inédite le processus d'élaboration de la théorie des oscillations non linéaires durant une période qui s'étend de 1880 à 1940 avec en toile de fond l'essor de la radiocommunication. Dans ce processus, la France joue un rôle déterminant tant au niveau de sa situation géographique lui donnant l'aspect d'un « carrefour » scientifique au sein duquel sont échangées et discutées les idées, qu'au niveau des résultats produits par la communauté scientifique française qui permettent de constituer les fondements sur lesquels cette théorie va peu à peu se formaliser. Ce livre fournit les clés permettant de comprendre la genèse de la Théorie du Chaos apportant ainsi une analyse complémentaire à celles qui ont décrit la naissance de cette théorie. Il intéressera tout autant les historiens des sciences que le grand public."
"Le concept de « cycle limite » fut introduit par Henri Poincaré dans son second mémoire « Sur les courbes définies par une équation différentielle » en 1882. Du point de vue de la Physique, un cycle limite stable (ou attractif) correspond à la représentation de la solution périodique d'un système (mécanique ou électrique) dissipatif dont les oscillations sont entretenues par le système lui-même. Inversement, l'existence d'un cycle limite stable garantit l'entretien des oscillations. Jusqu'à présent, l'historiographie considérait que le mathématicien russe Aleksandr' Andronov avait été le tout premier à établir une telle correspondance entre la solution périodique d'un système auto-oscillant et le concept de cycle limite de Poincaré. La découverte récente d'une série de conférences réalisées par Henri Poincaré en 1908 à l'Ecole Supérieure des Postes et Télégraphes (aujourd'hui Telecom Paris Tech) démontre qu'il avait déjà mis en application son concept de cycle limite pour établir l'existence d'un régime stable d'ondes entretenues dans un dispositif de la T.S.F. (Télégraphie Sans Fil.) Cet article a donc pour objet d'une part de retracer l'émergence de ce concept depuis sa création par Poincaré et, d'autre part de mettre en évidence l'importance de son rôle dans l'histoire des oscillations non linéaires."
"INFORMATIQUE-ROBOTIQUE-ELECTRONIQUE (61 ème section CNU) RESUME-Depuis les années 80 et l'avènement des caméras numériques, une multitude de techniques de capture ou de quantification du mouvement par traitement d'images vidéo ont émergées. Toutes ces méthodes, très différentes les unes des autres, ont pourtant toute un point commun : la dimension temporelle est directement déduite de la fréquence d'acquisition des images vidéo qui, par hypothèse, est toujours supposée connue et stable, tout au long de la séquence d'images analysée. Or, il s'avère que cette condition, fondamentale pour une bonne quantification du mouvement, n'est pas toujours scrupuleusement respectée par les systèmes d'acquisition. Dans cet article, nous proposons un dispositif permettant de montrer que pour certaines caméras numériques, les instants d'acquisition des images ne sont pas aussi réguliers qu'on aurait pu l'espérer. Nous verrons ensuite comment dater précisément chaque image de la séquence pour compenser ce défaut lors de la détermination du modèle cinématique du mouvement observé. Mots-clés-Traitement d'images, capture de mouvement."
"Le but de cette étude était de comprendre le comportement des fractions d'acide fulvique et d'acide humiquedans un système de sols podzolisé, compte tenu des caractéristiques du sol et d'étudier la capacité decomplexation de Cu+ 2 et Al + 3 dans des échantillons de ces sols. Pour cela, les méthodes conventionnellesde pédologie et de Spectroscopie de l'absorbance UV-Visible, Spectroscopie Infrarouge à Transformée deFourier et Fluorescence Quenching avec le traitement CP / PARAFAC ont été utilisés pour atteindre lesobjectifs suivants: (i) déterminer les étapes de transformation du pedo-paysage à l'étude des sols et à larépartition spatiale des sols podzols et du gleysol dans un bassin hydrographique; (ii) caractériser substanceshumiques et comprendre les schémas de distribution des acides fulviens et humiques dans une séquence desol podzols le long des profils et dans les horizons du sols en tenant compte des caractéristiquesmorphologiques, de la texture, du pH, carbone variation totale et souterraine dans les horizons des solsd'une séquence de sols podzols; (iii) étudier la capacité de complexation des substances humiques (acidesfulviques et humiques) des échantillons de sol de podzols dans le bassin du Alto Rio Negro, en identifiantles composants fluorescents des substances humiques, en évaluant les capacités et les constantescomplexantes de ces substances avec les métaux Cu+ 2 et Al+ 3 et la comparaison avec les groupesfonctionnels avec la fluorescence infrarouge à transformée de Fourier (FTIR). Encore les acides fulviquessoient plus aromatiques et condensés, leur caractère est principalement aliphatique et hydrophile etsecondairement aromatique et carboxylique. Les acides humiques sont moins aromatiques et moinscondensés que les acides fulviques et sont caractérisés par des fonctions polysaccharide et éther/alcool,mais secondairement leur caractère est aliphatique et hydrophile. Ces différences sont associées à laprésence des groupes fonctionnels qui répondent aux différences dans la dynamique de complexation desmétaux, dans les complexes acide humique, la complexation des métaux Cu+ 2 et Al+ 3 est associée auxfonctions ether/alcool et CO, polysaccharides pendant la période acide fulvic la corrélation est la plus élevéeavec les groupes fonctionnels -CH, -OH et COOH carboxylique."
"L'étude des interactions métaux traces – matière organique naturelle marine est rendue difficile par leurs faibles concentrations en milieux marins et côtiers. Ce travail propose un protocole d'analyse permettant la détermination des propriétés de complexation de la matière organique naturelle dissoute (MOND), et plus spécifiquement la MOND marine, vis-à-vis de contaminants métalliques comme le cuivre. L'approche par concentration de la MOND est habituellement utilisée, mais il est possible de travailler directement sur l'échantillon naturel, sans lui faire subir d'étapes de pré-concentration ou de traitements physico-chimiques susceptibles d'altérer significativement sa structure. La mise en place de ce protocole analytique fournit des paramètres de complexation directement utilisables dans des logiciels de spéciation chimique, ou de transports de polluants, pour prédire la biodisponibilité des métaux traces. Les principaux obstacles au développement de ce protocole ont été d'adapter la manière de travailler aux très faibles concentrations de métaux et de carbone, en milieu principalement marin. La technique électrochimique de DPASV permet d'atteindre une limite de détection suffisamment basse pour étudier les échantillons directement dans les conditions naturelles. L'utilisation d'une titration métallique, en mode logarithmique, permet de balayer une gamme de concentration métallique suffisamment large pour déterminer les paramètres de complexation de la MOND pour des concentrations en carbone aussi basses que celles pouvant être rencontrées dans le milieu naturel. Des résultats identiques sont obtenus en traitant la cinétique de complexation des échantillons analysés. Ce protocole, appliqué à des MOND concentrées et non concentrées, et testé sur différents échantillons de MOND marines et estuariennes, montre qu'une approche rapide sans concentration de la MOND est possible. Toutefois, la concentration de la MOND a aussi été utilisée afin de déterminer les paramètres de complexation d'une MOND marine modèle tenant compte du calcium, du cuivre et du proton."
"Le présent projet propose de rechercher une procédure d'inertage des résidus de bauxite pour produire un résidu de bauxite modifié (RBM) à faible phytotoxicité et qui soit valorisable dans le traitement in-situ d’anciens sites industriels contaminés et dans le traitement ex-situ d’autres déchets ultimes tels que les scories du site de l’Escalette. Le projet vise donc à traiter un sol contaminé et des scories par le RBM et à identifier l'efficacité et les risques d'un tel traitement par (i) la compréhension des mécanismes physico-chimiques régissant la mobilité des contaminants et (ii) l’évaluation de la phytotoxicité du RBM après traitement des matériaux contaminés par des tests de croissance de plantes et analyses des éléments chimiques accumulés. Par ailleurs, la présence de l’entreprise Altéo, à cause de sa forte production de déchets, peut engendrer des perceptions négatives de son activité industrielle. La réutilisation des boues rouges pour réhabiliter des sites contaminés pourrait alors être localement perçue soit comme une double solution, soit, compte tenu de l’attachement local - entre autres dynamiques représentationnelles – comme l’addition d’un deuxième problème à un environnement déjà pollué. La mise en oeuvre d’un tel dispositif d'inertage pourrait ainsi être freinée par la résistance des communautés locales ou de professionnels de l’environnement sur la base de leur attribution de risques à ces dispositifs. Il s’agit donc d’identifier les perception locales à ce projet d’inertage par le RBM."
"L’étude réalisée lors de ce travail de thèse porte sur l’évaluation de l'impact de l’agglomération Marseillaise sur le milieu côtier à travers la quantification des éléments traces métalliques (ETM) et du carbone organique. Sur le littoral méditerranéen français, Marseille représente la plus grande agglomération (~ 1.7M Ha) et possède la station d’épuration (STEP) enterrée la plus grande d’Europe (capacité de 1.62M équivalent-habitants), avec une façade directement ouverte sur la Méditerranée. L’impact de cette zone urbaine et industrialisée (flux brut) sur la zone côtière reste mal compris, de par la multiplicité des sources (apports directs(rivières/effluents) vs. apports diffus (friches industrielles côtières, aérosols, ...)) en partie contrôlée par le climat. Il en est de même des mécanismes de transfert des contaminants conditionnant leur devenir dans le milieu marin (flux nets vers le large). Dans ce contexte, différentes campagnes de prélèvements d’eau et de sédiments ont été mises en oeuvre sous des conditions climatiques contrastées (temps sec vs. pluie) dans les rivières ainsi qu'en mer, le long d’un transect allant de la côte à plus de 2 km au large des rejets. Les objectifs de cette étude visaient à déterminer les sources de contaminants au milieu côtier et à comprendre leurs mécanismes de transfert et leur devenir en mer. Par temps sec, les résultats obtenus ont montré que la dynamique du système est principalement contrôlée par les rejets de la STEP qui, par exemple, est responsable à plus de 75% des apports en Ag, Cu ou Pb au milieu côtier. Une fois en mer, les différents ETM analysés présentent un profil non conservatif, dû à un fractionnement dissous/particulaire hors équilibre dans les émissaires associé à un relargage rapide au début du gradient de salinité. Ces résultats ont été confirmés par une expérience de remobilisation réalisée au laboratoire permettant de mieux comprendre la cinétique de désorption des ETM. Dans ces conditions, il a été démontré qu'il était indispensable de filtrersur le terrain les échantillons pour ne pas sous-estimer la fraction dissoute des ETM. Par temps de pluie, le suivi des apports au cours d'une crue a montré la très grande réactivité du système, typique de rivières côtières. La majorité des ETM, transportés principalement sous forme particulaire, subissent une fois en mer des processus de désorption avec des cinétiques plus lentes et à des salinités plus importantes que par temps sec. Ces différences sont probablement liées à la nature des particules, urbaines et très organiques par temps sec, plutôt terrigène et inorganiques par temps de pluie. Enfin, une expérience de vitesse de chute des particules transitant dans le système par temps sec et de pluie a été développée au laboratoire. Elle a permis de caractériser les particules étudiées par des paramètres physico-chimiques intégrables au modèle hydro-sédimentaire de l'IFREMER, permettant de mieux évaluer le devenir des particules en zone côtière."
"La proposition, faite dans le cadre de la remobilisation des sédiments du Rhône, de démanteler les casiers Girardon, se heurte au fait que les casiers en eau (20% des aménagements) peuvent jouer un rôle d’annexe fluviale artificielle. Cette étude apporte des réponses sur la biodiversité de ces systèmes aquatiques. Elle propose quelques pistes de gestion et met en lumière des facteurs « clé » permettant d’expliquer les fonctions de ces écosystèmes artificiels tels que le degré de connexion au chenal, la turbidité de l’eau et les interactions entre phytoplancton et macrophytes."
fr_abstract_s
"Cette thèse porte sur les techniques de simulations des interactions dynamiques entre un véhicule sous-marin et l'eau qui l'entoure. L'objectif principal est de proposer une solution satisfaisante pour pouvoir, en amont du processus de conception, tester des algorithmes de contrôle et des formes de coques pour véhicules sous-marins. Il serait alors intéressant de pouvoir simuler en même temps la dynamique du solide et celle du fluide. L'idée développée dans cette thèse est d'utiliser la technique Smoothed Particles Hydrodynamics (SPH), qui est très récente et qui modélise le fluide comme un ensemble de particules sans maillage. Afin de valider les résultats de simulations une première étude a été réalisée avec un balancier hydrodynamique. Cette étude a permis la mise au point d'une méthode innovante d'estimation de paramètre hydrodynamique (forces de frottement et masse ajoutée) qui est plus robuste que les méthodes existantes lorsqu'il est nécessaire d'utiliser des dérivées numériques du signal mesuré. Ensuite, l'utilisation de deux types de solveur SPH : Weakly Compressible SPH et Incompressible SPH, est validée en suivant la démarche de validation proposée dans cette thèse. Sont étudiés, premièrement, le comportement du fluide seul, deuxièmement, un cas hydrostatique, et enfin un cas dynamique. L'utilisation de deux méthodes de modélisation de l'interaction fluide-solide : la méthode de réflexion de la pression et la méthode d'extrapolation est étudiée. La capacité d'atteindre une vitesse limite due aux forces de frottement est démontrée. Les résultats d'estimation des paramètres hydrodynamiques à partir des essais de simulation est finalement discutée. La masse ajoutée simulée du solide s'approche de la réalité, mais les forces de frottement semblent actuellement ne pas correspondre à la réalité. Des pistes d'améliorations pour pallier à ce problème sont proposées."
"— Le rythme accéléré du renouvellement des produits entraîne une surexploitation des matériaux et de l'énergie. Pour remédier à cela, le présent document considère des systèmes upgradables, produit évolutif auquel est intégré au cours du temps des améliorations fonctionnelles (upgrades). La quantité et le timing d'intégration de ces upgrades définissent différents « scénario d'upgrade ». Une expérimentation basée sur l'outil UpMoS et les données du passé d'un appareil électronique simule différentes hypothèses de conception, au regard d'une évaluation multicritères: impact sur l'environnement-coût pour l'entreprise-attractivité pour les consommateurs. Avec une intégration non systématique des nouvelles fonctionnalités et des astuces de conception des modules upgradés comme leur recyclage ou leur surdimensionnement pour espacer leur remplacement, les résultats montrent que l'upgradabilité est d'ores et déjà une opportunité pour développer des systèmes durables. Les gains environnementaux sont de l'ordre de 5 à 10 %. D'autres leviers de conception prometteurs pour obtenir des résultats beaucoup plus positifs sont présentés en discussion."
"Cette thèse porte sur le problème du contrôle de la forme d'ombilicaux pour des robots sous-marins légers téléopérés (mini-ROVs), qui conviennent, grâce à leur petite taille et grande manoeuvrabilité, à l'exploration des eaux peu profondes et des espaces encombrés. La régulation de la forme de l'ombilical est cependant un tâche difficile, car ces robots n'ont pas une puissance de propulsion suffisante pour contrebalancer les forces de traînée du câble. Pour faire face à ce problème, nous avons introduit le concept de Cordée de mini-ROVs, dans lequel plusieurs robots sont reliés à l'ombilical et peuvent, ensemble, contrebalancer les perturbations extérieures et contrôler la forme du câble. Nous avons étudié l'utilisation des caméras embarquées pour réguler la forme d'une portion de l'ombilical reliant deux robots successifs, un leader et un suiveur. Seul le robot suiveur se chargera de la tâche de régulation de la forme du câble. Le leader est libéré pour explorer ses alentours. L'ombilical est supposé être légèrement pesant et donc modélisé par une chaînette. Les paramètres de forme du câble sont estimés en temps réel par une procédure d'optimisation non-linéaire qui adapte le modèle de chaînette aux points détectés dans les images des caméras. La régulation des paramètres de forme est obtenue grâce à une commande reliant le mouvement du robot à la variation de la forme de l'ombilical. L'asservissement visuel proposé s'est avéré capable de contrôler correctement la forme du câble en simulations et expériences réalisées en basin."
"L’étude de l’observabilité et la synthèse d’observateur ont pour vocation la reconstruction de l’état d’un système à l’aide des mesures reçues. Ces dernières n’apportent généralement qu’une connaissance partielle de cet état. Le filtre de Kalman est un observateur particulièrement étudié et employé. Plusieurs versions existent, adaptées aux systèmes linéaires ou non linéaires, en version discrète, continue voire continue-discrète, dans le cadre stochastique ou déterministe. Ces observateurs reposent cependant sur l’hypothèse que les mesures fournies par les capteurs sont synchrones. Or, cette supposition est assez éloignée de la réalité physique, notamment des problèmes étudiés en robotique.Nous proposons dans ce manuscrit un observateur adapté aux systèmes non linéaires continusdiscrets asynchrones. Nous entendons par cela des systèmes dont l’état est continu et les sorties échantillonnées à des fréquences différentes. En nous basant sur le Filtre de Kalman Etendu grand-gain existant pour les systèmes non linéaires continus et continus-discrets synchrones, nous développons un formalisme et construisons un observateur en adoptant un point de vue déterministe. Sa convergence est prouvée analytiquement et illustrée par une application sur un système robotique mobile."
"L’objectif de cette thèse est le développement d’une méthodologie d’aide à l’écoconception permettant l’optimisation multi-objectifs du cycle de vie de produits. Le premier axe de travail est focalisé sur la phase d’utilisation. Il s’intéresse à la manière dont différents segments d’utilisateurs vont utiliser un même produit et aux répercussions que vont avoir ces différentes utilisations sur les performances de son cycle de vie. Le deuxième axe vise à élaborer une méthodologie de modélisation réaliste et pertinente du cycle de vie de produits. Il est centré sur une problématique cruciale des problèmes d’optimisation : l’élaboration de l’objectif de résolution. Il propose l’utilisation de technique d’identification et de quantification des besoins clients afin de définir un objectif de résolution orientant vers les solutions de conception respectant au mieux les attentes des clients. Le troisième axe a pour but d’obtenir, par l’optimisation, une sélection d’alternatives de conception permettant de maximiser les performances du système. Une modélisation “gros grain” du produit incluant ses composants et leurs alternatives y est réalisée."
"L’optimisation topologique (OT) est un outil mathématique permettant d’obtenir une répartition optimale de matière. A partir d’un volume donné, soumis à des chargements, l’OT aboutit à un concept de pièce répondant à un objectif et respectant des contraintes. En règle générale, ce concept, de forme très complexe, est irréalisable par des procédés de fabrication conventionnels. Les procédés d’obtention par fabrication additive (FA), relativement récents, permettent de déposer le matériau là où il est nécessaire et rendent ainsi possible la fabrication de pièces topologiquement optimisées ne pouvant pas être obtenues par des procédés traditionnels. Dans la littérature scientifique, les méthodologies de conception pour la fabrication additive sont souvent appliquées à une seule pièce mécanique et peu d'articles traitent de la conception optimisée d'un système mécanique multicorps. Ce travail de thèse a donc pour thématique générale la conception de systèmes mécaniques multicorps pour la FA. Ceux-ci sont composés de pièces liées entre elles par des liaisons cinématiques et ayant des mouvements relatifs. L’objectif de la thèse est de proposer une méthodologie de conception permettant d’obtenir un produit fabricable par FA et optimisé à l’échelle du système par rapport aux besoins fonctionnels. Dans ce but, et afin de tirer profit de toutes les possibilités de la FA, ce mémoire propose, dans un premier temps, une classification des optimisations réalisables lors de la conception d’un produit. Trois optimisations sont identifiées : l’optimisation architecturale, l’optimisation fonctionnelle puis l’optimisation topologique. La chronologie d’application et une démonstration des apports de chacune de ces optimisations sont établies. Dans un deuxième temps, une méthodologie d’optimisation topologique de systèmes multicorps (TOMS Topological Optimization of a Mechanical System) est développée afin de prendre en compte l’impact de la diminution des masses et inerties de chacune des pièces du système sur les autres. Pour cela, une boucle d’optimisation est proposée pour réaliser des itérations d’OT. Puis, l’impact de l’ordre dans lequel sont optimisées les pièces (appelé un chemin d’optimisation) sur le résultat de conception est étudié. Des principes de choix de chemin d’optimisation ont ainsi pu être établis afin d’obtenir le mécanisme répondant au mieux aux besoins du concepteur. Enfin, les trois optimisations (architecturale, fonctionnelle et TOMS) sont intégrées au processus global de conception d’un produit. Une méthodologie globale de conception, intégrant chaque étape du processus avec toutes les données nécessaires, est ainsi proposée. Cette méthodologie permet de concevoir aussi bien une seule pièce qu’un système mécanique multicorps, de la rédaction du cahier des charges à la conception du brut fabricable par FA."
"La fabrication additive par dépôt sous énergie concentrée (DED) permet la fabrication rapide de petites séries de pièces. Cependant, les trajectoires usuellement utilisées pour les pièces présentant du porte-à-faux nécessitent l’utilisation de supports, matériau non utile à la pièce finale dont le dépôt et l’enlèvement sont chronophages. Si les trajectoires multi-axes permettent de s’en passer, elles présentent généralement des distances locales inter-couches hétérogènes, nécessitant d’ajuster la hauteur de couche par la paramétrie de dépôt, pouvant alors impacter les caractéristiques mécaniques de la pièce finie. Cette thèse propose, dans un premier temps, une méthode de génération de trajectoire multi axes à distance locale inter-couches constante pour des tubulures définies par des courbes guides paramétrées et pouvant présenter des variations de rayon de profil. Les trajectoires proposées ont ensuite été validées par la fabrication additive robotisée de démonstrateurs en matériau polymère. La rotation autour de l’axe d’un outil de dépôt coaxial n’ayant pas d’incidence sur le dépôt, l’utilisation de robots 6-axes admet une redondance. En utilisant cette redondance, une méthode d’optimisation couche par couche de la trajectoire dans l’espace articulaire est finalement proposée. Pour une configuration de robot contrainte, l’optimisation permet la fabrication de pièces impossibles à produire de manière classique et apporte une amélioration de leur qualité géométrique ainsi qu’une meilleure répétabilité."
"L’objectif de cette thèse est d’apporter différentes contributions méthodologiques en bioacoustique pour l’étude de la faune. En effet, la bioacoustique est une science récente, pluridisciplinaire et très efficace pour étudier et classifier un écosystème. Beaucoup d’études ont mis au point des procédés acoustiques pour étudier la faune à des échelles spécifiques, populationnelles, individuelles et comportementales. Ce travail de thèse propose d’étudier différents cas d’études présents dans ces quatre échelles d’analyses. L’objectif de cette thèse est de mettre en place des outils depuis la pose du matériel d’acquisition jusqu’à l’analyse des données pour l’ensemble des échelles présentées, de les discuter et de les mettre en perspective. La bioacoustique spécifique est illustrée ici par la classification automatique d’Orques, de Cachalots et d’oiseaux. Pour la bioacoustique populationnelle, la classification acoustique de clans d’Orques est étudiée. Puis l’échelle d’analyse s’affine et étudie les émissions sonores individuelles. Pour cela 3 cas d’études sont utilisés : la localisation individuelle d’Orques, de Cachalots et d’oiseaux. Ladernière échelle est appelée bioacoustique comportementale, elle a pour but de mettre en corrélation des comportements avec des émissions acoustiques. Pour cela, l’influence du trafic maritime sur les Dauphins tachetés pantropicaux et l’impact de stimuli chimiques chez la Baleine à bosse est étudié. Nous avons volontairement fait le choix de sélectionner différentes espèces produisant des types de signaux bien différents (stationnaires vs transitoires) évoluant dans des milieux différents (marins vs terrestres) afin d’homogénéiser les méthodes d’analyses pour faciliter le développement de nouvelles études en bioacoustique. Chaque cas d’étude présente des résultats intéressants en terme de bioacoustique et d’écologie comportementale. Ces résultats sont comparés avec la bibliographie. Puis, les résultats de chaque cas d’étude permettent de valider les méthodes proposées dans cette thèse. Les apports méthodologiques de cette thèse sont synthétisés, comparés et discutés, notamment l’impact des signaux stationnaires et transitoires, des milieux (marin et terrestre) sur la mise en place des méthodes. Les méthodes supervisées et non supervisées sont mises en comparaison. Les méthodes proposées ont été testées et validées sur certains protocoles de données massives (plusieurs dizaines de Tera). En conclusion, cette thèse montre que les méthodes supervisées (notamment le Deep Learning) étaient très bien adaptées pour la classification de signaux stationnaires en bioacoustique spécifique et populationnelle pour le milieu terrestre et marins. Puis les méthodes non supervisées (clustering et réduction de dimensionnalité) peuvent être utilisées dans le cadre des études en bioacoustique comportementale pour identifier les signaux d’intérêt. Enfin, la bioacoustique individuelle peut se traduire par des méthodes de localisation comme l’estimation du temps de délais d’arrivée inter-capteur, réalisable pour les signaux transitoires, et plus complexe pour les signaux stationnaires."
"La fabrication additive WAAM, basée sur un procédé de soudage Fil-Arc, permet l’obtention de pièces brutes métalliques de grandes dimensions à des conditions économiques intéressantes pour de petites séries. En revanche, la qualité des pièces dépend fortement de la gestion de la température au cours de la fabrication. En effet, le mode de dépôt implique un transfert de chaleur important à la pièce et il peut être nécessaire de procéder à des arrêts du dépôt pour permettre son refroidissement. Dans le but de déterminer l’influence de la chaleur sur la qualité géométrique de pièces de révolution à parois minces, une méthode de mesure de la température in-situ via l’utilisation d’un pyromètre ainsi qu’une stratégie de gestion de la température inter-couches est proposée. Les méthodes de mesure et de gestion de la température inter-couches sont appliquées pour la fabrication de cylindres à parois minces de différents diamètres nominaux. Le contrôle géométrique de ces derniers permet d’évaluer l’impact du diamètre sur la qualité géométrique à iso-conditions de fabrication. Le matériau étudié est un alliage d’aluminium 5183 déposé à l’aide d’une source CMT robotisée. La méthode de mesure et de gestion de la température inter-couches proposée permet ainsi de garantir le dépôt de chaque couche dans des conditions thermiques similaires, dans le but de maîtriser au mieux les conditions opératoires de fabrication."
"La fabrication additive est en plein essor pour la fabrication directe de pièces fonctionnelles. Toutefois, les technologies actuelles-comme la fusion sur lit de poudre-présentent un coût élevé et des limites dimensionnelles qui freinent leur diffusion dans certains secteurs industriels comme l'aéronautique ou le naval. La fabrication additive par dépôt de fil métallique sur robots industriels multi-axes équipés de torches de soudage représente un procédé d'avenir peu onéreux et plus ouvert, qui permettra notamment de faire varier l'orientation du dépôt afin de supprimer les supports de fabrication. Néanmoins, cela nécessite de pouvoir générer une trajectoire de dépôt tridimensionnelle complexe permettant la fabrication correcte de la pièce. Cet article présente les problématiques relatives à la génération de trajectoires tridimensionnelles de dépôt de fil, ainsi que des premiers développements dans ce domaine illustrés par l'exemple d'une tubulure torique décrite analytiquement."
"La fabrication additive représente de forts enjeux industriels en ouvrant de nouvelles possibilités quant à la conception et à la manière de fabriquer les pièces. Cependant, cette méthode de production n'est pas adaptée pour les volumes de pièces importants. La conjoncture actuelle demandant une forte personnalisation de la production de masse, la fabrication additive peut être une alternative économique aux moyens de production traditionnels, permettant une rationalisation des entités identiques de pièces, produites alors via des moyens conventionnels, puis un ajout d'entités personnalisées. La fabrication additive permet aussi le réemploi de pièces, en les réparant par rechargement de matière, ou bien en ajoutant des fonctionnalités pour offrir une seconde vie à ces dernières. Ce document présente une méthode de génération de trajectoire, appliquée aux tubulures à parois minces, permettant l'ajout d'entités présentant un porte-à-faux important sur une pièce pré-existante et sans utilisation de matériau support, diminuant ainsi le temps de fabrication et la quantité de matériau à déposer. La trajectoire de fabrication est générée de manière à ce que la distance locale entre les couches soit homogène tout le long de cette dernière et les axes outils déterminés de manière à supprimer l'utilisation de matériau support. Enfin, un essai expérimental a été mené afin de valider la trajectoire générée."
"Cet article vise à répondre à l'inexistence d'outils d'écoconception pour les phases amont de la conception de produits, particulièrement dans le domaine des systèmes complexes. Un système complexe possède de nombreux sous-systèmes interdépendants, plusieurs alternatives de conception pour chaque sous-système, des conditions d'utilisation variables affectant le choix des solutions technologiques et des performances environnementales ; c'est en outre un système qui évolue tout au long du cycle de vie et en interaction avec l'environnement extérieur. Ces caractéristiques mènent rapidement à un grand nombre de configurations à évaluer pour identifier le système optimal sur le plan de l'environnement (de façon multicritère et sur l'ensemble du cycle de vie). Pour éco-concevoir un tel système le plus en amont possible nous avons élaboré, à partir de la littérature, 3 approches méthodologiques différentes, chacune faisant intervenir l'outil "" Life Cycle Assessment "": (1) l'approche intuitive du concepteur basée sur des règles simplistes ou du Pareto combinée au LCA, (2) l'approche basée sur des plans d'expériences combinée au LCA, et (3) l'utilisation d'algorithmes de résolution de problème basé sur le calcul par intervalles combinée au LCA. Ces trois démarches "" Pareto/ACV "", Plan d'expériences/ACV et CSP/ ACV ont été testées dans un projet d'éco-conception d'un système complexe : une navette maritime de transport de passager à motorisation hybride. Les résultats obtenus par chacune des 3 démarches sont évalués selon différents critères : temps d'utilisation, de formation, d'exploration du champ du possible, d'interprétabilité des résultats et de ré exploitabilité. Les résultats font apparaître que la démarche CSP/ ACV présente des avantages en particulier sur les critères d'exploration du champ du possible, d'interprétation des résultats et sur le gain de temps permis par la ré exploitation des résultats obtenus sur des systèmes proches."
"The marine energy sector is experiencing a growing interest; large offshore wind farms continue to emerge, particularly in northern Europe. The electrical energy produced by these turbines is intended as clean and renewable. Operations and Maintenance (O&M) of offshore wind farms is however costly and generating environmental impacts. Indeed in view of the circumstances on the high seas the accessibility to Wind Turbine (WT) is greatly complicated causing long periods of unavailability compared to onshore wind farm. To remedy this, operators will mobilize significant and specific resources (Fast Crew Boat, large boat, helicopters). The use of these vehicles and systems of maintenance can improve the availability of WTs but also degrade the environmental performance of offshore wind farm. The optimization of O&M requires rethinking the concept of maintenance of renewable energy systems in mobilizing the concept of Eco-Maintenance. Eco-Maintenance aims to minimize the environmental losses due to maintenance while maximizing the environmental benefits generated assuming the energy recovered due to better maintenance / availability equivalent to an environmental gain corresponding to the environmental impacts of a thermal power plant should have been put into operation to produce this energy. This approach is tested on an offshore wind farm. In this context, modeling the complex system (Offshore Wind Farm + Maintenance Systems) was developed. The various models show that the environmental performance of Systems of Energy Production and System of Maintenance (SM) are interconnected."
fr_abstract_s
"On montre en l'observant de manière fragmentée, l'extension sans précédent du réseau internet devenu ""internet of everything"" (IOE). IOE donc comme Hyperobjet 1 prenant en charge le devenir du monde et son intelligence. Certaines de ses caractéristiques sont mises en évidence. Et on esquisse quelques-unes de ses évolutions dans le contexte des trois grandes crises écologiques (au sens de Felix Guattari) en insistant sur des variations décisives affectant les milieux d'intelligence qui en sont l'expression. Les problèmes soulevés par les développements de biopolitiques fondées sur l'épigénétique, les physiques sociales, et l'hyper-vectorisation des individus comme complexes relationnels, ne sont qu'évoqués."
"THEMATIQUE Les pratiques collaboratives, le numérique et la participation : comment ça marche ? La dynamique de projet collaborative dans une perspective pédagogique : l'apport des techniques de créativité La pédagogie par projet implique des modes d'apprentissage différents au sein duquel la relation communicationnelle classique entre l'enseignant et l'apprenant change de format (elle n'est plus pensée sur le mode émetteur-récepteur). D'autres modèles communicationnels accompagnent cette nouvelle forme de pédagogie introduisant un espace d'échange plus horizontal et d'interactions plus fortes entre les apprenants : ⇒ Ce type de projet bouscule les pratiques traditionnelles de l'enseignant"
"Au cours de ses recherches sur le potentiel viral des vidéos publicitaires en ligne en fonction des intentions de partage des individus qui regardaient ces vidéo, l’auteur s’est posé la question de leur représentation graphique. Il présente ici un outil pensé et adapté pour accompagner les différents travaux sur la viralité ainsi que pour présenter et exploiter les données récoltées au cours de ceux-ci : les diagrammes ternaires."
"Depuis quelques mois, les questions relatives à la production et à la circulation des « fake news » se sont clairement imposées parmi les thématiques les plus prégnantes relatives à l’évolution des écosystèmes informationnels contemporains. Ces « fake news » traduisent la forme avancée de la dérégulation contemporaine du « marché cognitif », dérégulation dont la clef de voûte est le phénomène bien connu de la « désintermédiation informationnelle ». Les écosystèmes informationnels subissent une transformation qui n’est pas sans rappeler celle que les intermédiaires bancaires ont subie, il y a une quarantaine d’années environ, dans le champ de l’économie financière. Le but de cet article est précisément de mettre ces deux évolutions en parallèles, d’en indiquer les points communs, d’en comprendre les différences pour tenter, in fine, de cerner les scénarios alternatifs de reconfiguration des écosystèmes informationnels. Les intermédiaires de l’information connaitront-ils finalement le même sort que, jadis, les intermédiaires de la finance ? Les voies de la mutation informationnelle sont-elles, à la lumière des enseignements de l’histoire économique récente, déjà tracées ? On le sait bien, comparaison n’est pas raison. Il s’agira ici de puiser dans cette histoire raisonnée des éléments de réflexion et des outils d’analyse en vue de baliser certains scénarios d’évolution crédibles. La prospective doit toujours envisager la pluralité des schémas imaginables du futur. Le recours à l’histoire financière récente permet justement d’étayer cette exigence de diversité."
"Le présent article revient sur nos différents travaux pour présenter les données qui ont servi à en dresser les conclusions (Roux, 2016, 2018, 2019). Nos travaux ont produit de nombreuses données que nous souhaitons désormais partager et diffuser auprès de la communauté scientifique, en particulier celle des sciences de l’information et de la communication, afin qu’elle puisse les exploiter à des fins heuristiques. Ces données n’ont pas encore révélé leur plein potentiel, car nombre d’entre elles n’ont été que peu, voire pas du tout, exploitées."
"Cette thèse CIFRE au sein d’une organisation de santé nous a conduit à nous intéresser aux fonctions exécutives, impliquées dans le contrôle cognitif intervenant dans les situations nécessitant une articulation des actions ou pensées dirigées vers un but finalisé. Ces processus sont sollicités pour se concentrer sur une tâche (Attention), mémoriser et manipuler des informations (Mémoire de travail), s’adapter à de nouveaux environnements ou règles (Flexibilité mentale, Inhibition) et plus généralement lorsque les habitudes et automatismes ne suffisent pas à atteindre ces buts (Planification, Stratégie). Les patients ne reconnaissant pas leurs troubles (anosognosie), leurs thérapeutes éprouvent des difficultés à les impliquer dans leur rééducation afin qu’ils retrouvent une autonomie suffisante pour regagner leur domicile. A travers notre démarche de Recherche-Intervention participative, nous avons mis en place un canevas méthodologique pluriel ancré dans l’organisation de santé. En nous inspirant du Design Thinking, de la Conception Centrée Utilisateurs et de la Conception Universelle, nous avons mené de multiples observations, entretiens, séances de créativité et tests utilisateurs auprès des thérapeutes et des patients de façon itérative tout du long de notre recherche. Ceci nous a permis de prendre en compte les représentations liées aux dynamiques de la situation vécue, construites dans le cours de l’action et dépendantes des stratégies d’acteurs afin de coconcevoir dynamiquement S’TIM. Ce Serious Game thérapeutique est basé sur les théories de l’esprit, de l’autodétermination, de l’engagement, et de l’activité. Une grande attention a de plus été portée à l’Expérience vécue à travers la scénarisation proposée ainsi qu’aux possibilités d’apprentissage dans un univers virtuel afin qu’il y ait transfert des acquis en situation de vie quotidienne. Par leur accompagnement, les thérapeutes conservent ici un rôle primordial. S’TIM contient ainsi des clés d’action, des moyens de mobiliser ces clés et des stratégies pour les utiliser de manière pertinente. Il emmagasine et articule donc un savoir-faire, un savoir-comprendre et un savoir-combiner qui le positionnent en tant que pièce maitresse de l’expertise. Cette approche peut également être appliquée dans d’autres contextes, multiples, où l’engagement à travers un SG est la clé de voute du projet dans toute situation en relation avec les formes d'apprentissage hybridées. Enfin, nous avons étudié l’implication de l’organisation apprenante dans le projet ainsi que son appropriation du dispositif. Tout en étant partie prenante, les changements induits à un micro-niveau par le numérique n’ont pas été réellement traduits dans les dynamiques organisationnelles. Nous retrouvons la question de la pérennisation, de la consolidation permettant un réel réagencement au sein de l’organisation. Un accompagnement s’avère ainsi indispensable pour observer un véritable déplacement de la relation entre patients et thérapeutes, point de départ pour l’émergence de nouvelles formes thérapeutiques puis organisationnelles globalement plus structurantes."
"Dans le contexte d’économie des territoires actuels, le secteur du tourisme Français se doit de développer des services innovants (Boiron, 2014) et emprunts de modernité afin de continuer à être attractif. La culture du numérique (Licoppe, 2009) guide inéluctablement ces transformations à travers un accès au patrimoine (Benhamou, Thesmar, 2011) et à la création contemporaine se voulant facilités, avec pour leitmotiv la démocratisation culturelle, la transmission des savoirs et l’éducation au patrimoine (Szafrajzen, 2015) au service de la diversité de l’offre culturelle. Ainsi, de nouvelles prestations touristiques voient le jour, conduisant à repenser l’engagement expérientiel entre la culture et le visiteur, notamment au sein de parcours pédagogiques innovants (Corroy, Roche, Savignac, 2017). Le présent article étudie une nouvelle expérience humanisée, celle du cinéma immersif du site des neuf écluses de Fonseranes à Béziers, dans l’Hérault (région Occitanie)."
"Le journalisme au défi des big data. Mort annoncée ou mutation salvatrice ? fr.ejo.ch/deontologie-qualite/journalisme-defi-big-data-mort-annoncee-mutation-salvatrice-datajournalisme-everyblock Les données massives sont capables de mettre en valeur le travail de terrain du journaliste. (Crédit photo: Wikipédia) La déferlante des données massives interroge le journalisme contemporain, ses pratiques, mais aussi les modèles économiques des médias et l'organisation des rédactions. Au-delà des résistances épistémologiques que cela suscite, les chercheurs de l'IMSIC Marc Bassoni et Alexandre Joux montrent comment les big data peuvent aussi contribuer à recentrer la pratique autour de ses fondamentaux, que sont notamment le terrain et le lien avec le public. Quand l'expression big data (données massives) est associée au mot journalisme, il y a fort à parier qu'un tel rapprochement déclenche, en ces temps de grands bouleversements techniques, un quasi-réflexe de représentation du journalisme automatisé (ou « robot-journalisme »). Ce dernier se caractérise par une production de contenus opérée par des logiciels à partir d'un traitement algorithmique de grandes masses de données."
"Depuis plus de vingt ans, la place des technologies de l'information et de la communication dans le paysage universitaire contraint les établissements d'enseignement supérieur à offrir des services numériques ouverts toujours plus concurrentiels, gages d'attractivité, de compétitivité et de qualité (Leleu-Merviel, 2008). En 2013, et sur une durée de quatre mois, nous réalisons une lecture communicationnelle des dispositifs d'enseignement à distance (EAD) mis en place au sein d'une école supérieure de commerce. Les enjeux de l'étude sont de permettre à la direction de l'école de piloter en interne le développement du centre de formation à distance. Près de trois ans plus tard, nous retournons sur ce terrain de recherche pour étudier la pertinence de ces sources ouvertes au sein de cet établissement d'enseignement supérieur recherchant l'excellence opérationnelle et l'équilibre financier. Pour ce faire, nous posons un cadre d'évaluation capable de mettre en évidence la distance qui sépare le projet du centre d'EAD de son application, dans sa ou ses réalités d'ouverture au quotidien."
"La déferlante des données massives (big data) interroge le journalisme contemporain, ses pratiques, mais aussi les modèles économiques des médias et l’organisation des rédactions. Les enjeux sont protéiformes. Nous mettons tout d’abord en évidence la résistance épistémologique du métier face aux promesses d’un journalisme intégralement robotisé. En effet, le data journalisme se réfère toujours à un terrain identifié auquel les données doivent renvoyer à des fins de validation. Nous montrons ensuite que l’usage des données massives permet aux médias non seulement de personnaliser plus finement l’offre d’information, mais également de favoriser un processus d’innovation qui tend à hybrider de plus en plus les savoir-faire journalistiques avec des compétences techniques extérieures au métier. A défaut de disparaître, le métier mute en se recentrant sur certains de ses fondamentaux."
"Le streaming musical accompagne la mondialisation de l’écoute de la musique en ligne. Il est une réponse, organisée par les acteurs du marché, au développement du piratage dans les années 2000. Il s’apparente ainsi à une mondialisation des logiques de marché à l’ensemble de la production musicale, y compris au « sud » où les majors investissent désormais dans la production nationale. Ses logiques de recommandation, en revanche, ne garantissent pas nécessairement une meilleure circulation planétaire des œuvres, au moins du point de vue de la diversité de la création."
"La mondialisation et le développement du numérique, phénomènes majeurs des sociétés modernes, impactent également le domaine de l'enseignement supérieur. Massification et mobilités apparaissent ainsi comme des facteurs incontournables dans la transformation des systèmes éducatifs internationaux. Pour répondre à des objectifs d’efficience, d’égalité des chances et à des enjeux pédagogiques à nouveau émergents (comme la formation à distance ou hybride), les outils numériques sont toujours envisagés comme des moyens innovants mis à disposition du monde académique. Pour autant, dans un contexte international facilitant l’accès en ligne à de nombreuses ressources numériques mutualisées, leurs usages et appropriation locaux dépendent de facteurs variés : culturels, interculturels, cognitifs, motivationnels, pédagogiques, institutionnels. Telle est la réflexion générale qui guide cet ouvrage collectif. À travers ses quatorze chapitres, il présente des analyses et études de cas fondées sur des terrains variés et issus de plusieurs zones géographiques : Europe, Afrique, Canada, Océan indien. L'ouvrage est divisé en trois sections thématiques : l'appropriation du numérique et des dispositifs mondialisés, les mobilités en éducation, les usages locaux des outils numériques. Il offre ainsi des éléments de réflexion sur les enjeux de la massification et de la mondialisation dans l'enseignement supérieur, et le rôle actuel qu’y jouent les dispositifs et ressources numériques."
"Le printemps est habituellement la saison où se tiennent les colloques Ticemed. Le douzième de la série, Ticemed12, préparé à l’Université Panteion d’Athènes pour les 7-9 avril 2020 a dû être annulé quelques jours avant son ouverture en raison de la pandémie de Covid-19. Des mois de travail desconférenciers, des revieweurs et des organisateurs1 risquaient d’être effacés purement et simplement. Or il est de tradition dans ces colloques de publier les textes des conférences dans des préactes avant même la tenue du colloque. Les bouleversements de l’année écoulée n’ont pas permis depublier dans ces délais, mais il a paru indispensable aux organisateurs et à la communauté Ticemed de réaliser l’équivalent de pré-actes et de les mettre à disposition du public sur le net. Cela est l’objet du présent ouvrage."
"Selon un rapport de l’Organisation des nations unies pour l'éducation, la science et la culture (UNESCO) datant de 2009, la mondialisation apparait comme une « réalité majeure du XXIème siècle, ayant déjà profondément influencé l’enseignement supérieur » (Altbach, Reisberg et Rumbley, 2009 : 5). Elle est ainsi décrite comme une situation déterminée par un certain nombre de facteurs tels que la globalisation économique, l’émergence des nouvelles technologies, la circulation des savoirs et l’anglicisation. Chacun de ces facteurs agit indépendamment des politiques d’établissements, les contraignant à des évolutions structurelles. Dans tous les cas, « globalisation » ou « mondialisation » invitent à redéfinir le rôle de l’université dans l’espace mondial : d’un côté, celui d’un rapport libéral et concurrentiel ; de l’autre, celui d’échanges supposant collaborations et partages des ressources. Face à la guerre d’images que se jouent les universités sur la toile ou face au développement des ressources éducatives libres (REL), les deux logiques coexistent et supposent d’être analysées et soumises au regard critique. Dans la suite de la 10ème édition du colloque TiceMed portant sur l’ouverture des sources numériques (Massou, Juanals, Bonfils, Dumas, 2019), la 11ème édition visait ainsi à analyser la façon dont se décline l’appropriation d’outils et de ressources numériques globalisés dans différents contextes locaux de l’enseignement supérieur"
"Une information fiable et comprise est l’un des fondements d’une bonne vitalité démocratique. Cette information nourrit nos connaissances, favorise la citoyenneté capacitaire et participe d’un récit commun. Elle est confrontée à l’incertitude scientifique, dont l’essence même est la ré-interrogation des connaissances. Elle est aussi tributaire de la confiance que les citoyens accordent à l’émetteur ou encore de leur capacité à se confronter à l’altérité. Pour les collectivités, ces questions sont cruciales puisqu’elles impactent leurs projets de territoire"
"Depuis une trentaine d’années, la musique contemporaine savante opère une mutation profonde. Cherchant à sortir de l’impasse de la seconde partie du XXeme siècle qui l’a amenée à devenir une musique de spécialiste financée dans son intégralité par les fonds publics, les jeunes compositeurs cherchent à retrouver le contact avec l’auditeur sans pour cela baisser le niveau d’exigence de leurs musiques. La musique contemporaine savante se confronte ainsi à la technologie, à la mondialisation, aux musiques populaires et industries culturelles ainsi qu’aux différents média audiovisuels… Dans cette dynamique, la scénographie numérique aujourd’hui joue un rôle de plus en plus notable en intégrant une dimension spectaculaire à une musique essentiellement cérébrale et en rendant visible certains processus de composition : une question de communication…"
"L’accès généralisé à l’information musicale favorise des mélanges stylistiques qui traversent tous les genres ainsi que toutes les sphères culturelles. Si cette hybridation musicale n’est pas nouvelle, elle prend aujourd’hui une dimension inédite avec les technologies numériques fondées sur la connaissance. Cet échange généralisé et mondialisé concerne les musiques mais aussi les connaissances musicales. Ce transfert de connaissances est favorisé à la fois par les dimensions multimédia mais aussi par la formalisation de la théorie musicale incluse dans les ontologies musicales et portée par le paradigme numérique fondé sur la connaissances et les métadonnées. Ce paradigme de la connaissance modifie fortement l’accès à la musique mais aussi renouvelle profondément les méthodes de création musicale. L’objet de cet article est donc de traiter la question de l’hybridation musicale sur ces deux plans : d’un côté, celui de l’échange communicationnel et de l’autre celui de son infrastructure technique."
"Cet article revient sur les origines de la notion d’acceptabilité sociale avant de décrire des stratégies participatives qui y sont associées. Certaines s’apparentent davantage à de la manipulation qu’à une volonté de co-construction. À travers deux études de cas sur l’implantation d’innovations technologiques, l’article présente des pistes pour éviter cet écueil. Pour chaque cas, les démarches ont permis d’éclairer le débat sur un problème public et ouvrir un espace de choix dans lequel plusieurs solutions techniques sont devenues discutables."
"On observe depuis quelques années la diffusion d’émissions qui présentent des parties de jeu de rôle sur table : les actual plays. Les actual plays répondent à des enjeux médiatiques qui se manifestent en termes d’auditoire, de notoriété et de divertissement. Des moyens conséquents sont alors mobilisés pour développer ces émissions afin de les rendre les plus attractives possibles pour les spectateurs comme pour les joueurs. Il apparaît cependant qu’aucune étude n’ait jamais formalisé ces moyens. L’objectif du présent travail est donc de faire ressortir les modalités de médiatisation et de spectatorisation des actual plays afin de les examiner et de voir si celles-ci font appel à une standardisation ou une fragmentation des pratiques. Pour ce faire, nous avons réalisé une analyse comparative exploratoire sur la base d’une grille d’analyse créée ad hoc. La comparaison semble dégager un modèle de pratiques standardisées dans les actual plays."
"Le développement des technologies de l’information et de la communication conduit les professionnels de l’éducation à s’intégrer dans une stratégie numérique, mettant en lien localement des activités de l’enseignement à distance dans des structures fédératrices et cohérentes,et apparaissant comme un levier déterminant de compétitivité (Szafrajzenet Kosmicki, 2014). Le présent article propose un retour sur un terrain de recherche (une école supérieure de commerce) quatre années après les débuts de la mise en place de dispositifs d’enseignement à distance par le centre de formation à distance. Un focus est réalisé sur un cours de géopolitique internationale dispensé à des étudiants de bachelor première année. L’étude se situe du point de vue des étudiants en analysant les usages (Jauréguiberry et Proulx, 2011) faits du numérique via cet enseignement organisé autour de séquences en présentiel et à distance. L’approche se veut centrée sur l’interaction entre l’apprenant et le dispositif d’apprentissage et l’usage est étudié en situation et comme un construit social (Jouët, 2000). In fine, la question de l’autonomie des apprenants est à soulever, voire à placer au cœur des réflexions des établissements d’enseignement supérieur, s’agissant de l’utilisation d’une pédagogie numérique."
"Il est observable depuis quelques années des diffusions en streaming de parties de jeu de rôle sur table. Le streaming, tel que nous l’entendons en tant qu’activité culturelle, se distingue des autres formes de diffusion de contenus vidéo en ligne par son haut niveau d’interaction qui en fait un canal communicationnel plus complet. Ce dispositif permet notamment aux spectateurs d’interagir entre eux mais aussi parfois avec les créateurs de contenus, et ce, souvent en temps réel. Ce faisant, le streaming étend à une audience les cadres de participation ainsi que les plans de présence du jeu de rôle papier. En intégrant le spectateur dans l’action ludique de manière toujours plus interactive, le streaming redéfinit l’espace de jeu dont les frontières peuvent parfois devenir ténues et poreuses avec le hors-jeu. Finalement, les logiques du jeu de rôle sur table se voient fréquemment repensées pour répondre aux nouveaux enjeux médiatiques."
"Nous ambitionnons, dans ce travail, d’explorer l’environnement inventif algérien à partir de la prise en compte de la production informationnelle qui s’exprime à travers les brevets. Ce travail a pour objectif d’évaluer la capacité d’innovation de l’Algérie par le biais d’un ensemble d’indicateurs représentatifs basé sur les informations contenues dans les brevets d’invention déposés sur le territoire de ce pays. L’étude menée montre un faible taux de dépôt des brevets caractérisé par une production médiocre des déposants algériens. Elle relève aussi une forme de dépendance stratégique qui s’illustre particulièrement dans l’industrie pharmaceutique et l’industrie de l’énergie. L’examen des brevets indique également que la majorité des inventeurs algériens activent dans des pays étrangers, ce qui suppose que le pays n’arrive pas à réunir les conditions favorables à la pratique de l’innovation."
"Les bibliothèques numériques qui émergent des ""sociétés de l'information"" ne concernent plus uniquement ces dispositifs numériques technodocumentaires patrimonialement, culturellement ou scientifiquement déterminés. Les réseaux sociaux et les sites marchands à forte audience partagent les mêmes technologies, proposent des fonctionnalités d'expérience utilisateur (UX) identiques, des ressources numériques hétérogènes et naissent au sein des mêmes communautés de concepteurs et d'ingénieurs. 'Bibliothèque numérique et innovation' montre ces recouvrements induits par la technologie, qui nourrissent un imaginaire de l'usage. Ce dernier irrigue un mouvement de transformation de l'innovation où usage et usager/utilisateur occupent une place centrale. Cet ouvrage examine les évolutions des bibliothèques numériques et montre qu'elles sont le fruit d'un mouvement d'innovation qui produit deux effets majeurs : autonomiser les usagers et en accroître le nombre. La conjonction de ces effets a toutes les chances d'avoir un impact positif non seulement d'un point de vue économique mais plus globalement d'un point de vue social."
"Notre recherche s'intéresse à la radicalité. Cependant, nous ne travaillons ni sur le terrorisme, ni sur l'islam radical, ni sur les processus de radicalisation correspondant à une quelconque religiosité (Khosrokhavar, 2014). Nous souhaitons comprendre la radicalité véhiculée par des objets communicationnels, et notamment audiovisuels, en tant que phénomène de communication ayant pour caractéristiques la rupture du contrat de communication et le déni de l'autre. Ainsi, nous avons fait le choix d'un terrain qui conjugue tension et apaisement, « où se joue l'enjeu d'une guerre des mémoires, d'une tentative d'échapper à la radicalisation en s'exprimant sur le web » (Durampart, 2012). Le Liban, mosaïque communautaire, identitaire, religieuse et confessionnelle, affiche des contradictions. Nos questionnements s'articulent autour d'une communication radicalisée par l'image."
Résumé
"Cette enquête porte sur les représentations des journalistes sur le traitement médiatique de la crise sanitaire entre le moment de l’annonce du confinement strict (16 mars 2020, « Nous sommes en guerre ») et l’annonce du déconfinement (13 avril 2020). Le projet de recherche porte sur le traitement médiatique de la crise du Covid-19 jusqu’à l’annonce du déconfinement par le président de la République. Il s’agit d’étudier la période au cours de laquelle la couverture médiatique s’est concentrée sur les difficultés des hôpitaux et l’avancement de la recherche médicale. Elle ne porte pas sur la partie déconfinement, quand les enjeux économiques sont revenus au cœur de l’actualité médiatique. C’est donc une enquête sur ce que les journalistes pensent de la couverture de cette « guerre » contre un ennemi invisible, avant qu’ils ne se penchent aussi sur les conséquences du « quoi qu’il en coûte ». Les journalistes étaient sur le terrain : ils ont fait partie de cette deuxième ligne qui a permis à tous de garder le contact avec le monde extérieur. Mais comment ont-ils procédé ? Avec quels experts ? Ont-ils tous joué le jeu du terrain ? Comment ont-ils rendu compte des chiffres, des déclarations, des doutes aussi sur cette crise sanitaire et sur sa gestion ? Ont-ils pu parler d’autre chose que de la Covid-19 ? Comment ont-ils tenté de montrer autrement la crise, en s’intéressant aux centres-villes désertés, aux activités à la maison ? Le journalisme s’est-il réinventé ?"
"Alors que la PQR joue la carte de la concentration pour faire face notamment à la baisse de la publicité, le Groupe La Provence ne peut pas mobiliser ce levier. Confronté à un marché local limité, il parvient toutefois à augmenter ses recettes publicitaires grâce à une stratégie de diversification. Il déploie un ensemble d'activités en ligne, mais aussi des « hubs » thématiques qui mobilisent les acteurs du territoire. Toutes ces activités sont fédérées sous une marque-ombrelle, « La Provence », portée par l'expertise de la rédaction qui sert ici à crédibiliser les nou-veaux projets. Les synergies ainsi créées permettent à la régie de proposer une offre de communication globale qui essaime même au-delà de son petit marché."
"Cet article tente de saisir la contestation citoyenne en Algérie qui fait face à un pouvoir politique autoritaire. Face à des espaces publics souvent confisqués, encadrés ou contrôlés, l’expression citoyenne trouve des supports alternatifs dans les réseaux sociaux numériques. L’analyse de contenu de données récoltées sur le web montre la constitution et l’expression d’un espace public émergent. Ainsi, ce travail répondra à la question suivante : le web peut-il incarner une expression démocratique qui prolonge autrement ou en coexistence celles qui s’expriment en présence ?"
"S’appuyant sur des entretiens avec les journalistes à l’origine du Décodex et ceux impliqués dans son fonctionnement, cet article interroge les représentations associées à ce dispositif de signalement de la fiabilité des sources d’information en ligne. Il questionne notamment la représentation de leur travail atypique de fact checking durant le lancement du dispositif, au moment de la campagne présidentielle française, et les ambitions affichées et sous-entendues de ses promoteurs. En effet, le Décodex a ceci de particulier qu’il ne dénonce pas les fake news, comme de nombreux sites de fact checking, mais ambitionne d’identifier les sources qui les propagent. Dès lors, il s’autorise le droit de distinguer parmi les émetteurs de messages en ligne au nom d’un idéal journalistique. Les journalistes du Décodex voient-ils dans leurs pratiques une réaffirmation de la prétention épistémologique du journalisme à dire le vrai contre les fake news ? Comment perçoivent-ils le dispositif dans l’environnement numérique et médiatique ? Au-delà du discours de réaffirmation de la légitimité journalistique face aux fake news, c’est aussi un discours de l’efficacité des médias de référence qui est promu et de leur utilité sociale. Ainsi, la portée limitée des fake news durant la campagne présidentielle française de 2017 sera expliquée par la responsabilité des médias ayant « pignon sur rue ». En refusant de relayer les fausses informations, les médias d’information ont joué le rôle social de garant d’un débat public équilibré. Mais ce succès revendiqué est aussi un moyen pour la profession de journaliste de ne pas s’interroger sur ses limites, celles-là même qui conduisent aujourd’hui les fact checkers à défendre un journalisme menacé et le Décodex à se présenter comme une entreprise nécessaire."
"Cet ouvrage se propose de présenter une rétrospective des recherches, travaux de thèse, une recherche doctorale en Sciences de l'Information et de la Communication (S.I.C.) qui porte sur les « Territoires et Mobilité durable : Complexité, Acteurs-Réseaux et hybridation des pratiques au croisement de l’Intelligence Territoriale et du Développement Durable ». Cette recherche est au croisement de l’« Intelligence Territoriale (I.T) et de la mobilité durable » avec une orientation sur le transport partagé, biens et personnes, facteur de communication sociale, et de développement d’équilibre territorial. Cette recherche a été conduite avec trois territoires de référence à savoir, l'Afrique de l'Est et les régions Corse et Paca. Nous avons mobilisé des dispositifs hybrides composés d’acteurs avec leurs modalités, leurs pratiques, leurs moyens, et leurs finalités pour montrer que la mobilité est un facteur de cohésion et d’inclusion sociale. Nous avons convoqué les notions d’innovation sociale, de collaboration et de partage, via la communication et l’organisation des déplacements pour étudier la relation qui existe entre le développement durable et l’intelligence territoriale."
"Notre thèse porte sur la communication contre les violences faites aux femmes, et en particulier sur des dispositifs mobilisant le détournement du genre (DDG). De plus, ces dispositifs intègrent les hommes (dans le champ de la caméra) et les incluent dans le public-cible. Ces violences sont des actes d’hommes en tant qu’hommes qui ciblent les femmes en tant que femmes. Elles relèvent d’un phénomène social d’ampleur et collectif. Elles s’envisagent par le prisme des femmes. Le DDG désigne les perturbations des relations, normes ou représentations socialement construites relevant des femmes et des hommes, du féminin et du masculin. Nous cherchons à mieux comprendre cette notion de DDG. Ces créations de DDG, communicationnelles, artistiques, ou les deux à fois, illustrent pour certaines des emprunts réciproques entre les champs de l’art et de la mobilisation sociale. Considérant la communication mobilisant le DDG comme forme d’interactions humaines et sociales médiatisées, trois axes de recherche se dessinent : l’étude des formes du DDG à travers ses dispositifs (ses créations) ; l’étude empirique de la création, les processus créatifs et intentions d’influence des concepteur·trice·s (les créatifs) de tels dispositifs ; l’étude empirique de la réception, des processus de réception et d’influence en réception, c’est-à-dire sur des publics constitués de sujets sociaux. Premièrement, une analyse des dispositifs de DDG hétérogènes permet d’identifier leurs formes multiples qui illustrent l’hypertextualité du genre et des violences (collecte et grille d’analyse systématisées de dispositifs de DDG). Deuxièmement, des entretiens semi-directifs et d’explicitation avec les créatifs permettent l’étude de la création du DDG. Ces derniers assertent la réalité des violences et du genre, ils·elles imitent puis transforment ces réalités sociales, visent à interpeller, faire sentir des affects négatifs et révéler les violences, dans une perspective socio-cognitivo-réflexive, mais aussi à pointer, montrer et renverser sur les auteurs de violence la responsabilité des violences, l’anormalité de leurs comportements. Enfin, troisièmement, nous nous penchons sur la réception du DDG (focus groups et quasi-expérimentation). Les sujets-sociaux relèvent la dimension collective des violences, ressentent l’inconfort psychologique lié à l’hypertextualité et co-construisent des opinions allant dans le sens des intentions initiales en création ou expriment des histoires de violences et sexisme vécues ou en tant que témoin. Même si certaines opinions ne vont pas dans le sens des dispositifs de DDG en condamnant la transgression du genre ou des positions féministes contemporaines, nous avons montré que le DDG peut modifier favorablement des attitudes sexistes."
"L'objet de cette recherche porte sur l'analyse de l'expérience vécue en classe d'élèves acculturés au numérique, quand celle-ci est confiée à des néo-enseignants. Nous nous sommes intéressés à la manière dont les acteurs de l'espace pédagogique de la classe construisent du sens dans le contexte d'une généralisation des technologies numériques et de leurs usages, pouvant induire des effets sur la conception-réception des cours. Nous nous sommes appuyés sur un protocole de recherche qui prolonge, en éducation, le projet REMIND, ayant servi à décrire et comprendre la visite muséale. Il s'agissait de demander à des élèves de filmer des séances pédagogiques de leur perspective située grâce à un équipement de lunettes-caméras, puis de les accompagner à commenter leur vidéo subjective. Les verbalisations des élèves ont été analysées au prisme d'un langage symbolique appelé le signe hexadique. Nous avons également mené des entretiens semi-directifs avec les néo- enseignants de ces élèves afin de comprendre leurs modalités de construction des cours. Nos résultats mettent en avant que l'expérience des professeurs débutants répond à des contraintes de la forme scolaire qui peuvent entraîner des décalages avec leurs élèves. Ces derniers développent alors une pensée d'autocontrôle par rapport aux formes d'expression pédagogiques élaborées par leur enseignant : pratiques hétérogènes du numérique, recherche de simplification, frustration et lassitude ... Ceci nous permet de conclure que les enjeux expérientiels des élèves sont difficilement pris en compte par les néo-enseignants pendant la phase de conception de leur cours, ce qui pourrait entre autres favoriser l'offre en médias et outils numériques portée par le secteur industriel et marchand."
"Cet article met en lumière la plateformisation du marché publicitaire sous l’influence des moteurs de recherche et des réseaux sociaux numériques. Il analyse la manière dont les médias tentent de s’y adapter en mixant data management et brand safety, ce qui nous permet de mettre en évidence des logiques de coopétition entre plateformes et médias. L’article analyse ensuite la stratégie de la régie du Groupe TF1, première régie « média » de France. Cette stratégie mobilise les atouts de la publicité sur Internet pour mieux valoriser l’offre publicitaire télévisée. Elle fait du spot TV sur les chaînes à forte audience un élément clé de toute campagne de communication, ce que ne peuvent pas proposer les régies des acteurs de l’Internet. Mais cette course à la taille, sur Internet comme à la télévision, pose la question de l’avenir des médias à l’audience plus confidentielle quand ils se financent avec la publicité."
"Poursuivre le programme ou assurer l'engagement? Analyse des critères de continuité pédagogique et des transformations pédagogiques et en contexte de pandémie Formation et profession 28(4 hors-série), 2020 • ésumé La pandémie a donné lieu à une transformation pédagogique de type disruptif. Cette étude a pour objectif de comprendre la transformation des usages du numérique en lien avec les critères de continuité adoptés par les enseignant•e•s des différents niveaux éducatifs. Les résultats montrent la priorisation de la relation pédagogique par rapport au suivi du programme ou à l'évaluation. Cependant, les transformations des usages se centrent davantage sur la classe virtuelle et les outils de communication en maternelle et au primaire. À partir du collège, l'usage des plateformes instituées préexistantes s'intensifie, mais engage également les apprenants dans le choix des outils."
"Formation et profession 28(2), 2020 • 1 ésumé La pédagogie créative engage les différents acteurs et composantes d'une situation d'apprentissage. Dans le présent article, nous décrivons le rôle que des médiations pédagogiques dites technocréatives peuvent jouer pour favoriser l' engagement des élèves dans le cadre d'une démarche de pédagogie créative. Nous présentons ces médiations pédagogiques en lien avec la notion d' expérience vécue dans le but d' enrichir leur conception. C' est en anticipant sur les ressources et le sens que les élèves vont leur attribuer que réside le potentiel de l'approche technocréative : leur capacité à appuyer le développement de l' esprit critique des élèves face aux dispositifs numériques qui les entourent. Mots-clés Expérience vécue, créativité, esprit critique, compétences, numérique. Abstract Creative pedagogy involves the different actors and components of a learning situation. In this article, we describe the role that so-called techno-creative pedagogical mediations can play in fostering student engagement; and show how these mediations consider the notion of lived experience to enrich the design phase. It is by anticipating the resources and give meaning that students will attribute to the potential of techno-creativity, i.e. their ability to enrich students' critical thinking facing the digital devices that surround them."
Des activités sur l'IA dans des « nouveaux lieux d'apprentissages » ?
"Pour toute entreprise, Internet est un Eldorado où l’investissement est minimal, le gain économique maximal, mais où la maintenance et l’évolutivité restent difficiles à garantir dans des coûts et des temps raisonnables. En parallèle à ces réalités de production qui illustrent la difficulté pour les entrepreneurs d’apprécier les qualités des technologies, cet ouvrage replace chacun des procédés traités dans leur contexte d’élaboration associé au World Wide Web Consortium (W3C). A partir de cas réels, il analyse l’utilisation des technologies Web et la façon dont l’imaginaire des entrepreneurs détermine les premières pierres d'applications Web. Les technologies du Web au défi de l'entreprise s’adresse aux étudiants inscrits dans des formations où la communication numérique tient une place importante. Il est un outil complet pour les entrepreneurs, les responsables de communication et les équipes Web pour décrypter le contexte d’exploitation effectif des plus fréquentes solutions logicielles du Web actuel."
"Le développement de bibliothèques numériques à travers le monde témoigne de la maturité technologique de ces dispositifs documentaires sophistiqués. Destinées à un large public dont les profils informationnels sont délicats à saisir, elles se confrontent aux usages en perpétuelle évolution des TIC. Les internautes disposent désormais d’espaces numériques hybrides où le rapport virtuel/réel est revisité. Les sites de réseaux sociaux recensant plusieurs millions de membres et les plates-formes de partage (photos, vidéos) voient se développer des usages fondés sur la participation et le contributif au sein de communautés virtuelles. C’est dans ce contexte où technologies du numérique et activités sociales s’amalgament, s’amplifient et se nourrissent mutuellement, produisant des fonctionnements inédits, qu’apparaissent ces bibliothèques numériques essentiellement technologiques. Cet ouvrage rassemble quelques pistes de réflexion sociotechniques, qu’il serait souhaitable de garder à l’esprit dans tout projet de bibliothèque numérique afin d’éviter que ces coûteuses machines technodocumentaires, au service de l’information et de la connaissance, et à l’usage de tous, ne soient reléguées au rang de dépôts numériques, extensions méconnues des usagers des bibliothèques traditionnelles."
"Malgré le discours politique et économique rôdé, qui accompagne la profusion techno-informationnelle d'où émerge la Société de l'Information, des situations inédites apparaissent, des interrogations se font jour, des complexités se révèlent, tant au niveau des individus censés maintenir un niveau d'activité satisfaisant dans cette société en mutation, qu'à celui de la macro-structure sociétale qui doit (re)trouver une nouvelle stabilité. Au regard des problématiques identifiées par de nombreux chercheurs, relatives à l'archivage de données numériques structurées ou non, l'accessibilité et le partage, la propriété intellectuelle, le document numérique, la Recherche d'Information, les modalités d'interaction avec les systèmes techniques, les compétences numériques et informationnelles, la qualification et la pertinence de l'information, les profils informationnels, etc., les planifications politiques de la Sociétés de l'Information ne peuvent que susciter circonspection voire embarras d'un point de vue scientifique. Cet ouvrage collectif, qui rassemble treize chapitres originaux de chercheurs en Sciences de l'Information et de la Communication, témoigne du caractère sensible de quelques problématiques scientifiques, présentées dans ce volume, qui sont pour certaines d'entre elles, anciennes, et pour d'autres, plus récemment formulées, et que la nouvelle ère numérique a mis en lumière."
"Le temps des sites internet/intranet statiques est révolu. Les difficultés rencontrées pour les mettre à jour et en assurer l'évolutivité conduit de nombreuses entreprises à s'orienter vers des solutions dynamiques où les pages HTML sont produites à la volée , à partir de structures préétablies alimentées en informations par des bases de données. La capacité à interfacer serveur Web et système de gestion de bases de données devient donc cruciale. Les bases de données pour l'internet et l'intranet présente l'ensemble des technologies (IDC, ASP, JDBC, PHP, Phraséa, Jasmine, etc.) qui sont utilisées dans ce domaine. Il s'adresse aussi bien à un public non rompu aux exigences de la programmation qu'à celui plus averti des développeurs."
"L'interopérabilité technologique des bibliothèques numériques doit être repensée pour s'adapter aux nouveaux usages et réseaux. Les environnements numériques documentaires destinés à répondre aux demandes patrimoniales, culturelles, scientifiques ou commerciales ont investi le cyberespace mondial et ont redessiné le paysage technodocumentaire du Web. Pourtant, alors que les modèles technologiques démontrent leur efficacité et expliquent en grande partie la création de bibliothèques, archives et dépôts numériques, le concept sous-jacent des usages continue à faire débat. Les technologies informatiques utilisées par les bibliothèques numériques hétérogènes permettent une interopérabilité technique des contenus. Celle-ci n'est pas suffisante pour permettre l'adhésion d'un public connecté aux profils informationnels et techniques très différents. Cet ouvrage explore les pistes d'une interopérabilité orientée usage où les questions des interfaces de consultation et des procédés de description des contenus sont étudiées"
"Comme depuis les commencements de l'Ecriture, l'Hypertextualité ne cesse de venir vers nous, dès l'émergence des villes ou des cités, 1 l'Hyperville ne cesse de venir à notre rencontre selon des formes et des devenirs très hétérogènes. L'Hyperville de Franck Cormerais n'est pas la mère de toutes les villes passées, présentes et à venir, mais une actualisation incarnation spécifique singulière de ce Virtuel qui donc nous précède, la ville contributive. Cette Hyperville à la fois déjà là sous des formes locales et modèles limités, se différencie d'autres actualisations en cours, d'autres incarnations associées à des économies politiques dominantes, à des manières de faire converger physique sociale et biopolitique pour définir des territoires urbains d'hypercontrôle des populations. Les diverses formes de "" Smart cities"" sont ici visées mais aussi des formes hybrides où sont tissés à la fois les forces et les énergies, les ressources et les peaux du monde, le tissage continue des êtres et des choses, des artefacts et des objets…."
"En 2008, la crise économique a précipité du côté des annonceurs le processus d’internalisation de leurs activités à travers le concept d’agences intégrées (K. Brizard-Kim, 2015) ; l’objectif étant de réduire les dépenses publicitaires . Aujourd’hui, avec l’exploitation de la data, la créativité recherchée dans les formes d’expression et la production de contenus destinés à mettre en place des stratégies de communication personnalisées, cabinets conseils, Entreprises de Services Numériques et GAFA empiètent sur le territoire des agences de communication. Face à cette situation, certains acteurs inquiets de l’avenir de la filière prévoient depuis quelques années, une « ubérisation du métier d’agences » avec pour conséquence « la constitution d’un cimetière d’agences, jonchée des plus vieux noms du secteur » . Tandis que d’autres préfèrent adopter des positionnements différenciant et rationalisant le modèle d’agence d’un point de vue organisationnel et méthodologique. C’est le cas notamment des agences de communication digitales dites Inbound, qui cherchent à « s’approprier des espaces sur lesquels les groupes professionnels voisins ne cherchent pas ou ne parviennent pas à s’implanter » (C. Gadéa, S. Olivesi, 2016, P.8). Elles font la promesse d’une méthode de conception centrée sur les besoins de l’utilisateur et capables de générer un bon retour sur investissement des actions sur la base d’une analyse de leur langage corporel numérique. Pour ce faire, des techniques inspirées des théories méta professionnelles du design UX sont mobilisées par ces agences pour faire reconnaître le professionnalisme des acteurs sur la conception centrée utilisateur. Toutefois, nos séances d’observations participantes et entretiens d’autoconfrontation réalisées – à partir d’une démarche ethnométhodologique (H. Garfinkel, 2007) – dans le cadre d’un projet de création chez notre partenaire industriel nous amènent à constater une activité de conception très technicisée. Autrement dit, nous assistons à une activité ponctuée par l’usage d’outils qui interviennent comme des assistants voire des architextes de conception (L. Collet, 2018). Loin de souscrire à une sorte de déterminisme technologique, ces outils conçus par Hubspot préconfigurent d’une part, les espaces de pratiques des professionnels vers une logique de conception marquée avant tout par les préceptes de l’algorithme Page Rank de Google. Nous identifions cette tendance sous le terme de « googletisation » ou « googling » en référence aux travaux de D. Duvernay et P. Rasse (2009). D’autre part, ils donnent à voir une extension du modèle gestionnaire de la communication, incitant les acteurs en agence à épouser une démarche d’amélioration continue. Ce qui suscite à certains égards, des tensions et renforce le diktat du commanditaire."
"Pourquoi s’intéresser à un concept qui est a priori balisé, voire désuet ? L’objectif de ce texte est de montrer que le concept d’identité est toujours opérationnel, notamment s’agissant de terrains de recherche complexes. À travers une étude articulant une approche conceptuelle qui s’appuie sur un cadre théorique en sciences de l’information et de la communication ainsi qu’une enquête par entretien, auprès d’acteurs sociopolitiques et religieux libanais, le présent travail tente d’apporter un éclairage empirique quant à la question de l’identité au Liban."
"Être radical c’est avoir une pensée irréductible et surtout ne pas reconnaître l’autre et ses opinions. En communication, la radicalité peut se traduire par le refus de l’altérité. Outre la violence, l’objectif du radical ne vise-t-il pas à perturber ou à réduire, sinon aliéner la pensée de l’autre ? Depuis l’éruption de la dernière pandémie, l’agenda médiatique est focalisé sur la Covid-19. Or les médias ne sont pas les seuls relais de l’information. Les médias dits sociaux ont envahi nos écrans et notre quotidien. Nous sommes conscients de la viralité des fausses informations qui circulent sur le web, on sait aussi la portée de la rumeur dans l’espace public. Mais qu’en est-il de la cabale contre le coronavirus à l’ère de l’incertitude et de la peur ? Ne sommes-nous pas au temps de la radicalité ?"
"Avant de tomber dans l’actuelle crise sanitaire qui fragilise les États-nations, quelques pays du sud de la Méditerranée comme le Liban, l’Irak et l’Algérie vivaient des soulèvements, voire des révolutions populaires. Quels effets sur les pouvoirs et institutions hégémoniques ? Quel bilan des revendications proclamées par le Peuple dans l’espace public ? Quelles sont les réalités politiques et communicationelles du Hirak algérien ?"
"Reconnaître l’altérité, c’est accepter les principes d’un contrat en mouvement qui se renouvelle grâce à la communication. Ce cadre rend peut-être possible la confiance en l’autre. Mais quand les incompréhensions dépassent un certain seuil, la méfiance conduit parfois à un geste brutal. L’un refuse la différence, pendant que l’autre le voit basculer vers la radicalité. Albert Camus nous le montre dans L’homme révolté (1951) avec l’égarement, la démesure et la déraison de celui qui tente de s’identifier à l’autre. L’homme radical est, lui aussi, celui qui veut tout ou rien. Ne rencontrons-nous pas de plus en plus la radicalité ? Ne sommes-nous pas face à une banalisation du radical et de ce qui l’anime ? Mais qu’est-ce donc qu’être radical ?"
"La littérature récente, consacrée à la question de la radicalité, en sciences de l’homme et de la société en général et en sciences de l’information et de la communication plus particulièrement, témoigne d’un grand intérêt porté au phénomène de la radicalisation plus qu’à la radicalité, au processus conduisant à l’action violente plus qu’à la forme d’expression qui engloberait des actions violentes et non-violentes. L’objectif de cet article est de proposer, d’une part, une approche conceptuelle de la radicalité, et, d’autre part, de proposer une définition de la radicalité en communication."
"Les plateformes dans leur grande variété, participent de la création continuée du monde. Il n'est pas dans notre intention de tenter une typologie des différentes plateformes. Ce que nous visons c'est de saisir, à travers un certain nombre d'exemples, les conditions essentielles de leur productivité et à l'occasion de soulever un certain nombre de questions anthropologiques et politiques. Les plateformes offrent des fonctionnalités variées selon les régimes relationnels qu'elles portent, selon les traces qu'elles fabriquent ou captent, selon les opérations cognitives ou selon les inférences qu'elles mettent en oeuvre. Pour un grand nombre d'entre-elles elles assurent des fonctions cognitives et connectives essentielles qui animent les intelligences collectives, qui définissent ce dont elles sont l'expression et l'exprimé."
"Dans les systèmes d’informations instrumentés par les TIC, les usages et les pratiques révèlent des phénomènes complexes que construisent les processus cognitifs des utilisateurs, les principes fonctionnels des dispositifs techno-documentaires et les visées organisatrices des concepteurs. Cet ouvrage rassemble des contributions de natures différentes autour de problématiques liées aux usages et aux pratiques dans les bibliothèques numériques. Chercheurs en psychologie, en ergonomie, en sciences de l'Information et en informatique, professionnels de l'information et responsables de services institutionnels se trouvent associés pour communiquer, et finalement partager, leurs analyses à différents degrés et sous différents angles, des usages planifiés, espérés, pressentis, constatés, originaux ou en genèse de ces dispositifs techno-documentaires qui construisent la société de l'information émergente"
"L'hypertextualisation automatique, processus empirique débouchant de l'hypertexte, repose sur la réutilisation de documents linéaires de nature technique, saisis à partir de logiciels de traitement de textes, afin de créer dynamiquement les noeuds et les liens réseaux hypertextes. La phase d'extraction des noeuds utilise la structure physique pour mettre en évidence les différentes entités logiques composants les documents. Les liens références (particulièrement les références croisées), dont la syntaxe est préalablement définie par l'auteur, sont extraits au moyen d'un programme d'analyse utilisant une description générique de la grammaire des références croisées. L'hypertextualisation automatique conduit à la création d'un méta-réseau hypertexte ou la mise à jour locale de documents peut remettre en cause la cohérence des noeuds et des liens. comme les systèmes de gestion de bases de données relationnelles ont montré leur efficacité pour préserver l'intégrité des données, nous proposons une normalisation relationnelle des hyperdocuments automatiquement générés afin de gérer la mise a jour des liens références. L'accroissement du volume d'information est une autre conséquence de la création automatique de réseaux hypertextes puisqu'elle accentue davantage les problèmes de désorientation et de surcharge cognitive. Une solution consiste à coupler le processus d'hypertextualisation à un programme d'indexation automatique, ce qui permettrait d'associer à chaque noeud du méta-réseau un ensemble de termes pertinents représentatifs du contenu du noeud. Ainsi, le lecteur disposerait non seulement de mécanismes de navigation structurelle mais aussi de possibilités de recherche sémantique."
"En questionnant une société civile en mouvement à travers ses dispositifs médiatiques, cette communication tente de comprendre le phénomène de la radicalité d’un point de vue communicationnel. Aujourd’hui, l’usage de la radicalité est ambigu. Il désigne dans l’espace public la radicalisation ou le radicalisme, sinon le terrorisme. Il est donc important de re-questionner la radicalité à partir d’un regard communicationnel. C’est pourquoi, afin de mieux l’appréhender, nous proposons d’étudier la société civile libanaise qui oscille entre émancipation et radicalité à partir d’une approche sémio-pragmatique."
"Les fablab sont des lieux d'expérimentation et d'innovation sociale portée par l'idéologie d'une économie collaborative et durable (Nedjar-Guerre, Gagnebien, 2015). Ils permettent aussi le développement d’une contre-culture du bricolage, de l'innovation (Bosqué, Noor, Ricard, 2015) et de l'hybridation culturelle (Eychenne, 2012). Ils renvoient à la conception d'une société où les notions de transparence, gouvernance, interactivité, transmission horizontale de l'information sont en parfaite adéquation avec l'idéologie portée par les technologies numériques. Ces technologies numériques sont-elles mises à contribution pour dynamiser les formes d'investissement des acteurs citoyens sur un territoire dans une démarche d'économie collaborative ?"
"Le monde d’aujourd’hui est en pleine transition, celle du numérique. Les nouvelles technologies et en particulier l’Intelligence Artificielle, transforment nos quotidiens, nos modes de vie. Le Département souhaite accompagner ses concitoyens dans la compréhension de ce monde nouveau au travers d’un cycle de conférences et tables rondes afin de mettre l’Intelligence Artificielle à la portée du plus grand nombre. Ces rendez-vous réguliers, les #IADATES, vous permettront de mieux comprendre et connaître les enjeux et perspectives de l’I.A. dans les domaines qui touchent tout un chacun : santé, mobilité, éthique, territoire… Pour ce faire, la Maison de l’Intelligence Artificielle et le Département des Alpes-Maritimes s’associent avec l’Institut EuropIA qui a pour vocation de sensibiliser le public aux questions de l’Intelligence Artificielle. Cette fois-ci, les #IADATES nous emmènent vers une nouvelle thématique liée aux enjeux éducatifs à l’ère de l’intelligence artificielle. « De la salle de classe à la formation des enseignants » nos intervenants réaliseront un parcours sur les enjeux que les différents acteurs éducatifs doivent considérer à l’ère de l’IA. Ils aborderont la formation à l’IA des élèves aux enseignants, en mettant l’accent sur les usages créatifs de l’IA en éducation, pour ensuite partager des initiatives de formation citoyennes comme le MOOC IAI. Simon Collin partagera ses travaux sur les aspects éthiques de l’IA en éducation et les autres conférenciers termineront la conférence en abordant le potentiel de l’IA pour la recherche en sciences de l’éducation et de la formation."
"Contexte. Le gaspillage alimentaire (GA) fait référence à « Toute nourriture destinée à la consommation humaine qui, à une étape de la chaîne alimentaire, est perdue, jetée, dégradée » (Pacte national de lutte contre le gaspillage alimentaire, 2013, France). Selon FAO, le GA est quatre fois plus important en restauration hors domicile, et notamment en cantines, où sont pris près de la moitié de ces repas. Cependant, les preuves empiriques concernant les facteurs expliquant le GA à la consommation en cantines scolaires et l'efficacité des leviers - stratégies d’intervention testés pour réduire celui-ci restent disparates. Pour remédier à ce problème, une analyse systématique a été réalisée pour synthétiser les résultats d’études dans le contexte français (les facteurs et les leviers du GA), puis comparer expérimentalement l’impact de trois leviers communicationnels incitant les convives à gaspiller moins d’aliments dans la restauration scolaire. Se basant sur les recherches théoriques et sur le contexte de l’étude, nous avons souhaité comparer les effets de trois leviers en particulier : la communication participative (CP), la communication engageante (CE), la communication incitative douce ‘nudge’ (CN). Méthodes. Le design initial d’étude expérimentale comprenait le test des trois dispositifs ci-dessus dans trois collèges du département des Bouches du Rhône, permettant comparer leurs effets respectifs : le dispositif CP (collège 1) ; les deux dispositifs combinés CP+CE (collège 2) ; les trois dispositifs combinés CP+CE+CN (collège 3). Toutefois, les contraintes organisationnelles et la crise sanitaire ont empêché le déroulement expérimental prévu. Après deux ajustements au cours du projet, un seul collège a pu appliquer la quasi-totalité du protocole, et un nombre de facteurs de ce contexte particulier ont influencé désormais les mesures et le déroulement de l’expérience. Le design final réalisé comprend donc : un collège « contrôle » où deux séries de mesures du GA ont été réalisées, sans intervention (collège 2) ; un collège de « test » où deux séries de mesures ont été réalisées avec l’intervention par trois dispositifs combinés CP+CE+CN (collège 3). Résultats. Dans ce contexte, nous avons pu analyser les résultats descriptifs du GA scolaire dans les deux collèges ; ainsi qu’avoir quelques résultats comparatifs mais dont l’interprétation est limitée. Sur les deux collèges, on observe globalement que les ‘cuidités’ (légumes cuits) génèrent le plus du GA chez les convives, ce qui va dans le sens des résultats d’études antérieures. On observe l’augmentation du GA moyen dans le collège temoin de +20% environ, d’une année à l’autre. On observe l’absence d’évolution du GA moyen dans le collège « test » d’une année à l’autre, avec le protocole de test appliqué mais incomplet (3 dispositifs, CP+CE+CN). Concernant le dispositif CP, on observe néanmoins que les élèves Eco-ambassadeurs/éco-délégués « participants » auraient le GA moindre, comparé aux autres convives (-4,5%, malgré leur participation moins importante que prévu), ce qui irait dans le sens des travaux antérieurs. Ces élèves « participants » ont pu compléter la sensibilisation de 2/3 des classes. Concernant le dispositif CE, 73% des convives ont réaffirmé par le mini-questionnaire « pré-engageant » leur « capacité d’agir » sur au moins une des 7 actions de lutte contre le GA. Concernant le dispositif CN (nudge « feedback » et « norme sociale »), les observations permettent de suggérer, qu’il aurait réussi à attirer l’attention des convives, qui venaient retrouver leurs questionnaires. Conclusion. L'efficacité des trois leviers testés dans ce contexte scolaire a été limitée, mais il est impossible d’en tirer des conclusions définitives. En effet, les résultats comparatifs du GA moyen ont subi des biais méthodologiques, - du fait que le protocole n’a pas été réalisé intégralement, et que le contexte temporel, organisationnel, situationnel fut très restrictif et non identique. Toutefois, les résultats de cette étude fournissent des indications et des pistes de réflexion sur des différents leviers de lutte contre le GA scolaire à prendre en compte et à continuer à tester plus amplement."
"L’article propose une analyse des interactions entre ville durable et dispositifs numériques du point de vue de la citoyenneté environnementale. Centré sur les jeux en ligne permettant la simulation de la ville du futur et sa mise en débat, il développe la problématique de l’engagement du joueur. L’enjeu n’est pas tant d’analyser les résultats d’un débat démocratique augmenté par le numérique, que de mettre en évidence les effets d’une mise en technologies des débats sur la ville durable et de leur appropriation par les citoyens. La mobilisation d’un corpus rétrospectif – 4 jeux en ligne publiés entre 2006 et 2018 – permet de montrer l’émergence progressive de ces dispositifs de jeu, la diversité des acteurs qui en sont à l’origine et l’oscillation entre l’objectif d’éducation à l’environnement et celui de transmission d’un message positif sur l’action publique environnementale (marketing politique). Nous observons un glissement vers une certaine standardisation du sujet, mais aussi une ludification de l’activité citoyenne ordinaire. Enfin, l’identification des diverses formes d’éthique mobilisées par le joueur nous permet de conclure sur la portée de ces dispositifs de jeu sur l’engagement citoyen en matière d’environnement."
"Contexte: Le gaspillage alimentaire (GA) fait référence à « Toute nourriture destinée à la consommation humaine qui, à une étape de la chaîne alimentaire, est perdue, jetée, dégradée » (Pacte national de lutte contre le gaspillage alimentaire, 2013, France). Le GA est quatre fois plus important en restauration hors domicile, et notamment en cantines, où sont pris près de la moitié de ces repas. Cependant, les preuves empiriques concernant les facteurs expliquant le GA à la consommation en cantines scolaires et l'efficacité des leviers - stratégies d’intervention testés pour réduire celui-ci restent disparates. Pour remédier à ce problème, une analyse systématique a été réalisée pour synthétiser les résultats d’études dans le contexte français, comparer les facteurs et l’impact de leviers incitant les convives à gaspiller moins d’aliments dans la restauration scolaire. Méthodes. Une recherche systématique de la littérature a été effectuée sur les bases de données scientifiques BDSP, EBSCO BSP, EBSCO PsyArticles/PsyInfo, CAIRN, Archives HAL SHS de CNRS, ainsi que sur le Google Scholar. Après évaluation de l’éligibilité, 23 publications (17 études) ont été retenues pour la synthèse descriptive. Seize publications (11 études) contenaient suffisamment d’informations pour en synthétiser les résultats à l’aide du logiciel d’analyse de données qualitatives Nvivo12 Plus. Résultats. La synthèse montre que les facteurs du gaspillage alimentaire des convives mis en lumière dans les études seraient davantage comportementaux. D’autre part, les leviers d’intervention visant à réduire ce gaspillage ont un effet modéré. Les mesures et les résultats sont hétérogènes, avec la qualité des études en milieu naturel moyennement ou peu robuste (ou non précisée). Conclusion. Les résultats de cette revue fournissent l’indication des facteurs du GA scolaire à prendre en compte ou à explorer plus amplement, ainsi que l’indication de l'efficacité des leviers testés dans les contextes scolaires, - tout en soulignant les problèmes à résoudre avant de tirer des conclusions définitives. http://www.academia.edu/45625328/Gaspillage_alimentaire_à_la_consommation_dans_la_restauration_collective_scolaire_en_France_comportements_facteurs_interventions_Revue_systématique_de_littérature"
"Je souhaite aborder l'opposition entre imaginaires urbains autour du commun, du paysage partagé, et au contraire autour du privé, des gated communities et de l’entre-soi., en l'illustrant par un projet qui tente de porter l'occupation sur ce que l’on nomme les “franges ou tiers paysages”(Clément, 2020). La nouvelle place faite à la nature est en tension avec les dynamiques spontanées des urbains depuis les deux derniers confinements. Ce projet “en Lisière” vise à structurer un collectif multi-acteurs pour co-construire et gérer de manière intégrée un espace à l’interface entre la ville de Marseille et le Parc national des Calanques (PnCal) avec l’approche de la permaculture et ainsi contribuer à la « transition écologique » (Den Hartig Cyrielle, 2013). Il s’agit de faire réfléchir et faire travailler ensemble des associations de citoyens et des acteurs publics et privés et plus récemment des artistes invités afin de créer un espace ouvert à tous d’observation, d’initiation, d’apprentissage, de recherche, de coopération et de transmission des enjeux de la biodiversité. Il s'agit d’une ancienne parcelle agricole située à la fois dans le périmètre du PnCal et à proximité de grands ensembles urbains dont un quartier populaire du 11ème arrondissement de Marseille. Ce paysage en interface ville/nature - à la fois fondamentalement urbain et caractéristique d’une nature exceptionnelle et protégée, invite à imaginer et valoriser des liens géographiques (rurbain - périurbain), écologiques (de la ville à l’agricole jusqu’à l’espace naturel du PnCal) et culturels, (du vernaculaire et des historiques d’usages aux enjeux contemporains pour l’environnement et la ville durable). Cette parcelle a permis à des générations d’habitants, dont beaucoup de populations immigrées, de développer des usages. Mais depuis 2021, deux ensembles de logements individuels construits sur le modèle de lotissements pavillonnaires clos souhaitent fermer cet espace. Durant le confinement, cette frange a vu sa fréquentation augmenter : balades, promenade des chiens, se rendre au PnCal sans passer par les accès principaux bondés etc. Des pétitions fleurissent contre la privatisation des accès, qui ont mené des actions pour valoriser la parcelle depuis plus de 80 ans (Khouri, 2016). Je souhaite interroger les rôles tenus de la nature et de ses imaginaires spécifiques dans nos environnements urbains, nos jardins privés et collectifs (Brondeau Florence, 2017 ; Demailly Kaduna-Eve, 2017), nos places, parcs et paysages La méthodologie d’analyse est ethnographique et ancrée , avec participations aux diverses réunions de sensibilisations avec les collectifs et acteurs en présence, promenades avec les habitants sur la parcelle, captation photographique. Quel seront les rôles futurs cette nature à Valbarelle ? Rôles envisagés en tension visant à garder cet espace non encore clos avec son charme du « non exploité », voire soit prétexte au « non-agir » (collectif Loco, 2016). Bibliographie Brondeau Florence, « Jardiner pour coproduire la ville », Géographie et cultures [En ligne], 103 | 2017, mis en ligne le 18 octobre 2018 Clément Gilles, 2020, Manifeste du Tiers Paysage, Édition du commun, coll. Culture des précédents, 81 p. Demailly Kaduna-Eve, 2017, Les jardins partagés franciliens. Des territoires de transition environnementale ? Géographie et cultures, L’Harmattan, pp.79-95. Den Hartig Cyrielle, 2013, Jardins collectifs urbains : leviers vers la transition, Mouvements, n°75, pp. 13-20 Khouri Liliane, 2016, Il était une fois la Cité Michelis à Marseille, Gaussen, 160 p. Collectif Loco, 2015, Machines de guerre urbaines, Manola Antonioli sd., Ecole nationale supérieure d’Art de Dijon, Editions Loco 303 p."
"Cette étude s’intéresse à la réception de dispositifs communicationnels contre les violences sexistes, issus de la société civile et mobilisant le détournement du genre (DDG : stratégie créative perturbant les rapports genrés). Elle vise à mieux comprendre la réception et les influences de ces dispositifs, envisagés comme interactions humaines et sociales médiatisées, à travers une méthodologie mixte et triangulée (focus group et protocole de mesure des attitudes « avant-après »). Les résultats montrent, qu’en réception, (1) les violences sont envisagées de manière collective; l’éducation prime pour lutter contre celles-ci, quand la loi ou le féminisme contemporain constituent notamment des résistances au changement. Les personnages et l’ancrage des violences dans la réalité agissent comme marqueurs forts du DDG. (2) En outre, le DDG provoque un inconfort psychologique mais peu d’empathie virtuelle. Enfin, nous montrons que (3) le DDG diminue significativement des attitudes de sexisme hostile et bienveillant."
"Les communautés connaissent un essor considérable depuis l’instrumentation numérique réalisée à partir des Technologies de l’Information et de la Communication qui façonnent et organisent le monde du Web et des réseaux socionumériques. En analysant l’émergence et le potentiel d’Intelligence Collective d’une communauté de développeurs hors de l’organigramme de la Direction des Systèmes d’Information d’une grande entreprise de l’énergie, nous avons identifié une approche d’organisation décentralisée autorisant la résolution communautaire de problèmes liés aux activités-cœur de l’entreprise et ouvrant en parallèle des perspectives d’innovation sociale dans le fonctionnement historique de l’entreprise."
"Dans l’univers des troubles «dys», les personnes atteintes de troubles dysexécutifs sont en attente de nouvelles solutions pour leur rééducation. A partir d’une analyse approfondie de la littérature, un consortium multidisciplinaire a été créé afin de spécifier et de développer un Serious-Game qui immerge les patients dans un monde virtuel à travers un scénario élaboré et des missions variées. Il s’utilise sur une table tactile robotisée de 48’ facile à appréhender. Ce dispositif a trois enjeux majeurs. Tout d’abord la levée de l’anosognosie ainsi que le passage à une motivation intrinsèque des patients afin qu’ils soient acteurs de leur rééducation. Ensuite, l’atteinte d’une métacognition suffisante pour qu’ils puissent développer des stratégies qui leurs sont propres et qu’ils sélectionnent la plus appropriée dans chaque contexte. Enfin, la facilitation du transfert de ces stratégies dans la vie quotidienne. Des changements dans l’organisation des thérapeutes sont également attendus."
"Voici un éclairage original, inspiré par les sciences de l'information et de la communication, sur les transformations actuelles des industries et institutions culturelles, ainsi que des arts vivants, à l'ère du numérique. Les auteurs proposent de repenser les catégories de la création, de la médiation ou de la réception au travers des perspectives de réinvention et de dépassement offertes par l'environnement numérique."
"Introduction. Nous proposons une méthodologie et des outils pour explorer les contextes de citation des publications savantes sur le web. Méthode. A partir des logs des pages de l'OpenEdition, une plateforme qui héberge des publications scientifiques en sciences humaines, et des traces numériques laissées sur le web par les visiteurs, nous avons identifié les pages web qui citaient des publications de la plateforme OpenEdition et délimité les contextes de citation en utilisant la bibliothèque de segmentation thématique Texttiling. Avec Unitex, un moteur de traitement du langage naturel, nous avons identifié des expressions nominatives à partir des contextes de citation des pages web citantes et construit un graphique de cooccurrence reliant les publications OpenEdition citées aux expressions trouvées dans leurs contextes de citation sur le web. Analyse. Nous avons généré une interface graphique dynamique et interactive permettant à l'utilisateur de naviguer entre les publications citées de l'Open Edition et leurs contextes de citation. Résultats. L'exploration des graphiques a révélé les thèmes de citation, leur provenance (citers) et d'autres caractéristiques telles que le but de la citation, les acteurs et les événements entourant les publications citées et la polarité de la citation (positive ou négative)."
"Cette thèse porte sur la communication médiatée par des avatars dans un espace numérique le « jeu de rôle en ligne massivement multijoueur » (souvent désigné par l’acronyme « MMORPG ») : World of Warcraft. Si nous considérons les espaces numériques à l’aune des interactions qui s’y déroulent, il apparaît que l’une de leurs particularités est la disparition du corps physique de l’interactant (et de ses extensions que sont la voix ou ses postures) au profit d’un corps numérique : l’avatar. Cette substitution pourrait être problématique dans la mesure où le corps physique est une source d’informations essentielle à la définition de la situation permettant le bon déroulement d’une interaction. Pourtant, les recherches antérieures montrent que les individus s’avèrent tout de même capables de fonder des groupes, voire de les pérenniser. C’est ainsi que nous avons été amenés à nous demander : dans quelle mesure une communication médiatée par des avatars est-elle suffisante pour favoriser la pérennité d’un groupe d’individus ? Pour répondre à cette question nous avons poursuivi la lignée conceptuelle de Edward Castronova proposant d’appréhender les espaces numériques comme des espaces à part entière, distincts de l’espace physique. Traduisant cet axe de recherche dans les Sciences de l’Information et de la Communication, cela nous a conduit à nous focaliser sur les interactions menées dans un espace numérique et à considérer que la distinction faite entre les espaces pouvait également s’appliquer à leurs habitants ; du point de vue des interactions, l’avatar est donc une entité distincte de l’utilisateur. Afin de tenter de répondre à ce questionnement et de mettre à l’épreuve ce cadre théorique, nous avons mis en place une triangulation méthodologique. Le sommet principal de celle-ci a consisté en une ethnographie, pendant plus de deux ans, au sein de deux guildes (des regroupements institutionnalisés d’avatars) de World of Warcraft. Cette observation participante a été complétée par la conduite d’entretiens formels (second sommet) et la captation audiovisuelle des périodes d’interactions les plus intenses (troisième sommet). Cet appareillage méthodologique nous a permis de saisir que les avatars étaient à même de produire des groupes et de les pérenniser, non seulement en produisant des sentiments forts, ancrés spatio-temporellement comme la nostalgie, mais aussi en donnant une dimension rituelle à des évènements essentiels dans la vie quotidienne de ces guildes. L’espace numérique, lorsqu’il est appréhendé depuis le point de vue des interactions, apparaît alors comme marqué par une forme d’indépendance et d’autonomie qui se heurte toutefois à certaines limites essentielles qui nous conduisent à considérer que les interactions menées dans un espace numérique peuvent relever davantage d’un cadre de communication que d’un cadre social."
"Au cours des dernières années, le domaine du documentaire interactif s’est progressivement développé en raison des changements survenus dans le monde de l’Internet et d’études académiques croissantes sur le sujet. Pourtant, on sait relativement peu de choses sur la relation entre l’usager et le documentaire interactif. L’objet de cette étude est précisément de mesurer les attitudes et les interactions de l’usager exposé à un documentaire interactif décliné en différentes versions, disposant chacune d’un degré d’interactivité plus ou moins développé. L’étude de l’attitude des usagers nous a conduit à approfondir les notions d’engagement narratif, d’interactivité perçue, d’engagement perçu et d’attitude à l’égard du site Web documentaire interactif. Un autre objectif de cette étude est d’examiner la relation entre interactions réelles et perceptions des usagers. L’étude a cherché à comparer l’interactivité et la linéarité en terme d’engagement narratif et d’engagement perçu. Un travail de terrain a été conduit auprès de 360 étudiants jordaniens. L’échantillon a été divisé en trois groupes, chaque groupe visualisant un des 3 documentaires interactif et répondant au questionnaire relatif. L’étude a également utilisé deux logiciels pour tracer le comportement réel de l’usager. Les résultats de cette étude mettent à jour une relation significative entre d’une part le haut niveau d’interactivité réelle et d’autre part l’interactivité perçue et l’attitude à l’égard du site Web documentaire interactif. D’autre part, les résultats ont révélé une corrélation positive entre d’une part l’interactivité perçue et de l’autre l’engagement perçu et l’attitude à l’égard du site Web documentaire interactif. Cependant, l’étude n’a pas trouvé de corrélation entre l’interactivité perçue et l’engagement narratif. De plus, les résultats ont montré que l’interaction réelle des participants est positivement corrélée à leurs perceptions. Enfin, les participants qui ont regardé le documentaire linéaire sont significativement plus engagés dans la narration documentaire que les autres groupes. Cette étude présente enfin les résultats, les discute et envisage des perspectives futures."
"Les citoyens déplorent de plus en plus les mauvaises nouvelles rapportées par les journalistes, et expriment une défiance envers les médias. Pour répondre à ces attentes, des rédactions se lancent dans le journalisme de solutions, le « sojo ». Quelle est son histoire, quelles sont ses caractéristiques, quels sont les médias qui l’ont adopté et pourquoi ? « Ne pas masquer les mauvaises nouvelles, mais redonner leur juste place aux informations enthousiastes, aux réussites, au développement de l’humanité », c’est ainsi que Pauline Amiel décrit le « sojo » : il a pour ambition de traiter une question de société en présentant les solutions potentielles pour la résoudre. Voilà un moyen de fédérer les journalistes autour de pratiques exigeantes, proches de l’investigation, et de tenter de regagner la confiance du lectorat. Le sujet est traité sous l’angle opérationnel, orienté métier. Complété d’interviews des pionniers de la pratique, l’ouvrage propose une boîte à outils pour le journaliste de solutions : quels sujets aborder, où chercher ses sources, comment construire son article, son interview…"
"L’auteure propose une lecture diachronique et synchronique de l’épistémisation des SIC. Elle souligne certains apports et tournants accompagnés de tensions épistémiques et de certains points aveugles qui permettent de comprendre comment les chercheurs en SIC élargissent, au fil du temps, les frontières disciplinaires, formulent de nouvelles problématiques, mais aussi passent parfois à côté de certaines questions vives. Elle s’attache à identifier certaines filiations qui préparent la place, actuellement modeste, occupée par la communication environnementale dans le paysage des SIC et des humanités environnementales. Elle met en perspective les travaux des chercheurs en communication consacrés aux thématiques environnementales et questionne la pertinence et la potentialité de la notion d’Anthropocène comme aiguillon pouvant aider au dépassement de certaines formes de dualisme dans la pensée communicationnelle."
"L’usage d’internet en classe est un sujet d’actualité au sein de l'institution scolaire. À travers une étude de cas, nous analysons un dispositif permettant aux élèves de puiser dans les ressources disponibles en ligne pour étudier les textes littéraires. Celui-ci s’appuie à la fois sur l’utilisation du numérique dans la classe et sur une pratique de l’analyse des textes reposant sur les recherches et propositions des élèves plutôt que sur les choix du professeur. Nous analysons comment la mise en œuvre de ce dispositif accentue certaines préoccupations de l’enseignant et, en étudiant comparativement deux entretiens menés au début et en fin d’expérimentation, nous cherchons à cerner certains signes d’appropriation. L’analyse établit que le processus repose notamment sur la manière dont l’enseignant parvient à mettre en relation ses habitudes avec les contraintes techniques et didactiques induites par le dispositif, mais aussi avec les pratiques médiatiques numériques hétérogènes effectives des élèves. Ce cas tend à illustrer que l’introduction d’un dispositif didactique numérique exige un temps long d’appropriation et souligne la nécessité d’un enseignement explicite des compétences en littératie médiatique numérique reposant sur la prise en compte des pratiques informelles des élèves dans la classe."
"Internet est devenu un outil incontournable pour développer une stratégie de communication dans l’organisation ; dans ce flux des supports imagés, il s’agit désormais de se faire connaître, de séduire et d’affirmer son identité visuelle. Le Buzz marketing est précisément usité comme une des nouvelles stratégies de communication, rendant le consommateur actif. Pourtant, faire du client potentiel l’acteur privilégié de la communication marketing ne s’improvise pas : à nouveau mode de communication, nouveaux outils, et nouvelles professions. Nous pouvons alors nous demander quels sont les enjeux du buzz marketing dans ses différents terrains d’application ? Est-ce un phénomène spontané ou stratégiquement organisé ? Pour répondre à ces questions, l’article s’appuie sur les médias numériques avant de se centrer sur un terrain d’application pédagogique pour éviter la marchandisation des idées."
"Dans le contexte d’économie des territoires actuels, le secteur du tourisme Français se doit de développer des services innovants (Boiron, 2014) et emprunts de modernité afin de continuer à être attractif. La culture du numérique (Licoppe, 2009) guide inéluctablement ces transformations à travers un accès au patrimoine (Benhamou, Thesmar, 2011) et à la création contemporaine se voulant facilités, avec pour leitmotiv la démocratisation culturelle, la transmission des savoirs et l’éducation au patrimoine (Szafrajzen, 2015) au service de la diversité de l’offre culturelle. Ainsi, de nouvelles prestations touristiques voient le jour, conduisant à repenser l’engagement expérientiel entre la culture et le visiteur, notamment au sein de parcours pédagogiques innovants (Corroy, Roche, Savignac, 2017). Le présent article étudie une nouvelle expérience humanisée, celle du cinéma immersif du site des neuf écluses de Fonseranes à Béziers, dans l’Hérault (région Occitanie)."
"La réforme structurelle de l’organisation territoriale française destinée à améliorer l’efficacité de l’action publique dans les territoires a engendré une lame de fond dont l’impact a été considérable sur les composantes structurelles des territoires : régions, départements et communes. Les médiathèques municipales, notamment celles situées dans les grandes agglomérations françaises (Paris, Marseille, Lyon, Lille, etc.) en raison d’une grande densité des communes limitrophes et d’un nombre considérable de bibliothèques dans cette concentration urbaine (certaines communes pouvant être dotées de plusieurs bibliothèques) sont invités à participer simultanément à des périmètres différents : celui de la commune, du département et de la région. Ces périmètres structurels clairement identifiés se complètent de périmètres variables liés à des caractéristiques locales : sous-secteur départemental, sous-réseau de coopération, d’inspiration géographique, entre établissements d’une même intercommunalité ou d’une même commune, coopération entre des établissements situés sur des intercommunalités différentes, etc. Si cette superposition de périmètres urbains pour un même établissement engendre une complexification des activités de collaboration des bibliothèques en matière d’offres documentaires et de services aux publics, elle représente une aubaine pour chaque usager, qui voit augmenter, grâce à ce jeu de superposition de périmètre d’activité, la panoplie des offres documentaire et de services. En renversant cette situation, complexe pour les bibliothèques, en la considérant du point de vue de l’usager, et plus particulièrement de l’usager scolaire, nous avons observé qu’elle pouvait constituer une aubaine sous la forme de la création d’un cocon documentaire, permettant à l’usager scolaire de ces communes de bénéficier des offres documentaires des établissements situés dans un périmètre élargi, et mouvant, compris entre ses lieux de vie et d’études. Au sein de ces grandes concentrations urbaines, notamment en région parisienne, les lieux de vie et d’études de ces élèves peuvent être distants sans pour autant générer de difficultés de mobilité compte tenu d’une offre de transports très conséquente. Il s’agit alors d’offrir une visibilité des fonds documentaires adaptée à chaque usager scolaire en fonction de sa configuration de résidence, de mobilité et de rattachement à un établissement scolaire. La mise à disposition d’un catalogue en ligne personnalisable, adaptée d’une part à ces configurations géographiques spécifiques et paramétrable d’autre part afin d’envisager des interopérabilités inédites entre des structures documentaires qui fonctionnent jusqu’à présent indépendamment les unes des autres, apparaît alors indispensable. Notre proposition de communication vise à concevoir l’instrumentation numérique d’un dispositif numérique paramétrable qui permette de rassembler, à façon, les données bibliographiques des catalogues des établissements documentaires en fonction du périmètre de mobilité de chaque usager. Notre travail s’inscrit dans le prolongement de la théorie de l’activité et de l’action située (Vygostky, Leontiev) et des émergences de genèses instrumentales décrites par Pierre Rabardel lorsque des dispositifs technologiques mobilisés rentrent dans la boucle d’activité constructive du sujet."
"La viralité est un phénomène communicationnel qui a connu son essor depuis le développement d’Internet, du Web 2.0 et des médias et dispositifs socionumériques, et qui demeure aujourd’hui un phénomène majeur de la culture Web. Elle constitue un phénomène complexe qui est la résultante d’une multiplicité de facteurs potentiellement à son origine, qui lui sont favorables et/ou qui l’orientent. Malgré plusieurs travaux étudiant ces divers facteurs, il ne semblerait cependant qu’aucun d’entre eux ne se soit penché sur l’influence du genre dans un phénomène viral. L’enjeu du présent article est de démontrer, par la méthode expérimentale, la significativité de l’influence d’un tel effet sur les intentions de partage selon les genres. Nos premiers résultats permettent déjà de dégager une première tendance quant à l’hypothèse formulée : le genre ne semble avoir aucun effet significatif sur le phénomène viral."
"Cet article propose d’étudier le forum de discussion en ligne comme société, en tant que produit des interactions entre les individus, comme lieu d’action et dispositif rituel, lieu de l’action rituelle et de l’émergence des marqueurs de la cyberlangue associés, un espace de lien social comme un système d’accords entre interacteurs par considération pour l’intimité d’autrui, la « sphère idéale ». À la suite de la mise en relief de codes communs, sur la base d’un rapport sémiotisant entre la cyberlangue et le rite, variable cachée, cette étude analyse un marqueur de certains rites d’interaction et une forte activité créatrice, pour mettre en valeur et discuter l’action pacificatrice de la cyberlangue lorsqu’elle se trouve couplée au rite d’interaction."
"Pour une organisation, communiquer sur le web c’est évidemment rédiger du texte, agrémenter d’images, de vidéos ou de sons puis référencer pour établir une présence sur les index. En règle générale, cette communication dispense un message en phase avec les objectifs d'image à propager par ladite organisation. Mais, la communication écrite sur le web c’est aussi implanter des hyperliens (Lakel & Deuff, 2017) en mettant en relation des contenus (et, par induction de supposer d'une relation entre des organisations ou ces contenus). En effet, il n’y a pas de règles établies pour la description des hyperliens entre les sites : la motivation de leur mise en place est aussi variée que les choix terminologiques (Reymond, 2007; Thelwall, 2006), des ""signes passeurs""(Jeanneret & Souchier, 2005) qui les inscrivent. Ces hyperliens, au-delà des effets sur le degré d'indexation dans notre propos, constituent des chemins de visites, des trajets que peuvent suivre les internautes au cours de leur quête (informationnelle ou achats, peu importe). Dans cette trajectoire, l'usager est alors transporté d'un espace informationnel (organisation A) à un autre (organisation B) qui peuvent conforter ou compléter le message de départ mais, quelquefois produire des dissonances pour l'une ou l'autre des organisations (généralement B, sinon l'existence du lien entre A et B est à remettre en cause). Dans le domaine du tourisme, nous prenons le Parc National Port-Cros comme terrain de recherche du fait de la forte pression anthropique (Larrère, 2009) dû à une surfréquentation touristique en période estivale à laquelle il est confronté. En opposant les acteurs économiques et le Parc national, l'hypothèse initiale est que certains acteurs utilisent l’image du parc pour servir leur propre communication, ce qui peut desservir ou déformer le message informationnel promu par les parcs. Nous pouvons aussi formuler quelques hypothèses contextuelles à ce domaine d’analyse : organisations concurrentes ou organisations collaboratives, par exemple, sous-tendent les interprétations potentielles de ces hyperliens. La proximité géographique peut aussi constituer des éléments informationnels explicatifs. En s’appuyant sur la théorie d’acteur-réseau (Akrich et al., 2006), nous étudions simultanément les réseaux, les acteurs et les artefacts pour analyser les processus communicationnels et informationnels (Serres, 2002) de la communication environnementale. Ainsi, nous partons d’une analyse quantitative des hyperliens qui sont en interconnexion avec le site web organisationnel (Pinède & Reymond, 2013) du Parc National Port-Cros pour réaliser une analyse qualitative en catégorisant et en réalisant la typologie de ses hyperliens pour comprendre le processus info-communicationnel qui se forme autour des différents acteurs."
"Le débat de l’entre-deux tours de la présidentielle française de 2017 est révélateur des ambiguïtés du fact-checking quand il prétend dénoncer les mensonges propagés par les acteurs publics. Drapées dans un discours de vérité, les pratiques de fact-checking visent d’abord à identifier le faux plus qu’à dire le vrai. Elles délèguent l’établissement de la vérité à des sources fiables que le fact-checker pourra mobiliser. L’analyse révèle toutefois qu’il s’agit d’abord de sources institutionnelles considérées comme collectivement légitimes. Le fact-checking déploie ainsi une approche potentiellement conservatrice de l’information journalistique qu’il applique ensuite à l’ensemble des propos tenus dans l’espace public."
"Contexte et formalisation des œuvres 1 % et commandes publiques, qui constituent depuis 1952 un modèle d’intervention de l’État et des collectivités territoriales (première Loi 1 % artistique)1, puis 1983 (commandes publiques), et de leur relation avec le numérique. Le développement des technologies numériques nécessitant autant un changement matériel et de savoir-faire, qu’un changement épistémologique, nous verrons, d’une part, comment sont formatées les commandes, utilisant uniquement des technologies numériques, réalisées à partir des modèles de commande mis en place pour créer des œuvres plus classiques (sculptures et dessins, etc.). D’autre part, considérant que les œuvres ont à résister au temps et aux regards et provoquer d’autres usages, générer du sens, et s’inscrire dans l’Histoire de l’art, quelle peut être la capacité des œuvres numériques issues des commandes à s’installer durablement dans l’espace public et donc à y trouver leur forme ? Deux œuvres issues de commandes publiques réalisées à Toulouse illustreront notre propos : L’œuvre commémorative de Gilles Conan, réalisée pour l’usine AZF en 2012, et l’œuvre de Sophie Calle, créée en 2007 pour le métro Toulousain"
"Nous allons nous intéresser dans cet article aux fonctions exécutives, ou de contrôle, qui constituent les fonctions élaborées impliquées dans le contrôle cognitif intervenant dans les situations qui nécessitent une articulation des actions ou pensées dirigées vers un but finalisé (Godefroy et GREFEX 2004). Ces processus cognitifs de contrôle sont sollicités lorsqu'il faut se concentrer sur une tâche, mémoriser et manipuler des informations, s'adapter à de nouveaux environnements ou règles et plus généralement quand les habitudes et automatismes ne suffisent pas à atteindre ces buts (par exemple, lorsque la route habituelle est barrée et qu'il faut trouver un autre chemin) (Diamond 2013 ; Godefroy et al., 2008). Ces fonctions sont également liées à certains processus attentionnels comme la sélectivité de l'information et la division de l'attention notemment en cas de double tâche (par exemple, songer aux courses à faire tout en surveillant qu'une voiture n'arrive pas lorsqu'on traverse la route). Les troubles touchant ces fonctions compromettent sévèrement l'autonomie (Bottger et al., 1998). Les patients se retrouvant en incapacité de retourner à leur domicile se voient réorientés vers les Établissements d'hébergement pour personnes âgées dépendantes (EHPAD) ou d'autres établissements similaires. Si la structure cérébrale principalement impliquée est le lobe frontal, l'aspect fonctionnel est préféré puisque des déficits sont également observés dans des pathologies non frontales et notamment à prédominance sous corticale (Albert, Feldman, Willis, 1974). Le domaine thérapeutique est encore peu développé et les soignants sont en demande de solutions pour la rééducation des patients. Pour aller plus loin que les outils classiques basés sur les jeux de société ou encore des activités sur papier, la réalité virtuelle peut offrir de nouvelles alternatives et dépasser les limites rencontrées jusqu'ici comme le côté artificiel de la situation ou encore la limitation des occasions où le patient doit prendre des initiatives. Des applications sur ordinateur à celles sur tablettes ou smartphone en passant par la Kinect ou encore les visiocasques, les technologies numériques ouvrent un nouveau champ des possibles encore inexploré. Les serious games sont, en particulier, un moyen privilégié pour proposer une nouvelle expérience thérapeutique en permettant de plus un paramétrage adapté à chaque patient tout en standardisant les mesures. Nous nous proposons dans ce chapitre de donner quelques informations sur le syndrome dysexécutif et sur les difficultés de la rééducation actuelle en présentant les questions se posant sur ce plan. Ensuite, nous ouvrirons sur les potentiels que les technologies numériques peuvent apporter pour de nouvelles formes de rééducation relevant de ce syndrome."
"La Science Ouverte est un mouvement mondial prônant la libre circulation des connaissances scientifiques, principe déjà ancien, mais qui a pris toute sa grandeur et son importance dans notre société grâce au numérique et au développement d'initiatives autour de ces questionnements. Les pratiques des enseignants-chercheurs sont analysées et deviennent des modèles à suivre ou, au contraire, à améliorer. Des études ont déjà été menées dans différentes universités françaises et ont montré des résultats plutôt encourageants. Dans cet article, nous proposons de revenir sur les résultats d’une enquête menée au sein de l'Université de Toulon."
"Les dynamiques de production, de circulation et de diffusion de la connaissance qui se développent dans l’écosystème numérique témoignent d’une mutation profonde du capitalisme. En marge du couple traditionnel marchés de la connaissance/ droits de propriété exclusifs, la notion de commun culturel ouvre la voie à de nouveaux modes de production articulant des agencements marchands hybrides et une conception inclusive de la propriété. Cet ouvrage étudie les fondements de l’économie politique des communs culturels dans l’écosystème numérique. Il présente les lieux et les espaces de réflexion dans lesquels cette notion a émergé et identifie les enjeux socio-économiques, techniques et politiques qui lui sont associés. Il analyse également les conditions concrètes de déploiement de l’économie des communs culturels dans un écosystème numérique spécifique, celui du livre, en traitant des bibliothèques numériques et des plateformes d’auto-édition."
"Diffuser et mettre débat la connaissance élaborée à propos des territoires de la Région Sud, en collaboration avec l’Insee, les agences d’urbanisme et les universités de Provence-Alpes-Côte d’Azur. La Région Sud s’associe ainsi à Aix-Marseille Université pour interroger la manière dont les territoires s’adaptent, face aux multiples défis posés par la Covid-19. Cette réflexion est portée et organisée par Connaissance du territoire. Une des missions que se donne la Région avec la Stratégie régionale de la connaissance du territoire adoptée fin 2017, c’est de mieux connaître son territoire, ses forces et ses faiblesses. Pour autant, il est nécessaire que ces connaissances se diffusent et soient mises en débat par des rencontres et des conférences. C’est bien le propos de ces échanges qui cherchent à faire un point sur la crise et ses impacts socio-économiques. La pandémie de la Covid-19 a conduit l’État à mettre à l’arrêt pendant plusieurs mois la vie économique et sociale de l’ensemble du pays, à compter de mars 2020. Cette période de crise sanitaire, l’une des plus graves qu’a connues le pays, est à l’origine d’une crise qui souligne nos faiblesses structurelles (fragilités du secteur industriel, forte dépendance à l’économie-monde, dette publique, inégalités sociales, chômage endémique, baisse d’attractivité des villes). Si tous les territoires ont été à l’arrêt lors du confinement, certains resteront marqués plus durablement par la fragilisation de leur tissu économique (en lien avec la forte dominance de secteurs d’activités industriels ou touristiques) ou de leurs populations plus vulnérables (chômage, mal-logement, jeunes adultes entrant sur le marché du travail, isolement des plus âgés, liens familiaux fragilisés). La crise sanitaire a par ailleurs de multiples conséquences sur les modes de vie et d’habiter, notamment ceux des citadins des plus grandes villes. Les espaces urbains sont davantage exposés à la pandémie, le sentiment d’inconfort y est prégnant si l’on songe au manque d’espaces verts. Par ailleurs, on a pu constater des mobilités de population, le temps du confinement, vers des territoires moins denses, un déploiement accru du télétravail, reports modaux sur la voiture et le deux-roues, un recours accru au e-commerce et à la « ville sans contact », des difficultés d’approvisionnement. Le modèle urbain en ressort affaibli, certains y voyant même une forte remise en cause de la place des métropoles et une revanche des villes moyennes et des campagnes. Dans ce contexte singulier, se posent des questions essentielles : Comment relancer ce « monde à l’arrêt » ? Quels sont les enjeux territoriaux adaptées à chaque contexte ? Quelles peuvent être les réponses économiques et sociales ? Comment repenser les pratiques de l’aménagement du territoire ?"
"La diffusion de la culture scientifique, technologique et industrielle (CSTI) par les chercheurs de toutes disciplines est depuis quelques années fortement encouragée et financée par les pouvoirs publics en France. Celle à destination des adolescents, visant à les inciter à poursuivre dans les cursus scientifiques et technologiques de l’enseignement supérieur, fait même l’objet de programmes spécifiques associant chercheurs, enseignants du secondaire et leurs élèves : elle occupe une place de premier rang dans les rapports officiels. A priori dès le dé-part nécessairement transdisciplinaire, notre programme décrit ici est à la fois théorique et expérimental puisqu’il consiste en la construction collaborative, entre chercheurs, enseignants et élèves, d’archives numériques à travers un web-documentaire multimodal sur le patrimoine historique des représentations de la science par l’art"
"Pour l’édition 2020 de la Nuit européenne des chercheur·e·s, dont l’inspiration thématique nationale est “Petits secrets nocturnes”, il a été demandé aux participants de répondre à un format de médiation autour de la correspondance entre un·e chercheur·e et… son objet d’étude ! À travers les correspondances de chercheur·e·s, il s’agit de donner au public un aperçu de la relation complexe et intime que les chercheur·e·s peuvent avoir avec leur objet d’étude. Cette plongée dans notre quotidien de chercheur·e, en faisant appel aux émotions, facilitera l’identification et la création de liens entre nous et le public qui lira ou écoutera notre correspondance. Ici, il s'agit d'une correspondance entre une doctorante et son objet de recherche : la Science Ouverte. A travers cette discussion ""sms"", vous pourrez découvrir des informations précieuses, pour ainsi susciter l'envie du lecteur pour nous rejoindre et nous écouter lors de notre passage pendant le bureau de désinformation prévu pendant l'évènement. Les correspondances pourront être découvertes en amont de l’événement, sur les réseaux de communication de la Nuit européenne des chercheur·e·s et/ou le 27 novembre sur Aix-en-Provence."
"Notre papier propose d’approfondir la notion d’innovation sociale numérique, domaine de recherche en gestation, au travers d’une étude circonscrite à l’un de ses champs d’application : la diffusion de l’open hardware dans les fablabs, lieux singuliers de fabrication collective. Il va mettre en évidence l’originalité de ces dynamiques d’innovation contributives originales, qui réside dans la promesse d’un univers de production post-industriel créateur de communs de connaissance."
"Le développement des mémoires numériques dans le domaine des sciences, couplé avec la crise des humanités et la difficulté de faire apparaître les relations internes des savoirs, imposent de nouvelles exigences aux infrastructures de l’édition. Sont en jeu les écritures du web, les technologies intellectives qui rendent productifs ces nouveaux « milieux » d’intelligence. L’article met en évidence les principaux points d’application des forces pour surmonter l’encyclopédisme en éclats."
"La collaboration dans les organisations sous l’angle de l’agencement relayé par les dispositifs numériques est étudiée à travers deux niveaux imbriqués. En premier lieu, l’impératif à collaborer se tient, d’une part, entre une exaltation de l’individuation et, d’autre part une attente d’homogénéité, d’intérêt collectif, de contrôle. À travers l’exemple des organisations de santé, nous verrons ce qui, paradoxalement, contrarie, contredit la qualité des démarches collaboratives. En second lieu, une référence issue d’un contrat de recherche (avionneur AH) illustre la notion « d’organisation apprenante » numérisée envisagée comme une forme d’idéal. A contrario, la suprématie de l’agencement technicisé, se substituant aux ajustements, exerce des tensions vives entre organisation du travail et forme sociale."
"L’autre est imprévisible, tantôt « risque » pour le sujet, tantôt source de sensations de déplaisir. De fait, les réseaux socionumériques proposent une simplification de la relation en intégrant des fonctionnalités allant de la discussion instantanée à la ponctuation affective des contenus échangés. Constitutives de la relation en face-à-face, les émotions sont cependant des états somatiques individuels qui ne peuvent faire l’objet d’une atomisation informationnelle par les plates-formes. Notre propos portera en ce sens sur les enjeux de la communication humaine au moment où l’on assiste à une dilution de la relation au profit des dispositifs. Que reste-t-il de notre humanité face à la réduction informationnelle ? Nous centrerons notre approche sur certains éléments insolubles, comme l’embarras, en nous intéressant à des sujets adolescents que nous suivons dans différentes enquêtes de terrain."
"À de rares exceptions, il n’existe plus de villes (hors Paris) ou de départements français servis par deux ou plusieurs quotidiens ; le processus de concentration est désormais abouti. La recherche a montré qu’un journal en position de monopole perd des lecteurs, pertes compensées par les gains de tarifs publicitaires monopolistiques. La disruption numérique est en passe de faire voler en éclats le statu quo dans une majorité de pays. En France, la création de sites d’information locale et régionale reste embryonnaire. Il est probable que la rentabilité de sites nationaux comme Médiapart va accélérer le processus d’implantations locales en concurrence avec les sites des entreprises « print » historiques. Quels sont les effets de la compétition ? Faute de pouvoir encore l’observer in situ sur une lutte entre entreprises numériques, nous avons choisi une étude de cas, un journal breton, Le Télégramme, performant face à Ouest-France, leader français de diffusion payée, tous deux étant en phase de transition « web » (paywall récent)."
"La formation initiale des enseignants fera l’objet de nouveaux changements annoncés dès la rentrée universitaire 2020. À cette occasion, il s’agit de relancer la place du renforcement de la culture du numérique au sein de l’école. Nous décrivons notre module de formation en TICE qui vise à apprendre à développer des usages créatifs du numérique en classe. Cette expérimentation se place dans une perspective d’émancipation des élèves afin d’augmenter leur savoir et savoir-faire. Nous abordons également les perspectives et les limites de cette pédagogie à partir de la notion d’expérience vécue de l’élève en classe."
"Nous avons choisi de travailler sur une étude de cas centrée sur l’entreprise Nestlé, car son public a été dévoyé, au sens de retourné contre elle, sur sa propre page Facebook, suite à l’attaque de l’ONG Greenpeace, lui reprochant de recourir à un fournisseur d’huile de palme malhonnête. Nous verrons alors comment le succès de cet appel au militantisme peut être expliqué par l’instrumentation de la participation des internautes sur les réseaux socionumériques, en recourant à une forme de « prescription engageante », dans un effet de symétrie comparable à celle déployée par Nestlé lors de sa campagne de marketing viral, « Have a break, Have a Kit-Kat ». Cette crise opposant Greenpeace contre Nestlé sera à l’origine de la prise de conscience des problématiques environnementales et de santé publique soulevées par la production de palmiers à huile et la consommation de produits contenant de l’huile de palme."
"Les réseaux socionumériques tentent de proposer une alternative à la relation face à face par des fonctionnalités promettant le partage d’affects en ligne. De fait, les instants contraignants comme les situations d’embarras peuvent s’évacuer vers des échanges moins « risqués » via les dispositifs numériques. En ligne, pas de regards qui s’entrechoquent, ni cet inattendu de la rencontre avec l’autre. On voudrait éviter la communication humaine en usant d’échanges informationnels, mais on ne peut réduire ce qui fait le sensible de chaque relation, comme l’embarras. En ce sens, notre approche par les récits de vie auprès d’un corpus d’adolescents de la région toulonnaise nous permet de mettre en perspective les éléments inhérents à l’irréductibilité de l’embarras dans la communication."
"Cet article propose d’éclairer un champ créatif émergeant : les « audio-games » ou « vidéo-ludiques sonores », se situant au carrefour du jeu vidéo et de l’informatique musicale. Aujourd’hui, une multitude de petites applications, qui proposent des expériences audiovisuelles ludiques où la dimension sonore est prépondérante, sont disponibles sur des consoles de jeux, des ordinateurs, des téléphones portables. Ces expériences représentent un nouvel espace où la notion de « jouabilité » venue du jeu vidéo et appliquée à la création musicale favorise de nouveaux ponts entre les deux disciplines. Ce champ créatif, en proposant la manipulation de représentations que nous appelons « a-musicologiques » (c’est-à-dire utilisant des symboles ne faisant pas référence à la musicologie classique pour manipuler et produire du sonore) interroge de manière nouvelle la représentation du sonore et des structures musicales et produit ainsi de nouveaux gestes instrumentaux et compositionnels qu’il s’agit d’étudier. Après avoir défini les caractéristiques et les limites de ce champ et repéré quelques-unes de ses filiations historiques (cinéma abstrait, théorie du jeu en musique, partition graphique et d’actions…), nous étudions quelques exemples de jeux sonores et proposons des pistes de recherche pour l’élaboration d’outils d’analyse de ces nouveaux objets."
"Cet essai prolonge les éditions précédentes du ‘Petit Guide’ et des expériences professionnelles antérieures(4e édition). Pour le titre de ce ‘Traité d’initiation’, nous avons préféré le pluriel au singulier parce que l’idée d’une méthode est illusoire car comme l’a écrit Roland Barthes (1964), « les mêmes qui insistent le plus sur la méthodologie sont souvent ceux qui apportent le moins à la Recherche». Nous nous sommes placés dans la situation d’un Apprenti-Chercheur confronté à la formulation, la résolution d’une problématique de recherche en Sciences Humaines et Sociales en général, et, plus spécifiquement en Sciences de l’Information & de la Communication, à un besoin d’ancrage territorial épistémologique et de repérage méthodologique, parce que les S.I.C ‘ne parlent d’une seule voix’ (Bougnoux, 2004) et que bon nombre d’étudiants emportent bien souvent leur bagage d’origine en rejoignant les Sciences de l’Information & de la Communication. Et qu’ils sont perplexes ces étudiants devant le foisonnement épistémologique exposé."
"La relation avec l’autre, souvent compliquée, parfois désagréable, peut aujourd’hui être évitée par l’usage d’interfaces d’échanges comme les réseaux socionumériques. De nouvelles sociabilités moins contraignantes évacuent alors la question des corps qui se scrutent, s’impactent, et se synchronisent. Cet article propose d’articuler la communication et les émotions de déplaisir en montrant plus particulièrement en quoi l’embarras est constitutif de la communication humaine. À la fois émotion sociale et expérience sensorielle, l’embarras se conceptualise au prisme d’un nécessaire examen pluridisciplinaire et d’une ethnographie auprès de sujets adolescents particulièrement concernés par ces bouleversements communicationnels."
"Les enjeux de l’apprentissage par le numérique sont essentiels au système éducatif pour développer les compétences disciplinaires et transversales et susciter le développement de nouvelles pédagogies. Depuis 2010, Moodle, système de gestion de parcours pédagogiques en ligne, est présent dans chaque établissement du secondaire de l’académie de Nice. Ce dispositif apporte un gain en efficacité pédagogique, mais, étant perçut comme trop complexe, sa prise en main constitue des freins. Comment aider l’enseignant à s’approprier ce dispositif dans un contexte d’apprentissage ? Comment l’aider à transformer ses pratiques pour la réussite de ses élèves ?"
"Dans cet article, nous proposons une piste de réflexion épistémologique sur le nécessaire renouvellement des grilles de lecture des services de renseignement face à des phénomènes qui ne répondent plus aux modèles classiques issus de la Guerre froide et, plus récemment, de la lutte contre les réseaux terroristes. Nous prendrons le cas de la lutte contre la radicalisation violente comme exemple des difficultés et des paradoxes auxquels sont aujourd’hui confrontés des services de renseignement toujours mieux équipés et pourtant souvent démunis face à ces phénomènes. En combinant trois concepts théoriques empruntés librement à diverses disciplines (émergence, mémétique et intégration conceptuelle), nous proposons un modèle théorique d’analyse des phénomènes de radicalisation."
"Si de plus en plus d’organisations (musées, entreprises commerciales, organisations publiques…) cherchent à améliorer la médiation avec leurs publics en utilisant des technologies de communication interactive, peu évaluent leur efficacité. C’est le cas des organisations marchandes faisant de la publicité avec un écran tactile sur des lieux de vente. Deux expérimentations menées en milieu « naturel » ont étudié ses effets : elle incite davantage à acheter des produits de la marque par rapport à des conditions où, premièrement, la publicité audiovisuelle est diffusée sur un écran non interactif et, deuxièmement, sans publicité. Si la publicité interactive améliore certains jugements cognitifs sur la marque, elle serait moins efficiente pour modifier l’attitude à l’égard des produits de la marque. Pour mieux comprendre les processus psychosociaux impliqués dans la communication interactive tactile, nous interprétons les résultats à la lumière de la théorie de la probabilité d’élaboration et du concept d’actes préparatoires. Les limites et nouvelles perspectives de recherche sont également indiquées."
"Dans cet article, nous nous intéressons aux activités d’organisation des connaissances des spécialistes de l’Enseignement Supérieur et de la Recherche dans la mesure où elles contribuent à la circulation des connaissances et à l’évolution des systèmes d’organisation des connaissances (SOC). Les domaines scientifiques auxquels ces spécialistes sont institutionnellement rattachés se recomposent dans des communautés épistémiques qui conduisent des activités similaires tant en matière de recherche d’information, de production de connaissances, que d’accès à l’information. Pour effectuer ces activités, ces communautés, notamment en sciences humaines et sociales, disposent d’outils, de services et de dispositifs qui participent du mouvement des « Humanités numériques ». Les deux grandes agences bibliographiques françaises (ABES et BnF) prennent part à ce mouvement en appliquant aux données le modèle entité-relation qui s’étend progressivement à l’indexation matière (répertoires d’autorités matières, thésaurus, etc.). Les connaissances produites par les communautés épistémiques sont donc étroitement liées aux infrastructures numériques de recherche basées sur la normalisation et la mutualisation qui composent un socle technologique distribué et commun. Tandis que la communauté scientifique exprime le caractère pluridisciplinaire, multidisciplinaire, voire transdisciplinaire des objets qu’elle étudie, nous nous intéressons aux conditions dans lesquelles le web de données peut répondre aux besoins de structuration et d’organisation des connaissances de ces communautés épistémiques. Dans quelle mesure les SOC conservent-ils une place prépondérante au sein des infrastructures des « Humanités numériques » et de leurs dispositifs sociotechniques ? Comment valoriser les propriétés heuristiques des SOC lors du processus d’indexation effectué par les spécialistes de ces communautés épistémiques ? Afin de répondre à ces questions nous étudions les orientations techniques, les composants et le fonctionnement de trois Infrastructures de Recherche (HAL, ISIDORE, Persée). Nous constatons que la production et la circulation des connaissances sont fondées sur la puissance technologique des dispositifs et soulignons la présence de nombreux SOC et une valorisation fragmentée de l’indexation matière. Aussi, nous nous intéressons aux conditions de possibilités d’une fertilisation croisée entre les SOC et proposons de nous appuyer sur le Linked Open Data (LOD) pour construire un modèle d’interopérabilité entre les SOC des communautés épistémiques afin de composer un continuum documentaire."
"Cet article a pour objectif de discuter d’un point de vue communicationnel la notion d’équilibre dans les environnements immersifs. Il s’agit de tenter de cerner un des aspects complexes des expériences vécues par les utilisateurs de ce type d’environnements dans des situations plutôt ludiques typiques des jeux vidéo. Les auteurs questionnent cette notion en articulant leur réflexion autour des couples théoriques engagement-distanciation et présence-attention (voire hyper-attention). Ils s’appuient pour cela sur des travaux antérieurs qui ont tenté de saisir et de conceptualiser la complexité de ces expériences communicationnelles particulières (Bonfils, 2014 ; 2015), et dans le même temps de relire ces questionnements à l’épreuve d’un terrain (World of Warcraft) investigué actuellement par l’un des deux auteurs. Le constat de départ porte sur le fait que les expériences vécues par les utilisateurs en immersion nécessitent de leur part de produire des efforts cognitifs importants pour vivre pleinement ces situations. En effet, les environnements immersifs conçus aujourd’hui par les designers et les développeurs sont de plus en plus riches visuellement et proposent de plus en plus d’interactions sophistiquées qui permettent aux utilisateurs d’utiliser de manière croissante leur corps ou sa projection (en l’occurrence son avatar) entre espaces physique et numérique. L’idée avancée alors par les auteurs consiste à faire l’hypothèse que ces utilisateurs en quête de flow (Csikszentmihalyi, 2008) et de plaisirs sensibles sont en proie à des conflits récurrents entre les activités de perception et les modes d’actions possibles liés aux activités de production. Il en résulte de fait une quête constante et souvent inconsciente pour les utilisateurs d’équilibre, d’engagement, de distanciation et d’auto-régulation au sein de plusieurs sphères, à l’articulation de différents espaces."
"L’ouvrage de Marie-Pierre Fourquet Courbet et Didier Courbet s’inscrit dans un contexte que plusieurs auteurs nomment la « révolution anthropologique », car les technologies de l’information et de la communication investissent presque toutes les sphères de la quotidienneté. Ces outils techniques sont désormais au cœur de notre sociabilité et de nos interactions collectives et individuelles. En s’appuyant sur de nombreux travaux scientifiques, les auteurs montrent comment les écrans et les claviers envahissent notre environnement, s’intègrent dans notre subconscient jusqu’à avoir une incidence sur notre santé physique et mentale."
Distances et médiations des savoirs Distance and Mediation of Knowledge 31 | 2020 Des ressources aux pratiques éducatives libres : quelle réappropriation dans la formation ouverte et à distance ?
"Quel est l’impact, d’une part, de l’agentivité – la capacité d’un sujet à s’attribuer ses propres actions et leurs effets – et, d’autre part, de la présence personnelle – la sensation d’être dans un lieu alors qu’il n’existe pas – et environnementale – le fait d’attribuer le caractère réel à un lieu qui n’existe pas – sur la représentation que l’usager construit de la réalité à l’issue de l’expérience immersive dans un environnement virtuel ? C’est sur la base d’une étude empirique qu’il s’agit d’examiner la pertinence d’un modèle causal mettant, notamment, en exergue le rôle central de l’agentivité à plusieurs niveaux d’analyse dans l’expérience immersive."
"Dans le cadre des problématiques exposées récemment par Matteo Treleani dans son ouvrage Qu’est-ce que le patrimoine numérique ? Une sémiologie de la circulation des archives, se pose la question de la mise en perspective numérique sur le web de nombre de patrimoines (archives papier, manuscrits, livres et revues, photographies, films, etc.), anciens comme récents. Nous montrerons ici le travail en cours sur trois thèmes très différents et pourtant liés. Tout d’abord le patrimoine des revues et ouvrages scientifiques du 19e siècle et les différentes numérisations, mises en perspective (et obsolescence de certaines d’entre elles) réalisées ou en cours, y compris via deux programmes ANR successifs. Ensuite le patrimoine industriel et plus précisément celui des anciens chantiers navals de La Seyne-sur-Mer (Var) où il est là question d’inventorier et de médiatiser – sous quelle forme, pourquoi, pour qui ? – à la fois d’innombrables archives papiers, familiales ou institutionnelles, mais aussi photographiques et filmées : on est là en même temps dans la démarche de la nouvelle archivistique et dans le registre de la communication en CSTI (culture scientifique, technologique et industrielle) encouragée par l’état. Étonnamment, le sujet précédent a croisé deux autres formes de patrimonialisation, elles-mêmes liées depuis une quarantaine d’années, dont nous montrerons quelques exemples : la représentation de la science par l’art à travers le temps (on est là à la fois dans la CSTI et l’histoire de l’art), et l’histoire du street art qui est en train de s’écrire du fait de son « artification » au sens défini par Nathalie Heinich et Roberta Saphiro, c’est-à-dire au croisement de la sociologie, de la philosophie et de l’histoire de l’art."
"Cette recherche est partie du constat que le marché de l’UX design, auquel n’échappe pas le secteur de la communication, est en pleine expansion. Elle s’est donc intéressée à l’appropriation des méthodes et outils de l’UX design par les agences de communication situées en région Provence-Alpes-Côte-d’Azur (Paca) et aux enjeux de cette démarche. Celle-ci est lente et partielle car les clients de ces agences ne le réclament pas et n’accordent pas de moyens suffisants : par exemple pour les tests avec les utilisateurs. Seules certaines agences, localisées à Paris, arrivent à surmonter et à dépasser la logique contraignante de la commandite d’un support de communication qui constitue le quotidien des agences en région Paca. Ces agences parisiennes sont, en effet, situées à proximité des sièges sociaux de grands groupes industriels ou de services qui utilisent de plus en plus l’UX pour faire évoluer leurs offres."
"Les connaissances, au cœur de la création de valeur pour l’entreprise, font l’objet de nombreux conflits d’appropriation qui ont donné naissance à un mouvement critique pour préserver des communs de connaissance (E. Ostrom). Des structures comme les fablabs, basées sur le faire ensemble, cherchent également à lutter contre cette privatisation de la connaissance. À partir de deux variables déterminant la nature d’un commun de connaissance (mode de gouvernance et systèmes de droits de propriété), les auteurs observent dans six « fablabs » quels facteurs favorisent ou freinent les projets en commun et pour le commun."
"Les campagnes de communication d'appel aux dons, indispensables aux organisations humanitaires et caritatives pour collecter des fonds, utilisent des messages persuasifs pour amener le grand public à les soutenir financièrement. Cependant, malgré les enjeux fondamentaux de ces campagnes, dans la littérature aucune recherche n'a été menée sur les représentations sociales que les producteurs mobilisent lorsqu’ils conçoivent leurs messages. Réalisée sur dix-huit concepteurs de campagne d’appel aux dons pour des organisations caritatives et humanitaires, cette enquête qualitative vise à mieux comprendre comment sont conçus les messages persuasifs. Les résultats montrent que les producteurs créent à partir de théories causales naïves sur les effets psychologiques provoqués par différents procédés persuasifs. Les représentations sociales fondamentalement différentes entre les producteurs expérimentés et moins expérimentés les conduisent à concevoir des messages ouvertement différents. Elles concernent le mode de réception et d’influence, les récepteurs perçus, leur motivation, le degré de complexité du message et le types d‘affects que ce dernier doit générer. Après avoir discuté la validité scientifique des représentations sociales à la lumière de recherches expérimentales sur la communication persuasive, nous donnons quelques recommandations aux producteurs, indiquons les limites et nouvelles perspectives de recherche."
"L’internet muséal a été identifié comme une des problématiques émergentes dans les sciences de l’information (Papy, 2008 ; Vidal, 2008). La Conférence permanente des Directeurs·trices des unités de recherche en Sciences de l’Information et de la Communication (CPDirSIC, 2018) a circonscrit le domaine « médiations mémorielles, culturelles et patrimoniales » comme l’un des dix (10) domaines dont les sciences de l’information et de la communication (SIC) ont accordé une grande attention en plaçant le concept de médiation au cœur de leur approche à travers des dynamiques de recherches menées en collaboration avec des spécialistes d’autres disciplines. Cette double reconnaissance positionne les projets muséaux dans des approches transversales et pluridisciplinaires où les SIC constituent un outil privilégié de médiation entre le sujet de l’étude et les différents acteurs qui y interviennent sur la même étude. Selon Vidal (2008), les professionnels des musées en s’appropriant les technologies interactives, inventent de nouvelles médiations et de nouveaux accès aux arts et sciences, qui traduisent la façon dont les musées pensent la circulation de leurs ressources. C’est ce que nous tenterons de mettre en exergue à travers un exposé des principales technologies de réalité virtuelle, de réalité augmentée et de réalité mixée qui constituent de nouvelles médiations et de nouveaux accès aux arts et sciences et qui traduisent la façon dont les musées pensent la conservation, la circulation et la diffusion de leurs ressources. Nous tenterons ensuite, de montrer comment les musées explorent les opportunités qu’offre cette (r)évolution afin d’opérer des transformations profondes vers de nouvelles formes d’organisation que nous appellerons génériquement « smart muséums » , aptes à exploiter diverses technologies alliant réalité virtuelle et réalité augmentée. De ce point de vue, ces smart muséums seront de plus en plus, ambidextres au sens de March (1991). En effet, l’ambidextrie de ce type de musées, apparait d’une part, dans leur capacité à atteindre l’excellence opérationnelle dans l’exploitation de leurs activités traditionnelles de conservation et d’exposition d’artéfacts et d’autre-part, à explorer tout moyen permettant de s’adapter aux transformations qu’exigent les visiteurs contemporains et dans le futur. Ces visiteurs ne veulent plus être des consommateurs passifs. Ils exigent désormais des informations, des divertissements et une participation active aux stimulations multisensorielles combinés à un design innovant (Pine et Gilmore, 1999) en moyens de diffusion et de médiation. Pour cela, les smart muséums auront pour mission de répondre aux attentes qui traduisent les trois facettes du visiteur (en tant qu’humain) : (1) homo sapiens , humain qui sait et qui est en quête permanente d’informations et de savoir ; (2) homo ludens qui souligne l’importance de l’acte de jouer pour l’être humain, notamment pour agrémenter sa quête du savoir et enfin, (3) homo faber , humain qui fabrique des outils qui lui facilite la vie notamment, ceux nécessaires à sa quête du savoir. Nous présenterons enfin, quelques projets concrets qui illustrent les nouvelles missions de ces « smart muséums », les process et les techniques utilisés pour la conception d’expériences muséales immersives et augmentées qui contribuent à la restauration numérique des artéfacts ainsi qu’à la sauvegarde numérique durable du patrimoine mondial."
"Une des particularités de l’information‐documentation est qu’elle s’inscrit dans une longue tradition de pratiques et de recherches qui ont conduit à la production d’ouvrages de portée réflexive, de recommandations, de traités qui sont ou s’apparentent à une élaboration théorique. Les thématiques abordées par leurs auteurs constituent le sous‐bassement de la discipline. C’est ainsi que nous avions introduit notre appel à article afin de mettre en lumière des recherches en cours. Que travaillons-nous ? Quels sont nos fondements théoriques ? Comment la science de l’information (SI)1 a-t-elle évolué ? Nous avons proposé de répondre à ces questions par l’examen des apports de personnalités qui nous ont précédées. L’entrée privilégiée est alors non pas de dresser la biographie de ces auteurs, même si elle doit être posée pour contextualiser le propos, mais de se centrer sur un apport méconnu, d’approfondir une voie de recherche ouverte ou encore de mettre en évidence un passé qui a des répercussions dans le présent. Il s’agissait aussi d’explorer à partir de sources nouvelles, ou encore peu travaillées, des parcours intellectuels ou politiques méconnus."
"Autrefois, les érudits étaient reconnus pour leurs connaissances en histoire, en langues anciennes et dans la recherche documentaire. Aujourd’hui, comment peut-on prétendre être érudit en ayant accès aux informations en quelques clics ? Les bibliothèques surchargées de manuscrits ont laissé place aux plateformes numériques et aux outils de recommandations bibliographiques. La recherche minutieuse des multiples sources est devenue un raccourci clavier (le CTRL+F). A l’heure où l’information est abondante, gratuite, commune et où le libre accès est devenu un enjeu dans notre société du numérique, que devient la figure de l’érudit ? A travers cet article, nous tentons de répondre aux questions en proposant que l’accumulation des informations n’est pas une accumulation de connaissances comme c’était le cas pour l’érudition."
"L’objectif du programme « Transition bibliographique », initié par la BnF et l’ABES et qui implique l’ensemble des bibliothèques municipales, universitaires et de recherches, est d’exposer les catalogues des bibliothèques dans le web de données pour répondre à l’évolution des usages des internautes, en structurant les données selon le modèle FRBR pour se rapprocher du nouveau code de catalogage Resource, Description and Access (RDA). Cette transition vise délibérément à abandonner la logique des documents et à la substituer à celle d’œuvres autour desquelles s’agrégeront données catalographiques et données d’autorité sur des personnes, des organisations, des concepts, des évènements que pourvoiront, à partir de leurs entrepôts de données les deux institutions de référence. Cette transformation majeure n’est pas sans conséquence pour les bibliothèques, « lieux du savoir » construits comme des « espaces de cohérence » autour de collections principalement physiques que le catalogue informatisé transpose pour en améliorer l’accès. Avec l’approche « data » du modèle FRBR et l’enrichissement sémantique du catalogue, par le truchement de données externes, c’est un modèle acquis au mythe technologique du « big is beautiful » et désenchâssé de la réalité des services à rendre aux publics qui est imposé aux bibliothèques au nom de l’innovation et des usages, et ce malgré l’absence d’étude d’usage sérieuse. En imposant cette FRBRisation, c’est d’abord la nature fonctionnelle de l’OPAC qui se transforme, passant alors, sans préavis, du statut de dispositif de recherches de ressources documentaires à celui de dispositif de recherches d’œuvres virtuelles et de ses instances réelles. Enfin, en terme d’usage, cet épaississement sémantique du catalogue introduit les nouveaux objets conceptuels « œuvre », « expression », « manifestation » qui viennent complexifier la relation directe « notice <-> exemplaire » en une relation « œuvre<->expression<->manifestation<->exemplaire ». Déjà bousculé cognitivement par l’hybridité de la bibliothèque depuis l’avènement du numérique, l’usager risque d’être plus encore malmené par ce nouvel artefact technodocumentaire aux propriétés fonctionnelles inhabituelles sur des objets documentaires pourtant bien habituels avec lesquels il interagit dans l’espace connu, organisé et inchangé de la bibliothèque."
"Internet est devenu un outil incontournable pour développer une stratégie de communication dans l’organisation; dans ce flux des supports imagés, il s’agit désormais de se faire connaître, de séduire et d’affirmer son identité visuelle.Le Buzz marketing est précisément usité comme une des nouvelles stratégies de communication, rendant le consommateur actif. Pourtant, faire du client potentiel l’acteur privilégié de la communication marketing ne s’improvise pas : à nouveau mode de communication, nouveaux outils, et nouvelles professions. Nous pouvons alors nous demander quels sont les enjeux du buzz marketing dans ses différents terrains d’application? Est-ce un phénomène spontané ou stratégiquement organisé?Pour répondre à ces questions, l’article s’appuie sur les médias numérique savant de se centrer sur un terrain d’application pédagogique pour éviter la marchandisation des idées"
"Ancrées dans les SHS, les questions relatives aux usages se posent légitimement aux bibliothèques numériques (BM) considérées avant tout comme des objets sociotechniques et socioculturels. Ces questionnements s’imposent d’autant plus que les usages des BM demeurent incertains. Élaborés sur une transposition illusoire de la bibliothèque physique, ces artefacts techno-documentaires imposent une désintermédiation aux usagers au profit d'une interaction instrumentée de la RI par les TIC. L’interopérabilité technocentrée de ces BM paroxyse la désintermédiation et malmène acceptabilité et utilisabilité. De nombreuses recherches soulignent que les usagers ne parviennent pas à s’approprier les dispositifs qui devraient répondre à leurs attentes d’association et de contextualisation de l’information que les actions en faveur de l’Open Data et des Linked Open Data encouragent. Parmi les technologies du Web présentes dans les BM patrimoniales, l’architecture REST est négligée alors que les GAFA l’utilisent – et la mettent à disposition - dans leurs BM pour améliorer l’appropriation de leurs services. A la lumière des travaux de P. Rabardel sur les genèses instrumentales et le nécessaire assujettissement de l’instrument aux formes de l’organisation du travail, les API REST, en réajustant la vocation de l’interopérabilité technologique, sont de nature à réhabiliter l’usage réel des BM et à offrir aux usagers des instruments adaptés à leurs activités constructives."
"La société de l’information et de la connaissance est promue comme l’archétype de l’innovation. Son fonctionnement requiert l’usage des données de l’Open Access, de l’Open data et de l’Open science. Cet usage et la contribution des acteurs de la société à ce modèle impliquent la prise en compte des problématiques d’organisation des connaissances (OC). Alors que les Infrastructures de Recherche se tournent vers l’interopérabilité technique, l’OC s’opère selon des logiques de communautés épistémiques qui rencontrent les problématiques d’interopérabilité sémantique et culturelle. La formation des cultures de l’information citoyennes place les activités d’OC des communautés épistémiques au centre des processus info-documentaires. Nous présentons un cas d’usage impliquant plusieurs communautés épistémiques."
"Les réseaux sociaux en se diffusant sur l’intégralité de la société sont également entrés dans le monde de la recherche. Ces outils accélèrent la circulation de l’information, et pourraient atteindre une audience différente du circuit universitaire. Parallèlement les plateformes de savoir ouvert se développent et rendent accessible à tout le monde le savoir scientifique. Notre étude se focalise sur l’étude des tweets émis entre 2013 et 2017 pointant vers un contenu d’OpenEdition. Nous avons analysé les réseaux de retweets ainsi que les contenus textuels des tweets par étude lexicométrique. Les résultats tendent à montrer une conservation des pratiques institutionnelles avec un cloisonnement linguistique et disciplinaire tant dans les pratiques de retweets que dans les contenus textuels."
"Du fait du développement des technologies numériques, peut-être plus qu'avant, « la sphère communicationnelle pose avec acuité la question éthique ». Dans cet article, nous développerons une réflexion philosophique autour d'une pratique numérique du web 2.0 : le tagging collaboratif. Y at -il une place pour l'éthique dans ce mode d'usage d'Internet ? Quels sont les enjeux ?"
"La déferlante des données massives (big data) interroge le journalisme contemporain, ses pratiques, mais aussi les modèles économiques des médias et l’organisation des rédactions. Les enjeux sont protéiformes. Nous mettons tout d’abord en évidence la résistance épistémologique du métier face aux promesses d’un journalisme intégralement robotisé. En effet, le data journalisme se réfère toujours à un terrain identifié auquel les données doivent renvoyer à des fins de validation. Nous montrons ensuite que l’usage des données massives permet aux médias non seulement de personnaliser plus finement l’offre d’information, mais également de favoriser un processus d’innovation qui tend à hybrider de plus en plus les savoir-faire journalistiques avec des compétences techniques extérieures au métier. A défaut de disparaître, le métier mute en se recentrant sur certains de ses fondamentaux."
"Face au développement d'un capitalisme informationnel dont les acteurs les plus visibles sont connus comme les « géants du web », un autre modèle de partage de l’information et des connaissances tente de se structurer autour des sources ouvertes numériques – aussi désignées par les termes open educational resources, open courses, open licences, open edition, open data, open science… Dans le domaine éducatif, ce modèle concerne les contenus pédagogiques, le savoir scientifique et les dispositifs et environnements d’apprentissage. L’ouverture des sources numériques questionne aussi bien les stratégies des établissements d’enseignement que les dispositifs pédagogiques et les modalités d’apprentissage. Dans le secteur public en général, les données s’ouvrent progressivement aux citoyens et touchent des sphères sensibles telles l’information sur la santé ou l’environnement. Pour autant, le développement d’une politique de l’offre n’implique pas une appropriation spontanée par les publics visés, qu’ils soient enseignants, apprenants ou simplement citoyens. Ainsi ces ouvertures forment-elles le socle et le réceptacle d’un croisement de logiques d’acteurs, de l’expression de connaissances et compétences multiples et de discours experts ou profanes ayant de nombreuses conséquences en termes d’information et de communication."
"Cette interview réalisée par le Centre Algérien de Diplomatie Économique revient sur la thèse ""L'intelligence économique en Algérie : analyse des brevets comme indicateurs de la puissance innovatrice"". Au-delà de la problématique de recherche et de l'intérêt de l'information brevet, l'interview aborde une série de recommandations pour le pilotage de l'innovation à travers un système plus efficace de gestion de l’information technique et scientifique (IST) en Algérie."
"Du réel de l'émotion à l’émotion réalisée dans la création musicale contemporaine Florence Lethurgez, Aix-Marseille Université, EA 4262 IMSIC Pascal Terrien, Aix-Marseille Université, EA 4671 ADEF-GCAF L’émotion en sciences humaines reste un objet discret et discrédité par l’étalement de l’intime au détriment de la raison. Pourtant, le champ émotionnel se manifeste dans la relation anthropologique de l’homme (compositeur) à la création musicale. Considérer l’image de « tournant émotif » de l’art contemporain, identifié par la critique philosophique, comme rupture socio-historique laisse supposer que les émotions n’interviennent que de manière contingente, voire parasite, dans le processus créatif. Pourtant, tout un champ de la recherche scientifique en psychologie a inspiré des travaux plus récents en sciences de l’information et de la communication et en musicologie didactique ; raisons pour lesquelles nous proposons une approche de l’anthropologie des processus de création musicale fondée sur les émotions en partant des outils méthodologiques de ces sciences sociales. Notre recherche étudie les contenus des notices rédigées par les compositeurs à l’occasion de la parution. Ces propos sont confrontés aux verbatims sur leur tâche d’écriture (notice et composition) recueillis par un chercheur. Elle souhaite dégager un régime d’apparition des émotions tant dans les notices que les entretiens, et leur éventuelle reprise dans la composition. Or, les deux ordres, verbal et musical, montrent que l’émotion est impactée par la dimension mésogénétique liée à la contingence (moyens compositionnels). L’émotion, qui est à l’origine du projet compositionnel, passe par le filtre de choix successifs que l’analyse musicale et discursive permet de saisir. Notre corpus réunit une production de quatre compositeurs (C. Groult, G. Finzi, S. Yoshida, S. Reich). Nous détaillons deux processus dynamiques de création musicale. L’un en entrée, où les émotions inspirent le projet, l’autre en sortie, où elles sont « composées ». Notre étude permet de saisir la réalité de l’image sonore originelle propre au compositeur en prise aux contraintes rencontrées dans l’exercice compositionnel."
"Dans cet essai théorique, nous discuterons de l'évolution des approches paradigmatiques et théoriques en Communication internationale et en Communication pour le développement/changement social, dans une perspective comparative entre les écoles anglo-saxonne et francophone. Nous nous arrêterons d'abord sur les rapports étroits entre ces deux domaines de recherche (Aubin et Agbobli, 2014) au sein de la communication institutionnelle et organisationnelle ; sur les définitions et les redéfinitions de leurs frontières respectives pendant les dernières décennies. Nous présenterons ensuite un bref aperçu des évolutions des paradigmes et des approches de la communication internationale et pour le développement, respectivement dans le cadre des écoles anglo-saxonne et francophone. Il s'agira plus particulièrement de décrire l'évolution de la place de la communication pour le développement vis-à-vis la communication internationale. Dans un troisième temps, nous allons discuter des points de convergence et de divergence de ces approches respectives. Sur la base de ces analyses, nous allons apporter notre appui théorique à la convergence des approches (Wilkins, 2008) dans un cadre multi-méthodologique et interdisciplinaire (Loum, 2014). Nous proposerons notamment une grille de lecture, intégrant les deux modes de communication, à savoir << persuasive >> (communication push, la logique de l'offre publique) et << participative >> (communication pull, la logique de la demande sociale), ainsi que les deux prismes d'analyse, étant << macro >> (structures) et << micro >> (acteurs)."
"Les éditeurs de la Presse Quotidienne Régionale (PQR) doivent s’adapter à la présence des plateformes qui ont créé une intermédiation entre eux et leurs publics et ont su se rendre indispensables. Depuis 2018, le rapport aux plateformes s’est encore complexifié puisque ces dernières ont institué la proximité et l’information locale comme axe de développement. Pour mieux comprendre la façon dont la presse locale s’adapte à cette nouvelle donne, des responsables de trois groupes de PQR ont été interrogés. Ces directeurs généraux adjoints, directeur de régie, président directeur général, directeurs de la rédaction, rédacteur en chef web et directeur du marketing ont été questionnés pour identifier les motifs qui président à l’élaboration des stratégies numériques des groupes de presse locale et leurs conséquences dans l’organisation des groupes. Une analyse des parcours et des positions des interrogés permet de mettre en perspective le regard des managers sur les plateformes et leur acculturation au numérique. Leurs représentations et leurs décisions ont des conséquences sur le rapport à l’éditorial et le rapport aux outils des plateformes. Les trois groupes étudiés ont des conceptions des plateformes très différentes et mettent en place des stratégies numériques diverses. A Nice-Matin, le choix a été fait de tenir à distance les plateformes, mais le développement d’une offre éditoriale en ligne qui en reprend en partie les codes est au cœur de la stratégie d’entreprise. Au Groupe La Provence, la stratégie numérique est revendiquée et implique davantage les services annexes à la rédaction. Les plateformes y sont plutôt perçues comme des partenaires de développement. Le Groupe La Dépêche est le plus réticent dans sa façon de construire ses rapports aux plateformes et semble être celui qui a le moins intégré le numérique dans sa stratégie."
"Cette communication s’appuie sur une étude menée en Algérie, Côte-d’ivoire, Liban, Sénégal et Tunisie, à l’occasion de la crise sanitaire du Covid-19. À partir d’entretiens et de questionnaires contextualisés, elle a étudié les mouvements citoyens en ligne et hors ligne. Elle s’est posée la question des possibles transformations des formes traditionnelles et autoritaires de contrôle social et des dynamiques citoyennes. Il en ressort plutôt un renforcement de phénomènes existant que de véritables transformations."
"Les professionnels des musées, en s’appropriant les technologies interactives, inventent de nouvelles médiations et de nouveaux accès aux arts et sciences, qui traduisent la façon dont les musées pensent la circulation de leurs ressources. C’est ce que nous tenterons de mettre en exergue à travers un exposé des principales technologies de réalité virtuelle, de réalité augmentée et de réalité mixée, qui constituent de nouvelles médiations et de nouveaux accès aux arts et sciences, et qui traduisent la façon dont les musées pensent la conservation, la circulation et la diffusion de leurs ressources. Nous tenterons de montrer comment les musées explorent les opportunités qu’offre cette (r)évolution afin d’opérer des transformations profondes vers de nouvelles formes d’organisation que nous appellerons génériquement smart museum , aptes à exploiter diverses technologies alliant réalité virtuelle et réalité augmentée. Les visiteurs ne veulent plus être des consommateurs passifs. Ils exigent désormais des informations, des divertissements et une participation active aux stimulations multisensorielles, combinés à un design innovant en moyens de diffusion et de médiation. Pour cela, les smart museum auront pour mission de répondre aux attentes qui traduisent les trois facettes du visiteur (en tant qu’humain) : homo sapiens , humain qui sait et qui est en quête permanente d’informations et de savoir ; homo ludens , qui souligne l’importance de l’acte de jouer pour l’être humain, notamment pour agrémenter sa quête du savoir, et enfin, homo faber , humain qui fabrique des outils qui lui facilite la vie, notamment ceux nécessaires à sa quête du savoir. Nous présenterons quelques projets concrets qui illustrent les nouvelles missions de ces smart museum, les process et les techniques utilisés pour la conception d’expériences muséales immersives et augmentées, qui contribuent à la restauration numérique des artefacts ainsi qu’à la sauvegarde numérique durable du patrimoine mondial."
"Conférence proposée dans le cadre des « Jeudis du CNRS » Délégation Provence et Corse du CNRS, Salle de conférences Pierre Desnuelle, Marseille, 9 janvier 2020. Depuis quelques années, le déferlement des « fake news » au coeur des systèmes d'information s'est imposé comme un phénomène majeur qui traduit-comme le dit Arnaud Mercier (2018)-un « grave symptôme de délitement politique ». Ce déferlement est souvent assimilé à une « pollution » informationnelle dangereuse. Eu égard à un tel déferlement, l'objet de cette conférence est de décliner un état des lieux et un diagnostic. En matière d'état des lieux, il s'agit, d'une part, de souligner l'ancienneté des phénomènes informationnels contenus aujourd'hui dans l'expression-valise de « fake news » ; il s'agit, d'autre part, de montrer comment ces phénomènes sont réactivés et amplifiés dans le contexte actuel des dispositifs socio-numériques d'information et de communication. En matière de diagnostic, il convient d'indiquer des voies d'accès à une résilience collective, basée essentiellement sur une acculturation précoce et critique à l'univers numérique et à ses codes."
"Cette contribution s’intéresse au traitement de la radicalité par un média libanais dans un contexte socio-politique spécifique, ancré dans un espace méditerranéen, chrétien et arabe. En effet, plutôt que d’essayer de définir en soi ce qu’est un comportement ou un contenu radical à la manière d’Olivier Roy (2004, 2016), de Gilles Kepel (2003, 2018) ou de Farhad Khosrokhavar (2014), nous nous sommes intéressés à son traitement dans un espace public particulier. Une analyse de contenu des éditoriaux du journal francophone libanais L’Orient-Le jour a ainsi été menée sur un corpus constitué de 216 éditos publiés pendant le vide présidentiel au Liban, entre le 25 mai 2014 et le 31 octobre 2016. L’outil utilisé pour mener à bien ce travail a été le logiciel Tropes afin d’automatiser l’analyse. Nos résultats montrent une interprétation communautaire du terme dans un pays multiconfessionnel où chaque communauté se définit par rapport aux autres sachant qu’il en existe 18 dans le pays du Cèdre."
"A l’heure du Web sémantique, les données bibliographiques des catalogues des bibliothèques municipales sont invitées à s’exposer sur le Web. Elles sont encouragées à prolonger les initiatives de la BnF et l’ABES en matière de FRBRisation, en lien avec les mouvements de l’Open Data et du Linked Open Data. Mais c’est une autre alternative orientée vers les usagers que nous présentons. Elle s’appuie sur le portail data.gouv.fr auquel les bibliothèques municipales peuvent contribuer au titre de producteurs de données. Notre article vise à concevoir l’instrumentation numérique d’un dispositif infodocumentaire paramétrable qui permette de rassembler, à façon, les données bibliographiques des catalogues des établissements documentaires en fonction du périmètre de mobilité de chaque usager. Notre travail s’inscrit dans le prolongement de la théorie de l’activité et de l’action située et des émergences de genèses instrumentales décrites par Pierre Rabardel lorsque des dispositifs technologiques mobilisés rentrent dans la boucle d’activité constructive du sujet."
"Bien que le néologisme de « datafication » ne soit pas encore installé dans les usages, une page wikipédia dédiée témoigne de l'existence du terme. Il fait référence à la tendance actuelle qui consiste à fonder le pilotage des activités de nombreux secteurs sur l'exploitation de grandes masses de données générées par nos interactions avec des dispositifs numériques. La datafication renvoie également à une croyance répandue que l'exploration de grandes masses de données a la capacité d'accélérer le rythme des découvertes et des innovations dans les secteurs concernés. Avant d'aborder les enjeux sociétaux de la datafication, il nous faut clarifier la notion de « donnée » (datum) et de « données massives » (big data). Qu'est-ce qu'une donnée ? Le sens courant du mot « donnée » laisse entendre qu'il s'agit de quelque chose de neutre ou d'objectif qui serait prêt à l'emploi. Son étymologie latine ""dare"" est également trompeuse car elle signifie l'acte de donner (""to give""). Bien qu'il existe des données qui résident dans des éléments naturels (celles dégagées par le temps qu'il fait, par la couleur du ciel ou des feuilles sur les arbres), elles ont besoin d'être interprétées, souvent collectivement, pour construire des significations sur lesquelles nous pouvons être d'accord. Le type de données qui nous préoccupe ici nécessite un effort conscient pour les produire et les mettre en forme. Elles sont donc construites socialement, techniquement, économiquement et situées spatialement et culturellement (comme par exemple, les données sur la consommation d'eau ou d'électricité d'une commune de France). En fait, elles devraient être appelé des « obtenues » 3 ou des « capta » (du latin ""capere"") qui signifie prendre. Comme l'observe Jensen (1950) cité dans Rob Kitchin (2014b : 2) : « C'est un hasard malheureux de l'histoire que datum plutôt que captum soit venu à symboliser le phénomène basique en science. Car la science traite non pas de ""ce qui a été donné par la nature au scientifique"" mais de ""ce qui a été capté ou sélectionné de la nature par le scientifique "" en accord avec le but de celui-ci »."
"A l’aune des procédés d’écriture classiques et créatifs, présents dans les œuvres d’illustres auteurs, tel Rabelais, l’auteur étudie l’impact de la cyberlangue, sur les situations de communication médiatée, en environnement socio numérique. Ce travail mobilise un pluralisme théorique et méthodologique, une approche quantitative et typologique. Il s’appuie sur un terrain de données nativement numériques sur un forum de discussion en ligne, dans le domaine de la téléréalité. En appui sur les concepts de langue et langage, réduction de l’incertitude, cyberlangue, dispositif, du lien social, réseaux socio-numériques, cette étude exploratoire permet d’identifier des procédés d’écriture élaborés et créatifs et d’établir une typologie enrichie des marqueurs de la cyberlangue. Le poids et la concentration des marqueurs cyberlangue, révèlent un nouveau type de langue véritable, dans une dynamique de diffusion et de co-construction sur le web pour le maintien du lien social."
"Cette communication vise à étudier un acteur politique, confessionnel et communautaire : le Hezbollah. Ceci dans l’objectif d’interroger ses modes d’énonciations et ses formes d’expression qui sont constitutifs de ce que nous appelons la communication radicalisée. La communauté internationale le (re)connaît comme une fraction terroriste, le constat est en contraire plus ambivalent. Le Hezbollah utilise de nombreux outils numériques dans un objectif de médiatisation de ses actions mais surtout dans le but de donner à voir un autre visage. Ainsi, nous souhaitons saisir l’image en tant que vecteur des représentations et forme d’expression de la radicalité comprise au sens d’une rupture du contrat de communication, établit par des acteurs partageants les mêmes espaces et dispositifs de communication."
"La mobilité, en particulier la mobilité urbaine est aujourd’hui un thème d’une grande actualité politique et scientifique, qui soulève des questions et engage des démarches qui vont bien au-delà des problématiques habituelles des transports (Bonnet, Desjeux, 2000, p.201). Au centre de la vie quotidienne, économique et/ou sociale des acteurs locaux, la mobilité est un problème aux multiples enjeux : impact sur le réchauffement climatique, droit à la mobilité, économie, urbanisme et cadre de vie, équilibre entre ville et campagne, sécurité routière et santé publique. Le besoin de mobilité peut être traité, voire satisfait, de plusieurs façons, soit en apportant une réponse au besoin de déplacement soit en apportant une réponse au mode de mobilité. La question de la mobilité est un des enjeux majeurs pour l'accès aux emplois comme aux services dans les territoires et lorsque celle-ci se heurte au manque de transports collectifs, publics et privés, ce manque entraîne l'isolement voire l’exclusion, surtout pour les populations les plus fragiles avec pour corollaire la saturation des infrastructures routières par le recours à l’utilisation intensive des véhicules individuels dans les zones péri-urbaines. Ainsi, le secteur des transports a besoin d’utiliser les technologies de l’information. Ces technologies de l’information (TI) sont au centre de nombreuses réflexions sur la mobilité et la non mobilité, en particulier dans le cadre de la réduction des déplacements physiques rendus nécessaire par la maitrise des émissions de gaz à effet de serre. Les technologies de l’information en hybridant les territoires, peuvent être des outils d’une gestion globale et durable des déplacements territoriaux. Dans cette recherche, nous avons mis l’accent sur le croisement de l’« Intelligence Territoriale et mobilité durable » avec une orientation sur le déplacement partagé, biens et personnes, facteur de communication sociale, et de développement d’équilibre territorial en menant en parallèle une double étude entre l’Afrique de l’Est (la CAE) et l’Euro Méditerranée (PACA et Corse), pour en tirer des enseignements. Bertacchini, Girardot, et Grammacia (2006), présentent l’intelligence territoriale (IT) comme étant une théorie, posture, et démarche ascendante d’intelligence collective fondée sur une approche citoyenne de la valorisation territoriale. Nous avons insisté sur la nécessité de fonder l'action sur une analyse fine des besoins des habitants en matière de déplacement et comment inventer de nouvelles modalités d'organisation des services par le développement des stratégies de communications qui s’inspirent de l’intelligence territoriale (IT), la cohésion sociale, la convivialité, l’équité, les hypothèses de l’IT et la capacité de la communication à promouvoir la médiation territoriale"
"Devenant également une « crise pédagogique » (Magnard, 2020), la crise sanitaire d'une ampleur inédite a contraint les établissements d'enseignement à se réinventer en mettant en oeuvre en urgence des politiques de continuité de l'activité enseignante, dans un contexte de distanciation sociale. Face à des apprenants totalement virtualisés, cela a de quoi interroger l'enseignant dans sa pratique quotidienne : nous pouvons alors nous demander si l'enseignant est réellement préparé à travailler uniquement avec des étudiants à distance. Le présent article s'interroge sur cette injonction du numérique à l'aune du contexte de la crise sanitaire en étudiant les différents dispositifs d'apprentissage innovants (Caron-Fasan, Parmentier, 2019) mis en place dans l'urgence par les enseignants d'une école supérieure de commerce. Il s'agira également de comprendre, à l'aide d'entretiens semi-directifs actifs, les représentations des enseignants face au prisme du tout numérique."
"Les objets d'étude dans les Sciences Humaines et Sociales (SHS) sont nécessairement complexes, multidimensionnels et imbriqués. Leur investigation sollicite les regards et les apports de théories épistémologiques différentes avec leurs méthodes adossées. Dès lors, on peut postuler que le pluralisme épistémologique et méthodologique est la norme plutôt que l'exception en SHS, et a fortiori dans les SIC (sciences de l'information et de la communication) qui est une interdiscipline, constituée de plusieurs disciplines ayant chacune sa tradition de recherche, ses paradigmes et ses méthodes. Les six contributions retenues dans ce numéro spécial abordent sous différents angles cette question épineuse mais passionnante des présupposés épistémologiques sous-jacents aux champs de recherche en SIC. Certaines contributions se sont employées à retracer la façon dont des concepts fondateurs, ou dits comme tels, sont rediscutés, repensés voire critiqués ou redéfinis par différentes communautés scientifiques qui constituent les SIC en France comme à l'étranger, d'autres se sont employées à démontrer comment des théories élaborées en dehors de la discipline ou comment des travaux d'auteurs importants situés en dehors des SIC peuvent néanmoins être fertiles pour certains champs des SIC."
"Les réseaux socionumériques ont partie liée avec l’activité de l’usager-récepteur théorisée par l’école de Columbia. Dans leur modèle de la communication à deux étages, les chercheurs de Columbia avaient pour objet les réseaux sociaux non numérisés. Le lien entre système médiatique et système social qu’avait anticipé l’école de Columbia semble d’autant plus d’actualité avec la prise d’informations via les réseaux socionumériques. Désormais, les médias doivent compter avec les réseaux socionumériques et par conséquent avec les usagers-récepteurs. Par son partage d’information, chaque usager-récepteur peut devenir leader d’opinion à court terme en influençant ses groupes secondaires et en suscitant des gratifications. L’acte ponctuel de partage concrétise ce nouveau filtrage qui symbolise le passage au deuxième étage de la communication. Le partage est ainsi la réification conjoncturelle de l’influence personnelle qui transforme l’usager-récepteur en leader d’opinion. Dans ce modèle de l’usager-récepteur 2.0 du nouvel écosystème médiatico-socionumérique, les leaders d’opinion 2.0 sont assimilables à des partageurs d’opinion. Les usagers-récepteurs 2.0 ne sont plus seulement influencés par les discussions animées par des leaders d’opinion au sein des groupes auxquels ils appartiennent, mais ils sont également influencés dès leur prise d’information sur les réseaux socionumériques par le filtrage opéré par ces mêmes leaders d’opinion 2.0. En mobilisant une littérature scientifique européenne et nord-américaine, nous souhaitons montrer la pertinence d’un cadre théorique canonique pour l’analyse des usages et pratiques des réseaux socionumériques au prisme de la réception de l’information des jeunes adultes. Le but de notre démarche est de relier l’information (qui est diffusée par les médias) à la communication de l’information (par les usagers-récepteurs). En vue de comprendre les situations d’influence d’opinion à l’œuvre dans les activités de circulation et de réception, les processus de filtre de l’information seront étudiés en reprenant les éléments structurants du modèle proposé par l’école de Columbia ; dans ce modèle, ce sont les leaders d’opinion qui sont les véritables relais et filtres de l’information. Notre démarche, tant sur le plan théorique que sur le plan méthodologique, consiste en un retour à la littérature originelle de l’école de Columbia. En accord avec cette même littérature, nous ambitionnons de déployer une analyse sociale empirique imprégnée de méthodologie à la fois quantitative et qualitative. Nous nous intéressons à ce que les « vrais gens de la vie quotidienne » choisissent et font des médias sur les réseaux socionumériques, à l’instar de l’école de Columbia qui s’intéressait au choix des gens et notamment à ce que les gens faisaient des médias traditionnels. L’objectif de cette recherche est de transposer ce modèle de Columbia au contexte des réseaux socionumériques afin de l’actualiser et de redéfinir, en son sein, la notion de leader d’opinion dont l’acception a été altérée. Notre apport est donc celui d’une analyse sociale de la communication humaine de l’information via les réseaux socionumériques en sciences humaines et sociales."
"Le cas du blog Fdesouche par Stéphanie LUKASIK. Le terme fake-news est couramment assimilé à des informations fausses, réfutables par une simple contre-information qui rétablit la vérité. Or, il existe un objet frontière moins catégorique : le blog qui se dit de « réinformation ». Les producteurs de cet objet frontière défendent leur sélection de l'information par une volonté de « réinformer ». Le but étant de rétablir une « vraie » information cachée des médias ou de mettre en lumière une information noyée sous la masse d'informations quotidiennes : « Il apparait que la notion de réinformation est surtout un mot au fort potentiel normatif pour désigner un discours d'opinion auquel les grands médias n'accordent pas de publicité. » (Jammet, Guidi, 2017, 255). Cette « réinformation » dans la manière dont elle est construite n'est pas sans lien avec la désinformation. Car dès l'instant où l'on présente une information selon sa seule vision, l'information donnée aux récepteurs est biaisée, tronquée, et par conséquent imprécise voire erronée. En découpant l'information, n'en montrant qu'une partie, on confisque au lecteur son esprit critique. Ici, informer est effectué dans un seul but : appuyer son opinion et la diffuser."
"Les réseaux socionumériques constituent un mirage pour l’érudition. Alors que l’accès à une information diversifiée est une condition nécessaire à l’érudition, accéder aux sources via les réseaux socionumériques peut limiter la pluralité informationnelle. En raison de leur fonctionnement homophile, ces plateformes ne confrontent pas les contenus. Concrétisation à grande échelle de la communication à deux étages et de la figure du leader d’opinion, les réseaux socionumériques sont des plateformes au sein desquelles les usagers-récepteurs s’informent par l’intermédiaire des membres de leurs réseaux socionumériques. Cette réception de l’information par le biais du partage du leader d’opinion change la compréhension. Sans les différents aspects d’une information, l’érudition, qui nécessite l’approfondissement et la confrontation aux sources, semble difficilement atteignable. Seule la reconstruction personnelle d’une diversité de sources pourrait restituer l’hétérogénéité nécessaire à l’érudition."
"Avec les réseaux sociaux numériques, la figure du leader d'opinion du modèle de la communication à deux étages de l'école de Columbia nécessite d'être reformulée. En mobilisant une littérature scientifique européenne et nord-américaine, nous souhaitons montrer la pertinence d'un cadre théorique historique pour l'analyse des usages et pratiques des médias numériques dans la réception de l'information des jeunes. Afin de comprendre les situations d'influence d'opinion à l'oeuvre dans les activités de circulation et de réception, les processus de filtre de l'information seront étudiés."
"Les boucles et groupes sont l’autre facette de la mobilisation anti-passe sanitaire et anti-vaccination. La conversation s’alimente aux diverses sources de la « réinformation » mais se nourrit aussi d’une profonde défiance institutionnelle, favorisée par l’horizontalité des outils numériques."
"Entretien | Dernièrement, certains manifestants s'en sont pris à des journalistes lors de rassemblements contre le pass sanitaire. Des comportements hostiles qui découlent d'une défiance envers les médias traditionnels et d'une volonté de réinformation."
"Cette présentation évoquera en 10 points les enjeux artistiques et pédagogiques de la collaboration entre la licence TSI (techniques sons images) à l’UFR Ingémédia et le Pôle National Supérieur de Danse Cannes-Mougins Marseille. Depuis une dizaine d’années, je questionne le rapprochement entre les écoles d’art et l’université- pour la question de la recherche : direction d’ouvrage pour la revue Hermès avec Jean-Marc Réol L’artiste, un chercheur pas comme les autres ; représentant au Res CAM de l’Université de Toulon ; séminaire en création « la science sans nom » – pour les questions pédagogiques : campagne d’évaluation HCERES ; responsable de mention de la licence TSI dirigé par Hervé Zénouda et surtout la collaboration avec des artistes divers, notamment Régine Chopinot, Jean-Christophe Paré, Ian Simms, Rocio Berenguer, Fabrice Hyber, et aussi la compagnie Castafiore qui accompagna en 2017 et 2018 le projet pédagogique de la licence TSI et qui servira pour amorcer mon intervention."
« L’enfant que tu n’auras pas été » est celui pour qui la détresse d’origine ne fut pas partagée dans un premier temps par un proche mais par un algorithme : le « cri » devient alors un « langage cri ». L’enfant avant le langage (l’infans) pousse un cri qui est transformé par cet algorithme en données interprétables. Le cri qui devient signal n’est plus un appel. Nous verrons en quoi l’humain sera ainsi augmenté d’un « langage cri » qui réduit la fonction subjective. « Le langage cri » exprime un désir illusoire de combler l’inachèvement de l’humain en le saturant de données. Or l’inachèvement est constitutif de notre condition comme le montre Dany-Robert Dufour. L’être néoténique reçoit dès son origine l’incidence de l’autre dont la réponse le structure en tant que sujet. Comme le souligne François Ansermet l’incidence des technologies se joue sur d’autres registres car les logiques temporelles des algorithmes de la médecine prédictive ne s’inscrivent pas naturellement dans celles du sujet. Supprimer l’incidence de l’autre en inventant un « langage cri » est une transgression car le sujet naît d’une simultanéité fondatrice avec l’autre. Il n’y a pas de sujet sans réponse de l’autre.
"L’érudition est-elle une simple parure, un trompe-l’œil, ou une forme de recherche foncièrement libre et inclassable ? Cette dualité se retrouve dans les stéréotypes entourant la figure de l’érudit, admiré pour l’étendue de ses connaissances ou moqué pour ses postures singulières. L’érudition suscite des questions qui intéressent d’autant plus la communication que les nouvelles technologies modifient les productions, l’accès et l’usage des connaissances et des cultures. À l’heure des industries de l’information et des moteurs de recherche, l’érudition fait débat. Le Web et les réseaux socio-numériques produisent-ils de nouvelles pratiques érudites ou conduisent-ils au contraire à les marginaliser ? Le flot vertigineux d’informations bouleverse le statut de l’érudition et de la communication. L’expert qui s’affiche sur le Web serait-il donc le nouvel érudit ? Dans un monde algorithmique qui engendre simultanément communication, incommunication et acommunication, les nouvelles formes numériques produisent une rationalisation supplémentaire. Qu’en est-il alors de l’érudition, cette si ancienne et importante activité culturelle qui apporte de la complexité, à l’heure de l’instantané ? Les articles rassemblés ici engagent ce débat. On y trouvera des études consacrées aux formes historiques et littéraires de l’érudition, des analyses qui portent sur le numérique, la traduction, les bibliothèques ou les plateformes. Des entretiens précisent les défis et les traits de l’« érudit » aujourd’hui. En somme, l’érudition ne cesse de se réinventer."
"Dans le but de reconstituer une cartographie de l'expertise des chercheurs d'un établissement universitaire, SOVisuHAL a pour objectif de séduire les chercheurs par une interface de médiation qui d'une part, en appelle à la promotion et les usages de la science ouverte par des outils de cartographie et de bibliométrie et, d'autre part, reconstruit un reflet élaboré de leurs propres publications pour situer et rectifier leur propre lisibilité scientifique publique et sur des niveaux de référence interne. Le même dispositif alimente la production de cartographies de l'expertise d'un laboratoire définie par un ensemble de mots, termes, expressions, entités nommées (noms de méthodes, noms de projets, d'infrastructures, noms de jeux de données etc.) ou d'un individu par indexation de termes clés reconstruits aux différentes échelles (ce à des fins de pilotage de politique et de collaboration scientifique). Nous présentons un panorama des différents domaines disciplinaires convoqués pour élaborer un prototype, les différents choix techniques, opérés pour la réalisation à des fins critiques et performatives."
"Dans le but de reconstituer une cartographie de l'expertise des chercheurs d'un établissement universitaire, SOVisuHAL a pour objectif de séduire les chercheurs par une interface de médiation qui d'une part, en appelle à la promotion et les usages de la science ouverte par des outils de cartographie et de bibliométrie et, d'autre part, reconstruit un reflet élaboré de leurs propres publications pour situer et rectifier leur propre lisibilité scientifique publique et sur des niveaux de référence interne. Le même dispositif alimente la production de cartographies de l'expertise d'un laboratoire définie par un ensemble de mots, termes, expressions, entités nommées (noms de méthodes, noms de projets, d'infrastructures, noms de jeux de données etc.) ou d'un individu par indexation de termes clés reconstruits aux différentes échelles (ce à des fins de pilotage de politique et de collaboration scientifique). Nous présentons un panorama des différents domaines disciplinaires convoqués pour élaborer un prototype, les différents choix techniques, opérés pour la réalisation à des fins critiques et performatives."
"Les problématiques liées au libre accès des publications scientifiques et la réutilisation des données de la recherche concernent de nombreux pays. La France est un pays à la fois porteur et coordinateur de la science ouverte au niveau national et européen. Ce pays a également exprimé son engagement à l’international en rejoignant des initiatives et des coalitions d’ordre mondial. Depuis que le mouvement de l’Open Science a pris de l’ampleur en France, de nombreuses initiatives ont vu le jour et des projets se sont développés autour de ce sujet. La science ouverte concerne avant tout la communauté scientifique, actrice majeure des enjeux liés aux données, aux publications, à l’évaluation et à la communication. Face au pouvoir de certains acteurs privés sur le marché de l’édition, la France insiste sur sa position actuelle : le but n’est pas de les détruire mais plutôt de ne pas en être totalement dépendant. Dans la même lancée, la France a proposé récemment sa contribution à l’UNESCO en cas de besoin d’experts français dans le cadre d’une création d’équipe spécialisée dans la science ouverte."
"Cet article prend acte de nouvelles pratiques et de nouveaux usages liés aux réseaux sociaux numériques, alors que se développe la figure de l’influenceur. Nombre de plateformes adoptent une définition de l’engagement relativement éloignée de celle forgée en Sciences de l’Information et de la Communication (SIC), au travers du paradigme de la communication engageante, voire, de façon plus récente, de la communication numérique engageante. Il s’agit donc de décrire certaines innovations tout en les situant par rapport à la proposition robuste qu’est celle forgée en SIC. Se présente alors un fort enjeu d’interdisciplinarité et des questions de traduction ou de significations sont posées voire problématisées."
"Devant l'explosion planétaire du nombre d'utilisateurs d'Internet, des fréquences et des durées de connexion, les controverses scientifiques sur la question des « addictions » liées à Internet (aux réseaux sociaux, jeux vidéo en ligne, jeux d'argent, sites pornographiques, smartphones) sont nombreuses et les débats intenses pour savoir s'il s'agit ou non de véritables addictions. Cet article dresse une synthèse critique des modèles et des principales recherches scientifiques sur ces thèmes : comment définir les addictions en lien avec Internet et les réseaux sociaux ? Est-ce de nouveaux troubles mentaux ou une conséquence d’autres troubles psychologiques ou psychosociaux préexistants ? Quels sont les facteurs de risque prédisposant aux comportements excessifs et problématiques et quelles sont les conséquences pour les personnes qui en souffrent ? Nous ouvrons ensuite de nouvelles perspectives de recherche pluridisciplinaire pour dépasser le seul débat addiction/non addiction. À la lumière d’une approche communicationnelle liant production et usages des dispositifs numériques d’une part, et contextualisant davantage les comportements problématiques avec Internet d’autre part, nous donnons des pistes pour mieux comprendre et saisir les pratiques excessives dans leur complexité à la fois biologique, psychologique, sociale et économique."
"Si l’on parle aujourd’hui d’indexation pour désigner l’enregistrement d’un site internet dans la base de données d’un moteur de recherche, la notion d’indexation des connaissances remonte quant à elle à l’Encyclopédie de Diderot et d’Alembert ainsi que les projets qui s’en sont suivis (projet d’Otlet, la classification de Dewey…). L'indexation des connaissances tire son origine du champ disciplinaire de la documentation, elle reste néanmoins au cours des questionnements et des débats liés à l’interdisciplinarité. L’indexation des connaissances peut-elle s’exercer au profit de l’interdisciplinarité ? Nous tentons d’y répondre dans cet article. Nous proposons une expérimentation visant à utiliser la classification internationale des brevets (CIB) comme pivot de classement interdisciplinaire. Le terrain étudié concerne la « science en train de se faire », ici les thèses de doctorat, mais peut être calqué à d’autres sources de données. Nous détaillons les résultats par un exemple lié directement aux sciences dures, mais qui peut être également appliqué aux sciences humaines et sociales. Cette étude originale est au croisement du monde académique et du monde technologique."
"La privatisation croissante de la culture, durant ces dernières décennies, a fait naître un mouvement de révolte intellectuelle qui a réactualisé la notion de commun (commons en anglais) et en a fait l’étendard d’une approche alternative de la propriété intellectuelle et des modes de gouvernance qui lui sont associés. Cet article propose une reconstruction intellectuelle de la notion de commun culturel. Partant d’une quête sur ses origines, pour mieux en éclairer le contexte, les enjeux qui lui sont attachés et ses fondements conceptuels, il propose une grille de lecture permettant d’identifier les contenus culturels (écrit, son, image) éligibles a priori à un tel statut"
"Cet article s’appuie sur une recherche menée au sein d’un cabinet médical d’anatomo-cytopathologie et porte sur les transformations du travail d’analyse diagnostique du tissu prostatique dans un contexte d’intégration d’un logiciel d’accompagnement au diagnostic basé sur une intelligence artificielle (IA). Des chercheurs en sciences de l’information et de la communication se sont intéressés à l’expérience vécue par un public de huit médecins de ce cabinet médical lors de la phase de test du logiciel d’IA, c’est-à-dire au sens subjectif que ces individus attribuent à leurs activités dans une perspective critique d’émancipation (comme augmenter ses savoirs et savoir-faire) versus aliénation (diminution des savoirs et savoir-faire par rationalisation technique) dans le cadre d’un changement de leurs habitudes de travail."
"L’objectif de cette communication est double et à deux voix, reflet des recherches partagées dans le domaine des SIC, à savoir comment les discours sur le thème de l’environnement et du DD, notamment ceux qualifiables de discours Sciences et techniques mis à disposition au sein des musées, influencent les perceptions et les comportements des visiteurs-citoyens-récepteurs. Nous nous intéresserons donc à la façon dont les musées participent, aux côtés d’autres organisations, à la sensibilisation des citoyens aux questions du développement durable : comment un musée peut-il influer l’action citoyenne ? Notre originalité consistera à mobiliser d’une part la narratologie, et, d’autre part la sémiologie, pour étudier les dispositifs d’exposition des musées."
"Objectifs. Le taux d’adhésion rapporté dans la polyarthrite rhumatoïde (PR) est très variable (entre 10 et 98%). La variabilité peut résulter en partie des différentes méthodes utilisées pour mesurer l'adhésion, de la définition employée ou des facteurs associés. Les objectifs de cette étude étaient d'identifier les facteurs associés à l'adhésion thérapeutique médicamenteuse dans la PR et les stratégies d’amélioration de celle-ci. Méthodes. La revue systématique de la littérature (RSL) a interrogé les bases de données scientifiques : PubMed, Web of Science, PsyArticles/Info, BDSP, HAL CNRS, ainsi que Google Scholar et la documentation supplémentaire (les suggestions, la recherche manuelle, les listes de références des revues), - dès l’insertion au mai 2020 inclut, sans limitation de pays. Articles contenant des données sur l’observance ou l’adhésion thérapeutique (médicamenteuse) chez des patients adultes atteints de PR ont été inclus, ainsi que les données sur les facteurs et les leviers associés. Résultats. Nous mettons à jour les RSL précédentes sur les facteurs associés. Les 137 facteurs pouvant affecter l’adhésion sont identifiés au total et classés selon la catégorisation de l’OMS : sociodémographiques/économiques, liés au patient, liés au traitement, liés à la maladie, liés au système/équipe de soins. Les 12 facteurs associés à l’adhésion ont été trouvés dans au moins 2 études différentes, sans un résultat contraire. Il s'agit de : l'âge avancé, l’ethnie blanche, l’éducation supérieure, la croyance en la nécessité des médicaments, le différentiel entre les préoccupations vs la croyance en la nécessité des médicaments, l'usage de différents types de médicament (au régime thérapeutique qui leur est propre), la durée de la maladie, le degré élevé de l’handicap, les coûts hebdomadaires du traitement. Un nombre de facteurs associés présentent des résultats conflictuels. Les 22 stratégies potentielles d’amélioration ont été identifiées au total et classées en cinq catégories de l’OMS. Les RSL rapportent 4 stratégies d’amélioration testées ayant l’impact positif sur l’adhésion dans au moins 1 étude, sans résultat contraire. Ce sont notamment les stratégies : la thérapie en groupe, les thérapies cognitives-comportementales (TCC), comportementales, combinées. Certains résultats sont conflictuels ou mitigés. Conclusion. Le taux d’adhésion était associé aux différents facteurs en fonction de certaines variables sociodémographiques, de croyances de patient, des médicaments pris, et d’autres. Le taux d’adhésion peut être amélioré à l’aide de certaines stratégies ; toutefois, plus amples recherches seraient nécessaires pour augmenter la fiabilité des résultats et tester de nouvelles stratégies. http://www.academia.edu/45625377/Adhésion_thérapeutique_chez_les_patients_atteints_de_polyarthrite_rhumatoïde_les_facteurs_associés_et_les_leviers_d_accompagnement_Revue_systématique_de_littérature"
"L'intégration du patrimoine local dans le territoire s'inscrit dans la dynamique de création d'une ""marque territoire"". La démarche apparaît comme l'expression d'une action collective qui résulte d'un processus de co-construction entre imaginaires touristiques, symbolique du territoire, aspirations culturelles, identité citoyenne... Pour autant, ces reconfigurations organisationnelles et symboliques induisent-elles de nouveaux comportements chez les touristes ? Par ailleurs, que signifie se comporter ""en touriste"" à l'ère des univers immersifs et des applications géo-localisées ? Quelle esthétique de la réception se retrouve dans un univers de réalité augmentée ? La patrimonialisation d'un territoire, par des dispositifs communicationnels permettant de multiples registres d'appropriation de l'expérience culturelle du touriste comme de l'habitant, pourrait ainsi être apparentée à un dispositif qui permettrait aux territoires urbains, ruraux mais aussi immatériels de se réinventer dans les expériences touristiques comme dans les imaginaires collectifs. Ainsi ces marques territoires font l'objet d'un processus continu de réécriture pour se réinventer. Dans une perspective interdisciplinaire, cet ouvrage interroge les rapports économiques et les logiques sociales, les forces de cohésion et de coopération entre le touriste co-créateur d'une expérience et la mise en récit de celle-ci."
"Un pré-print d'Article invité pour la plateforme EEDD de la MAMP, réalisé suite à la conférence-atelier « Communiquer le développement durable et inciter au changement du comportement : quelques outils de l'approche nudge », aux Journées d'échanges Fertiles Rencontres pour l'EEDD, Séminaire 1 : Agir et réagir avec des publics qui changent. Arbois, 11 mars 2019. Métropole Aix-Marseille Provence. Résumé : Dans les services liés à l’environnement et au développement durable, la santé publique et autres domaines d’utilité sociétale, l’intérêt de donner un ‘coup de pouce’ au changement des comportements sociaux, sans contraindre mais en utilisant des leviers à priori plus efficaces de l’incitation douce, est plutôt indéniable. D’autant que toutes les études de méta-analyse, comparant les résultats de recherche antérieure, présentent à peu près les mêmes conclusions – la communication de sensibilisation « classique » par l’éducation et l’information n’aurait que des effets très modérés sur le changement effectif des comportements (entre 5% et 15% d’effet sur l’action effective, malgré les éventuels changements d’attitude). Mais, qu’est-ce donc plus précisément le Nudge – un nouveau mot à la mode, une technique, une théorie, un modèle ? Quels en sont les tenants et les aboutissants ? Une organisation, peut-elle en faire ; et enfin, faudrait-il s’en méfier ? Dans ce bref article vulgarisé, nous rappellerons les points clés du Nudge, reprendrons quelques nudges principaux selon les auteurs de l’approche, et donnerons quelques précisions sur leur application dans une organisation."
"Nous souhaitions pour ce colloque envisager la fabrique du héros par ceux qui la définissent et qui la développent en regard de notre époque, de nos sociétés. Il s’agit pour nous de poursuivre dans cette proposition notre analyse déjà présente dans nos travaux respectifs dans le domaine des Sciences de l’information et de la communication, à savoir comment les discours et productions culturelles et médiatiques influencent les perceptions et les comportements des citoyens par une mise en récit de héros ordinaires. Nous décidons de poursuivre cette analyse avec deux nouvelles approches celles de Cyrulnik (2016) qui démontre que nous choisissons nos héros selon notre histoire personnelle et Campbell (1949) qui définit les étapes du héros, toujours identiques dans les récits. La réunion de deux terrains nous semble innovante quand l’un interroge les processus et l’origine de la création d’une mise en portrait de personnalités héroïques et l’autre construit la narration d’un mouvement de pensée autour de l’écologie. D’abord nous identifions le héros transmis par l’art avec le regard et l’analyse d’un artiste plasticien Fred Kleinberg, qui travaille la notion depuis deux ans (Kleinberg, 2012), et produit actuellement une recherche graphique (avec son Panthéon de Héros qui est une « galerie de portraits » de ses héros toujours inachevée puisque peut-être s’y cacherait un seul héros). En regard de ce travail, nous étudions une autre galerie, présentée dans le journal Le Monde durant l’été 2020 sur les nouveaux écosophes, la série de portraits est intitulée « Les penseurs du nouveau monde » . Ce qui nous a semblé prégnant était que ces derniers étaient présentés sur une page pleine, avec pour moitié leur portrait réalisé par l’artiste Yann Legendre. Ainsi, en référence à Cyrulnik et Campbell, respectivement les choix de nos héros et les étapes du monomythe, avec comme toile de fond la possibilité d’un seul héros au final et divers visages qui justifierait la série, une galerie de portraits, nous questionnons un possible glissement des hérauts en héros, des héros en hérauts, mobilisant l’intime et le geste (à offrir, à entendre, pour laisser une trace) que nous mettons en perspective avec l’usage en communication environnementale de personnalités qui incarneraient l’écologie."
"De la ""frontier""à la frange de parc Résumé Notre propositionà double voix traite du rôle des imaginaires urbains dans la production d'espaces organisés d'un côté autour du commun, du partage, du renouveau de mouvements coopératifs, et de l'autre côté, organisés au contraire autour du privé et de nouvelles formes de ségrégation. En quoi les imaginaires actuels et du futur de la ville sont-ils traversés par les transitions et questionnements propres au 21e siècle, et en particulier par les enjeux liésà la nature età l'écologie ? Nous y répondons en nous intéressantà deux villes dans lesquelles les imaginaires urbains oscillent entre ces deux pôles ; plus spécifiquement, en examinant les rapports entre imaginaires et pratiques dans des territoires de l'entre-deux qui marquentà la fois la séparation et la rencontre entre ces deux mondes. L'imaginaire américain de la frontier, associéà la conquête de l'Ouest par les pionniers américains, représente dans l'imaginaire collectif le point de rencontre entre civilisation et sauvagerie, ou entre culture et nature (Slotkin, 1998). Il aété utilisé pour décrire le phénomène de gentrification dans les villes américaines (Smith, 1996) et donc l'appropriation des quartiers populaires et de minorités ethniques par les classes moyennes blanches. A Detroit, cet imaginaire fait référenceà la ville en décroissance, avec ses espaces où la nature a repris ses droits, devenue en partie vide et sauvage, récemment transformée par d'importants investissements privés, y compris dans l'exploitation forestière, qui ont aboutià une gentrification rapide (Safransky, 2014). En même temps, l'imaginaire autour de la ville se nourrit des pratiques alternatives fondées sur la valeur d'usage (Paddeu, 2012), relevant de l'autosubsistance, du "" Do-it-yourself "" (Kinder, 2016) ou de l'agriculture urbaine. Eń etudiant un projet alternatif et contestataire développé par un collectif d'artistes, de fermiers urbains et de militants dans le quartier du North End, situé aux franges du centre-ville gentrifié, nous verrons la façon dont cet imaginaire s'exprime dans des visions opposées du devenir de la ville."
"Nous avons travaillé sur l’analyse de onze synopsis de courts métrages écrits en période de confinement par une trentaine d’auteurs . Il s’agissait pour eux, de répondre à l’appel à projet de Canal + On s’adapte dont les dates d’ouverture (11 mars 2020) et de fermeture (5 juin 2020) ont été corrélées à la période d’un premier confinement en France. Un cadre d’écriture était imposé pour que ces histoires proposent aux spectateurs des « récits d’un futur possible et souhaitable », les fictions devaient représenter « ce que nos sociétés pourront devenir dans les décennies à venir, sans candeur, ni catastrophisme » . Il s’agissait donc pour les auteurs de créer une diégèse permettant de projeter le spectateur dans un monde qui aurait accompli une transition écologique en lien avec les connaissances scientifiques dont les auteurs avaient un accès à travers une plateforme dédiée. Nous avons mené cette étude en considérant les onze synopsis comme un seul corpus et nous inspirant du travail de Chalvon-Demersay (1994), de Blandin (1977), afin de rendre compte de l’imaginaire social proposé. Un des résultats de cette micro recherche est que les auteurs ont situé leurs intrigues dans une époque « post quelque chose » mais pas dans un « futur souhaitable ». En revanche la part « optimiste » de ces synopsis figurait dans les rapports humains. Des explications sont soulevées pour expliquer ce phénomène quand le futur imaginé ne peut être la perpétuité du monde présent. Nous reviendrons sur un imaginaire « empêché » , due au contexte spécifique où l’agir en temps de crise reste incertain (Callon et al., 2001). Alors que la fiction cinématographique est un outil subtil d’influence sociale qui pourrait participer à concrétiser une transition écologique (Sussfeld, 2021), nous observons que c’est à travers le film documentaire que les questions écologiques sont davantage traitées. Nous pouvons proposer une communication sur ce manque actuel dans la fiction cinématographique et télévisuelle d’une culture de la transition qui s’exprime alors dans des documentaires (le film Demain, Cyril Dion, 2015), dont les auteurs vont d’ailleurs amener le spectateur à fictionnaliser (Odin, 2000). Quelle temporalité narrative pour aider la transition écologique : faut-il imaginer le futur où inscrire le récit dans le présent ? Bibliographie Bardin, L., (1977), L’analyse de contenu, PUF, France Botero, N., (2017), « Catastrophisme en écologie : le double statut narratif de la peur », in Zinna A. et Darrault-Harris I. (éds), Formes de vie et modes d’existence ‘durables’, Collection Actes, Toulouse, Éditions CAMS/O, p. 41-65. Callon M., Lascoumes P., Barthe Y., (2001), Agir dans un monde incertain. Essai sur la démocratie technique, Paris, Le Seuil (coll. ""La couleur des idées"") Chalvon-Demersay,S., (1994). Mille scénarios. Une enquête sur l’imagination en temps de crise. Éditions Métailié Lahire B. (2018), L’interprétation sociologique des rêves, Paris, La Découverte Odin, R., (2000), De la fiction, Editions De Boeck Université. Sussfeld, F., (2021), Entre expériences, récits et actions, la communication du mouvement de la transition écologique traduit-elle un changement d’heuristique ?, thèse IMSIC."
"Condamnée par Bruxelles le 16 septembre 2021 pour « détérioration continue de la liberté des médias et de l’État de droit en Pologne », Varsovie a finalement renouvelé son autorisation d’émettre à la chaîne télévisée TVN24 après dix-neuf mois d’attente. Sous les oripeaux d’un affrontement culturel, il s'agit-là essentiellement de liberté du journalisme, de pluralisme médiatique et de conformité au droit européen."
"Le numérique va prendre toute sa part dans les futures maquettes de formation des enseignants pour que ces derniers développent une pédagogie plus active. Notre thèse nous a conduit à proposer à des enseignants débutants du secondaire d’équiper un ou deux élèves de lunettes caméras pendant qu’ils faisaient la classe. Une méthode de collecte de données qui nous a ensuite permis de faire verbaliser naturellement les élèves sur leur vidéo. Les fonctionnalités de la communication éducative médiatisée, en particulier le cahier de texte en ligne, mais également d’autres usages des technologies éducatives comme Moodle, provoquent des usages, chez les élèves, les orientant vers des plateformes non institutionnelles. À l’heure où ces derniers trouvent leurs dispositifs personnels de plus en plus attractifs, nous discutons d’un modèle qui pourrait servir aux enseignants afin d’éviter certains décalages entre leur pédagogie et la simplification que promettent, aux élèves, les industriels du numérique. Haut de page"
"Ce rapport répond à une commande du Bureau des paysages du Medde portant sur l'étude du corpus réuni à l'occasion du concours ""Mon paysage au quotidien, la France vue par ses habitants"". Lancé à l'occasion de l'anniversaire de la loi ""Paysage""(1993) en octobre 2013, ce concours photographique est destiné au grand public et aux scolaires. Il fait écho au concours national organisé en 1992 ""Mon paysage, nos paysages"", dont les résultats ont fait l'objet d'une analyse par la sociologue Françoise Dubost (1995) . Vingt ans plus tard, les 8 000 contributions collectées permettent d'apprécier l'évolution de la perception du territoire hexagonal. Qu'en est-il aujourd'hui alors que la population française localisée en périurbain n'a cessé de croître depuis 20 ans, que les centres-villes convergent vers les berges, que la banlieue se gentrifie et que les écoquartiers se multiplient. A quoi ressemble la France d'aujourd'hui dans le viseur des français? sources : http://www.side.developpement-durable.gouv.fr/Default/doc/SYRACUSE/344271/mon-paysage-au-quotidien-une-pratique-ordinaire"
"Médecin devenu star des réseaux sociaux numériques, le Professeur Raoult et sa stratégie de communication scientifique en ligne interrogent les figures d'autorité dans l'information. Comment un scientifique, jusque-là relativement peu médiatisé, a pu s'imposer dans l'agenda médiatique ? Est-il le symbole d'un rapport bouleversé entre champ scientifique et médiatique ?"
"À l’aube des années 2000, la chaîne payante Canal+ diversifie sa gamme de programmes en lançant plusieurs séries dramatiques ambitieuses, exigeantes et réalistes. Celles-ci portent le label de “Création Originale.” Cette stratégie de labellisation participe à une différenciation de l’offre des chaînes gratuites françaises. Elle consiste aussi à transposer la stratégie déployée à la fin de la décennie précédente par le diffuseur premium nord-américain, HBO. Partant d’entretiens sociologiques réalisés avec des producteurs, scénaristes, et chargés de programmes de séries de Canal+, cet article met en évidence deux corollaires de cette politique éditoriale: une modification des méthodes de travail en matière de fictions télévisées, ainsi qu’une recomposition des sphères professionnelles de la télévision française."
"Les accidents de la vie courante (AcVC) étant la première cause de décès chez les enfants et adolescents, ce rapport propose des recommandations fondées sur des données scientifiques probantes pour l'élaboration d'une politique de communication de santé publique pour la prévention, en France, de ce problème majeur. À partir d'une synthèse de la littérature scientifique et dans une perspective pluridisciplinaire, le rapport vise deux objectifs. Premièrement, il synthétise les principales connaissances nécessaires pour l'élaboration d'une politique publique efficace (les déterminants des accidents de la vie courante liés aux enfants /parents/environnement, les niveaux d'actions possibles individuels/communautaires, les modèles théoriques du changement de comportement et ceux de la résistance au changement, l’efficacité de différents types d'actions de communication/prévention notamment via les médias traditionnels/digitaux). Deuxièmement, le rapport présente, d'une part, les recommandations préconisées pour concevoir des stratégies de communication pour prévenir les accidents de la vie courante chez les jeunes de moins de 15 ans en France et, d'autre part, les différentes étapes à suivre pour réaliser une politique publique de prévention efficace. L’ensemble fournit des pistes prometteuses pour promouvoir l’efficacité de la prévention des AcVC en France. ►Ces travaux ont bénéficié d’un financement de la Direction générale de la santé et de l’appui de l’Union nationale des associations familiales (UNAF)."
"L'UMR GEAU propose depuis près de 10 ans des modules de formation à la décision collective sur la gestion de l'eau basés notamment sur la construction collective de jeux de rôle avec le dispositif Wat-A-Game (WAG). Ce dispositif, qui se place dans la lignée de la modélisation d'accompagnement (voir http://commod.org ), fournit des briques matérielles et des éléments méthodologiques associés et a pour but de faciliter l'autonomisation de groupes d'acteurs et de réduire l'intervention des spécialistes (chercheurs ou bureau d'étude) dans la maitrise et l'utilisation de l'outil jeu de rôles dans des processus de gestion de l'eau. WAG fait partie d'un ensemble plus large d'outils dédié à l'autonomisation de groupes d'acteurs locaux dans l'accompagnement et la mise en oeuvre de processus de décision participatifs pour la gestion des ressources naturelles, et de l'eau en particulier (voir http://cooplaage.watagame.info).Les modules de formation, d'une demi-journée à une semaine guident les participants à déployer en petits groupes de 4 à 6 personnes les outils sur des études de cas concrètes (les leurs en formation continue, avec si possible tests en directs avec des groupes d'acteurs locaux, des études de cas issues de projets en cours en formation initiale). Nous disposons aujourd'hui d'une importante expérience d'utilisation de ce dispositif dans des modules de formation initiale (niveau master le plus souvent) ou continue (professionnels ou grand public, le plus souvent dans le cadre de projet). Ainsi en 2019, plus de 1000 personnes ont suivi un module de formation incluant le dispositif Wat-A-Game. L'hypothèse est que via la mise en capacité de groupes d'acteurs locaux à maitriser ce type d'outils, du changement puisse être produit via la dissémination de protocoles de modélisation participative produisant des jeux de rôles permettant à d'autres groupes d'explorer et de structurer leur futur. Dans un objectif d'évaluation et d'amélioration de nos outils et de nos modules de formation, nous avons mené une étude sur les apprentissages générés par ces formations, sur la base des formulaires d'évaluation que nous faisons remplir systématiquement en fin de formation, d'une enquête quantitative "" à froid "" des anciens participants aux modules, et d'entretiens avec quelques anciens participants choisis. Nous présenterons les résultats de cette étude portant sur différents types d'apprentissages : - Apprentissage expérientiel des outils et approches de modélisation participative réutilisables de manière autonome . - Apprentissages substantifs relatifs au système socio-écologique traité (enjeux, dynamiques, options technique, états possibles et désirés...) et à la gestion de l'eau en général (enjeux, solutions, acteurs). - Effets relationnels au sein et à l'extérieur du groupe de travail - Effets normatifs sur les valeurs des participants. Nous discuterons des résultats de cette étude au regard de nos objectifs de transfert et de dissémination d'outils et de postures visant à produire du changement dans la gestion de l'environnement et des limites que nous rencontrons dans cette démarche. Nous aborderons notamment en discussion la stratégie de développement de ressources en ligne que nous poursuivons depuis 2015 afin d'améliorer et consolider la dissémination de nos outils, avec notamment le MOOC Terr'Eau and Co est en ligne depuis 2018 sur la plate-forme Agreenium (https://lms.agreenium.fr/enrol/index.php?id=12). Ce MOOC est conçu pour permettre à des groupes de suivre ensemble à distance les enseignements que nous dispensions jusqu'à présent en présentiel, et pour les accompagner dans la conception de leur propre dispositif participatif. Ce MOOC s'accompagnera à terme d'outils en ligne, dont certains sont en cours de développement, permettant aux apprenants de structurer la conception de leur jeu. Le MOOC constitue un support de transmission stabilisé nous permettant d'alléger nos charges d'enseignement en présentiel, et d'accompagner des enseignants ou des facilitateurs déjà sensibilisés à la démarche dans l'animation d'atelier de développement de jeux de rôle. Son utilisation en support autonome d'enseignement à distance n'a aujourd'hui pas encore été expérimenté, mais elle pose de nombreuses questions : comment l'utilisation du numérique pourrait-elle faciliter la compréhension de consignes sur des taches collectives complexes comme la modélisation ou la conception de jeux ? Comment le numérique peut-il s'adapter aux modes de pédagogie active basés sur l'expérience et l'accompagnement ? Comment faire évoluer nos méthodes d'évaluation à ces nouvelles pratiques d'enseignement ?"
"Le champ de la communication corporate sur le développement durable se trouve aujourd’hui à la croisée de plusieurs dynamiques organisationnelles et de mouvements d’opinion riches d’enseignements. La communication développement durable semble s’inscrire de plus en plus naturellement dans les stratégies corporate, et parallèlement, la démarche de communication reste prioritairement orientée sur l’image de l’entreprise et ses valeurs.Cette problématique est d’autant plus forte pour les entreprises qui ont toujours eu une vocation dite d’intérêt général. Notre terrain d’analyse se concentre sur des entreprises longtemps sous tutelle étatique et désormais régies par d’autres statuts, mais qui ont conservé leur ‘mission statement’ originelle dans leurs valeurs et dans la culture d’entreprise, telles que les Caisses d’Epargne ou la CDC pour le secteur financier, EDF ou GDF pour le secteur énergétique, la SNCF ou Air France pour les transports par exemple."
"Lorsqu’on nécessite la participation du public et des changements de comportement, comment mobiliser le sujet individuel et collectif dans des campagnes d’actions et d’utilité sociale (CAUPS) ? Notre projet de recherche scientifique financé cherchait à comprendre les leviers nécessaires à la participation collective. Mené sur le terrain autour d’une campagne sur la prévention et l’hygiène dans un espace public, les piscines communautaires, ce projet a permis d’envisager de nouvelles perspectives d’action pour une communication institutionnelle. Grâce au croisement d’un dispositif de recherches en SIC « classique », sémio-pragmatique et qualitatif, d’un dispositif expérimental et d’un dispositif quantitatif, nous avons pu obtenir de nouvelles données, et démontrer l’efficacité d’un dispositif à tiroir, informationnel et engageant, sur l’agrément de la campagne mais aussi et surtout sur le cours de l’action des récepteurs, devenus alors acteurs. Ces résultats nous permettent également d’avancer la grande simplicité du dispositif ainsi préconisé, et son ample efficacité."
"Le présent article dresse un bilan bibliographique diachronique des recherches publiées en langue française dans le champ de la communication environnementale (articles scientifiques, thèses doctorales et livres). Les auteures retracent d'abord l'émergence de ce champ avant de définir, en deuxième partie, quelques termes cruciaux. Après avoir présenté la méthodologie, elles reconstruisent, dans une troisième partie, le parcours historique du développement des recherches sur la communication environnementale, ce qui leur permet de contribuer à une cartographie des recherches existantes aujourd'hui. Les conclusions portent sur l'état actuel de ce domaine de recherche."
"Cet ouvrage collectif apporte un éclairage original, inspiré par les sciences de l'information et de la communication, sur les transformations actuelles des industries et institutions culturelles, ainsi que des arts vivants, à l'ère du numérique. Ses auteurs sont issus du réseau CREAMED, qui regroupe artistes et chercheurs en région Provence-Alpes-Côté d'Azur-Corse, autour de la problématique des liens entre art, culture et communication. Ils proposent de repenser les catégories de la création, de la médiation ou de la réception au travers des perspectives de réinvention et de dépassement offertes par l'environnement numérique."
"Une plateforme en ligne déployée le 30 janvier 2019 en parallèle et concurrente de celle, plus instituée et plus médiatisée, du « Grand débat national ». Une arène dédiée à la consultation sur les préoccupations des citoyens qui a davantage circulé dans les réseaux de Gilets jaunes que dans les médias ou les relais plus institutionnels. Une consultation dont les résultats laissent voir, plutôt qu'un recueil d'opinions ou un débal- lage d'intérêts hétéroclites et peu structurés, un nouveau répertoire d'actions de la part d'un mouvement ou de ses sympathisants. Au-delà d'un simple porte-vue d'opinions, ou un recueil thématisé de sondages à peu de frais, les contributions visent à établir une négociation, voire à l'imposer. Identifié par la CNDP puis sollicité par le collectif qui diligente le « Vrai débat », notre collectif de chercheur(e)s de l'Université de Toulouse1 a réalisé l'analyse complète de la base de données et propose ici son interprétation préliminaire. Sans nuages de mots ni Intelligence artificielle (IA) ajoutée."
"Pour ses pratiquants comme ses producteurs, le jeu de rôle n’est pas un loisir isolé et s’inscrit dans un ensemble de références culturelles et de compétences génériques au service de l’imaginaire ludique et de la sociabilité. Il peut alors être considéré comme le centre d’un dense réseau de pratiques qu’il permet d’actualiser et de mettre en commun dans une logique convergente. Cet article établit les liens historiques et sociologiques entre ce type de jeu et le répertoire culturel qui rend possible son existence."
"L’objet de cet article est de démontrer qu’il ne suffit pas de jouer à des jeux vidéo pour se considérer comme gamer. Au contraire, cela passe par tout un ensemble d’attitudes et de discours vis-à-vis de la pratique qui impliquent la mise en place de normes d’authenticité d’un.e vrai·e gamer. Ces normes sont avant tout construites autour d’une double temporalité : synchronique (avoir été là avant) et diachronique (y passer beaucoup de temps), mais aussi de références fondatrices qui forment les bases du bon (et en creux du mauvais) goût dans ce que l’on peut alors qualifier de sous-culture gamer, qui s’est construite au fil de la massification de la pratique. La construction sociale de cette catégorie subculturelle est explorée ici à partir d’un corpus d’entretiens portant de manière générale sur la culture geek, et dans lequel la question de ce qu’est l’identité gamer s’est avérée centrale."
"La question qui anime cette réflexion, s’appuyant sur plusieurs travaux menés sur les cultures fans et sur un travail de thèse portant sur la culture geek1, est finalement assez simple : qu’est-ce qu’un auteur dans le contexte médiatique contemporain ? Mais si la formulation est facile, la réponse l’est beaucoup moins. Les questions « qu’est-ce qu’un auteur, qu’est-ce qui lui confère son autorité, et à l’inverse quelle autorité ont les publics sur les œuvres ? » sont très discutées depuis déjà de nombreuses années et il serait impossible de résumer ici tous les débats sur le sujet qui animent les chercheurs en sociologie, en narratologie, en sémiologie ou en études littéraires depuis la naissance de ces disciplines. On peut avancer, pour commencer, que d’après de nombreuses recherches menées depuis une quinzaine d’années, les places respectives des publics et des auteurs, les frontières de leurs rôles dans l’espace social, et dans l’espace de production semblent de plus en plus ténues, floues, et se croiser. Le plus souvent, pour exemplifier cette évolution largement basée sur la démocratisation des outils numériques, mais aussi sur une évolution sociale de prise en main des objets par les individus, on s’appuie sur l’étude de ce qu’on appelle la culture participative. Celle-ci est définie entre autres par Henry Jenkins comme : une culture dans laquelle les faibles obstacles à l’expression artistique..."
"Après avoir mis en ligne le document ""Comment rédiger un projet de thèse (ou un projet de recherche)"" (voir à la fin de ma page web : http://irsic.univ-amu.fr/Didier-COURBET?lang=fr) plusieurs jeunes chercheurs de l'IRSIC/IMSIC (Universités d'Aix-Marseille et de Toulon) m'ont demandé d'établir un petit guide de logique identique pour les aider à rédiger l'introduction d'un article de recherche destiné à une revue scientifique (ou l'introduction d'un rapport de recherche de type mémoire ou thèse de doctorat, encore que pour une thèse, la logique est un peu différente). Pour être précis, j'entends ici par « introduction » les toutes premières lignes de l'article (qui se situent donc avant d'expliciter le contexte théorique et la problématique de manière précise) Voici donc ce document. Je le mets en ligne pour en faire également profiter aux jeunes chercheurs d'autres équipes de recherche. Je le modifie régulièrement, n'hésitez pas à revenir régulièrement télécharger la toute dernière version."
fr_abstract_s
"La dynamique des modèles de piles de sable décroissantes en une dimension laisse apparaître des motifs réguliers après une phase d'évolution à l'allure désordonnée. Dans cet article, nous présentons les résultats de simulations mettant en avant des phénomènes d'émergence entretenus par différents mécanismes. La définition des modèles est argumentée, ses premières propriétés sont démontrées, et les observations sont discutées autour de la formulation de deux conjectures."
"Adapter une preuve SAT par résolution en une preuve valide pour Max-SAT sans augmenter considérablement sa taille est une question longtemps restée ouverte. Cet article, qui résume les travaux publiés dans [3], contribue à y répondre en présentant des adaptations linéaires de réfutations SAT en réfutations Max-SAT, dans les cas tree-like regular, tree-like et semi-tree-like. On étend également ces résultats en proposant une adaptation complète pour toute réfutation SAT qui est exponentielle dans le pire des cas. * Cet article est un résumé de [3]."
"Les solveurs Conflict Driven Clause Learning (CDCL) sont efficaces pour résoudre des instances structurées avec un grand nombre de variables et de clauses. Un composant important de ces solveurs est l'heuristique de branchement qui sélectionne la prochaine variable de décision. Dans cet article, on propose d'utiliser un bandit manchot pour combiner deux heuristiques de l'état de l'art pour SAT, à savoir Variable State Independent Decaying Sum (VSIDS) et Conflict History-Based (CHB). Le bandit évalue et choisit de manière adaptative une heuristique adéquate à chaque redémarrage. Une évaluation expérimentale est menée et montre que la combinaison de VSIDS et CHB avec un bandit est compétitive et surpasse les deux heuristiques."
"Cet ouvrage, organisé en 3 volumes, est issu de la communauté française des chercheurs en intelligence artificielle (IA). Il a pour objectif de dresser un panorama des recherches effectuées en IA allant de travaux fondamentaux aux applications et aux frontières, en mettant l’accent tout autant sur les résultats obtenus que sur les problématiques actuelles. Il s’adresse à un public d’étudiants de master et de doctorat, mais aussi de chercheurs et d’ingénieurs intéressés par ce domaine. L’ouvrage est organisé en trois volumes : - le premier volume regroupe vingt chapitres traitant des fondements de la représentation des connaissances et de la formalisation des raisonnements (Volume 1. Représentation des connaissances et formalisation des raisonnements) ; - le deuxième volume offre une vue de l’IA, en onze chapitres, sous l’angle des algorithmes (Volume 2. Algorithmes pour l’Intelligence Artificielle) ; - le troisième volume, en onze chapitres également, décrit les principales frontières et applications de l’IA (Volume 3. L’Intelligence Artificielle : frontières et applications). Si chaque chapitre peut être lu indépendamment des autres, les références croisées entre chapitres sont nombreuses et un index global de l’ouvrage permet d’aborder celui-ci de façon non linéaire."
"La thèse se place à l’intersection de trois sujets de recherche : logiques conditionnelles, théorie de la démonstration et sémantique de voisinage. La famille de logiques conditionnelles considérées provient dès ouvrages de Stalnaker et Lewis. Elle est une extension de la logique classique propositionnelle avec un opérateur modal à deux places, qui exprime une notion affinée de conditionnalité. La sémantique de ces logiques est définie en termes de modèles de voisinage. Le but de la recherche est d’étudier la théorie de la démonstration des logiques conditionnelles, en précisant leur calculs des sequents. Les calculs définis sont des extensions du calcul des sequents de Gentzen ; ils sont étiquetés, c’est à dire définis en enrichissant le langage, ou internes, qui rajoutent des connecteurs structurels aux sequents. La thèse est organisée en six chapitres. Le chapitre 1 présente les axiomes et la sémantique des logiques conditionnelles et le chapitre 2 introduit la théorie de la démonstration. Les contributions originelles au sujet sont traitées dans les chapitres 3– 6. Le chapitre 3 introduit des calculs de sequents étiquetés basées sur la sémantique de voisinage pour les logiques conditionnelles préférentielles. Le chapitre 4 présente différents systèmes internes de calcul pour les logiques counterfactuelles, une sous-famille des logiques préférentielles. Le chapitre 5 analyse la relation parmi les systèmes de preuve en présentant les deux cotés d’une traduction entre un calcul étiqueté et un calcul interne. Finalement, au chapitre 6, les méthodes de la théorie de la démonstration conditionnelle sont appliquées à une logique épistémique multi-agente."
"Dans cet article, nous introduisons une méthodologie par-ticulì ere fondée sur l'analyse séquentielle de plusieurs corpus : ` a partir d'un corpus d'interaction humain-humain pour construire un système semi-autonome ayant pour objectif de recueillir un nouveau corpus d'interaction humain-machine, uné etape avant le développement d'un systèmeentì erement autonome construit sur la base de l'analyse des corpus collec-tés. La méthodologie présentée est illustrée dans le contexte du développement d'une plateforme pour la formation en réalité virtuelle des médecinsmédecins`médecinsà l'annonce d'´ evènements in-désirables graves."
"La notion de rection a été mise en usage par des grammairiens du 12 e siècle (P. Hélie, A. de Villedieu) pour décrire les rapports verbe-arguments en latin. Dès l'origine, cette notion allie deux aspects, l'un sémantique, l'autre formel [Chevalier 1968 : 55 ; Auroux 1992 : 164 sq. ; Bouard 2007 : 81] :-Au plan sémantique, un verbe signifie une action mais pas les êtres qu'elle concerne, et il appelle donc des arguments (sujet, objet) nécessaires pour en compléter le sens ;-Au plan formel, les noms d'arguments dépendent du verbe parce qu'ils portent une marque de cas imposée par celui-ci. 1.2. En grammaire française. 1.2.1. Cette conception de la rection, avec ses deux aspects, se retrouve ne varietur chez les grammairiens du français jusqu'au 18 e siècle : Un verbe doit être suivi d'autant de noms déterminants, qu'il y a de sortes d'émotions que le verbe excite nécessairement dans l'esprit. J'ai donné, quoi ? et à qui ? [Du Marsais, Principes de grammaire] Ce sont les mots indéterminés [= les verbes, ndlr] qui, dans le langage des Grammairiens, gouvernent ou régissent les noms déterminans. […] C'est une métaphore prise d'un usage très-ordinaire de la vie civile. Un grand gouverne ses domestiques, & les domestiques attachés à son service lui sont subordonnés ; il leur fait porter sa livrée, le public la reconnoît & décide au coup d'oeil, que tel homme appartient à tel maître. Les cas que prennent les noms déterminatifs sont de même une sorte de livrée ; c'est par là que l'on juge que ces noms sont, pour ainsi dire, attachés au service des mots qu'ils déterminent. [Encyclopédie, art. Gouvernement]"
"La notion de décomposition arborescente est un sujet important pour l'étude et la résolution de problèmes NP-difficiles en intelligence artificielle, et notamment en programmation par contraintes. D'un point de vue théorique, son exploitation, dans le cadre général des modèles graphiques (réseaux bayésiens, CSP (pondérés),. . .), a conduit, sous certaines hypothèses, à la définition d'algorithmes de résolution polynomiaux. Par ailleurs, ces dernières années, des solveurs l'exploitant ont fait montre de leur intérêt pratique pour la résolution de problèmes de décision, d'optimisation ou de comptage. Dans cet article, nous nous intéressons aux décompo-sitions arborescentes optimales et à leur intérêt pratique pour la résolution d'instances CSP. La motivation de ce travail est double. D'une part, des progrès considérables ont été accomplis au niveau des méthodes calculant de telles décompositions, rendant envisageable leur exploitation pratique. D'autre part, la complexité temporelle des méthodes de résolution exploitant une décomposi-tion arborescente est directement liée à la largeur de la décomposition employée et donc employer une décom-position optimale permet théoriquement de minimiser cette complexité. Nous évaluons d'abord la capacité des méthodes calculant des décompositions optimales à dé-composer des instances CSP avant de mesurer l'apport de ces décompositions optimales au niveau de l'efficacité de la résolution. Ce papier est un résumé de [2]. 1 Contexte Une instance CSP (pour Problème de Satisfaction de Contraintes) est définie par la donnée d'un triplet (X, D, C), où X = {x 1 ,. .. , x n } est un ensemble de n * Ce travail est soutenu par l'Agence Nationale de la Recherche dans le cadre du projet DEMOGRAPH (ANR-16-C40-0028) variables, D = {d x1 ,. .. , d xn } est un ensemble de do-maines finis de taille au plus d, et C = {c 1 ,. .. , c e } est un ensemble de e contraintes. Chaque contrainte c i est un couple (S(c i), R(c i)), où S(c i) = {x i1 ,. .. , x i k } ⊆ X définit la portée de c i , et R(c i) ⊆ d xi 1 × · · · × d xi k est une relation de compatibilité. La structure d'une instance CSP est donnée par un hypergraphe, appelé hypergraphe de contraintes, dont les sommets correspondent aux variables et les arêtes aux portées des contraintes. Une affectation d'un sous-ensemble de X est dite cohérente si toutes les contraintes portant sur ce sous-ensemble sont satisfaites. Une solution est une affectation cohérente de toutes les variables. Déterminer si une instance CSP possède une solution est un problème NP-complet. Il en découle que les algorithmes de résolution habituellement em-ployés ont une complexité en O(exp(n)). Toutefois, certains algorithmes, comme BTD [3], sont capables de fournir de meilleures bornes de complexité tem-porelle en exploitant une décomposition arborescente [4] de l'hypergraphe de contraintes pour identifier des sous-problèmes indépendants. Définition 1 Une décomposition arborescente d'un graphe G = (X, C) est un couple (E, T) où T = (I, F) est un arbre (I est un ensemble de noeuds et F un ensemble d'arêtes) et E = {E i : i ∈ I} une famille de sous-ensembles de X, telle que chaque sous-ensemble (appelé cluster) E i est un noeud de T et vérifie : (i) ∪ i∈I E i = X, (ii) pour chaque arête {x, y} ∈ C, il existe i ∈ I avec {x, y} ⊆ E i , et (iii) pour tout i, j, k ∈ I, si k est sur un chemin de i à j dans T , alors E i ∩ E j ⊆ E k. La largeur d'une décomposition est égale à max i∈I |E i | − 1. La largeur arborescente dite tree-width w * de G est la largeur minimale pour toutes les décompositions arborescentes de G."
"Dans le but d'exploiter les opinions dans les tweets, cet article présente une classification à partir du sentiment contenu au sein des tweets. Nous présentons une méthode d'identifi-cation de nouveaux mots-germes. Ils sont utilisés pour la prédiction de l'intensité de sentiments des mots en co-occurrence avec ces mots-germes. Ensuite, le calcul de similarités entre sentiments est appliqué en utilisant: la mesure de la similarité entre deux mots et l'utilisation de plongement de mots (e.g. word2vec, GloVE) couplé à la mesure cosinus. Les résultats montrent l'importance de l'utilisation de mots-germes adaptés aux tweets, ainsi que la taille et le prétrai-tement de corpus. Pour conclure, nous avons obtenu les meilleurs résultats grâce à l'application de la méthode utilisant le plongement de mots couplée à la mesure cosinus. ABSTRACT. For the purpose of opinion exploring in tweets, this article presents a sentiment classification of tweets content. First, we present a method to identify new sentiment similarity seed words. These seed words are used for predicting sentiment intensity of other words and short phrases in co-occurrence. Then, for testing sentiment similarity, we use: Similarity Measures methods between words and cosine similarity measure between the word embedding representations (e.g. word2vec, GloVE). The experiments results highlight the importance of adapted for tweets seed words. In addition of the corpora size and its pre-treatement. As a conclusion, best results were achieved using cosine similarity measure between the word embedding representations. MOTS-CLÉS : Mots-germes, Twitter, Mesure de la Similarité, Plongement de mot, Word2vec, GloVe."
"Dans cet article, nous présentons un outil permettant la prédiction et la visualisation de l’activité cérébrale locale d’un individu au cours d’une conversation. Cet outil a été utilisé en particulier pour comparer l’activité cérébrale de conversation d’individu suivant que l’interlocuteur est un humain ou une tête parlante. Le module de prédiction de cet outil est construit à partir de classifieurs entraînés sur un corpus de conversations humain-humain et humain-machine incluant des données comportementales et neurophysiologiques. Le module prend en entrée des variables comportementales calculées à partir de données brutes, principalement la parole du participant et de l’interlocuteur, la vidéo de l’interlocuteur et les mouvements oculaires du participant, et permet de prédire l’activité cérébrale de l’individu. Le module de visualisation montre en temps réel la dynamique des zones actives cérébrales synchronisées avec les données comportementales brutes, permettant ainsi de faire un lien entre les comportements et l’activité cérébrale. De plus, il montre quelles variables comportementales permettent de prédire l’activité des zones cérébrales."
"Une grande partie des traces des utilisateurs exprimées par des signaux sociaux (ex. j’aime, +1, rating) sont attribuées aux ressources web. Ces signaux sont souvent exploités par les systèmes de RI comme des sources d’évidence additionnelles pour trier les résultats de recherche. Notre objectif dans cet article est d’étudier l’impact des nouveaux signaux sociaux, appelés Facebook reactions (j’adore, haha, grrr, wouah, triste) sur le tri de ces résultats. Ces réactions permettent aux utilisateurs d’exprimer des émotions plus nuancés par rapport aux signaux classiques (ex. like, share). Nous analysons tout d’abord ces réactions et montrons la manière dont les internautes les utilisent pour interagir avec les ressources (ex. posts, vidéo, etc). Ensuite, nous évaluons l’impact de ces réactions dans le tri des résultats de recherche en les comparant à un modèles de tri textuel et à un modèle qui prend en compte le signal j’aime. Ces caractéristiques sociales sont modélisées comme une probabilité a priori du document et elles sont intégrées dans un modèle de langue. Nous avons effectué une série d’expérimentations sur la collection INEX IMDb. Nos résultats révèlent que la prise en compte de ces signaux améliore significativement la qualité des résultats de recherche"
"L'introduction des technologies émergentes en classe - comment les interfaces tangibles et de robotique éducative impliquent des changements sur l'apprentissage, sur la structuration du cours, sur la posture des enseignants. Les enseignants expriment le besoin de ressources bien structurées pour repenser l'ingénierie didactique du cours ainsi qu'une valorisation de l'apprentissage collaboratif. Donc, face à l'évolution technologique et à ces enjeux amenés aussi par la crise du COVID-19, il semble important d'identifier les contours actuels du travail collaboratif afin de le soutenir et de contribuer à catégoriser les pratiques enseignantes les plus favorables avec les interfaces tangibles & la robotique éducative pour soutenir tous les élèves et notamment ceux en difficulté. Nous présentons un projet en cours de développement qui adresse les questions suivantes : quelles sont les modalités et les scénarios d'apprentissage techno-créative en matière de programmation et de robotique éducative à mettre en œuvre auprès des enfants ? Comment la robotique éducative peut-elle améliorer les apprentissages fondamentaux ? L'objectif principal est de développer une série de scénarii pédagogiques impliquant l'utilisation de la robotique éducative et des interfaces tangibles (TUI) dans des contextes d'apprentissage collaboratif permettant d'améliorer les apprentissages fondamentaux à destination des formateurs. Le projet souhaite faciliter la mise en place d'approches pédagogiques mobilisatrices d'apprentissages collaboratifs supportés par le numérique, et ce au sein même de la classe. Dans ce but, le projet explore les potentialités des interfaces tangibles (TUI) et de la robotique pour accompagner les pratiques enseignantes associées. Pour répondre au mieux aux besoins et aux attentes des élèves et des enseignants, tant ergonomiques, pédagogiques que technologiques, le projet couple les principes et méthodes d'interaction homme-machine et d'ingénierie pédagogique. Il se fonde sur les principes de la conception participative et du co-design en living lab et en classes de Cycles 3 et 4. Il associe ainsi praticiens, chercheurs, ingénieurs, mais aussi élèves et start-up et associations tout au long du projet. Nos travaux s'inscrivent dans le cadre de l'articulation recherche-formation-terrain. La collecte de scénarios (directement auprès des enseignants) permet d'avoir une vision réelle des usages en classe et la modélisation de ces scénarios à l'aide de la recherche scientifique permet d'identifier les variables significatives en faveur de l'apprentissage collaboratif pour les fondamentaux. Enfin, la conception soutenue pour un outil d'étalage peut aider les formateurs dans la pratique professionnelle en classe en temps réel avec une application physique et / ou numérique et avec une collecte de données en temps réel pour soutenir l'apprentissage collaboratif."
"L’heuristique de choix de variables est une brique importante pour les algorithmes de résolution du problème de satisfaction de contraintes (CSP). Elle a une influence souvent considérable sur l’efficacité de la recherche et permet, d’une certaine manière, d’exploiter la structure des instances. Dans cet article, nous proposons l’heuristique CHS (pour Conflict-History Search) qui est une heuristique dynamique et adaptative pour la résolution d’instances CSP. Elle repose sur les échecs rencontrés durant la recherche et considère leur temporalité tout au long de la résolution. Une technique d’apprentissage par renforcement est exploitée pour estimer l’évolution de la difficulté des contraintes durant la recherche. Les expérimentations réalisées sur des instances au format XCSP3 permettent de montrer que l’intégration de CHS au sein d’un solveur basé sur l’algorithme MAC s’avère pertinente, conduisant notamment à des résultats meilleurs que ceux obtenus avec des heuristiques de l’état de l’art comme dom/wdeg et ABS."
"Récemment, une heuristique efficace, appelée Conflict History Search (CHS), a été introduite pour la résolution du problème de satisfaction de contraintes (CSP). Elle repose sur une technique d'apprentissage par renforcement appelée Exponential Recency Weighted Average (ERWA) pour estimer la dureté des contraintes. CHS favorise les variables qui apparaissent souvent dans les échecs récents. Le paramètre de pas utilisé dans CHS est important car il contrôle l'estimation de la dureté des contraintes. Dans cet article, nous envisageons un raffinement de ce paramètre à l'aide d'un bandit manchot. Le bandit sélectionne une valeur appropriée de ce paramètre lors des redémarrages effectués par l'algorithme de recherche. Chaque bras correspond à l'heuristique CHS avec une valeur donnée pour le paramètre de pas et est récompensé selon sa capacité à mener une recherche efficace. Une phase d'entraînement est introduite en amont de la recherche pour aider le bandit à choisir un bras pertinent. L'évaluation expérimentale montre que cette approche conduit à des améliorations significatives."
"Les agents virtuels peuvent être utilisés pour promouvoir des comportements pro-sociaux. Dans cet article, nous explorons le potentiel des agents virtuels et de la réalité virtuelle pour lutter contre les discriminations sociales à travers l’induction d’empathie chez l’utilisateur. Nous présentons différents systèmes développés dans cette perspective et discutons les facteurs d’influence, les outils de mesures des performances de ces systèmes, ainsi que leslimites actuelles de ces travaux."
"Comme tous les ans, les Journées Francophones de Programmation par Contraintes (JFPC) sont l'occasion pour notre communauté de se réunir et d'échanger dans un cadre à la fois scientifique et convivial. Cette année, la co-localisation des Journées d'Intelligence Artificielle Fondamentale (JIAF) nous offre en plus l'opportunité d'interagir avec la communauté francophone travaillant sur l'Intelligence Artificielle Fondamentale et de renforcer les liens existants entre nos deux communautés."
"L'article présente les principes et les critères qui ont présidé à l'élaboration de la table des parties du discours et à l'organisation du lexique correspondante, mis en oeuvre dans l'analyse syntaxique automatique du corpus Orféo. La comparaison est établie avec le Lexique des Formes Fléchies du Français (Lefff) utilisé dans d'autres outils de traitement automatique du langage. Les enjeux linguistiques et informatiques sont abordés. Un développement particulier est consacré au traitement des locutions ou expressions multi-mots. Des perspectives d'amélioration sont envisagées."
"Notre travail se situe dans le contexte de recherche d'information sociale (RIS) et s'intéresse plus particulièrement à l'exploitation du contenu généré par les utilisateurs dans le processus de la recherche d'information. Le contenu généré par les utilisateurs (en anglais User-generated content, ou UGC) se réfère à un ensemble de données (ex. signaux sociaux) dont le contenu est principalement, soit produit, soit directement influencé par les utilisateurs finaux. Il est opposé au contenu traditionnel produit, vendu ou diffusé par les professionnels. Le terme devient populaire depuis l'année 2005, dans les milieux du Web 2.0, ainsi que dans les nouveaux médias sociaux. Ce mouvement reflète la démocratisation des moyens de production et d'interaction dans le Web grâce aux nouvelles technologies. Parmi ces moyens de plus en plus accessibles à un large public, on peut citer les réseaux sociaux, les blogs, les microblogs, les Wikis, etc. Les systèmes de recherche d'information exploitent dans leur majorité deux classes de sources d'évidence pour trier les documents répondant à une requête. La première, la plus exploitée, est dépendante de la requête, elle concerne toutes les caractéristiques relatives à la distribution des termes de la requête dans le document et dans la collection (tf-idf). La seconde classe concerne des facteurs indépendants de la requête, elle mesure une sorte de qualité ou d'importance a priori du document. Parmi ces facteurs, on en distingue le PageRank, la localité thématique du document, la présence d'URL dans le document, ses auteurs, etc. Une des sources importantes que l'on peut également exploiter pour mesurer l'intérêt d'une page Web ou de manière générale une ressource, est le Web social. En effet, grâce aux outils proposés par le Web 2.0 les utilisateurs interagissent de plus en plus entre eux et/ou avec les ressources. Ces interactions (signaux sociaux), traduites par des annotations, des commentaires ou des votes associés aux ressources, peuvent être considérés comme une information additionnelle qui peut jouer un rôle pour mesurer une importance a priori de la ressource en termes de popularité et de réputation, indépendamment de la requête. Nous supposons également que l'impact d'un signal social dépend aussi du temps, c'est-à-dire la date à laquelle l'action de l'utilisateur est réalisée. Nous considérons que les signaux récents devraient avoir un impact supérieur vis-à-vis des signaux anciens dans le calcul de l'importance d'une ressource. La récence des signaux peut indiquer certains intérêts récents à la ressource. Ensuite, nous considérons que le nombre de signaux d'une ressource doit être pris en compte au regard de l'âge (date de publication) de cette ressource. En général, une ressource ancienne en termes de durée d'existence a de fortes chances d'avoir beaucoup plus de signaux qu'une ressource récente. Ceci conduit donc à pénaliser les ressources récentes vis-à-vis de celles qui sont anciennes. Enfin, nous proposons également de prendre en compte la diversité des signaux sociaux au sein d'une ressource."
"L’étude de l’observabilité et la synthèse d’observateur ont pour vocation la reconstruction de l’état d’un système à l’aide des mesures reçues. Ces dernières n’apportent généralement qu’une connaissance partielle de cet état. Le filtre de Kalman est un observateur particulièrement étudié et employé. Plusieurs versions existent, adaptées aux systèmes linéaires ou non linéaires, en version discrète, continue voire continue-discrète, dans le cadre stochastique ou déterministe. Ces observateurs reposent cependant sur l’hypothèse que les mesures fournies par les capteurs sont synchrones. Or, cette supposition est assez éloignée de la réalité physique, notamment des problèmes étudiés en robotique.Nous proposons dans ce manuscrit un observateur adapté aux systèmes non linéaires continusdiscrets asynchrones. Nous entendons par cela des systèmes dont l’état est continu et les sorties échantillonnées à des fréquences différentes. En nous basant sur le Filtre de Kalman Etendu grand-gain existant pour les systèmes non linéaires continus et continus-discrets synchrones, nous développons un formalisme et construisons un observateur en adoptant un point de vue déterministe. Sa convergence est prouvée analytiquement et illustrée par une application sur un système robotique mobile."
"Un schéma d’étiquetage est un procédé permettant de distribuer la représentation d’un graphe sur ses sommets. Il consiste en une fonction d’encodage qui attribue des étiquettes binaires (courtes) à chaque sommet, et d’une fonction de décodage qui, étant données les informations locales contenues dans deux étiquettes, et sans connaissance supplémentaire sur le graphe, permet de répondre (rapidement) à un type de requête pré-établi. Une partie des résultats de cette thèse sont initialement motivés par la construction de telles représentations distribuées. Ce document traite cependant de problèmes d’intérêt plus généraux tels que l’étude de bornes sur la densité de graphes, de la VC-dimension de familles d’ensembles, ou de propriétés métriques et structurelles. Nous établissons dans un premier temps des bornes supérieures sur la densité des sous-graphes de produits cartésien de graphes, puis des sous-graphes de demi-cubes. Pour ce faire, nous définissons des extensions du paramètre classique de VC-dimension (utilisé par Haussler, Littlestone et Warmuth en 1994 pour majorer la densité des sous-graphes d’hypercube). De ces bornes sur la densité, nous déduisons des bornes supérieures sur la longueur des étiquettes attribuées par un schéma d’adjacence à ces deux familles de graphes. Dans un second temps, nous nous intéressons à des schémas de distance et de routage pour deux familles importantes de la théorie métrique des graphes : les graphes médians et les graphes pontés. Nous montrons que la famille des graphes médians, sans cube, avec n sommets, admet des schémas de distance et de routage utilisant tous deux des étiquettes de O(log^3 n). Ces étiquettes sont décodées en temps constant pour retourner, respectivement, la distance exacte entre deux sommets, ou le port vers un sommet rapprochant (strictement) une source d’une destination. Nous décrivons ensuite un schéma de distance 4-approché pour la famille des graphes pontés, sans K_4, avec n sommets, utilisant des étiquettes de O(log^3 n) bits. Ces dernières peuvent être décodées en temps constant pour obtenir une valeur entre la distance exacte et quatre fois celle-ci."
"Dans le cas des modèles graphiques, même les plus simples comme les CSP (réseaux de contraintes), les dé-compositions généralement proposées dans la littérature s'intéressent pour l'essentiel à la structure. Cela étant, afin de mieux appréhender un problème dans sa globalité, il serait souhaitable aussi de prendre en compte sa séman-tique, au sens des domaines des variables, et des relations de compatibilité qui leur sont associées via les contraintes. Dans cette note, il est proposé une nouvelle approche de la problématique qui prend justement en compte ces deux aspects pour définir une nouvelle forme de décom-position et son paramètre associé. L'une des ambitions d'une telle démarche et qui peut d'ailleurs constituer un réel défi pour la communauté, est de dépasser les limites actuellement posées par les décompositions arbo-rescentes de graphes de tree-width (ou largeur) bornée. On montre ainsi que le paramètre que nous proposons minore systématiquement la tree-width d'un réseau de contraintes binaires (cf. théorème 1) qui constitue ""la"" borne fondamentale actuelle pour le traitement de nom-breux problèmes combinatoires, dès lors qu'ils s'expriment en termes de graphes et qu'il s'agit de les résoudre par des approches fondées sur leur structure. Toutefois, si ce premier résultat semble démontrer la pertinence d'une telle approche, cette contribution doit être considérée comme une contribution très préliminaire à une éventuelle nouvelle voie de recherches."
"Cette thèse aborde le problème de la détection des feux de forêt par des outils de traitement d’images et apprentissage machine. Un incendie de forêt est un feu qui se propage sur une étendue boisée. Il peut être d'origine naturelle (dû à la foudre ou à une éruption volcanique) ou humaine. Dans le monde entier, l’impact des feux de forêts sur de nombreux aspects de notre vie quotidienne se fait de plus en plus apparente sur l’écosystème entier. De nombreuses méthodes ont montré l’efficacité pour la détection des incendies de forêt. L’originalité du présent travail réside dans la détection précoce des incendies par la détection de la fumée de forêt et la classification des régions de fumée et de non fumée à l’aide d’apprentissage profond et des outils de traitement d’image. Un ensemble de techniques de prétraitement nous a aidé à avoir une base de donnée importante (ajout du bruit aux entrées, augmentation des données) qui nous a permis après de tester la robustesse du modèle basée sur le DBN qu’on a proposé et évaluer la performance en calculant les métriques suivantes (IoU, Précision, Rappel, F1 score). Finalement, l’algorithme proposé est testé sur plusieurs images afin de valider son efficacité. Les simulations de notre algorithme ont été comparées avec celles traités dans l’état de l’art (Deep CNN, SVM…) et ont fourni de très bons résultats."
"Cette thèse s'inscrit dans le domaine des EIAH. Elle vise à lever certaines limitations existantes concernant l'évaluation des Jeux Sérieux de Gestion de Crise (JSGC). Les JSGC constituent une alternative innovante dans laquelle les joueurs essaient de gérer une crise. Les constats motivant ce travail sont: (1) la nécessité grandissante pour les chercheurs d'étudier les facteurs de succès contribuant à l'efficacité des JSGC, (2) l'insuffisance des travaux d'évaluation prenant en compte l'aspect émotionnel, et (3) la nécessité d'exploiter les résultats d'évaluation pour adapter le jeu aux profils des joueurs dans le but d'améliorer leur apprentissage. Face à ces constats, cette thèse tente d’apporter des contributions aussi bien sur le plan théorique que pratique qui s’articulent autour de deux grands axes. Premièrement, nous proposons un cadre théorique pour l’évaluation dans les JSGC. Ce cadre est basé sur une grille d’analyse détaillant les critères et les indicateurs permettant de les mesurer et qui peuvent être classés sous trois facettes d’évaluation : pédagogique, émotionnelle et sociale. Deuxièmement, nous proposons une approche d’évaluation des joueurs dans les JSGC basée sur deux méthodes permettant d’évaluer les facettes sociale et émotionnelle. La première méthode vise à produire une évaluation aussi bien quantitative que qualitative des interactions entre les joueurs. La deuxième méthode vise à produire une évaluation formative et sommative des états affectifs des apprenants à deux niveaux : individuel et collectif. Les résultats d’évaluation sont exploités pour proposer une adaptation d’un jeu simulant un exercice d’évacuation d’un bâtiment en cas d’incendie."
"Dans les expériences de neuroimagerie fonctionnelle, les participants effectuent un ensemble de tâches pendant que leur activité cérébrale est enregistrée, par exemple en utilisant l’électroencéphalographie (EEG), la magnétoencéphalographie (MEG) ou l'imagerie par résonance magnétique fonctionnelle (fMRI). L'analyse des données d'un groupe de participants, souvent appelée analyse de groupe, vise à identifier des invariants de population qui se rapportent aux tâches accomplies par les participants. Ceci permet de comprendre l'organisation fonctionnelle du cerveau chez les sujets sains et ses dysfonctionnements dans les populations pathologiques. Tandis que les analyses de groupes univariées, basées sur le modèle linéaire généralisé, ont fait l'objet d'études approfondies, de nombreuses questions restent ouvertes pour les analyses de groupe fondées sur des méthodes d'apprentissage machine multivariées. Cette thèse étudie donc sur les analyses de groupe multivariées pour les expériences de neuroimagerie fonctionnelle. Nous nous focalisons sur un schéma d’analyse de groupe multivarié sous utilisé, que nous désignons “analyse de motifs inter-sujet”, qui consiste à entraîner un modèle sur des données d’un ensemble de sujet et à évaluer sa capacité à généraliser sur des données enregistrées dans d’autres sujets. Nous effectuons d’abord une comparaison des résultats fournis par l'analyse de motifs inter-sujet avec ceux obtenus en utilisant la méthode standard. L'analyse inter-sujet offre à la fois une plus grande capacité de détection et facilite l'interprétation des résultats obtenus à un coût de calcul comparable. Dans ce contexte, notre deuxième contribution introduit une formalisation unifiée de l'analyse de motifs inter-sujet, que nous modélisons comme un problème d'apprentissage par transfert transductif multi-sources. Ensuite, nous produisons une revue de la littérature des méthodes développées pour l’analyse de motifs inter-sujet. Puis, nous effectuons une série d’études expérimentales qui examine le bien-fondé de la formalisation par transfert transductif multi-sources de l'analyse de motifs inter-sujet. La quatrième contribution de cette thèse est une nouvelle méthode d'analyse multivariée au niveau du groupe pour les expériences de neuroimagerie fonctionnelle. Notre méthode est basée sur le transport optimal, qui tire parti des propriétés géométriques des cartes d’activité cérébrales pour surmonter les différences inter-individuelles qui ont un impact sur les analyses de groupe traditionnelles."
"Les commentaires sur des ressources Web (ex. des cours, des films) deviennent de plus en plus exploitées dans des tâches d'analyse de texte (ex. détection d'opinion, dé-tection de controverses). Cet article étudie l'intensité de contradiction dans les commentaires en exploitant diffé-rents critères tels que la variation des notations et la variation des polarités autour d'entités spécifiques (ex. aspects, sujets). Premièrement, les aspects sont identifiés en fonc-tion des distributions des termes émotionnels à proximité des noms les plus fréquents dans la collection des commen-taires. Deuxièmement, la polarité est estimée pour chaque segment de commentaire contenant un aspect. Seules les ressources ayant des commentaires contenant des aspects avec des polarités opposées sont prises en compte. Enfin, les critères sont évalués, en utilisant des algorithmes de sé-lection d'attributs, pour déterminer leur impact sur l'effi-cacité de la détection de l'intensité des contradictions. Les critères sélectionnés sont ensuite introduits dans des mo-dèles d'apprentissage pour prédire l'intensité de contradiction. L'évaluation expérimentale est menée sur une collection contenant 2244 cours et leurs 73873 commentaires, collectés à partir de coursera.org. Les résultats montrent que la variation des notations, la variation des polarités et la quantité de commentaires sont les meilleurs prédicteurs de l'intensité de contradiction. En outre, J48 est l'approche d'apprentissage la plus efficace pour cette tâche"
"L'analyse des avis (commentaires) générés par les utilisateurs devient de plus en plus exploitable par une variété d'applications. Elle permet de suivre l'évolution des avis ou d'effectuer des enquêtes sur des produits. La détection d’avis contradictoires autour d’une ressource Web (ex. cours, film, produit, etc.) est une tâche importante pour évaluer cette dernière. Dans cet article, nous nous concentrons sur le problème de détection des contradictions et de la mesure de leur intensité en se basant sur l’analyse du sentiment autour des aspects spécifiques à une ressource (document). Premièrement, nous identifions certains aspects, selon les distributions des termes émotionnels au voisinage des noms les plus fréquents dans l’ensemble des commentaires. Deuxièmement, nous estimons la polarité de chaque segment de commentaire contenant un aspect. Ensuite, nous prenons uniquement les ressources contenant ces aspects avec des polarités opposées (positive, négative). Troisièmement, nous introduisons une mesure de l'intensité de la contradiction basée sur la dispersion conjointe de la polarité et du rating des commentaires contenant les aspects au sein de chaque ressource. Nous évaluons l’efficacité de notre approche sur une collection de MOOC (Massive Open Online Courses) contenant 2244 cours et leurs 73873 commentaires, collectés à partir de Coursera. Nos résultats montrent l'efficacité de l’approche proposée pour capturer les contradictions de manière significative."
"Les outils informatiques peuvent assister la découverte de nouvelles unités phraséologiques dans les corpus grâce à leur facilité pour calculer rapidement des statistiques à partir de grands volumes de données textuelles. Alors que la communauté de recherche s'est concentrée sur le développement et l'évaluation d'algorithmes originaux pour la découverte automatique d'unités phraséologiques, la transformation de ces méthodes sophistiquées en logiciels utilisables est souvent ignorée. Ce chapitre présente un bref résumé des principales approches informatiques disponibles pour la découverte d'unités phraséologiques. Nous présenterons des exemples détaillés de l'application de ces approches avec le mwetoolkit, un logiciel libre pour la découverte et l'identification d'unités polylexicales. L'utilité des unités extraites automatiquement dépend de plusieurs facteurs comme la langue, la taille du corpus, les unités cibles, et les étiqueteurs et analyseurs disponibles. Néanmoins, le mwetoolkit permet un paramétrage fin, de manière à ce que cette variabilité soit prise en compte dans l'adaptation de l'outil à chaque environnement lexicographique."
"Cet article propose une approche de recherche d'information (RI) en langue arabe sur Facebook, qui exploite toutes les traces des utilisateurs (ex. polarité, partage, j'aime, haha) laissées sur des publications Facebook pour estimer leur importance sociale. Notre objectif est de montrer comment ces signaux peuvent jouer un rôle vital dans l'amélioration de la recherche en langue arabe sur Facebook. Premièrement, des polarités (positive ou négative) portée par les signaux textuels (ex. commentaires) et non textuels (ex. les réactions j'adore et triste) ont été identifiées pour chaque publication Facebook. Par conséquent, la polarité de chaque com-mentaire exprimé sur une publication donnée, est estimée sur la base d'un modèle neuronal de sentiment en langue arabe. Deuxièmement, des signaux en fonction de leur complémentarité ont été regroupés en utilisant des algorithmes de sélection. Troisièmement, des algorithmes de learning to rank ont été appliqués pour re-ordonner les résultats de recherche de Facebook en fonction des groupes de signaux sélectionnés. Enfin, des expérimentations sont réalisées sur 13500 publications Facebook, collectées à partir de 45 requêtes en langue arabe. Les expéri-mentations révèlent des résultats prometteurs pour la RI en langue arabe sur Facebook."
"Les solveurs basés sur la méthode séparation et évaluation (BnB) pour Max-SAT exploitent la max-résolution, la règle d'inférence pour Max-SAT, pour s'as-surer que chaque sous-ensemble inconsistant (IS) détecté n'est compté qu'une seule fois. Cependant, les transformations par max-résolution peuvent affecter négative-ment leurs performances. Elles sont alors généralement apprises de manière sélective : quand elles respectent cer-tains motifs. Dans ce papier, on s'intéresse à des motifs particuliers appelés k-UCSs binaires. On prouve que ces motifs vérifient une caractérisation récente des transformations par max-résolution appelée UP-résilience et on décrit comment ce résultat peut aider à étendre les motifs actuels. Enfin, ce travail s'inscrit d'une démarche globale de caractérisation de la pertinence des transformations par max-résolution."
"Le changement de croyances dans le cadre de fragments de la logique propositionnelle est l'objet d'un intérêt croissant. Le raffinement d'opérateurs de changement de croyances a été proposé pour adapter des opérateurs connus afin qu'ils opèrent dans des fragments et que leur résultat reste dans le fragment considéré. Alors que la notion de raffinement permet de définir des opérateurs concrets de révision et de mise à jour adaptés aux fragments propositionnels cette notion nécessite d'être adaptée pour la contraction et l'effacement. Nous proposons une notion plus spécifique de raffinement pour ces opérations, appelée raffinement raisonnable. Celle-ci nous permet de définir des opérateurs de contraction et d'effacement raffinés qui satisfont les postulats de base. Nous étudions les propriétés logiques de deux opérateurs concrets pour la contraction et pour l'effacement. Notre approche ne se limite pas au fragment de Horn mais s'applique à de nombreux fragments propositionnels."
Habilitation à Diriger des Recherches spécialité « Automatique » par Frédéric Lafont Soft-computing et observateurs appliqués à des systèmes environnementaux soutenue le
"Afin de motiver l'utilisateur à changer son comportement ou ses attitudes, par exemple pour la pratique d'activités physiques pour améliorer son bien-être, les agents virtuels doivent être dotés de capacités de persuasion. La force persuasive de l'agent virtuel ne dépend pas seulement de son argumentation, mais aussi de son comportement verbal et non verbal, de son attitude sociale et de sa capacité à adapter son comportement à celui de l'utilisateur. La persuasion peut s'analyser soit de manière directe, soit de manière indirecte via des dimensions telle la confiance ou la sociabilité. Nous proposons dans cet article de faire la revue des différents recherches sur les comportements persuasifs en comparant les travaux sur les interactions humain-humain à ceux concernant les interactions humain-machine."
"Les actes de la 9e édition de la Conférence Internationale Francophone sur la Science des Données (CIFSD, https://cifsd-2021.sciencesconf.org) regroupe l'ensemble des contributions présentées à la conférence entre le 9 et le 11 juin 2021. Cette édition a été organisée par Aix-Marseille Université et le Laboratoire d'Informatique et Systèmes (LIS UMR 7020). En raison de la situation sanitaire, elle s'est déroulée en distanciel depuis Marseille (France). La thématique mise en avant pour cette édition a été la science de données pour la santé."
"La dématérialisation des processus de recrutement n'a pas fait disparaître toutes les frictions inhérentes à cette activité. La recherche automatisée d'un candidat idéal se heurte toujours à la difficulté à modéliser correctement les besoins exprimés en langage naturel dans une offre d'emploi. Le recrutement d'experts, notamment, est particulièrement difficile. En effet, ces profils concernent une proportion réduite des recrutements et leur prise en charge informatisée nécessite une connaissance précise du secteur d'activité concerné. Dans cet article, nous proposons l'architecture d'un système de recommandation de profils experts dans l'industrie des procédés afin d'assister ce type de recrutements."
"La représentation des textures dynamiques (TD), considérée comme une séquence de textures en mouvement, est un défi en analyse des vidéos dans des applications diverses de la vision par ordinateur. Cela est en partie causé par la désorientation des mouvements, les impacts négatifs des problèmes bien connus dans la capture des caractéristiques turbulentes: bruit, changements d'environnement, illumination, transformations de similarité, mise en échelles, etc. Dans le cadre de cette thèse, nous introduisons des solutions significatives afin de traiter les problèmes ci-dessus. Par conséquent, trois approches principales suivantes sont proposées pour le codage efficace des TDs : i) à partir de trajectoires denses extraites d'une vidéo donnée; ii) basé sur des réponses robustes extraites par des modèles de moment; iii) basé sur des résultats filtrés qui sont calculés par des variantes de noyaux de filtrage gaussien. En parallèle, nous proposons également plusieurs opérateurs discriminants pour capturer les caractéristiques spatio-temporelles des codages de TD ci-dessus. Pour une représentation TD basée sur des trajectoires denses, nous extrayons d'abord des trajectoires denses à partir d'une vidéo donnée. Les points de mouvement le long des trajectoires sont ensuite codés par notre opérateur xLVP, une extension des modèles vectoriels locaux (LVP) dans un contexte de codage complémentaire, afin de capturer des caractéristiques directionnelles basées sur une trajectoire dense pour la représentation efficace de TD. Pour la description TD basée sur des modèles de moment, motivée par un modèle d'images de moment, nous proposons un nouveau modèle de volumes de moment basé sur des informations statistiques des régions de support sphériques centrées sur un voxel. Deux de ces modèles sont ensuite pris en compte dans l'analyse vidéo pour mettre en évidence des images/volumes de moment. Afin d'encoder les images basées sur le moment, nous nous adressons à l'opérateur CLSP, une variante des modèles binaires locaux terminés (CLBP). De plus, notre opérateur xLDP, une extension des modèles de dérivés locaux (LDP) dans un contexte de codage complémentaire, est introduit pour capturer les caractéristiques spatio-temporelles basés sur les volumes des moments. Pour la représentation DT basée sur les filtrages Gaussiens, nous étudierons de nombreux types de filtrages dans l'étape de prétraitement d'une vidéo pour mettre en évidence des caractéristiques robustes. Après cette étape, les sorties sont codées par des variantes de LBP pour construire les descripteurs de TD. Plus concrètement, nous exploitons les noyaux gaussiens et des variantes de gradients gaussiens d'ordre élevé pour le filtrage. En particulier, nous introduisons un nouveau noyau de filtrage (DoDG) en tenant compte de la différence des gradients gaussiens, qui permet de mettre en évidence des composants robustes filtrés par DoDG pour construire des descripteurs efficaces en maintenant une petite dimensionalité. Parallèlement aux filtrages gaussiens, certains novels opérateurs sont introduits pour répondre à différents contextes du codage TD local: CAIP, une adaptation de CLBP pour résoudre le problème proche de zéro causé par des caractéristiques bipolaires; LRP, basé sur un concept de cube carré de voisins locaux; CHILOP, une formulation généralisée de CLBP. Les résultats de reconnaissance TD ont validé que nos propositions fonctionnent de manière significative par rapport à l'état de l'art. Certaines d'entre elles ont des performances très proches des approches d'apprentissage profond. De plus, nos descripteurs qui ont une dimensionalité très petite par rapport à celle des méthodes d'apprentissage profond sont appréciées pour les applications mobiles."
"Les consommateurs ont l'habitude de consulter les critiques postées sur internet avant d'acheter un produit. Mais, il est difficile pour le consommateur de connaître l'opinion globale du produit vu le nombre important de ces critiques. L'analyse des sentiments permet de détecter la polarité (positive, négative ou neutre) sur une opinion exprimée et donc de classer ces critiques. Notre but est de déterminer l'influence de l'expression des émotions sur l'analyse de la polarité des critiques de livres. Nous définissons des modèles de représentation ""sac de mots"" de critiques qui s'appuient sur un lexique contenant des mots porteurs de sentiments (positif, négatif) et d'émotions (anticipation, tristesse, peur, colère, joie, surprise, confiance, dégoût). Ce lexique permet de mesurer les types d'émotions ressenties par les lecteurs. L'apprentissage supervisé mis en oeuvre est de type forêt aléatoire (Random Forest). L'application concerne des critiques de la plateforme Amazon."
Nous étudions l'impact de l'ajout de plusieurs modalités à la transcription de la parole pour la prédiction d'émotions. Ce travail a pour objectif de contribuer aux thérapies pour l'éducation émotionnelle des adolescents TSA (Trouble du Spectre Autistique) à partir d'une recommandation automatique de passages de séries TV.
"Les agents virtuels peuvent être utilisés pour promouvoir des comportements pro-sociaux. Dans cet article, nous explorons le potentiel des agents virtuels et de la réalité virtuelle pour lutter contre les discriminations sociales à travers l'induction d'empathie chez l'utilisateur. Nous présentons différents systèmes développés dans cette perspective et discutons les facteurs d'influence, les outils de mesures des performances de ces systèmes, ainsi que les limites actuelles de ces travaux."
"Le cachalot, Physeter macrocephalus, possède le plus grand biosonar de la nature. Composé de plusieurs poches d’huile, le sonar du cachalot est conçu pour fonctionner de la surface de la mer jusqu’à une profondeur de 2 kilomètres, émettant des clics pouvant aller jusqu’à 236 dB, et est polyvalent, car il produit des clics pour l’écholocation ou la socialisation. Cependant, la cire liquide qui compose le sonar a fait des cachalots la cible de la chasse jusqu’en 1986, lorsque la population restante était beaucoup trop petite pour rester commercialement viable, en particulier avec l’arrivée de produits similaires développés par l’industrie pétrochimique. La population de cachalots est toujours confrontée à certaines menaces humaines, comme l’ingestion de plastique et la collision avec des bateaux qui continuent de faire des ravages sur leur nombres. L’étude des cachalots donne ainsi des résultats dans de multiples domaines, en conservation, en éthologie, ainsi qu’en bioacoustique. Comprendre le mécanisme qui régit le sonar du cachalot aidera à étudier ces autres domaines, car il s’agit d’un élément clé de la vie du cachalot. Dans ce but, cette thèse analyse trois bases de données aux caractéristiques distinctes, obtenant la trajectoire des plongées de cachalots. Les clics enregistrés ont été également reliés au cachalot qui les avait émis, et ce sur plusieurs années d’enregistrement fait sur la même population. Un modèle original End-to-End par Deep Learning est construit pour classer efficacement les formes d’onde de biosonars. Une simulation de propagation des ondes à travers la tête du cachalot a également été développée pour mieux comprendre le mécanisme complexe de ce sonar. Enfin, une méthode de couplage a été développée pour améliorer les paramètres de la simulation en utilisant les clics enregistrés des bases de données précédemment citées. Un résumé de la thèse en français est présent au début du manuscrit."
"Un réseau d’automates (RA) est un réseau d’entités (les automates) en interaction. Ces automates ont un nombre fini d’états possibles et sont reliés les uns aux autres par une structure de graphe appelée graphe d’interaction. Chaque automate évolue au cours du temps discret en fonction des états de ses voisins dans le graphe d’interaction, ce qui définit un système dynamique. Ce travail de thèse explore deux questions principales : a) quel est le lien entre les propriétés dynamiques et calculatoires d’un RA ? et b) quel est l’impact de la topologie du graphe d’interaction sur la dynamique globale d’un RA ?. Pour aborder la première question, une notion de complexité calculatoire est définie au regard de problèmes de décision liés à la dynamique des RA. De même, une notion de complexité dynamique est définie en termes de l’existence d’attracteurs de période exponentielle. Un lien fort entre ces deux définitions est présenté qui met en exergue le concept de simulation entre familles de RA. Dans ce contexte, la complexité se caractérise d’un point de vue localisé en étudiant l’existence de structures appelées gadgets qui satisfont deux propriétés : i) ils peuvent interagir localement de manière cohérente comme des systèmes dynamiques et ii) ils sont capables de simuler un ensemble fini de fonctions définies sur un ensemble fini. La deuxième question est quant à elle abordée dans le contexte des RA “freezing”. Un RA est “freezing” s’il y a un ordre sur les états de telle sorte que l’évolution de l’état de n’importe quel automate ne diminue pas quelle que soit l’orbite. Un problème général de model-checking capturant de nombreux problèmes de décision classiques est présenté. De plus, lorsque trois paramètres de graphe, le degré maximum, la largeur arborescente et la taille de l’alphabet sont bornés, un algorithme parallèle efficace résolvant le problème mentionné est donné. De plus, il est montré que ce problème est peu susceptible d’être FPT (fixed-parameter tractable) lorsque le paramètre de largeur arborescente ou celui de taille de l’alphabet sont considérés comme unique paramètre."
no abstract
"En dépit de leur caractère distribué, les chaînes logistiques peuvent se révéler très performantes dans les conditions idéales de production et d’échange. Toutefois, leur complexité les rend de plus en plus fragiles. Cette thèse propose des modèles et des méthodes pour l’analyse des risques, de façon à renforcer la robustesse et la résilience des chaînes logistiques. Pour nous aider à mieux positionner nos travaux et à tirer les caractéristiques essentielles des chaînes logistiques, nous avons analysé ce domaine suivant une démarche ontologique à l’aide de la méthode KOD. En nous appuyant sur un état de l’art du domaine des risques dans les chaînes logistiques, et sur les bases de cas réels, nous avons identifié les indicateurs des vulnérabilités les plus significatifs. A partir des connaissances extraites, et des modèles mathématiques proposés dans la littérature, nous avons construit un modèle de chaîne logistique multi-étages à l’aide de modèles ARIMA intégrant l’aspect aléatoire de la demande. Pour adapter ce modèle aux situations de vulnérabilité et de risques, nous avons ajouté des contraintes de capacité et de positivité sur les commandes et sur les stocks. Sous l’effet d’événements dangereux, certaines contraintes du système peuvent être atteintes et par conséquence, son évolution peut s’écarter fortement de la dynamique nominale. Nous avons proposé des indicateurs de vulnérabilités comme des indicateurs de fréquence des retards de livraison, ou de surcoût d’immobilisation de produits. Enfin, l’occurrence d’événements dangereux a été représentée par des scénarios. Nous avons alors obtenu des résultats de simulation sous MATLAB, qui nous ont permis d’évaluer leurs conséquences pour différentes configurations du système, en particulier sous perturbation des flux d’informations (demande) et des flux physique (qualité de produits approvisionnés)."
no abstract
"L'annotation de données est un problème majeur dans toutes les tâches d'apprentissage automatique. Dans le domaine du TAL, ce problème est multiplié par le nombre de langues existantes. De nombreuses langues se retrouvent sans annotations, et sont alors mises à l'écart des systèmes de TAL. Une solution possible pour intégrer ces langues dans les systèmes est de tenter d'exploiter les langues disposant de nombreuses annotations, d'apprendre des informations sur ces langues bien dotées, et de transférer ce savoir vers les langues peu dotées. Pour cela, il est possible de se reposer sur des initiatives comme les Universal Dependencies, qui proposent un schéma d'annotation universel entre les langues. L'utilisation de plongements de mots multilingues et de traits typologiques issus de ressources comme le WALS sont des solutions permettant un partage de connaissances entre les langues. Ces pistes sont étudiées dans le cadre de cette thèse, à travers la prédiction de l'analyse syntaxique, de la morphologie et des parties du discours sur 41 langues au total. Nous montrons que l'impact du WALS peut être positif dans un cadre multilingue, mais que son utilité n'est pas systématique dans une configuration d'apprentissage zero-shot. D'autres représentations des langues peuvent être apprises sur les données, et donnent de meilleurs résultats que le WALS, mais ont l'inconvénient de ne pas fonctionner dans un cadre de zero-shot. Nous mettons également en évidence l'importance de la présence d'une langue proche lors de l'apprentissage des modèles, ainsi que les problèmes liés à l'utilisation d'un modèle de caractère pour les langues isolées."
"Ce document représente l’étude préalable du projet Musées Résilients aux Inondations (MRI) et constitue le premier de ses livrables. Il s’agit d’un projet de recherche contractuel établi entre le Ministère de la Transition Écologique et Solidaire, le réseau Alliance de Villes Euro-­‐méditerranéennes de Culture (AVEC) et l’Institut de Prévention et de Gestion des Risques urbains (IPGR)."
"Pour résoudre les problèmes de satisfaction de contraintes pondérés, les méthodes basées sur une dé-composition arborescente constituent une approche inté-ressante selon la nature des instances considérées. Sou-vent, les décompositions exploitées visent à réduire la taille maximale des clusters, connue comme étant la lar-geur de la décomposition. En effet, l'intérêt de ce pa-ramètre est lié à son importance par rapport à la com-plexité théorique de telles méthodes. À ce niveau, Min-Fill constitue l'heuristique de référence pour le calcul de décompositions. Cependant, son intérêt pratique pour la résolution de problèmes demeure limité au vu de ses multiples défauts, notamment au niveau de la restriction de la liberté de l'heuristique de choix de variables. Ainsi, nous proposons, dans un premier temps, d'ex-ploiter de nouvelles décompositions pour le problème d'optimisation sous contraintes. Le but de ces décom-positions est de capturer des critères permettant d'aug-menter l'efficacité de la résolution. Dans un second temps, nous proposons d'exploiter ces décompositions plus dynamiquement dans le sens où la résolution d'un sous-problème ne se baserait sur la décomposition que lorsque cela semble utile. Les expérimentations réalisées montrent l'intérêt pratique des nouvelles décompositions ainsi que l'apport de leur exploitation dynamique. Abstract When solving weighted constraint satisfaction problems , methods based on a tree-decomposition constitute an interesting approach depending on the nature of the considered instances. The exploited decomposi-tions often aim to reduce the maximal size of the clusters , which is known as the width of the decomposition. Indeed, the interest of this parameter is related to its importance with respect to the theoretical complexity of these methods. However, its practical interest for the solving of instances remains limited if we consider its multiple drawbacks, notably due to the restriction imposed on the freedom of the variable ordering heuristic."
"– Nous proposons un algorithme de diagonalisation conjointe par similitude d'un ensemble de matrices avec contrainte de non négati-vité sur les valeurs propres. Ce problème est au coeur de récentes méthodes de dé-mélange de signaux multidimensionnels. L'approche proposée garantit la non négativité des valeurs propres estimées et nous montrons à l'aide de simulations numériques que l'utilisation de cette contrainte permet d'améliorer l'estimation des valeurs propres. Abstract – In this paper we propose an algorithm for the joint eigenvalue decomposition of a matrix set with a non-negativity constraint on the eigenvalues. This kind of problem notably occurs for the separation of multidimensional signals. The proposed approach ensures the non-negativity of the estimated eigenvalues besides, our numerical simulations show that using the non-negativity constraint allows to improve the estimation of the eigenvalues."
"Développer des applications multiplateformes pour les appareils mobiles est une tâche assez complexe car les différents systèmes qui les animent se sont rendus parfaitement incompatibles en termes de portage d'applications. Cordova, supporté par le groupe Apache, est une alternative de développement multiplateforme mobile se basant sur HTML5, CSS3 et JavaScript. Cordova est une forme de conteneur pour interfacer l'application Web avec les fonctionnalités natives de l'appareil mobile. Pour les étudiants R&T cette plateforme met en valeur les connaissances acquises dans les modules de développement Web et la programmation pour appareils mobiles."
"Cet article propose une nouvelle méthode de prédiction de l'état du trafic routier sur de courtes fenêtres temporelles. Cette méthode bénéficie des méthodes de partitionnement spatial, d'extraction de motifs et de modélisation Markovienne. À partir de trajectoires, nous extrayons des régions fréquentes où des véhicules passent de manière récurrente en utilisant les motifs fermés fré-quents et nous détectons l'état du trafic en se basant sur l'évolution des motifs dans le temps. Nous prédisons ensuite l'état suivant du trafic pour les régions fréquentes en se basant sur les chaînes de Markov. Les expérimentations effec-tuées sur des données réelles montrent que la méthode proposée est plus efficace qu'une méthode de base."
"L'´ etude des triangles cassés devient de plus en plus ambitieuse, par la résolution desprobì emes de satisfaction de contraintes (CSP) en temps polynomial d'un coté, et par la réduction de l'espace de recherchè a tra-vers l'´ elimination de variables et la fusion de valeurs de l'autre. Pour cela, plusieurs extensions de ce concept ontétéétudiées ontétéontétéétudiées dans le passé récent, tel que les triangles cassés duaux et les triangles légèrement cassés. Ces extensions ontétéontété introduites dans le but de maximiser soit le nombre de valeurs fusionnées et/ou le nombre d'ins-tances traitables capturées. Mais, aucune d'entre elles n'a préservé toutes les caractéristiques de BTP. Ici, nous introduisons une nouvelle version légère de BTP, que nous appelons m-fBTP (pour flexible broken-triangle property). m-fBTP permet la fusion de valeurs, l'´ elimination de variables et définit une plus grande classe polynomiale pour laquelle la cohérence d'arc est une pro-cédure de décision. Une version plus détaillée en langue anglaise a ´ eté publiéè a AAAI'17 [4]."
"Cette thèse porte sur la synthèse de séquences de motion capture avec des modèles statistiques. La synthèse de ce type de séquences est une tâche pertinente pour des domaines d'application divers tels que le divertissement, l'interaction homme-machine, la robotique, etc. Du point de vue de l'apprentissage machine, la conception de modèles de synthèse consiste à apprendre des modèles génératifs, ici pour des données séquentielles. Notre point de départ réside dans deux problèmes principaux rencontrés lors de la synthèse de données de motion capture, assurer le réalisme des positions et des mouvements, et la gestion de la grande variabilité dans ces données. La variabilité vient d'abord des caractéristiques individuelles, nous ne bougeons pas tous de la même manière mais d'une façon qui dépend de notre personnalité, de notre sexe, de notre âge de notre morphologie, et de facteurs de variation plus court terme tels que notre état émotionnel, que nous soyons fatigués, etc.Une première partie présente des travaux préliminaires que nous avons réalisés en étendant des approches de l'état de l'art basées sur des modèles de Markov cachés et des processus gaussiens pour aborder les deux problèmes principaux liés au réalisme et à la variabilité. Nous décrivons d'abord une variante de modèles de Markov cachés contextuels pour gérer la variabilité dans les données en conditionnant les paramètres des modèles à une information contextuelle supplémentaire telle que l'émotion avec laquelle un mouvement a été effectué. Nous proposons ensuite une variante d'une méthode de l'état de l'art utilisée pour réaliser une tâche de synthèse de mouvement spécifique appelée Inverse Kinematics, où nous exploitons les processus gaussiens pour encourager le réalisme de chacune des postures d'un mouvement généré. Nos résultats montrent un certain potentiel de ces modèles statistiques pour la conception de systèmes de synthèse de mouvement humain. Pourtant, aucune de ces technologies n'offre la flexibilité apportée par les réseaux de neurones et la récente révolution de l'apprentissage profond et de l'apprentissage Adversarial que nous abordons dans la deuxième partie.La deuxième partie de la thèse décrit les travaux que nous avons réalisés avec des réseaux de neurones et des architectures profondes. Nos travaux s'appuient sur la capacité des réseaux neuronaux récurrents à traiter des séquences complexes et sur l'apprentissage Adversarial qui a été introduit très récemment dans la communauté du Deep Learning pour la conception de modèles génératifs performants pour des données complexes, notamment images. Nous proposons une première architecture simple qui combine l'apprentissage Adversarial et des autoencodeurs de séquences, qui permet de mettre au point des systèmes performants de génération aléatoire de séquences réalistes de motion capture. A partir de cette architecture de base, nous proposons plusieurs variantes d'architectures neurales conditionnelles qui permettent de concevoir des systèmes de synthèse que l'on peut contrôler dans une certaine mesure en fournissant une information de haut niveau à laquelle la séquence générée doit correspondre, par exemple l'émotion avec laquelle une activité est réalisée. Pour terminer nous décrivons une dernière variante qui permet de réaliser de l'édition de séquences de motion capture, où le système construit permet de générer une séquence dans le style d'une autre séquence, réelle."
Nous introduisons une nouvelle méthode d'apprentissage de clauses dites nobetters pour les solveurs séparation etévaluationetévaluation pour Max-SAT. Elle s'inspire de l'apprentissage de clauses nogoods utilisé par les solveurs 5 SAT basés sur l'analyse de conflits (CDCL). Elle a pour objectif de permettre une meilleure résolution des instances industrielles par une meilleure prise en compte de leurs structures.
"Les systèmes de traitement automatique des langues reposent souvent sur l'idée que le langage est compositionnel, c'est-à-dire que le sens d'une entité linguistique peut être déduite à partir du sens de ses parties. Cette supposition ne s'avère pas vraie dans le cas des expressions polylexicales (EPLs). Par exemple, une personne qui est une poule mouillée n'est ni une poule, ni nécessairement mouillée. Les techniques modernes de calcul pour déduire le sens des mots en fonction de leur distribution dans le texte ont obtenu de bons résultats sur plusieurs tâches, en particulier depuis l'apparition des modèles communément appelés word embeddings. Cependant, la représentation des EPLs reste toujours un problème non résolu. En particulier, on ne sait pas comment prédire avec précision, à partir des corpus, si une EPL donnée doit être traitée comme une unité indivisible (par exemple carton plein) ou comme une combinaison du sens de ses parties (par exemple eau potable). Cette thèse propose un cadre méthodologique pour la prédiction de compositionnalité d'EPLs fondé sur des représentations de la sémantique distributionnelle, que nous avons instancié à partir d'une variété de paramètres. Nous présenterons une évaluation complète de l'impact de ces paramètres sur trois nouveaux ensembles de données modélisant la compositionnalité d'EPLs, en anglais, français et portugais. Finalement, nous présenterons une évaluation extrinsèque des niveaux de compositionnalité prédits par le modèle dans le contexte d'un système d'identification d'EPLs. Les résultats suggèrent que le choix spécifique de modèle distributionnel et de paramètres de corpus peut produire des prédictions de compositionnalité qui sont comparables à celles présentées dans l'état de l'art"
"Cette thèse présente de nouveaux algorithmes de diagonalisation conjointe par similitude. Cesalgorithmes permettent, entre autres, de résoudre le problème de décomposition canonique polyadiquede tenseurs. Cette décomposition est particulièrement utilisée dans les problèmes deséparation de sources. L’utilisation de la diagonalisation conjointe par similitude permet de paliercertains problèmes dont les autres types de méthode de décomposition canonique polyadiquesouffrent, tels que le taux de convergence, la sensibilité à la surestimation du nombre de facteurset la sensibilité aux facteurs corrélés. Les algorithmes de diagonalisation conjointe par similitudetraitant des données complexes donnent soit de bons résultats lorsque le niveau de bruit est faible,soit sont plus robustes au bruit mais ont un coût calcul élevé. Nous proposons donc en premierlieu des algorithmes de diagonalisation conjointe par similitude traitant les données réelles etcomplexes de la même manière. Par ailleurs, dans plusieurs applications, les matrices facteursde la décomposition canonique polyadique contiennent des éléments exclusivement non-négatifs.Prendre en compte cette contrainte de non-négativité permet de rendre les algorithmes de décompositioncanonique polyadique plus robustes à la surestimation du nombre de facteurs ou lorsqueces derniers ont un haut degré de corrélation. Nous proposons donc aussi des algorithmes dediagonalisation conjointe par similitude exploitant cette contrainte. Les simulations numériquesproposées montrent que le premier type d’algorithmes développés améliore l’estimation des paramètresinconnus et diminue le coût de calcul. Les simulations numériques montrent aussi queles algorithmes avec contrainte de non-négativité améliorent l’estimation des matrices facteurslorsque leurs colonnes ont un haut degré de corrélation. Enfin, nos résultats sont validés à traversdeux applications de séparation de sources en télécommunications numériques et en spectroscopiede fluorescence."
"L'existence de modèles universels pour décrire la syntaxe des langues a longtemps été débattue. L'apparition de ressources comme le World Atlas of Language Structures et les corpus des Universal Dependencies rend possible l'étude d'une grammaire universelle pour l'analyse syntaxique en dépendances. Notre travail se concentre sur l'étude de différentes représentations des langues dans des systèmes multilingues appris sur des corpus arborés de 37 langues. Nos tests d'analyse syntaxique montrent que représenter la langue dont est issu chaque mot permet d'obtenir de meilleurs résultats qu'en cas d'un apprentissage sur une simple concaténation des langues. En revanche, l'utilisation d'un vecteur pour représenter la langue ne permet pas une amélioration évidente des résultats dans le cas d'une langue n'ayant pas du tout de données d'apprentissage."
"Les objets qui se trouvent au centre de cette thèse sont les systèmes dynamiques discrets, appréhendés à travers les réseaux d’automates finis, qui se définissent comme des vecteurs de fonctions locales de transition associées à chaque automate. Ces réseaux d’automates peuvent aussi bien être vus comme un outil pour modéliser des systèmes d’interactions naturels (réseaux biologiques, réseaux de particules physiques…) et les phénomènes qu’ils induisent que comme modèle de calcul que l’on peut étudier per se. Ces objets sont abordés à travers le prisme de la complexité et selon celui de la simulation (théorique), chacun de ces concepts formant le cœur d’une partie du document. La première partie est consacrée au rapport entre le graphe d’interaction d’un réseau, qui permet de représenter les influences entre ses automates, et la dynamique sous-jacente. Dans un premier temps, le focus est mis sur le problème des points fixes, ou configurations stables du réseau, qui vise à comprendre les informations que donne un graphe d’interaction sur le nombre de points fixes d’un réseau, et plus précisément sur la complexité algorithmique de ce comptage. Dans un second temps, l’intérêt est porté sur une propriété quelque peu opposée, à savoir l’expansivité. Un réseau est expansif si on peut prédire sa configuration de départ en observant une seule de ses composantes pendant suffisamment longtemps. Autrement dit, un réseau est expansif si sa dynamique est instable et que toute perturbation locale initiale a des répercussions visibles sur chaque automate. Nous caractérisons les graphes d’interactions qui admettent un réseau expansif et étudions la possibilité d’un réseau d’être expansif en fonction de plusieurs paramètres comme la taille de l’alphabet, le « temps d’expansivité » ou le type des fonctions calculées (linéaires, abéliennes...). La seconde partie est consacrée à la simulation intrinsèque, c’est-à-dire la capacité d’un réseau à contenir la complexité comportementale et la richesse calculatoire d’un autre réseau. Plus précisément, nous nous intéressons à une simulation spécifique fondée sur la capacité d’un réseau à simuler pas à pas toute la dynamique d’un autre réseau. L’un des paramètres sur lesquels l’accent est mis est le mode de mise à jour, qui représente les étapes de temps au cours desquelles les automates du réseau mettent à jour leur état. Il est bien connu que la dynamique d’un réseau dépend fortement des modes de mise à jour. Une question naturelle dans ce contexte est de comprendre quel type de dynamique peut être simulé avec un mode de mise à jour donné. Pour commencer, nous mettons en évidence le fait qu’un réseau mis à jour séquentiellement est moins « puissant » qu’un réseau mis à jour en parallèle. Alors que toute dynamique peut-être simulée par un réseau évoluant en parallèle, nous montrons que pour être simulée séquentiellement, elle peut nécessiter des réseaux plus grands dont nous bornons la taille. Ensuite, nous donnons les caractéristiques de réseaux « complets » dans le sens qu’ils peuvent simuler tous les réseaux d’une taille donnée en faisant varier leurs modes de mise à jour. Nous présentons enfin plusieurs réseaux de taille minimale, ou de « temps de simulation » minimal, et étudions les relations entre ces deux paramètres."
"La lecture est fondamentale pour tout ce qu’un enfant doit apprendre pendant son parcours scolaire. D’après des rapports nationaux (MJENR 2003) ou internationaux (PISA 2009), 20% à 30% des élèves français sont de faibles lecteurs et ont des difficultés pour comprendre les textes écrits, 5% à 10% sont des enfants dyslexiques. Ces lecteurs sont en grande difficulté face à des textes complexes ou avec un vocabulaire peu courant. Ces dernières années, un nombre important de technologies ont été créées pour venir en aide aux personnes ayant des difficultés pour lire des textes écrits. Les systèmes proposés intègrent des technologies de la parole (lecture à « voix haute ») ou des aides visuelles (paramétrage et/ou mise en couleur des polices ou augmentation de l’espace entre lettres et lignes). Cependant, il est essentiel de proposer aussi des transformations sur le contenu afin d’avoir des substituts de mots plus simples et plus fréquents. Cela permettra de rendre les textes plus accessibles et plus faciles à lire et à comprendre. Le but de cette thèse est de contribuer à un système d’aide à la lecture permettant de proposer automatiquement une version simplifiée d’un texte donné tout en gardant le même sens des mots. Le travail présenté adresse le problème de l’ambiguïté sémantique (très courant en traitement automatique des langues) et vise à proposer des solutions pour la désambiguïsation sémantique à l’aide de méthodes non supervisées et à base de connaissances provenant de ressources lexico-sémantiques. Dans un premier temps, nous proposons un état de l’art sur les méthodes de désambiguïsation sémantique et les mesures de similarité sémantique (essentielles pour la désambiguïsation sémantique). Par la suite, nous comparons divers algorithmes de désambiguïsation sémantique afin d’identifier le meilleur. Enfin, nous présentons nos contributions pour la création d’une ressource lexicale pour le français proposant des synonymes désambiguïsés et gradués en fonction de leur niveau de difficulté de lecture et compréhension. Nous montrons que cette ressource est utile et peut être intégrée dans un module de simplification lexicale de textes."
"L'objectif principal de cette thèse est d'aider le diabète de type 1 (DTl) à contrôler et stabiliser son taux de glycémie. Pour cela, une analyse de l'évolution de la glycémie est nécessaire, ensuite et après l'enregistrement des valeurs de la glycémie à l'aide des CGM, une bonne méthode de prédiction de la glycémie est indispensable pour que le patient puisse ajuster la dose d'insuline injecté sur la base de ces valeurs prédites. Dans ce cadre, nous avons focalisé dans le premier chapitre une étude sur le principe de la régulation de la glycémie, dont nous avons présenté l'homéostasie glycémique, l'évolution de la glycémie, les organes responsables dans la régulation de la glycémie, et les ensembles des mécanismes pour la régulation de la glycémie. Ainsi, pour mieux comprendre le diabète, nous avons présenté des généralités sur le diabète : histoire du diabète, répartition du diabète dans le monde, les types de diabète et la différence entre eux, les moyens de traitement du diabète de type 1 et les matériels techniques utilisés pour la gestion du diabète.Dans le deuxième chapitre, nous avons étudié l'évolution de la glycémie, de ce fait nous avons montré que la glycémie a un aspect chaotique. Par conséquent, la glycémie est imprédictible à long terme, dont la limite de prédictibilité est presque égale à 45 minutes. Le troisième chapitre a été une continuation du travail présenté dans le chapitre précédant. En effet, après la détermination de la limite de prédictibilité, nous étudions les approches de prédiction de la glycémie. En effet, une vaste recherche bibliographique a été lancée sur les tous les méthodes de prédiction de la glycémie dont on a les méthodes mathématiques et les méthodes d'intelligence artificielle. Dans ce travail, deux approches de prédiction de la glycémie ont été proposées. La première approche est une nouvelle ANN adaptative. En effet, en optimisant l'architecture des ANN pour chaque patient. La précision des ANN proposées est discutée sur la base de certains critères statistiques tels que RMSE et MAPE. La moyenne obtenue de RMSE est de 6,43 mg/ dL, et la moyenne de MAPE est de 3,87 % pour un Horizon de Prédiction HP = 15 min. En comparant avec les autres modèles techniques établies dans la littérature, la méthode proposée présente plusieurs avantages tels que la précision et l'adaptabilité. Ainsi, les expériences montrent lacapacité des ANN proposés pour une meilleure prédiction du niveau de la glycémie. La deuxième approche est un SVR pondéré basé sur l'algorithme DE, la moyenne obtenue de RMSE était de 9,44 mg/ dL pour un HP égal à 15 min. Une comparaison avec les techniques établies dans la littérature montre que la méthode proposée présente de nombreux avantages tels que la précision, l'adaptabilité et la facilité de mise en pratique. Selon les résultats expérimentaux, la combinaison proposée de l'algorithme d'optimisation SVR avec DE présente une meilleure précision de prédiction grâce à son efficacité dans la modélisation de séries de données non linéaires et complexes."
"Les modèles à base de méthodes à noyaux et d'apprentissage profond ont essentiellement été étudiés séparemment jusqu'à aujourd'hui. Des travaux récents se sont focalisés sur la combinaison de ces deux approches afin de tirer parti du meilleur de chacune d'elles. Dans cette optique, nous introduisons une nouvelle architecture de réseaux de neurones qui bénéficie du faible coût en espace et en temps de l'approximation de Nyström. Nous montrons que cette architecture atteint une performance du niveau de l'état de l'art sur la classification d'images des jeux de données MNIST et CIFAR10 tout en ne nécessitant qu'un nombre réduit de paramètres."
"Leprobì eme du dénombrement de solutions d'une instance CSP, appelé #CSP, constitue unprobì eme ex-trêmement difficile qui possède de multiples applications en Intelligence Artificielle. S'il est le plus souvent résolu par des méthodes approchées, ici, nous nous focalisons sur le dénombrement exact. Nous montrons comment il est possible d'améliorer les méthodes basées sur les décompositions structurelles en améliorant la recherche d'une nouvelle solution, qui est uné etape essentielle, en particulier pour de telles méthodes. De plus, si les res-sources en temps ou en espace sont insuffisantes, nous montrons comment notre approche est capable de four-nir une borne inférieure du nombre de solutions. Des expérimentations sur des benchmarks CSP mettent en avant l'intérêt pratique de notre approche par rapport aux meilleures méthodes de la littérature. Ce papier est un résumé de [6]. Abstract The problem of counting solutions in CSP, called #CSP, is an extremely difficult problem that has many applications in Artificial Intelligence. This problem can be addressed by exact methods, but more classically it is solved by approximate methods. Here, we focus primarily on the exact counting. We show how it is possible to improve the methods based on structural decomposition by offering to enhance the search for a new solution which is a critical step for counting, particularly for such methods. Moreover, if the resources in time or in space are insufficient, we show that our approach is still able to provide a lower bound of the result. Experiments on CSP benchmarks show the practical advantage of our approach w.r.t. the best methods of the literature. This is a summary of [6]."
"La bibliographie, les retours d’expérience et les témoignages mettent régulièrement en évidence les difficultés éprouvées par les musées pour faire face aux risques en général et aux risques inondations en particulier. Le Projet Musées Résilients aux Inondations s’inscrit dans une démarche d’accompagnement pour franchir ces difficultés. Il a pour objectifs d’aider : à l’évaluation des vulnérabilités des musées face aux aléas inondations, à la mise en place de mesures de prévention et de surveillance, à la mise en place d’une organisation de gestion de crise et au retour à une situation normale, ainsi qu’à la mise en place d’exercices de simulation et au retour d’expérience. L’objet de cette communication est de montrer comment tous ces points ont été traités et formalisés au sein d’un même document : le guide d’accompagnement à l’élaboration, la mise en oeuvre et la mise à jour des Plans de Sauvegarde des Biens Culturels (PSBC)."
"Cette thèse se place dans le cadre de services web en dépassant leur description pour considérer leur structuration en réseaux (réseaux d'interaction et réseaux de similitude). Nous proposons des méthodes basées sur les motifs, la modélisation probabiliste et l'analyse des concepts formels, pour améliorer la qualité des services découverts. Trois contributions sont alors proposées: découverte de services diversifiés, recommandation de services et cohérence des communautés de services détectées. Nous structurons d'abord les services sous forme de réseaux. Afin de diversifier les résultats de la découverte, nous proposons une méthode probabiliste qui se base à la fois sur la pertinence, la diversité et la densité des services. Dans le cas de requêtes complexes, nous exploitons le réseau d'interaction de services construit et la notion de diversité dans les graphes pour identifier les services web qui sont susceptibles d'être composables. Nous proposons également un système de recommandation hybride basé sur le contenu et le filtrage collaboratif. L'originalité de la méthode proposée vient de la combinaison des modèles thématiques et les motifs fréquents pour capturer la sémantique commune maximale d'un ensemble de services. Enfin, au lieu de ne traiter que des services individuels, nous considérons aussi un ensemble de services regroupés sous forme de communautés de services pour la recommandation. Nous proposons dans ce contexte, une méthode qui combine la sémantique et la topologie dans les réseaux afin d'évaluer la qualité et la cohérence sémantique des communautés détectées, et classer également les algorithmes de détection de communautés."
"Certains problèmes ne peuvent être résolus efficacement avec les ordinateurs actuels, dits “classiques”. Certains algorithmes quantiques apportent des solu- tions théoriques à ces problèmes, qui pourraient être résolus efficacement si un ordinateur quantique, sous-entendu, universel, pouvait implémenter ces algo- rithmes. Il se trouve que la construction d’un tel ordinateur quantique s’avère une tâche très compliquée, limitée aujourd’hui fortement par les technologies à notre disposition. Ceci étant dit, les recherches précédemment citées durant, des simulateurs quantiques specialisés ont déjà été capables de résoudre certaines versions modestes de ces problèmes. Les simulateurs quantiques actuels sont en effet, soit des ordinateurs quantiques effectuant une tâche spécifique, soit des machines quantiques analogiques mimant le phénomène physique d’intérêt. Les dénommées marches quantiques, évolutions quantiques locales sur graphes discrets, sont un outil très pratique pour simuler certains systèmes physiques. Nous nous limiterons à leur version à temps discret, les marches quantiques à temps discret (MQTD). Dans certaines limites en espace-temps continu, ces marches quantiques coincident avec des équations d’onde pour fermions rela- tivistes, dont l’archétype et pilier est l’équation de Dirac. Dans la présente thèse, nous poursuivons l’étude des propriétés des MQTD comme possibles schémas de simulation quantique. Nous pouvons résumer nos résultats en trois parties: i) Nous introduisons un schéma MQTD permettant de simuler, dans la limite au continu, la dynamique de fermions relativistes dans une théorie de branes; ceci ouvre la possibilité d’étudier différents modèles de théories Kaluza-Klein; ii) Nous discutons l’invariance de jauge U(1), i.e., électromagnétique, des MQTD, nous comparons notre modèle aux invariances précédemment introduites dans la littérature; notre invariance de jauge présente de fortes similitudes avec celle des théories de jauge sur réseau; iii) Nous introduisons des MQTD sur grilles non-rectangulaires, plus précisément, triangulaires et hexagonales, avec toujours comme condition de retrouver l’équation de Dirac au continuum; ces modèles peuvent être étendus au moyen d’opérateurs unitaires locaux spatiotemporelle- ment inhomogènes et n’agissant que sur l’espace interne du marcheur, afin de générer dans la limite au continu l’equation de Dirac en espace-temps courbe."
"Ce travail porte sur une application dans un contexte médical d'un descripteur de forme pour des maillages 3D. Ce descripteur permet de représenter un maillage polyédrique avec un graphe subdivisé de forme basé sur les courbures pouvant ensuite être utilisé pour extraire des caractéristiques de la forme. La méthode proposée divise d'abord le maillage en 8 catégories de carreaux en utilisant les courbures discrètes. Ces carreaux sont ensuite nettoyés, et pour ajouter des informations topologiques un nouveau carreau ""segmentation"" est ajouté. Il représente la jonction entre les ""clusters"" calculée par la segmentation du maillage réalisée précédemment. Cela permet d'extraire des sous-graphes entre ces noeuds qui peuvent être appariés pour déterminer l'auto-similarité de parties de l'objet ou pour les comparer à d'autres sous-graphes provenant de maillages similaires, ce qui permet l'examen d'éventuelles déformations de courbure de la portion de maillage observée. Nous présenterons les résultats de ce descripteur de forme appliqué à l'oreille interne afin de décrire l'auto-similarité et de comparer les sous-graphes des canaux semi-circulaires obtenus à partir d'une évaluation par tomodensitométrie à faisceau cônique chez six patients adultes."
"Depuis leur apparition en 1999, les emojis ont une popularité grandissante dans les systèmes de communication. Ces petites images pouvant représenter une idée, un concept ou une émotion, se retrouvent disponibles aux utilisateurs dans de nombreux contextes logiciels : messagerie instantanée, courriel, forums et autres réseaux sociaux en tout genre. Leur usage, en hausse constante, a entraîné l'apparition récurrente de nouveaux emojis, allant jusqu'à 2 789 emojis standardisés en fin d'année 2018. Le parcours de bibliothèques d'emojis ou l'utilisation de moteur de recherche intégré n'est plus suffisant pour permettre à l'utilisateur de maximiser leur utilisation ; une recommandation d'emojis adaptée est nécessaire. Pour cela, nous présentons nos travaux de recherche axés sur le thème de la recommandation d'emojis. Ces travaux ont pour objectif de créer un système de recommandation automatique d'emojis adapté à un contexte conversationnel informel et privé. Ce système doit améliorer l'expérience utilisateur et la qualité de la communication, en plus de pouvoir prendre en compte d'éventuels nouveaux emojis. Dans le cadre de cette thèse, nous contribuons tout d'abord en montrant les limites d'usage réel d'une prédiction d'emojis ainsi que la nécessité de prédire des notions plus générales. Nous vérifions également si l'usage réel des emojis représentant une expression faciale d'émotion correspond à l'existant théorique. Enfin, nous abordons les pistes d'évaluation d'un tel système par l'insuffisance des métriques, et l'importance d'une interface utilisateur dédiée. Pour ce faire, nous utilisons une approche orientée apprentissage automatique à la fois supervisée et non supervisée, ainsi que la conception de modèles de langues ou, plus précisément, de plongements lexicaux. Plusieurs composantes de ce travail ont donné lieu à des publications nationales et internationales, dont le prix du meilleur logiciel reproductible à la conférence CICLing 2018, ainsi que celui du meilleur poster pour la catégorie des médias sociaux SONAMA à la conférence SAC 2018."
"La spécialité ""Réseaux et Télécommunications"" forme des techniciens supérieurs capables de s'insérer dans les secteurs des réseaux informatiques, télécommunications et du web, ou de poursuivent leurs études en licence professionnelle orientée vers la sécurité et l'administration des réseaux informatiques (ASUR). Le programme pédagogique national a nettement mis l'accent sur l'administration des systèmes et des services de l'Internet en l'affectant d'une charge d'environ 700 heures réparties sur 7 modules dans les 3 premiers semestres. D'une part, parce que ce type d'enseignement est très difficile à mener dans des salles informatiques banalisées, d'autre part, parce que la mise en place d'une pédagogie par projet nous semble être un gage d'une formation de qualité, il nous semble donc important la mise en place du matériels adéquats (Serveurs, Stations de travail, Unités mobiles) à vocation purement pédagogique pour l'équipement d'une salle de travaux pratiques afin de mettre en oeuvre une solution répondant aux exigences pédagogiques de nos modules d'enseignements. Ainsi, dans ce papier nous présentons la solution technique mise en place dans notre département R&T pour favoriser la pédagogie par projet et l'évaluation par compétences dans les modules administration systèmes et services réseaux."
"Plusieurs quantificateurs de fragmentation du sommeil ont été proposés tels que l'indice de Fragmentation du Sommeil (IFS) et l'indice Pondéré de Fragmentation du Sommeil (IPFS). Mais, ces indices se contentent de quantifier la fragmentation du sommeil et ne proposent pas de seuil à partir duquel le sommeil peut être considéré comme fragmenté. D'où provient la nécessité de construire un modèle mathématique et/ou numérique de l'analyse de la fragmentation du sommeil fournissant aux spécialistes du sommeil une aide au diagnostic. En se basant sur trois paramètres du sommeil (le nombre de changement de stades (SSS ), le taux de micro-éveils (MAR), le nombre d'éveils intra-sommeil (ISA)) et à partir d'une base de données de 111 PSG avec 55 sujets sains et 56 patients avec une suspicion du syndrome d'apnée de sommeil obstructif (OSAS) qui sont diagnostiqués par neufs praticiens hospitaliers, nous avons construit pour chaque praticien un modèle mathématique et trois modèles automatiques. L'accord, selon l'indice de Kappa Cohen, entre le diagnostic de chaque clinicien et celui du modèle correspondant, varie du modéré au presque parfait."
"Cet article s’intéresse a l'apprentissage multi-vue par des méthodes à noyaux et d'apprentissage de métriques. Dans ce cadre, nous considérons MVML (multi-view metric learning), une méthode récemment développée, et nous proposons un algorithme basé sur les séparateurs à vaste marge (SVM) qui apprend conjointement un classifieur et des métriques entre les vues permettant ainsi de tenir compte des caractéristiques multi-vues du problème d'apprentissage. Des expérimentations sur données réelles ont été réalisées afin d'évaluer les performances de l'algorithme proposé."
"Les services sociaux de microblogging jouent un rôle important dans notre société. Twitter est l'une des plateformes de microblogging les plus populaires, utilisées par les internautes pour trouver des informations pertinentes (sujets d'actualité, tendances populaires, informations sur certains internautes, etc.). Dans ce contexte, la recherche d'information provenant de telles données a récemment gagné un intérêt majeur et ouvert de nouveaux défis. Cependant, la taille de ces données ainsi que des requêtes est généralement courte et peut avoir un impact sur le résultat de la recherche. Cette dernière peut être améliorée à l'aide de l'expansion de requêtes. En effet, les mots peuvent avoir plusieurs sens dont un seul est utilisé pour un contexte donné. Dans cet article, nous proposons une méthode d'expansion de requêtes prenant en compte le sens du contexte. Nous utilisons les motifs et les plongements de mots pour étendre les requêtes des utilisateurs. L'évaluation expérimentale de la méthode proposée est menée sur la collection TREC. Les résultats montrent l'efficacité de l'approche en combinant des motifs avec des plongements de mots pour améliorer significativement la recherche de microblogs"
"Le domaine de la synthèse réactive a pour objectif d'obtenir un système correct par construction à partir d'une spécification logique. Une approche classique consiste à se ramener à un jeu à somme nulle, où deux joueurs interagissent tour-à-tour dans un système de transitions, et à se demander si le joueur ""contrôleur"" peut garantir que son objectif sera rempli, et ce indépendamment des décisions du joueur ""environnement"". Nous étudions des spécifications temps-réel, modélisées par un automate temporisé équipé d'un objectif d'accessibilité ou de Büchi, et présentons des méthodes symboliques pour synthétiser des stratégies du contrôleur. Nos contributions concernent deux problématiques distinctes : on peut souhaiter que le contrôleur obtienne une stratégie robuste aux perturbations, ou bien le faire jouer de manière optimale dans un jeu pondéré."
"Les compagnies d'affrètement maritime cherchent à utiliser les prévisions météorologiques en vue d'optimiser les déplacements de leur flotte. En effet, si un bateau doit amener des marchandises depuis un port d'origine jusqu'à un port de destination, en partant à une date donnée, et ce, tout en veillant à minimiser la consommation de carburant, la détermination de la meilleure route à emprunter constitue un problème difficile au sens de la théorie de la complexité. De plus, cette meilleure route peut évoluer pendant le trajet, ce qui rend le problème pratique encore plus complexe. Pour répondre à ce type de problématique, de nombreux logiciels de routage existent, mais dès lors qu'il s'agit de prendre en considération plusieurs critères qui parfois sont contradictoires, on se rend compte de l'absence dans l'état de l'art d'algorithmes qui résoudraient efficacement ce problème. L'objectif de cette thèse est de répondre à cette problématique en proposant un cadre de modélisation adéquat et des algorithmes qui devront être validés dans des conditions industrielles. Le premier travail réalisé porte sur la mise en place d'une méthodologie permettant de transformer les données brutes récupérées en entrée (principalement les données géographiques et météorologiques), en un modèle mathématique exploitable. Cette première étape est essentielle puisqu'elle conditionne ensuite les algorithmes utilisés, et donc leur efficacité. Ainsi, nous choisissons de modéliser le problème sous la forme d'un graphe qui prend en compte le temps. La seconde contribution est la proposition d'un algorithme multi-objectif et dépendant du temps, permettant d'identifier les chemins pareto-optimaux dans ce graphe. Un troisième apport concerne le traitement des ces chemins, de sorte à optimiser la vitesse tout au long d'un trajet et par voie de conséquence, optimiser la consommation de carburant. Enfin, nous présentons le système logiciel opérationnel qui intègre l'ensemble de ces contributions et qui a par ailleurs permis de valider expérimentalement, sur des données réelles, le modèle de représentation que nous avons proposé ainsi que les algorithmes qui l'exploitent."
Nous présentons une interface de recommandation d'emojis porteurs de sentiments qui utilise un modèle de prédiction appris sur des messages informels privés. Chacun étant associé à deux scores de polarité prédits. Cette interface permet permet également d'enregistrer les choix de l'utilisateur pour confirmer ou infirmer la recommandation.
"Les environnements d'apprentissage virtuels utilisent souvent des personnages virtuels pour faciliter et améliorer l''apprentissage. Connus sous le nom d'agents virtuels pédagogiques, ils peuvent incarner différents rôles, tels que tuteur ou compagnon. Des recherches ont mis en évidence l'importance de diverses caractéristiques des agents virtuels, comme la voix ou le comportement non verbal. Cependant, peu d'attention a été accordée au genre des agents pédagogiques virtuels. Cela est assez surprenant compte tenu de l'importance des questions de genre dans l'éducation et plus particulièrement de l'influence des stéréotypes de genre. Dans cet article, nous passons en revue les recherches montrant comment et pourquoi les étudiants peuvent craindre d'être réduits à un stéréotype de genre négatif dans des conditions ordinaires en classe lors d'un test de performance. Cette peur provoque des pensées négatives, réduisant la capacité de mémoire de travail des individus. Cela peut entraîner une baisse de performance et, si cela se répète, un désintérêt pour l'éducation (phénomène connu sous le nom de Menace de Stéréotype). Nous nous concentrons sur le processus d'apprentissage des étudiantes dans les domaines des Sciences, de la Technologie, de l'Ingénierie et des Mathématiques (STIM), en particulier en mathématiques. Nous examinons comment ces stéréotypes peuvent s'infiltrer dans les environnements d'apprentissage virtuels. Enfin, nous examinons comment le genre d'un agent pédagogique virtuel peut être utilisé pour contrer les effets de la Menace de Stéréotype et améliorer l'apprentissage scolaire."
"Plusieurs recherches ont permis de montrer qu'un ovale à huit centres coïncide presque parfaitement avec l'ellipse construite sur les mêmes axes et peut être considéré comme une représentation de celle-ci à condition de choisir convenablement les rayons des arcs de cercles qui le composent. Le calcul de son périmètre se réduit alors à la simple somme d'arcs de cercles. Pourtant, il ne nous semble pas que ce calcul, qui pourrait s'avérer utile, n'ait jamais encore été effectué ni publié. L'objet de cette note est donc de présenter une démonstration géométrique de la détermination du périmètre de l'ovale à huit centres."
"L'algorithme de multiplication dans les corps finis de Chudnovsky a une complexité bilinéaire uniformément linéaire en le degré de l'extension. Randriambololona a récemment généralisé cette méthode en introduisant l'asymétrie dans la procédure d'interpolation et en obtenant ainsi de nouvelles bornes sur la complexité bilinéaire. Dans cette note, nous décrivons la construction de cette méthode asymétrique sans évaluation dérivée. Pour ce faire, nous traduisons cette généralisation dans le langage des corps de fonctions algébriques, et nous donnons une stratégie de construction et d'implantation."
"Cette étude concerne le développement d’une boussole optique inspirée de la boussole céleste de la fourmi du désert Cataglyphis. Cette activité pédagogique vise à comprendre différents phénomènes physiques impliqués dans la détection optique de cap : la diffusion de Rayleigh de la lumière du soleil dans le ciel, la polarisation de la lumière et la mesure du cap à partir de photocapteurs. Nous décrivons ici, de manière successive, la conception puis la réalisation d’un dispositif expérimental d’enseignement utilisé au niveau Master 2 pour permettre aux étudiants de se familiariser avec l’ingénierie bio-inspirée appliquée à la détection optique de cap au moyen d’un capteur non conventionnel. Ce dispositif expérimental a été introduit pour la première fois cette année à des étudiants du Master Ingénierie des Systèmes complexes de l’Université de Toulon et à des étudiants du Master de « Mechanical Engineering » (option Fluide et Structure) de Aix-Marseille Université. Nous avons ensuite mis l’accent sur l’utilisation de carte Arduino pour aborder les problématiques liées au traitement temps réel des cibles du type microcontrôleur ayant des capacités calculatoires limitées. Enfin, des exemples de production et de mesures faites par les étudiants sont présentés pour démontrer l’exploitation pédagogique qui peut être faite d’un tel dispositif expérimental."
"La compréhension automatique de texte est une tâche faisant partie de la famille des systèmes de Question/Réponse où les questions ne sont pas à portée générale mais sont liées à un document particulier. Récemment de très grand corpus (SQuAD, MS MARCO) contenant des triplets (document, question, réponse) ont été mis à la disposition de la communauté scientifique afin de développer des méthodes supervisées à base de réseaux de neurones profonds en obtenant des résultats prometteurs. Ces méthodes sont cependant très gourmandes en données d'apprentissage, données qui n'existent pour le moment que pour la langue anglaise. Le but de cette étude est de permettre le développement de telles ressources pour d'autres langue à moindre coût en proposant une méthode générant des questions à partir d'une analyse sémantique de manière semi-automatique. La collecte de questions naturelle est réduite à un ensemble de validation/test. L'application de cette méthode sur le corpus CALOR-Frame a permis de développer la ressource CALOR-QUEST présentée dans cet article."
"L'importance des problèmes CSP, WCSP et #CSP est reflétée par la part considérable des travaux, théoriques et pratiques, dont ils font l'objet en intelligence artificielle et bien au-delà. Leur difficulté est telle qu'ils appartiennent respectivement aux classes NP-complet, NP-difficile et #P-complet. Aussi, les méthodes qui permettent de résoudre efficacement leurs instances ont une complexité en temps exponentielle. Les travaux de recherche de cette thèse se focalisent sur les méthodes de résolution exploitant la notion de décomposition arborescente. Ces méthodes ont suscité un vif intérêt de la part de la communauté scientifique du fait qu'elles soient capables de résoudre en temps polynomial certaines classes d'instances. Cependant, en pratique, elles n’ont pas encore montré toute leur efficacité vu la qualité de la décomposition employée ne prenant en compte qu'un critère purement structurel, sa largeur. Premièrement, nous proposons un nouveau cadre général de calcul de décompositions qui a la vertu de calculer des décompositions qui capturent des paramètres plus pertinents à l'égard de la résolution que la seule largeur de la décomposition. Ensuite, nous proposons une exploitation dynamique de la décomposition pendant la résolution pour les problèmes (W)CSP. Le changement de la décomposition pendant la résolution vise à adapter la décomposition selon la nature de l’instance. Finalement, nous proposons un nouvel algorithme de comptage qui exploite la décomposition d'une façon différente de celle des méthodes standards afin d'éviter des calculs inutiles. L'ensemble des contributions ont été évaluées et validées expérimentalement."
"Cet article porte sur l'item de feedback "" oui "" produit par un patient dans un environnement médical spécifique d'annonce de mauvaises nouvelles. Cette étude a pour objet de déterminer le rôle de la prosodie (intonation et délai temporel) dans la perception de ce feedback. 15 auditeurs français ont écouté de courtes interactions humain-humain impliquant deux personnes jouant le rôle du docteur et de son patient (ou d'un proche de ce dernier). Les auditeurs devaient juger du caractère plus ou moins approprié du feedback produit par le patient, en s'appuyant sur la manière dont il était produit oralement. Nous avons mesuré les effets de l'intonation (neutre/bouleversé/questionnant), du délai (court/long), et du genre des auditeurs (homme/femme) sur le score (jugé sur une échelle de 1 à 5) et le temps de réaction. Les résultats montrent le rôle de l'intonation et du délai dans une phase spécifique du dialogue (Définition du Problème) les hommes et les femmes étant sensibles à différents aspects de la prosodie. Ceci a des implications sur la modélisation de la prosodie des feedbacks d'un patient virtuel dans un contexte d'interaction homme-machine."
"Ce papier présente un ensemble de données original d'interactions contrôlées, en se concentrant sur l'étude des feedbacks. Il s'agit d'enregistrements de différentes conversations entre un médecin et un patient, jouées par des acteurs. Dans ce corpus, le patient est principalement un auditeur et produit différents feedbacks, dont certains sont (volontairement) incongrus. De plus, ces conversations ont été resynthétisées dans un contexte de réalité virtuelle, dans lequel le patient est joué par un agent artificiel. Le corpus final est constitué de différents films de conversations humain-humain, ainsi que les mêmes conversations rejouées dans un contexte humain-machine, ce qui donne le premier corpus parallèle humain-humain/humain-machine. Le corpus est ensuite enrichi de différentes annotations multimodales aux niveaux verbal et non verbal. De plus, ce corpus a été enrichi de données de perception subjective et de données neurophysiologique. Nous avons en effet conçu une expérience au cours de laquelle différents participants devaient regarder les films du corpus et donner une évaluation de l'interaction. Au cours de cette tâche, nous avons mesuré l’activité cérébrale du participant par électroencéphalographie. L'ensemble de données Brain-IHM est conçu dans un triple but : 1/ étudier les feedbacks en comparant les feedbacks congruents versus incongruents 2/ comparer la perception des feedbacks produit par l’humain versus la machine 3/ étudier les bases cérébrales de la perception des feedbacks."
"La simplification de textes est une tâche complexe du traitement automatique des langues. Depuis quelques années, des corpus parallèles de textes originaux et simplifiés sont proposés, permettant d'apprendre différents types d'opérations de simplification à partir de corpus. Dans le but de pouvoir développer et évaluer des systèmes de simplification automatique de textes, cet article s’intéresse au corpus Newsela, un corpus parallèle de textes en langue anglaise avec quatre niveaux de simplification. Nous présentons en détail ce corpus et étudions les différentes transformations caractérisant le passage d’un niveau de simplification à l’autre sur un sous-ensemble de textes, en nous intéressant plus particulièrement aux transformations syntaxiques."
"Cet article présente notre contribution en DEFT 2018 tâche 2 : ""Polarité globale"", déterminant la polarité globale (Positif, Négatif, Neutre ou MixPosNeg) des tweets concernant les transports publics, en langue Française. Notre système est basé sur une liste de mots-graines de sentiment adaptés aux tweets de transport public français. Ces mots-graines sont extraits de corpus annoté de DEFT, et les relations entre les mots-graine et les autres termes sont capturées par la similarité en mesure de cosinus entre les vecteurs représentants les mots, en utilisant un modèle word2vec en langue Française de 683k mots. Notre système semi-supervisé a atteint un F1-measure égale à 0,64."
"Le pourcentage de personnes en situation de handicap poursuivant leurs études jusqu'à un diplôme de niveau I est très faible en France. Parallèlement, les entreprises perçoivent un manque d'ingénieurs informaticiens. Afin de combler cette injustice et ce besoin économique, l'Université d'Aix-Marseille et un collectif comprenant de grandes entreprises se sont associés pour créer une formation d'ingénieurs dédiée aux personnes en situation de handicap. La mise en place et la gestion d'une telle formation sont complexes car d'une part, le nombre d'acteurs impliqués dans le projet est important et d'autre part, certains acteurs doivent apprendre à travailler ensemble. L'objectif de cet article est de formuler un certain nombre de recommandations et de pièges à éviter afin de faciliter, au niveau pédagogique, la mise en place de formations similaires. Il s'agit principalement de permettre aux élèves de tirer le meilleur d'eux mêmes et de prévenir les abandons. Cet article contient également un retour d'expérience succinct concernant la formation HUGo."
"Nous présentons des résumés en français et en anglais de l’article (Marzinotto et al., 2019) présenté à la conférence North American Chapter of the Association for Computational Linguistics : Human Language Technologies en 2019."
"Le traitement à posteriori de transcriptions OCR cherche à détecter les erreurs dans les sorties d'OCR pour tenter de les corriger, deux tâches évaluées par la compétition ICDAR-2017 Post-OCR Text Correction. Nous présenterons dans ce papier un système de détection d'erreurs basé sur un modèle à réseaux récurrents combinant une analyse du texte au niveau des mots et des caractères en deux temps. Ce système a été classé second dans trois catégories évaluées parmi 11 candidats lors de la compétition. MOTS-CLÉS : OCR, detection d'erreurs, réseaux de neurones récurrents."
"La classification de documents imprimés est une tâche réalisée en entrée de multiples chaînes de traitement et d'analyse d'archives numériques, ce qui en fait un point critique dans de tel systèmes. Afin d'extraire des éléments caractéristiques de chaque catégorie parmi les-quels ces pièces doivent être classés, des données textuelles ou des images sont utilisés. Nous présentons dans cet article une analyse de différentes approches pour la catégorisation de documents exploitant des données textuelles ou des images en entrée, ainsi qu'un système de classification utilisant l'information du texte et de l'image de façon jointe en un modèle de réseau de neurone convolutionnel."
"La désambiguïsation des rattachements prépositionnels est une tâche syntaxique qui demande des connaissances sémantiques, pouvant être extraites d'une image associée au texte traité. Nous présentons et analysons les difficultés de cette tâche pour laquelle nous construisons un système complet entraîné sur une version étendue des annotations du corpus Flickr30k Entities. Lorsque la sémantique lexicale n'est pas disponible, l'information visuelle apporte 3 % d'amélioration."
Les sons pulsés sont des exemples intéressants de sons biologiques complexes. Nous proposons une classification utile de ces sons en deux catégories : sons tonaux ou non-tonaux. Deux modèles mathématiques permettent de mieux cerner les propriétés de ces sons dans ces deux cas. Cette classification s'avère aussi utile pour les mesures de paramètres de ces sons et pour distinguer entre deux moyens de leur production. Nous avons appliqué cette méthode aux chants de baleines bleues du pacifique sud-est et ainsi trouvé que la fréquence de pulsation correspond à la fréquence fondamentale (qui n'est pas exprimée dans le spectre). Ainsi nous renforçons l'hypothèse que le son n'est produit que par un seul organe et ensuite filtré par le corps du géant.
"Dans le cadre de la compréhension automatique de documents, cet article propose une évaluation intrinsèque et extrinsèque d’un modèle d’analyse automatique en cadres sémantiques (Frames). Le modèle proposé est un modèle état de l’art à base de GRU bi-directionnel, enrichi par l’utilisation d’embeddings contextuels. Nous montrons qu’un modèle de compréhension de documents appris sur un corpus de triplets générés à partir d’un corpus analysé automatiquement avec l’analyseur en cadre sémantique présente des performances inférieures de seulement 2.5% en relatif par rapport à un modèle appris sur un corpus de triplets générés à partir d’un corpus analysé manuellement."
"Les systèmes réactifs sont caractérisés par une interaction constante avec leur environnement : celui-ci fournit un signal d’entrée, auquel le système répond par un signal de sortie, et ainsi de suite à l’infini. L’objectif de la synthèse réactive est de générer automatiquement l’implémentation d’un tel système à partir de la spécification de son comportement. Classiquement, l’ensemble des signaux est supposé fini. Cependant, ce cadre échoue à modéliser des systèmes qui traitent des signaux accompagnés de données issues d’un ensemble potentiellement infini (un identifiant unique, la valeur d’un capteur, etc.), qui doivent être stockées et comparées entre elles. L’objectif de cette thèse est d’étendre la théorie de la synthèse réactive sur les mots à alphabet fini au cas des mots de données. Le domaine de données consiste en un ensemble infini, dont la structure est définie par des prédicats et des constantes, enrichi par un ensemble fini de signaux. Les spécifications et les implémentations sont alors respectivement représentées par des automates et des transducteurs à registres, qu’ils utilisent pour stocker les données. Pour déterminer la transition à prendre, ils comparent la donnée d’entrée au contenu de leurs registres à l’aide des prédicats du domaine. Dans une première partie, nous considérons les problèmes de la synthèse bornée et non-bornée. Dans le premier cas, l’algorithme prend en entrée une borne sur le nombre de registres de l’implémentation, en plus de la spécification à implémenter. Nous considérons plusieurs instances, selon que la spécification est un automate non-déterministe, universel (ou co-non-déterministe), ou encore déterministe, pour plusieurs domaines de données. Tandis que le problème de la synthèse bornée est indécidable pour les spécifications non-déterministes, nous élaborons une approche générique qui permet de le réduire au cas d’un alphabet fini. Celle-ci permet de redémontrer la décidabilité de la synthèse bornée à partir d’automates universels sur (ℕ,=) et d’étendre le résultat à (ℚ,<), y compris en autorisant l’automate à deviner des données, tout cela en 2-ExpTime. Quant à la synthèse non bornée, elle est indécidable pour les spécifications données par des automates non-déterministes ou universels, mais décidable et ExpTime-complète pour les automates déterministes sur (ℕ,=) et (ℚ,<). Nous exhibons également une sous-classe décidable dans le cas de (ℕ,<), à savoir les spécifications unilatérales. Dans une seconde partie, nous examinons comment étendre au cas non-réactif, où l’implémentation est autorisée à attendre d’obtenir plus d’information avant de sélectionner son signal de sortie, toujours dans le cadre des mots de données. Les spécifications sont modélisées par des transducteurs non-déterministes asynchrones, qui produisent un mot (possiblement vide) à chaque fois qu’ils lisent une entrée. Déjà dans le cas fini, un tel problème est indécidable pour cette classe de spécifications. Une manière de contourner la difficulté est de traiter le cas des spécifications fonctionnelles, pour lesquelles chaque suite infinie d’entrées admet au plus une suite de sorties. Pour les implémentations données par des transducteurs déterministes sur l’entrée, le problème est indécidable, aussi nous intéressons-nous au problème de la calculabilité au sens de Turing, classiquement étendue au cas des mots infinis. Nous lions cette notion à celle de continuité pour la distance de Cantor, ce qui nous fournit une caractérisation de la calculabilité qui est décidable pour les fonctions définies par des transducteurs non-déterministes asynchrones sur (ℕ,=) et pour la classe des domaines oligomorphes, qui englobe (ℕ,=) et (ℚ,<). L’étude se conclut par le cas de (ℕ,<), également décidable. Pour ces trois domaines, les problème de calculabilité et ses déclinaisons, ainsi que la fonctionnalité, sont décidables en espace polynomial (PSpace)."
"Une vaste quantité d'outils mathématiques permettant la modélisation et l'analyse des problèmes multi-agents ont récemment été développés dans le cadre de la théorie du transport optimal. Dans cette thèse, nous étendons pour la premi\`ere fois plusieurs de ces concepts à des problématiques issues de la théorie du contrôle. Le premier résultat présenté dans ce manuscrit est la généralisation du principe du maximum de Pontryagin aux problèmes de contrôle optimal multi-agent étudiés dans leur approximation par limite de champs moyen. La preuve de ces résultats repose sur la généralisation de techniques du contrôle géométrique au cadre de la structure Riemannienne des espaces de Wasserstein. Par la suite, nous investiguons des conditions suffisantes de régularité Lipschitz en espace pour les contrôles optimaux. Ces résultats sont généralement cruciaux pour assurer une correspondance stricte entre les modèles microscopiques et leurs approximations macroscopiques. Nous les obtenons en combinant une approximation par limite de champs moyens et un argument d'existence de feedback Lipschitz optimaux pour les modèles microscopiques sous-jacents. Nous nous intéressons ensuite aux modèles d'alignement. Nous proposons une analyse de convergence de type Lyapunov pour une classe de systèmes coopératifs présentant des défauts aléatoires de communication. Par la suite, nous présentons une stratégie de contrôle parcimonieuse permettant d'assurer la convergence de systèmes faiblement coopératifs vers un état de presque-alignement. Nous présentons enfin un résultat de géométrie sous-Riemannienne, dans lequel nous achevons la classification des singularités génériques du lieu conjugué pour les distributions de contact en dimension 3. Ce résultat se base sur des arguments de transversalité appliqués aux jets de la métrique au voisinage de l'origine."
"Les Systèmes-sur-Puce (Systems on Chip, SoC) sont de plus en plus embarqués dans des systèmes à risque comme les systèmes aéronautiques et les équipements de production d’énergie. Cette évolution technologique permet un gain de temps et de performance, mais présente des limites en termes de fiabilité et de sécurité. Ainsi, le développement d’outils de surveillance et de diagnostic des systèmes électroniques embarqués, en particuliers les SoC, est devenu l’un des verrous scientifiques à lever pour assurer une large utilisation de ces systèmes dans les équipements à risque en toute sécurité. Ce travail de thèse s’inscrit dans ce contexte, et a pour objectif le développement d’une approche de détection et identification des dérives des performances des SoC embarqués. L’approche proposée est basée sur un modèle incrémental, construit à partir de modules réutilisables et échangeables pour correspondre à la large gamme de SoC existants sur le marché. Le modèle est ensuite utilisé pour estimer un ensemble de caractéristiques relatives à l’état de fonctionnement du SoC. L’algorithme de diagnostic développé dans ce travail consiste à générer des indices de dérives par la comparaison en ligne des caractéristiques estimées à celles mesurées. L’évaluation des résidus et la prise de décision sont réalisées par des méthodes statistiques appropriées à la nature de chaque indice de dérive. L’approche développée a été validée expérimentalement sur des SoC différents, ainsi que sur un démonstrateur développé dans le cadre de ce travail. Les résultats expérimentaux obtenus, montrent l’efficacité et la robustesse de l’approche développée."
"La décroissance en fréquence des deux chants de baleine bleue de l'océan pacifique sud est est examiné sur plusieurs décennies en utilisant comme source des données acoustiques de l'Equateur à la Patagonie chilienne. La fréquence de pulsation et la fréquence pic des signaux sont mesurés en utilisant deux méthodes distinctes (auto-corrélation sommée et transformée de Fourier rapide). Les sources d'erreur associées à chaque mesure sont estimées. Il y a un déclin linéaire de ces deux fréquences pour le chant le plus commun de cette zone (chant du Pacifique Sud Est n°2, SEP2). Un analyse plus rapide montre aussi une baisse linéaire, entre 1970 et 2014, de la fréquence du chant SEP1, plus rarement enregistré dans cette zone. Ces deux baisses ont des amplitudes similaires. L'intérêt de mesurer la fréquence de pulsation et la fréquence pic de façon concomitante est estimé. Enfin, une comparaison globale des déclins en fréquence de tous les types de chants de baleines bleues est fournie."
"This paper presents a new way of interaction between modelers and solvers to support the Product Development Process (PDP). The proposed approach extends the functionalities and the power of the solvers by taking into account procedural constraints. A procedural constraint requires calling a procedure or a function of the modeler. This procedure performs a series of actions and geometric computations in a certain order. The modeler calls the solver for solving a main problem, the solver calls the modeler’s procedures, and similarly procedures of the modeler can call the solver for solving sub-problems. The features, specificities, advantages and drawbacks of the proposed approach are presented and discussed. Several examples are also provided to illustrate this approach."
"Cet article présente un système d'analyse automatique en cadres sémantiques évalué sur un corpus de textes encyclopédiques d'histoire annotés selon le formalisme FrameNet. L'approche choisie repose sur un modèle intégré d'étiquetage de séquence qui optimise conjointement l'identification des cadres, la segmentation et l'identification des rôles sémantiques associés. Nous cherchons dans cette étude à analyser la complexité de la tâche selon plusieurs dimensions. Une analyse détaillée des performances du système est ainsi proposée, à la fois selon l'angle des paramètres du modèle et de la nature des données. ABSTRACT FrameNet automatic analysis : a study on a French corpus of encyclopedic texts This article presents an automatic frame analysis system evaluated on a corpus of French encyclopedic history texts annotated according to the FrameNet formalism. The chosen approach relies on an integrated sequence labeling model which jointly optimises frame identification and semantic role segmentation and identification. The purpose of this study is to analyze the task complexity from several dimensions. Hence we provide detailed evaluations from a feature selection point of view and from the data point of view. MOTS-CLÉS : Analyse en cadres sémantiques, étiquetage de séquence, textes encyclopédiques."
"Les formes géométriques, qu’elles proviennent du monde naturel ou du monde manufacturé par l’Homme, ont tendance à être de plus en plus numérisées, cela, entre autres, à des fins de visualisation ou de mesure. Ce processus produit en général des maillages 3D, composés d’une multitude de polygones plans. Ces maillages sont la représentation discrète la plus commune pour caractériser la surface d’une forme virtuelle. Ces représentations surfaciques 3D sont traitées le plus souvent de manière automatique, parfois interactive, afin que leur structure globale ou certains détails soient analysés ou calculés. Cela peut être fait en extrayant des caractéristiques géométriques ou topologiques pertinentes. De telles caractéristiques de forme peuvent simplifier la façon dont l’objet est considéré, elles peuvent aider à la reconnaissance, et elles peuvent le décrire et le classifier selon des critères spécifiques. Ce cours traitera de la définition et du calcul de caractéristiques sur un maillage surfacique 3D et de leur utilisation pour l’analyse de forme. Des méthodes récentes seront décrites pour extraire des caractéristiques ayant une signification liée non seulement à la géométrie mais aussi à la topologie. Plusieurs applications seront développées au cours des travaux pratiques."
"Conversion efficiency of solar cells is now on average reaching 15%. Despite these acceptable yields, it remains important to raise a technological limitation: the improvement of the life span of photovoltaic modules. Effects are observed during the use of these components in intermittent weather: rain, snow, molds, dust UV rays, shock, corrosion etc... rapid losses of optoelectronic properties depending on the usage conditions. The objective of this thesis is in twofold: -to try to improve the lifespan of solar panels, and -to improve the reliability of the photovoltaic modules by decreasing their degradation rate. Our approach begins by taking into account the effects of aging of photovoltaic modules whose degradation is related to temperature, moisture, Ultra violet light, cracks etc... Faults are analyzed and studied to understand the impact and the importance of each parameter in the different modes of degradation to develop simulation models that take into account external environmental conditions. Control algorithms have been developed for a best utilization avoiding defects and allowing photovoltaic modules to operate in optimal conditions for mitigation of degradation processes."
"du système électrique qui la délivre. Les deux grandeurs (tension, courant) subissent gé- néralement beaucoup de perturbations liées à l'impédance des réseaux et à la circulation de courants perturbateurs, déséquilibrés et réactifs liés à la variation de charge non linéaire. Dans le réseau de bord d'un hélicoptère, la prolifération des perturbations électriques peut entraîner des dysfonctionnements, des dégradations et des échauements des récepteurs. La correction active du facteur de puissance ou Power Factor Correction (PFC) a pour but d'éviter les eets indésirables, les harmoniques et perturbation des signaux. Ce type de système, constitué par un convertisseur AC/DC lié à un autre DC/DC, utilise des composants et commutations à hautes fréquences en particulier les MOSFET au Carbure de Silicium (MOSFET SiC). Ce choix permet de garantir une bonne qualité de l'énergie et la réduction du rapport puissance/poids pour les systèmes embarqués. La contribution de la thèse se situe au niveau de la modélisation multi-physique, de la simulation et la synthèse des lois de commande pour les structures de puissance utilisées. Ainsi, la modélisation non linéaire plus précise, devrait conduire à l'exploitation de lois de commande robustes et une bonne maîtrise du comportement thermique. Cette thèse a donné lieu à la proposition d'un modèle non linéaire électrothermique plus précis pour des MOSFET SiC. Ce modèle prend en compte les couplages électrothermiques et rend les premières commandes appliquées (PI et PBC) plus robustes. D'autre part, ce modèle a été validé par la reproduction des caractéristiques comparées aux datasheet. Pour implémenter et valider le modèle proposé et eectuer le prototypage de convertisseurs AC-DC et DC-DC, nous avons utilisé les simulateurs Psim, PSpice et Saber. Pour ces deux derniers simulateurs, nous avons amélioré les modèles des composants proposés que nous avons ajoutés à leurs bibliothèques. L'étude est réalisée dans le cadre d'un projet FUI pour la conception de sources d'énergie ables et stables qui seront embarquées dans les hélicoptères."
"Les usagers de deux-roues motorisés (2RM) figurent parmi les plus vulnérables, le taux de mortalité et d’accidentologie peut être partiellement expliqué par les problèmes liés à la dynamique du véhicule spécifique aux 2RM. D’où la décision d’aborder cette problématique et d’étudier la dynamique de ces véhicules afin de modéliser le comportement d’un 2RM au cours de la phase de pré-choc en prenant en compte les spécificités nécessaires à la reconstruction d’accidents. Un modèle multicorps à 6 corps et 11 degrés de liberté capable de traduire le comportement dynamique d’une moto lors de la réalisation de manœuvres d’urgence a été développé. Une fois le modèle implémenté avec Sim Mechanics, nous l'avons validé grâce à une moto instrumentée lors de la réalisation de manœuvres de type slalom et évitement. Les actions du conducteur, à savoir l’effort appliqué sur le guidon et la vitesse du véhicule, ont été injectées dans le modèle et les résultats obtenus numériquement et expérimentalement ont été comparés. Une fois le modèle numérique validé, nous l’avons utilisé à des fins de reconstruction d’accident. Pour cela, nous avons mis en place une méthodologie : passer sur les lieux de l’accident avec la moto instrumentée afin de recueillir les actions du conducteur en situation normale de conduite, extrapoler ces données vers la situation d’urgence afin de les injecter dans le modèle dans le but de balayer les différentes configurations possibles. Après une analyse approfondie des résultats de simulation, nous pouvons estimer par exemple la vitesse au moment du choc ou l’action qu’à réalisé ou non le conducteur et qui a mené à l’accident."
"La qualité de l'énergie électrique dans les systèmes aéronautiques dépend directement de la complexité du système électrique qui la délivre. Les deux grandeurs (tension, courant) subissent gé- néralement beaucoup de perturbations liées à l'impédance des réseaux et à la circulation de courants perturbateurs, déséquilibrés et réactifs liés à la variation de charge non linéaire. Dans le réseau de bord d'un hélicoptère, la prolifération des perturbations électriques peut entraîner des dysfonctionnements, des dégradations et des échauffements des récepteurs. La correction active du facteur de puissance ou Power Factor Correction (PFC) a pour but d'éviter les effets indésirables, les harmoniques et perturbation des signaux. Ce type de système, constitué par un convertisseur AC/DC lié à un autre DC/DC, utilise des composants et commutations à hautes fréquences en particulier les MOSFET au Carbure de Silicium (MOSFET SiC). Ce choix permet de garantir une bonne qualité de l'énergie et la réduction du rapport puissance/poids pour les systèmes embarqués. La contribution de la thèse se situe au niveau de la modélisation multi-physique, de la simulation et la synthèse des lois de commande pour les structures de puissance utilisées. Ainsi, la modélisation non linéaire plus précise, devrait conduire à l'exploitation de lois de commande robustes et une bonne maîtrise du comportement thermique. Cette thèse a donné lieu à la proposition d'un modèle non linéaire électrothermique plus précis pour des MOSFET SiC. Ce modèle prend en compte les couplages électrothermiques et rend les premières commandes appliquées (PI et PBC) plus robustes. D'autre part, ce modèle a été validé par la reproduction des caractéristiques comparées aux datasheet. Pour implémenter et valider le modèle proposé et effectuer le prototypage de convertisseurs AC-DC et DC-DC, nous avons utilisé les simulateurs Psim, PSpice et Saber. Pour ces deux derniers simulateurs, nous avons amélioré les modèles des composants proposés que nous avons ajoutés à leurs bibliothèques. L'étude est réalisée dans le cadre d'un projet FUI pour la conception de sources d'énergie ables et stables qui seront embarquées dans les hélicoptères."
"Dans les expériences de neuroimagerie fonctionnelle, les participants effectuent un ensemble de tâches pendant que leur activité cérébrale est enregistrée, par exemple en utilisant l’électroencéphalographie (EEG), la magnétoencéphalographie (MEG) ou l'imagerie par résonance magnétique fonctionnelle (fMRI). L'analyse des données d'un groupe de participants, souvent appelée analyse de groupe, vise à identifier des invariants de population qui se rapportent aux tâches accomplies par les participants. Ceci permet de comprendre l'organisation fonctionnelle du cerveau chez les sujets sains et ses dysfonctionnements dans les populations pathologiques. Tandis que les analyses de groupes univariées, basées sur le modèle linéaire généralisé, ont fait l'objet d'études approfondies, de nombreuses questions restent ouvertes pour les analyses de groupe fondées sur des méthodes d'apprentissage machine multivariées. Cette thèse étudie donc sur les analyses de groupe multivariées pour les expériences de neuroimagerie fonctionnelle. Nous nous focalisons sur un schéma d’analyse de groupe multivarié sous utilisé, que nous désignons “analyse de motifs inter-sujet”, qui consiste à entraîner un modèle sur des données d’un ensemble de sujet et à évaluer sa capacité à généraliser sur des données enregistrées dans d’autres sujets. Nous effectuons d’abord une comparaison des résultats fournis par l'analyse de motifs inter-sujet avec ceux obtenus en utilisant la méthode standard. L'analyse inter-sujet offre à la fois une plus grande capacité de détection et facilite l'interprétation des résultats obtenus à un coût de calcul comparable. Dans ce contexte, notre deuxième contribution introduit une formalisation unifiée de l'analyse de motifs inter-sujet, que nous modélisons comme un problème d'apprentissage par transfert transductif multi-sources. Ensuite, nous produisons une revue de la littérature des méthodes développées pour l’analyse de motifs inter-sujet. Notre troisième contribution est une série d’études expérimentales qui examine le bien-fondé de la formalisation par transfert transductif multi-sources de l'analyse de motifs inter-sujet. La quatrième contribution de cette thèse est une nouvelle méthode d'analyse multivariée au niveau du groupe pour les expériences de neuroimagerie fonctionnelle. Notre méthode est basée sur le transport optimal, qui tire parti des propriétés géométriques des cartes d’activité cérébrales pour surmonter les différences inter-individuelles qui ont un impact sur les analyses de groupe traditionnelles."
"Une ligne de recherche présente dans la littérature depuis les années soixante est celle des \emph{théorèmes de représentation}. Son résultat fondateur est le théorème de Chomsky--Schützenberger qui stipule qu'un langage est algébrique si et seulement si il est l'image par homomorphisme de l'intersection entre un langage régulier et le langage de Dyck. Ce résultat a connu depuis diverses généralisations à différentes familles de langages. Dans cette thèse, nous proposons plusieurs généralisations de ce résultat aux langages d'ordres supérieurs. En particulier, nous introduisons une notion de langages de Dyck d'ordres supérieurs, nous définissons et étudions des classes de transductions que nous qualifions d'$\varepsilon$-sûres et nous montrons qu'un langage appartient à un niveau $k+l$ de la hiérarchie des ordres supérieurs si et seulement si il est l'image d'un langage de Dyck de niveau $k$ par une transductions $\varepsilon$-sûre de niveau $l$. Ces résultats nous permettent aussi d'obtenir d'autres types de caractérisations tels que des caractérisations logiques."
"Cette thèse présente un cadre de travail d'analyse de trajectoires contenant une phase de prétraitement et un processus d’extraction de trajectoires d’objets mobiles. Le cadre offre des fonctions visuelles reflétant le comportement d'évolution des motifs de trajectoires. L'originalité de l’approche est d’allier extraction de motifs fréquents, extraction de motifs émergents et analyse formelle de concepts pour analyser les trajectoires. A partir des données de trajectoires, les méthodes proposées détectent et caractérisent les comportements d'évolution des motifs. Trois contributions sont proposées : Une méthode d'analyse des trajectoires, basée sur les concepts formels fréquents, est utilisée pour détecter les différents comportements d’évolution de trajectoires dans le temps. Ces comportements sont “latents”, ""emerging"", ""decreasing"", ""lost"" et ""jumping"". Ils caractérisent la dynamique de la mobilité par rapport à l'espace urbain et le temps. Les comportements détectés sont visualisés sur des cartes générées automatiquement à différents niveaux spatio-temporels pour affiner l'analyse de la mobilité dans une zone donnée de la ville. Une deuxième méthode basée sur l'extraction de concepts formels séquentiels fréquents a également été proposée pour exploiter la direction des mouvements dans la détection de l'évolution. Enfin, une méthode de prédiction basée sur les chaînes de Markov est présentée pour prévoir le comportement d’évolution dans la future période pour une région. Ces trois méthodes sont évaluées sur ensembles de données réelles. Les résultats expérimentaux obtenus sur ces données valident la pertinence de la proposition et l'utilité des cartes produites."
"Cet article propose une méthode qui analyse des données représentant des trajectoires d'objets mobiles. La méthode se base sur les motifs fréquents et introduit différentes types de motifs, par exemple, les motifs latents, émergents, etc. Un ensemble d'algorithmes sont alors introduits pour pré-traiter les données, extraire les motifs fréquents et détecter les types des motifs. Ces motifs géolocalisés sont ensuite utilisés pour tagger une zone spatiale déterminée. La classification d'une trajectoire consiste alors en sa projection sur la zone spatiale ce qui conduit nécessairement à une classification multi-labels dépendant de la granularité spatio-temporelle. Pour finir, nous discutons de l'application de notre méthode sur des données réelles représentant des trajectoires de taxis."
"Cette thèse présente un modèle résilient pour piloter un avion basé sur une logique non monotone. Ce modèle est capable de gérer des solutions à partir d’informations incomplètes, contradictoires et des exceptions. C’est un problème très connu dans le domaine de l’Intelligence Artificielle, qui est étudié depuis plus de 40 ans. Pour ce faire, nous utilisons la logique des défauts pour formaliser la situation et trouver des actions possibles. Grâce à cette logique, nous pouvons transformer les règles de pilotage en défauts. Ensuite, lorsque nous calculons les solutions, plusieurs options peuvent en résulter. À ce stade, il existe un critère de décision opportuniste pour choisir la meilleure solution. Le contrôle du système se fait via la propriété de résilience. Nous redéfinissons cette propriété comme l’intégration de la logique non-monotone dans le modèle de Minsky. En conséquence, il est démontré que le modèle de résilience proposé pourrait être généralisé aux systèmes intégrant une connaissance du monde contenant des situations, des objectifs et des actions. Enfin, nous présentons les résultats expérimentaux et la conclusion de la thèse en discutant des perspectives et des défis pour les orientations futures. Différentes applications dans d’autres domaines sont prises en compte pour l’intérêt du comportement du modèle."
"La notion de symétrie est largement étudiée dans différents domaines, en particulier dans la program-mation par contraintes où plusieurs travaux ont été réalisés. L'élimination des symétries a permis d'amé-liorer de manière significative les performances de nombreux solveurs. Généralement, les problèmes combinatoires contiennent un nombre important de symétries rendant leurs résolutions impraticables par les solveurs qui ne les considèrent pas. En effet, ces symétries guident souvent les solveurs à explorer inutilement des branches symétriques et redon-dantes. Dans ce papier, nous nous sommes intéressé à la programmation par ensembles réponses (Ans-wer Set Programming, ASP). L'ASP est un paradigme bien connu dans la représentation et le raisonnement sur les connaissances. Cependant, seuls quelques travaux concernant l'exploitation des symétries dans l'ASP ont été faits. Dans ce travail, nous étudions la détection et l'élimination des symétries pour une nouvelle méthode ASP que nous introduisons. Cette méthode est basée sur une nouvelle sémantique qui étend celles des modèles stables. Pour montrer l'im-pact de l'élimination des symétries dans notre mé-thode, nous l'avons expérimenté avec et sans symé-tries sur une grande variété de problèmes combina-toires et nous l'avons comparé à d'autres systèmes ASP connus. Les résultats obtenus sont très promet-teurs."
"Le but des modèles traditionnels en classification (comme les partitions et les hiérarchies de parties) est de permettre de discriminer sans ambiguïté et donc de produire des classes non empiétantes (i.e. l’intersection de deux classes est vide ou une classe est incluse dans l'autre). Cependant, cette exigence de non ambiguïté peut conduire à occulter de l’information. Dans le cas des plantes hybrides en biologie par exemple ou encore de textes appartenant à plusieurs genres en analyse textuelle. Les modèles généraux comme les hypergraphes ou les treillis permettent de prendre en compte l’empiétance entre les classes. Ce travail porte sur les hypergraphes totalement équilibrés clos par intersection et leurs équivalents. Ces hypergraphes sont définis comme étant des hypergraphes sans cycle (cycle spécial, aussi appelés alpha-cycle) et constituent une généralisation des arbres. Ils sont équivalents aux treillis démontables (i.e. treillis tels qu'il existe récursivement un élément doublement irréductible) et présentent des propriétés structurelles et algorithmiques qui leur permettent de bien se prêter à de nombreux domaines comme la phylogénie et de traiter différents types de données comme les dissimilarités, les matrices individus/attributs ou encore les graphes. En apprentissage automatique, les arbres de décision sont un modèle très utilisé pour leur simplicité d'utilisation et de compréhension. Une partie de ce travail porte sur le développement de méthodes similaires aux arbres de décision mais s'appuyant principalement sur la structure des données et permettant l’empiétance des classes afin de fournir une représentation plus complète des données. Les modèles visés sont donc fortement interprétables et peuvent également être utilisés pour les tâches classiques d'apprentissage automatique comme la prédiction de classe. Cette thèse présente deux méthodes : - les arbres de décision K-Means, une méthode d'apprentissage automatique utilisant la structure des données plutôt que les sorties attendues et présentant des résultats en classification équivalents aux arbres de décision classiques; - les treillis de centre de gravité, proposant une première approche pour un modèle en classes empiétantes. Dans le cas des arbres de décision, l'usage veut que les arbres utilisés soient binaires. Nous définissons donc les hypergraphes binaires afin de conserver la simplicité propre aux arbres de décision. Nous proposons une caractérisation des hypergraphes binaires par une séquence d'arbres (similaire à celle donnée par Lehel en 1985 pour les hypergraphes totalement équilibrés) et prouvons l'équivalence entre les hypergraphes binarisables (i.e. tels qu'ils peuvent être plongés dans un hypergraphe binaire) et les hypergraphes totalement équilibrés, faisant de ces hypergraphes particuliers un bon candidat à la classification inspirée des arbres de décision. Nous proposons également une binarisation des treillis démontables pouvant être appliquée dans le cadre de l'analyse de concepts formels. Ce travail présente de plus un aspect métrique en définissant les dissimilarités totalement équilibrées (les dissimilarités associées à un système totalement équilibré) et en donnant un algorithme de reconnaissance de dissimilarités totalement équilibrées, un algorithme d'approximation et enfin un algorithme permettant de calculer le système de classes associé à une dissimilarité totalement équilibrée de manière polynomiale."
"Les consommateurs ont l'habitude de consulter les critiques postées sur internet avant d'acheter un produit. Mais, il est difficile pour le consommateur de connaître l'opinion globale du produit vu le nombre important de ces critiques. L'analyse des sentiments permet de détecter la polarité (positive, négative ou neutre) sur une opinion exprimée et donc de classer ces critiques. Notre but est de déterminer l'influence de l'expression des émotions sur l'analyse de la polarité des critiques de livres. Nous définissons des modèles de représentation ""sac de mots"" de critiques qui s'appuient sur un lexique contenant des mots porteurs de sentiments (positif, négatif) et d'émotions (anticipation, tristesse, peur, colère, joie, surprise, confiance, dégoût). Ce lexique permet de mesurer les types d'émotions ressenties par les lecteurs. L'apprentissage supervisé mis en oeuvre est de type forêt aléatoire (Random Forest). L'application concerne des critiques de la plateforme Amazon."
"La synthèse est un domaine de l’informatique consistant à générer des pro- grammes à partir de spécifications abstraites. Les spécifications sont souvent décrites à l’aide d’un formalisme logique et les programmes sont obtenus sous la forme de modèles de transformation. Alors qu’il est utile de pouvoir exprimer les propriétés des spécifications avec du non-déterminisme, nous souhaitons en général obtenir des modèles déterministes pour des raisons évidentes d’efficacité. Ceci nous amène à vouloir simplifier les modèles synthétisés afin d’optimiser leur évaluation ou leur représentation concrète dans un programme. Dans cette thèse, les modèles de transformations qui nous intéressent sont exprimés par des Streaming String Transducers (SSTs) [AC10 ; AC11]. Un SST est un automate fini déterministe équipé d’un nombre fini de registres qui peuvent être utilisés pour élaborer un mot de sortie. Ces registres peuvent être mis à jour en utilisant la concatenation de registres ou en les préfixant ou suffixant par des mots finis. Nous sommes intéressés par le problème ambitieux de la minimisation de registres, qui consiste, étant donné un SST, à calculer un SST équivalent avec un nombre minimal de registres. Comme première étape à la prise en compte de ce modèle très expressif, nous contraignons la manière dont les registres sont manipulés : ils ne peuvent pas être concaténés les uns aux autres (cette classe est appelée Concatenation-Free SST). Nous présentons deux contributions principales. Tout d’abord, nous élaborons une procédure permettant de minimiser le nombre de registres dans la classe des Copyless Appending SSTs (dans cette classe, les registres ne peuvent qu’être suffixés par un mot). Ensuite, nous montrons, étant donné un Copyful Concatenation-Free SST, comment décider s’il existe un Concatenation-Free SST équivalent à un seul registre. Lorsque l’on considère la simplification des Finite-State Transducers (FST), un problème classique est le problème de la séquentialité [Cho77], qui demande si un FST donné admet un FST séquentiel équivalent. Pour nos deux résultats, les techniques de preuves utilisées généralisent le cadre créé autour du problème de séquentialité."
"Dans les messageries sociales les emojis sont parmi les principaux vecteurs d'émo-tions et de sentiments des individus. Aujourd'hui, les utilisateurs naviguent dans des biblio-thèques contenant souvent des milliers d'emojis pour sélectionner celui correspondant à ce qu'ils souhaitent transmettre. Nos travaux visent à développer un système de recommandation automatique d'emoji permettant à l'utilisateur d'identifier un panel réduit d'emojis pertinents étant donnée sa conversation en évitant le parcours de bibliothèques conséquentes d'emojis. Cette recommandation pouvant permettre à l'utilisateur de requêter les phrases susceptibles de contenir cet emoji, et l'émotion qui y est associée. Pour ce faire, dans un premier temps, notre objectif est de développer un outil permettant de prédire automatiquement les emojis d'une phrase à partir d'un modèle de classification appris sur un corpus de messagerie sociale conte-nant des emojis. Plusieurs caractéristiques sont considérées pour l'apprentissage telles que le sentiment de l'utilisateur mais aussi son humeur. Dans cet article, nous décrivons l'impact de ces caractéristiques et les performances des modèles résultants. ABSTRACT. Emojis are among the main carriers of emotions and sentiment in social messaging applications. Nowadays users have to scroll down libraries of thousands of emojis in order to select the one they wanted to use. Our work aims to build an emoji automatic recommendation system to avoid scrolling emoji libraries. And which will allow the user to request emojis by the current sentence based on the emotion it conveys. To do so, we first contribute by building an emoji automatic prediction in sentences based on a classification model. This classification model is learned on an informal text messages corpus based on real data containing emojis. Several features are used to train the classifier. Such as the sentiment value of the text and the user's mood. In this paper we describe the features and models impact on the emoji prediction task. MOTS-CLÉS : Classification multi-étiquette, recommandation d'emoji, analyse de sentiment."
"Avec la dépendance du prix du pétrole et ses variations ainsi que les préoccupations environnementales, les acteurs du transport maritime sont de plus en plus nom-breuxàbreuxà chercher des outils d'aidè a la décision leur per-mettant d'optimiser le temps de trajet en même temps que la consommation de carburant (optimisation bi-objectif). Dans leprobì eme traité, la donnée d'entrée est un itinéraire, et nous nous intéressonsintéressonsà optimiser le temps de trajet et la consommation en fonction d'un unique paramètre : la vitesse le long de cet itinéraire. Nous présentons ainsi une modélisation duprobì eme, puis nous expliquons pourquoi une approche exacte ne semble pas pertinente. Nous proposons ensuite trois mé-thodes de résolution qui semblent adaptées au contexte et qui devront dans un second tempsêtretempsêtre implémentés puis testés afin de juger de leur intérêt pratique."
"Depuis l'arrêt définitif des essais nucléaires, la Direction des Applications Militaires (DAM) du CEA s'appuiesur le triptyque modélisation physique - validation expérimentale - simulation numérique pour comprendre,prévoir et garantir le fonctionnement des armes nucléaires. Parmi les grands équipements contribuant à lavalidation des modèles physiques implémentés dans les logiciels de calcul, le Laser MegaJoule permetd'étudier expérimentalement des phénomènes de même nature que ceux intervenant dans les armes. La familled'objets expérimentés sur cette installation est dénommée « microcibles laser». Les microballons intégrés dansces microcibles sont caractérisés par des techniques de radiographie X. Les brusques variations de niveau degris observées sur les clichés X de microballons témoignent de la 'présence d'un phénomène de contraste dephase, contraste s'ajoutant au contraste d'absorption. L'information contenue dans ce phénomène est utiliséepour une détection de contours du microballon radiographié. Les points trouvés lors de la détection de contourssont alors utilisés par un algorithme d'estimation des défauts de formes des surfaces des microballons. Uneétude paramétrique permet d'identifier le nombre de clichés nécessaire à l'estimation des défauts de forme enfonction du nombre de modes souhaités. L'incertitude de la méthode d'estimation est calculée permettant lacaractérisation complète du microballon."
"Dans ce papier, nous décrivons un schéma de distance approchée pour la famille des graphes pontés sans K_4, avec n sommets. Ce schéma utilise des étiquettes de O(log^3 n) bits.Étant données les étiquettes de deux sommets, la fonction de décodage du schéma calcule en temps constant une valeur entre la distance exacte séparant les sommets et quatre fois cette dernière."
"De nos jours, les systèmes de gestion et de traitement des données sont censés stocker et traiter de grandes séries temporelles. Comme le nombre de variables observées et liées augmente, leur prédiction est devenue de plus en plus compli- quée, et l’utilisation de toutes les variables pose des problèmes pour les modèles de prédiction classiques. Dans le domaine de la prédiction de séries temporelles massives, l’un des prin- cipaux problèmes qui se posent dans les applications réelles, est comment dé- terminer l’ensemble de variables prédictives les plus pertinentes, qui vont être utilisées dans un modèle de prédiction multi-varié en vue de prédire une série temporelle cible. Les modèles de prédiction sans facteurs externes étaient les premières modèles de prédiction des données. En vue d’améliorer la precision des prédictions de ces modèles, l’utilisation de multiples variables est devenue commune. Ainsi, les modèles qui tiennent en compte des facteurs externes, ou bien les modèles multi- variés, apparaissent et deviennent de plus en plus utilisés car ils prennent en compte plus d’informations que les modèles uni-variés. Avec l’augmentation des données liées entre eux, l’application des modèles multi-variés devient aussi discutable. Parce que l’utilisation de toutes les infor- mations existantes n’amène pas forcément aux meilleures prédictions. Par consé- quent, le challenge dans cette situation est de trouver les facteurs les plus perti- nents à partir de l’ensemble des données originales. Dans ce travail, nous étudions ce problème en présentant une analyse détaillée des approches proposées dans la littérature. Nous remarquons que ce qui intéres- sant dans cette problématique est l’aspect temporel lors de l’analyse des dépen- dances entre les variables, et cela est absent dans beaucoup de méthodes bien connues. Nous proposons donc une nouvelle démarche pour approcher ce pro- blème basée sur la sélection des variables en utilisant les graphes de causalité, avec des techniques d’exploration de ces graphes en vue de détecter les variables « Hubs » qui transfèrent l’information par rapports aux autres variables prédic- tives et aux variables à prédire. Finalement, nous présentons une implémenta- tion d’un processus complet pour la prédiction des séries temporelles larges, et comme perspectives, nous présentons une première proposition pour distribuer ce système de prédiction."
"Le corpus CALOR-Frame est un corpus annoté en cadres sémantiques, constitué de textes encyclo-pédiques dans le domaine de l'Histoire et produit conjointement par l'Université d'Aix-Marseille et Orange Labs. La constitution de cette ressource s'inscrit dans le cadre général de la recherche d'information avec pour objectif de favoriser l'accès aux contenus de la connaissance. La structuration en cadres sémantiques permet des recherches avancées dépassant le cadre de la simple recherche par mots-clés. Dans cet article est décrit le processus d'annotation en cadres sémantiques mis en place, qui utilise un outil de validation d'annotations automatiques à des fins d'optimisation. Le choix des textes et des cadres sémantiques considérés est également motivé."
"La surveillance par acoustique passive est un outil d'importance croissante pour l'étude des mammifères marins. Cette thèse pose des nouveaux modèles pour l'étude du plus grand d'entre eux, la baleine bleue (Balaenoptera musculus), qui émet de très basses fréquences. Pour ce faire, nous avons enregistré un corpus inédit dans l'archipel de Humboldt au nord du Chili. Nos données révèlent un chant caractéristique de la baleine bleue 'chilienne', dont nous étudions la structure pulsée et l'évolution au cours des dernières décennies. Le classement en signal tonal ou non-tonal nous permet, en nous focalisant sur la fréquence fondamentale mise à jour, de quantifier la baisse en fréquence des chants et d'effectuer une comparaison des signaux au niveau mondial. Notre troisième contribution est une méthode de localisation mono-hydrophone basée sur des simulations de propagation par éléments spectraux. C'est à notre connaissance le premier modèle de ce type, implémenté en milliers d'heures de calcul haute performance. Nous concluons en soulignant l'intérêt des méthodes en bioacoustique comme moyen de suivi et de connaissance du milieu marin."
"Les agents artificiels (robots humanoïdes et personnages virtuels), peuvent être exploités pour un monde meilleur : pour augmenter l'empathie entre les gens, améliorer leur bien-être, promouvoir les coopérations, encourager les comportements prosociaux, réduire les inégalités sociales par l'apprentissage, etc. S’intégrant dans le récent courant de recherche appelé l’Informatique Prosociale, un certain nombre de travaux dans le domaine des agents artificiels sont aujourd’hui menés avec cet objectif. Outre ces recherches dédiées, quelque que soit le domaine applicatif de nos travaux de recherche, il est impératif de veiller à la conception de nos agents artificiels. En particulier, l’apparence et le comportement de ces derniers peuvent avoir une influence inattendue et non souhaitable sur le comportement de l’utilisateur et ses croyances, non seulement au sein de l’environnement virtuel mais se poursuivant aussi ensuite dans le monde réel. Dans cet article, ces problématiques sont discutées à la lumière des résultats de recherche récents autour de l’influence des agents artificiels. De solutions sont discutés pour veiller à ce que nos agents artificiels puissent être utilisés pour tendre vers un monde meilleur."
"Les commentaires sur des ressources Web (ex. des cours, des films) deviennent de plus en plus exploitées dans des tâches d'analyse de texte (ex. détection d'opinion, détection de controverses). Cet article étudie l'intensité de contradiction dans les commentaires en exploitant différents critères tels que la variation des notations et la variation des polarités autour d'entités spécifiques (ex. aspects, sujets). Premièrement, les aspects sont identifiés en fonction des distributions des termes émotionnels à proximité des noms les plus fréquents dans la collection des commentaires. Deuxièmement, la polarité est estimée pour chaque segment de commentaire contenant un aspect. Seules les ressources ayant des commentaires contenant des aspects avec des polarités opposées sont prises en compte. Enfin, les critères sont évalués, en utilisant des algorithmes de sélection d'attributs, pour déterminer leur impact sur l'efficacité de la détection de l'intensité des contradictions. Les critères sélectionnés sont ensuite introduits dans des modèles d'apprentissage pour prédire l'intensité de contradiction. L'évaluation expérimentale est menée sur une collection contenant 2244 cours et leurs 73873 commentaires, collectés à partir de coursera.org. Les résultats montrent que la variation des notations, la variation des polarités et la quantité de commentaires sont les meilleurs prédicteurs de l'intensité de contradiction. En outre, J48 est l'approche d'apprentissage la plus efficace pour cette tâche."
"La recherche dans le domaine de la détection du feu et de la fumée devient un sujet de plus en plus traité. Les algorithmes conventionnels utilisent exclusivement des modèles basés sur des vecteurs de caractéristiques. Ces vecteurs restent difficiles à définir et dépendent largement du type de feu observé. Ces méthodes donnent des résultats avec un faible taux de détection et un haut taux de fausse classification. Une approche innovante pour résoudre ce problème est d'utiliser un algorithme permettant de déterminer automatiquement les caractéristiques utiles pour classifier le feu et la fumée. Dans cet article, nous proposons un réseau de neurones convolutif pour identifier le feu et la fumée d'une vidéo en temps réel. Les réseaux de neurones convolutifs ont montré leur grande performance dans le domaine de la classification des objets. Testée sur des séquences vidéos réelles, l'approche proposée atteint une meilleure performance de classification que les méthodes conventionnelles. Ces résultats indiquent clairement que l'utilisation des réseaux de neurones convolutifs pour la détection du feu et de la fumée est très prometteuse. Mots Clef Feu, fumé, détection, réseau de neurones convolutifl, apprentissage profond, cartes de caractéristiques, dropout, maxpooling. Abstract Research on video analysis for fire detection has become a hot topic in computer vision. However, the conventional algorithms use exclusively rule-based models and features vector to classify whether a frame is fire or not. These features are difficult to define and depend largely on the kind of fire observed. The outcome leads to low detection rate and high false-alarm rate. A different approach for this problem is to use a learning algorithm to extract the useful features instead of using an expert to build them. In this paper, we propose a convolutional neural network (CNN) for identifying fire in videos. Convolutional neural network are shown to perform very well in the area of object classification. This network has the ability to perform feature extraction and classification within the same architecture. Tested on real video sequences, the proposed approach achieves better classification performance as some of relevant conventional video fire detection methods and indicates that using CNN to detect fire in videos is very promising. It's the English version of the abstract."
"L'appariement d'entités (Entity Matching) est un problème crucial pour l'intégration de données. Dans cet article, nous nous intéressons à l'appariement d'entités dans le domaine du Transport et Logistique lesquelles peuvent être définies par une structure (raison sociale, adresse). Aux difficultés usuelles qui caractérisent la problématique (typos, données manquantes ou redondantes, similarités sémantiques, etc.), s'ajoutent des spécificités « domaine » comme les abréviations et les acronymes dans les noms de sociétés ou l'absence d'un format standard pour les adresses. La solution que nous proposons s'appuie sur un processus en deux phases : 1) Standardisation des entités en vue de leur prétraitement et du parsing d'adresses (données textuelles), et 2) Appariement par apprentissage supervisé sur des représentations vectorielles sémantiques des entités obtenues par des techniques de représentation distribuée des mots. Les expérimentations menées sur un jeu de données réel illustrent la performance de la solution."
"Depuis la fin des années 80, des études numériques ont mis en évidence des mouvements chaotiques dans les trajectoires des planètes du système solaire, en particulier d’un temps caractéristique de sensibilité aux conditions initiales de 5 millions d’années. Cela implique l’impossibilité de prédire sur une durée de 100 millions d’années le mouvement des planètes, celles-ci pouvant aller, après une telle durée, jusqu’à entrer en collision. Toutefois, pour le système des planètes extérieures seules, ces études numériques ont constaté que ce comportement chaotique n’a qu’un effet très faible, pendant la durée de vie du système solaire. Le but de cette thèse est de démontrer certains résultats de stabilité pour des sous-systèmes du système solaire dont les études numériques ont montré la grande stabilité effective. On met en place des méthodes d’analyse de la stabilité des trajectoires, valables pour des systèmes planétaires à n corps assez généraux. On fait appel à des techniques classiques : expression des équations du mouvement dans des variables liées aux éléments elliptiques, moyennisation et méthodes de perturbation (construction à un ordre élevé d’une forme normale de Birkhoff). Du fait d’une dégénérescence du problème séculaire, on montre que la construction de cette forme normale n’est pas réalisable en général. Pour résoudre ce problème, on donne une nouvelle manière de réduire partiellement le problème planétaire de n corps par le biais de l’intégrale du moment cinétique. Finalement, on réalise l’étude pratique d’un système approchant la réalité : le problème de trois corps (Soleil, Jupiter et Saturne) séculaire à l’ordre un du rapport des masses. On démontre un résultat de stabilité : les trajectoires restent proches d’un tore donné de l’espace des phases pendant une durée supérieure à 5 milliards d’années et proches d’une solution quasi-périodique donnée pendant 100 millions d’années."
"L’objectif de cette thèse est d’apporter différentes contributions méthodologiques en bioacoustique pour l’étude de la faune. En effet, la bioacoustique est une science récente, pluridisciplinaire et très efficace pour étudier et classifier un écosystème. Beaucoup d’études ont mis au point des procédés acoustiques pour étudier la faune à des échelles spécifiques, populationnelles, individuelles et comportementales. Ce travail de thèse propose d’étudier différents cas d’études présents dans ces quatre échelles d’analyses. L’objectif de cette thèse est de mettre en place des outils depuis la pose du matériel d’acquisition jusqu’à l’analyse des données pour l’ensemble des échelles présentées, de les discuter et de les mettre en perspective. La bioacoustique spécifique est illustrée ici par la classification automatique d’Orques, de Cachalots et d’oiseaux. Pour la bioacoustique populationnelle, la classification acoustique de clans d’Orques est étudiée. Puis l’échelle d’analyse s’affine et étudie les émissions sonores individuelles. Pour cela 3 cas d’études sont utilisés : la localisation individuelle d’Orques, de Cachalots et d’oiseaux. Ladernière échelle est appelée bioacoustique comportementale, elle a pour but de mettre en corrélation des comportements avec des émissions acoustiques. Pour cela, l’influence du trafic maritime sur les Dauphins tachetés pantropicaux et l’impact de stimuli chimiques chez la Baleine à bosse est étudié. Nous avons volontairement fait le choix de sélectionner différentes espèces produisant des types de signaux bien différents (stationnaires vs transitoires) évoluant dans des milieux différents (marins vs terrestres) afin d’homogénéiser les méthodes d’analyses pour faciliter le développement de nouvelles études en bioacoustique. Chaque cas d’étude présente des résultats intéressants en terme de bioacoustique et d’écologie comportementale. Ces résultats sont comparés avec la bibliographie. Puis, les résultats de chaque cas d’étude permettent de valider les méthodes proposées dans cette thèse. Les apports méthodologiques de cette thèse sont synthétisés, comparés et discutés, notamment l’impact des signaux stationnaires et transitoires, des milieux (marin et terrestre) sur la mise en place des méthodes. Les méthodes supervisées et non supervisées sont mises en comparaison. Les méthodes proposées ont été testées et validées sur certains protocoles de données massives (plusieurs dizaines de Tera). En conclusion, cette thèse montre que les méthodes supervisées (notamment le Deep Learning) étaient très bien adaptées pour la classification de signaux stationnaires en bioacoustique spécifique et populationnelle pour le milieu terrestre et marins. Puis les méthodes non supervisées (clustering et réduction de dimensionnalité) peuvent être utilisées dans le cadre des études en bioacoustique comportementale pour identifier les signaux d’intérêt. Enfin, la bioacoustique individuelle peut se traduire par des méthodes de localisation comme l’estimation du temps de délais d’arrivée inter-capteur, réalisable pour les signaux transitoires, et plus complexe pour les signaux stationnaires."
"Les benzénoïdes sont une sous-famille d'hydrocarbures (molécules composées uniquement d'atomes d'hydrogène et de carbone) dont les atomes de carbone forment des hexagones. Ces molécules ont fait l'objet de nombreuses études en chimie théorique et peuvent posséder différentes propriétés physico-chimiques (résistance mécanique, conductivité électronique,. . .) desquelles découlent de nombreuses applications concrètes. Ces propriétés peuvent notamment reposer sur l'existence ou l'absence de fragments de la molécule correspondant à un motif donné (certains motifs imposent la nature de certaines liaisons, ce qui a un impact sur la structure électronique totale). Générer des structures de benzénoïdes tout en maîtrisant la présence ou non d'un certain motif constitue donc une problématique importante en chimie théorique. Dans cet article, nous montrons comment la programmation par contraintes peut aider les chimistes à répondre à différentes questions autour de cette problématique. Pour ce faire, nous proposons différentes modélisations dont une basée sur une variante du problème d'isomorphisme de sous-graphes et nous générons les structures souhaitées à l'aide du solveur Choco."
"Les benzénoïdes sont une sous-famille d'hydrocarbures (molécules composées uniquement d'atomes d'hydrogène et de carbone) dont les atomes de carbone forment des hexagones. Ces molécules ont fait l'objet de nombreuses études en chimie théorique et possèdent de nombreuses applications concrètes. La génération de benzénoïdes ayant certaines propriétés structurelles (par exemple, ayant un nombre donné d'hexagones ou ayant une structure particulière du point de vue graphique) est un problème intéressant et important. Il constitue une étape préliminaire à l'étude de leurs propriétés chimiques. Dans cet article, nous montrons que modéliser ce problème dans Choco Solver et laisser son moteur de recherche générer les solutions constitue une approche rapide et très flexible. Elle permet notamment de générer des structures répondant aux besoins des chimistes simplement en ajoutant de nouvelles variables et/ou contraintes tout en évitant d'avoir à développer des méthodes algorithmiques ad-hoc. Ce papier est un résumé de [2]."
"Cet article développe un modèle (de contact véhicule-chaussée) pertinent pour l'estimation en ligne de l'état et des forces de contact par un observateur robuste et adaptatif. L'étude est considérée dans le but de la maîtrise des trajectoires du véhicule par la commande, et la détection de situations à situations à risques."
"Les expressions polylexicales (EP), et parmi elles plus particulièrement les EP verbales (EPV), se caractérisent par une grande variabilité idiosyncrasique de forme. La détection et l’identification de ces EPV variées pose ainsi un réel défi à la réalisation d’applications langagières robustes. Cet article met l’accent sur la tâche d’identification dans un corpus de variantes d’une EP verbale déjà rencontrées. Il propose une stratégie d’identification basée sur l’extraction de formes candidates à partir de patrons syntaxiques, suivie de leur classification basée sur des caractéristiques morphologiques et syntaxiques. Ces propriétés sont à la fois absolues (c.-à-d. concernent l’entité considérée) ou relatives (c.-à-d. issues de la comparaison avec des EPV déjà rencontrées). Les performances du système résultant ont été évaluées sur un corpus francophone. Elles montrent une amélioration de 4 points de F-mesure par rapport à une baseline bien établie."
"La recherche d’information et la recommandation correspondent à des processus cognitifs complexes. Pour être efficaces, les approches algorithmiques doivent s’adapter dynamiquement et disposer d’informations nombreuses, notamment sur les contextes d’utilisation et sur les préférences des utilisateurs. C’est ainsi que se sont développées des approches personnalisées d’apprentissage automatique, exploitant de très nombreuses données de façon peu transparente. Nous présentons les approches statistiques les plus populaires, du modèle vectoriel de recherche d’information aux approches connexionnistes à bases de réseaux neuronaux. Nous soulignons les limites inhérentes aux procédures d’apprentissage dans le cadre de tâches subjectives, notamment du point de vue des risques d’uniformisation contre lesquelles une pluralité de modèles est nécessaire. Nous proposons enfin d’étudier le comportement des modèles et des systèmes automatiques sous différents angles, selon des approches pluridisciplinaires."
"Notre objectif est l'élaboration d'un système de détection automatique de relations de coréférence le plus général possible, pour le traitement des anaphores pronominales et les coréférences directes. Nous décrivons dans cet article les différentes étapes de traitement des textes dans le système que nous avons développé : (i) l'annotation en traits lexicaux et syntaxiques par le système Macaon ; (ii) le repérage des mentions par un modèle obtenu par apprentissage sur le corpus ANCOR ; (iii) l'annotation sémantique des mentions à partir de deux ressources : le DEM et le LVF ; (iv) l'an-notation en coréférences par un système à base de règles. Le système est évalué sur le corpus ANCOR. ABSTRACT End-to-end coreference resolution for French. We aim at developing a general coreference resolution system, for processing pronoun anaphora as well as direct coreferences. We describe in this article the different processing steps of the developped system : (i) extraction of lexical and syntactic features using the Macaon system ; (ii) mention detection with a supervised sequence classifier trained on the ANCOR corpus ; (iii) semantic mention tagging with two resources : DEM and LVF ; (iv) coreference prediction with a rule-based system. The system is evaluated on the ANCOR corpus. MOTS-CLÉS : expression référentielle, mention, détection automatique de relations de coréférence."
"Dans cet article, nous proposons une architecture d'un Agent Conversationnel Animé (ACA) socio-affectif. Les différents modèles computationnels sous-jacents à cette architecture, permettant de donner la capacité à un ACA d'exprimer des émotions et des attitudes sociales durant son interaction avec l'utilisateur, sont présentés. A partir de corpus d'individus exprimant des émotions, des modèles permettant de calculer l'expression faciale émotionnelle d'un ACA ainsi que les caractéristiques de ses mouvements du corps ont été définis. Fondés sur une approche centrée sur la perception de l'utilisateur, des modèles permettant de calculer comment un ACA doit adapter son comportement non-verbal suivant l'attitude sociale qu'il souhaite exprimer et suivant le comportement de son interlocuteur ont été construits. Le calcul des émotions et des attitudes sociales à exprimer est réalisé par des modèles cognitifs présentés dans cet article."
"Dans ce rapport de recherche, nous faisons un premier bilan du projet d’enregistrement des dauphins côtiers du sud du Chili, en mai 2021, dans le canal de Puyuhuapi, dans la région de Aysen. Il contient la présentation du projet, le rapport d’expérience, les premiers résultats et les perspectives."
"De nos jours, la mobilité urbaine de biens et de personnes évolue très rapidement. Place à la mobilité douce (tramway, vélo, marche à pied, etc.), aux usages raisonnés de la voiture (covoiturage, véhicules en libre-service…) et aux véhicules électriques. Et si l'avenir de la mobilité urbaine était un nouveau mode de consommation des transports ? Il s'agirait alors de rassembler tous les moyens de transport à disposition dans une seule offre, avec la garantie d'avoir la meilleure solution pour aller partout, à n'importe quel moment, et en associant différents modes. Ce sont les nouveaux défis du déplacement en ville."
"http://nkms.free.fr/ Polytech Marseille, Aix Marseille Université ; Domaine Universitaire Saint Jérôme, av Escadrille Normandie Niémen ; 13397 Marseille Cedex 20 RESUME : L'objectif de cette communication est double : il présente l'approche pragmatique (EAPAPA) que nous proposons (en ingénierie de l'automatique) pour notre formation d'ingénieurs à Polytech Marseille et le projet Erasmus+ nommé WESET (dans lequel cette approche est développée et appliquée) auquel Polytech Marseille contribue comme acteur responsable du module Contrôle, Commande, Optimisation et Diagnostic."
"Dans cet article, nous nous intéressons à l'amélioration de la prédiction de la durée restante de fonctionnement utile d'un système complexe dont l'état est représenté par des séries temporelles de données multivariées. Notre contexte d'application est le domaine de la maintenance prédictive pour l'industrie navale. Nous présentons et évaluons deux approches différentes en mesurant l'amélioration de la prédiction de la durée de vie restante utile (Remaining Useful Life ou RUL) au moyen de quatre approches d'apprentissage automatique utilisant des réseaux de neurones profonds. La première méthode que nous proposons s'appuie sur un ré-échantillonnage de la base d'apprentissage afin de réduire localement les erreurs. La deuxième méthode proposée s'intéresse à la détection automatique et l'utilisation d'un point de rupture dans le signal multivarié pour améliorer la phase d'entraînement. Nous montrons que les techniques de détection de points de rupture permettent une amélioration significative de la performance de prédiction des durées de vie restantes avec des gains allant jusqu'à 27 % sur l'erreur moyenne absolue (MAE) quel que soit le réseau utilisé, ce qui démontre la généricité et l'intérêt de notre approche."
"Les modèles de langue pré-entraînés ont prouvé leur efficacité dans la classification de texte multiclasses. Notre objectif est d'étudier et d'améliorer ce type d'approches pour la classification multilabels de texte, une tâche étonnamment peu explorée au cours de ces toutes dernières années. Cette tâche a pourtant des applications industrielles importantes telles que la recommandation de contenu, l'extraction de méta-données pour l'enrichissement des bases de données ou le routage automatique multicritères des emails. Dans cet article, notre originalité est de proposer des méthodes d'exploitation des activations des couches de sortie des transformeurs pour améliorer la performance de ces modèles pour la classification multilabels. Notre contribution concerne l'évaluation de l'utilité des méthodes de seuillage sur plusieurs modèles d'apprentissage profond, en calculant un seuil de classification global pour optimiser l'ensemble des classes (SGO), ou un seuil individuel propre à chaque classe étudiée (SI). Elle concerne aussi la proposition de deux approches pour la classification multilabels de texte. La première approche (NPA) consiste à ajouter un paramètre pour l'apprentissage du nombre de classes et/ou labels N présentes pour un exemple donné, pour considérer les classes qui correspondent aux N activations les plus élevées comme étant des labels valides. La deuxième approche (TL) consiste à ajouter une couche au transformeur pour l'apprentissage des critères utiles pour la sélection des labels pertinents. Nous évaluons ces approches sur des corpus d'articles de journaux et d'articles scientifiques. Nous avons aussi constitué et mis à disposition un jeu de données de résumés d'articles scientifiques en français que nous avons conçu à partir du dépôt d'archives ouvertes 'HAL'. Ces évaluations montrent que la performance de nos propositions dépasse celles des méthodes de l'état de l'art de classification multilabels de texte pour les jeux de données étudiés, et sont transposables à tout problème de classification multilabels utilisant les réseaux de neurones."
Cet article présente des méthodes permettant l’évaluation de la satisfaction client à partir de très vastes corpus de conversation de type “chat” entre des clients et des opérateurs. Extraire des connaissances dans ce contexte demeure un défi pour les méthodes de traitement automatique des langues de par la dimension interactive et les propriétés de ce nouveau type de langage à l’intersection du langage écrit et parlé. Nous présentons une étude utilisant des réponses à des sondages utilisateurs comme supervision faible permettant de prédire la satisfaction des usagers d’un service en ligne d’assistance technique et commerciale.
Les conversations techniques en ligne sont un type de productions linguistiques qui par de nombreux aspects se démarquent des objets plus usuellement étudiés en traitement automatique des langues : il s’agit de dialogues écrits entre deux locuteurs qui servent de support à la résolution coopérative des problèmes des usagers. Nous proposons de décrire ici ces conversations par un étiquetage en actes de dialogue spécifiquement conçu pour les conversations en ligne. Différents systèmes de prédictions ont été évalués ainsi qu’une méthode permettant de s’abstraire des spécificités lexicales du corpus d’apprentissage.
"Les dialogues ont une place importante dans la société et celle-ci s’accroît au fur et à mesure que la technologie progresse. Il existe de plus en plus d’outils pour dialoguer à distance permettant la collecte d’une masse importante de données, utilisables pour réaliser différentes analyses et divers systèmes automatiques. L’analyse du discours conversationnel est une réponse partielle pour comprendre certains aspects de la production du langage dans les dialogues. Une telle analyse permet de caractériser les interactions entre les messages d’un dia- logue et ainsi faire ressortir les différents enjeux ou d’identifier les échanges nécessaires pour faire progresser le dialogue. Produire ces analyses est une tâche complexe. Le nombre important de théories d’analyse du discours illustre bien la complexité pour un humain à définir des structures discursives modélisant l’ensemble des interactions. Ceci rend la production d’un grand corpus annoté très couteuse et le peu de données annotées rend difficile l’utilisation d’algorithmes d’apprentissage supervisés. Dans cette thèse, je propose de produire des représentations du discours conversationnel en s’appuyant sur peu de données annotées discursivement. La thèse s’inscrit dans le cadre de l’ANR DATCHA me donnant accès à un grand corpus de tchats provenant de l’entreprise Orange. Ce corpus me permet d’explorer plusieurs stratégies pour produire des représentations du discours : s’appuyer sur un modèle bout-en-bout prédisant la satisfaction des clients ; se fonder sur des annotations en actes de dialogue pour produire des plongements de phrases ; utiliser des algorithmes supervisés sur un corpus enrichi automatiquement."
"Cet article présente un système d’identification de personnes dans des flux multimédia. Ce système a été engagé dans le défi REPERE, co-organisé par l’ANR et la DGA et qui s’est terminé en 2014. La tâche principale du défi consistait à identifier des individus apparaissant dans au moins une des modalités portées par la vidéo, qu’il s’agisse de locuteurs audibles ou de visages visibles à l’écran. Un des verrous scientifiques majeurs de cette tâche est lié à la combinaison des modalités audio et vidéo. Cet article présente une stratégie pour la reconnaissance des personnes basée sur une identification du locuteur reposant sur des descripteurs de haut niveau, modélisant différents aspects de la scène filmée : la transcription et l’analyse des textes incrustés, l’identification du type de la scène filmée (reportage, plateau, ...), le nombre de personnes présentes, la disposition des caméras... Nos expériences sur le corpus REPERE montrent l’intérêt de l’approche proposée."
"L'identification du rôle d'un locuteur dans des émissions de télévision est un problème de classification de personne selon une liste de rôles comme présentateur, journaliste, invité, etc. À cause de la non-synchronie entre les modalités, ainsi que par le manque de corpus de vidéos annotées dans toutes les modalités, seulement une des modalités est souvent utilisée. Nous présentons dans cet article une fusion multimodale des espaces de représentations de l'audio, du texte et de l'image pour la reconnaissance du rôle du locuteur pour des données asynchrones. Les espaces de représentations monomodaux sont entraînés sur des corpus de données exogènes puis ajustés en utilisant des réseaux de neurones profonds sur un corpus d'émissions françaises pour notre tâche de classification. Les expériences réalisées sur le corpus de données REPERE ont mis en évidence les gains d'une fusion au niveau des espaces de représentations par rapport aux méthodes de fusion tardive standard. ABSTRACT Multimodal embedding fusion for robust speaker role recognition in video broadcast Person role recognition in video broadcasts consists in classifying people into roles such as anchor, journalist, guest, etc. Existing approaches mostly consider one modality, either audio (speaker role recognition) or image (shot role recognition), firstly because of the non-synchrony between both modalities, and secondly because of the lack of a video corpus annotated in both modalities. Deep Neural Networks (DNN) approaches offer the ability to learn simultaneously feature representations (embeddings) and classification functions. This paper presents a multimodal fusion of audio, text and image embeddings spaces for speaker role recognition in asynchronous data. Monomodal embeddings are trained on exogenous data and fine-tuned using a DNN on 70 hours of French Broadcasts corpus for the target task. Experiments on the REPERE corpus show the benefit of the embeddings level fusion compared to the monomodal embeddings systems and to the standard late fusion method. MOTS-CLÉS : Identification du rôle du locuteur, fusion multimodale, émissions de télévision."
"Identifier et nommer à chaque instant d'une vidéo l'ensemble des personnes présentes à l'image ou s'exprimant dans la bande son fait parti de ces nouveaux outils de fouille de données. D'un point de vue scientifique la reconnaissance de personnes dans des documents audiovisuels est un problème difficile à cause des différentes ambiguïtés que présentent l'audio, la vidéo et leur association. Nous présentons dans cette étude le système PERCOL0, développé dans le cadre du défi REPERE, permettant de détecter la présence de personnes (audible et/ou visuelle) dans des documents vidéo, sans utiliser de modèles de locuteurs a priori. ABSTRACT Percol0-A multimodal person detection system in video documents The goal of the PERCOL project is to participate to the REPERE multimodal evaluation program by building a consortium combining different scientific fields (audio, text and video) in order to perform person recognition in video documents. The two main scientific challenges we are addressing are firstly multimodal fusion algorithms for automatic person recognition in video broadcast ; and secondly the improvement of information extraction from speech and images thanks to a combine decoding using both modalities to reduce decoding ambiguities. MOTS-CLÉS : Reconnaissance Automatique de la Parole, Segmentation en locuteurs, reconnaissance de l'écriture, détection de visages."
"Nous présentons une architecture pour l'analyse syntaxique en deux étapes. Dans un premier temps un analyseur syntagmatique construit, pour chaque phrase, une liste d'analyses qui sont converties en arbres de dépendances. Ces arbres sont ensuite réévalués par un réordonnanceur discriminant. Cette méthode permet de prendre en compte des informations auxquelles l'analyseur n'a pas accès, en particulier des annotations fonction- nelles. Nous validons notre approche par une évaluation sur le corpus arboré de Paris 7. La seconde étape permet d'améliorer significativement la qualité des analyses retournées, quelle que soit la métrique utilisée."
"Ce travail démontre la faisabilité d'entraîner des chatbots sur des traces de conversations dans le domaine de la relation client. Des systèmes à base de modèles de langage, de recherche d'information et de traduction sont comparés pour la tâche. ABSTRACT Training chatbots for customer relation management This work demonstrates the feasability of training chatbots on customer relation conversation traces. Systems based on language models, information retrieval and machine translation are compared."
"Le projet ANR PORTMEDIA avait pour objectif de compléter le corpus MEDIA afin de favoriser le développement de méthodes performantes, notamment statistiques, pour la compréhension automatique de la parole dans le cadre des systèmes de dialogues homme-machines. Les principaux axes traités sont : la robustesse aux erreurs de reconnaissance de la parole, la portabilité multilingue, la portabilité multi-domaines et la représentation sémantique haut-niveau. Ainsi tout en élaborant au sein du projet des éléments de solution à ces problématiques nous nous sommes principalement attachés à élaborer des données et meta-données permettant ensuite à d’autres groupes de recherche d’évaluer dans les meilleures conditions possibles leurs propres propositions."
"Systems on Chip are increasingly embedded in safety-critical systems, such as aeronautical systems and energy production equipment. Such technological evolution allows for significant improvements in performance but presents limits in terms of reliability and security. Therefore, the development of new tools for the monitoring and diagnosis of embedded electronic systems, Systems on Chip, in particular is currently one of the scientific challenges to overcome, in order to ensure a broader and safer use of these systems in safety-critical equipment. The work presented in this thesis aims to develop an approach for detecting and identifying drifts in embedded Systems of Chips characteristics and performance. The proposed approach is based on an incremental model built from reusable and exchangeable modules able to adapt and accommodate the broad range of Systems on Chips available on the market. This model is then used to estimate a set of characteristics relating to the state of operation of the SoC. The diagnostic algorithm developed in this work consists of generating drift signals though the online comparison of the estimated characteristics to those measured. Then, the assessment of residuals and decision making are performed by statistical methods appropriate to the nature of each drift. The developed approach has been experimentally validated on different Systems on Chip, as well as on a demonstrator developed as part of this work. The obtained experimental results validate and show the efficiency and robustness of the incremental model and the monitoring algorithm"
"Nous nous intéressons ici à l'analyse de conversation par chat dans un contexte orienté-tâche avec un conseiller technique s'adressant à un client, où l'objectif est d'étiqueter les énoncés en actes de dialogue, pour alimenter des analyses des conversations en aval. Nous proposons une méthode légèrement supervisée à partir d'heuristiques simples, de quelques annotations de développement, et une méthode d'ensemble sur ces règles qui sert à annoter automatiquement un corpus plus large de façon bruitée qui peut servir d'entrainement à un modèle supervisé. Nous comparons cette approche à une approche supervisée classique et montrons qu'elle atteint des résultats très proches, à un coût moindre et tout en étant plus facile à adapter à de nouvelles données."
"Les sous-graphes isométriques d'hypercubes (dit cubes partiels) constituent une classe centrale de la théorie métrique des graphes. Ils englobent des familles de graphes importantes (arbres, graphes médians, graphes de topes de complexes de matroïdes orientés, etc.), provenant de différents domaines de recherche, tels que la géométrie discrète, la combinatoire ou la théorie géométrique des groupes. Nous étudions tout d'abord la structure des cubes partiels de VC-dimension $2$. Nous montrons que ces graphes peuvent être obtenus par amalgamations à partir de cycles et de subdivisions entières des graphes complets. Cette décomposition nous permet d’obtenir diverses caractérisations. En particulier, tout cube partiel de VC-dimension $2$ peut être complété en cube partiel ample de VC-dimension $2$. Nous montrons ensuite que les graphes de topes des matroïdes orientés et des complexes de matroïdes orientés uniformes peuvent aussi être complétés en cubes partiels amples de même VC-dimension. En utilisant un résultat de Moran et Warmuth, nous établissons que ces classes vérifient la conjecture de Floyd et Warmuth, l'une des plus vielles conjectures en théorie de l'apprentissage. C'est-à-dire qu'elles admettent des schémas de compression (étiquetés non propres) de taille leur VC-dimension. Par la suite, nous décrivons un schéma de compression étiqueté propre de taille $d$ pour les complexes de matroïdes orientés de VC-dimension $d$, généralisant ainsi le résultat de Moran et Warmuth pour les amples. Enfin, nous fournissons une caractérisation par pc-mineurs exclus et par sous-graphes isométriques interdits des cubes partiels plongeables isométriquement dans la grille $\mathbb{Z}^2$ et dans le cylindre $P_n \square C_{2k}$ pour un certain $n$ et $k > 4$."
fr_abstract_s
"Depuis plus de trois décennies, les investissements directs étrangers deviennent de plus en plus incontournables dans le processus de développement des nations. Que se soient les pays développés ou les PVD, la course aux IDE est devenue l'objectif majeur de la politique économique des Etats, lesquels rivalisent d'idées dans l'attractivité envers les FMN. Après plus d'une décennie de croissance négative entre 1985 et 1995, les Pays de la CEMAC retrouvent une croissance positive avec une particularité pour la Guinée équatoriale dont le taux annuel moyen de croissance entre 2000 et 2005 était de 27%, alors que la moyenne de la zone se situait autour de 9,7%. Cette croissance particulière est due à la découverte du pétrole en Guinée équatoriale et au Tchad, à l'augmentation des gisements du pétrole marin au Cameroun et à un afflux exceptionnel des IDE dans ce secteur. Cette thèse analyse l'impact des IDE sur l'économie des pays d'Afrique Centrale (zone CEMAC) à travers trois objectifs principaux : 1) analyser l'évolution de la croissance et des flux d'IDE entrant dans la CEMAC, 2) faire ressortir l'ensemble des principaux déterminants des IDE de la zone CEMAC, 3) étudier les effets des IDE sur la croissance économique de chacun des pays de la CEMAC. Le chapitre 1 fait une analyse descriptive des flux d'IDE entrants et du taux de croissance du PIB dans la CEMAC entre 1960 et 2005. Il ressort de cette analyse que les flux d'IDE cumulés représentent environ 40% du PIB de la CEMAC (inégalement répartis entre les pays), même si la CEMAC ne compte que pour moins d'un pour cent de l'IDE mondial. On constate également que les périodes d'afflux massifs d'IDE sont suivies d'un regain de croissance dans le pays bénéficiaire. Le chapitre deux essaie de montrer les atouts dont dispose la CEMAC pour attirer les IDE. Ainsi, les politiques d'attractivités (économiques, monétaires et institutionnelles) mises en place sont passées en revue, de même que les abondantes ressources naturelles de la région pour lesquelles les investisseurs peuvent s'intéresser. Il est aussi fait cas des problèmes qui limitent les entrées d'IDE dans la région tels la corruption et l'instabilité politique. Le chapitre trois recherche les déterminants des IDE dans la zone : pour ce faire, un modèle gravitationnel enrichi est construit et testé dans un panel dynamique GMM. Les résultats obtenus montrent que les IDE vers la CEMAC sont des IDE qui privilégient les ressources naturelles, avec une orientation particulière vers le secteur pétrolier où les investissements atteignent parfois plus de 90% des IDE totaux du pays concerné comme en Guinée Equatoriale et au Tchad. Il est également net que les problèmes institutionnels constituent les principales limites aux flux d'IDE dans cette zone. Enfin, le chapitre quatre analyse les effets des IDE sur les économies des pays de la CEMAC à l'aide d'un modèle structurel estimé par la méthode des triples moindres carrés. Le résultat obtenu montre que l'IDE contribue positivement à la croissance dans CEMAC prise comme entité économique. Toutefois, les simulations montrent que l'effet de l'IDE sur la croissance du PIB est plus sensible au Cameroun, faible au Gabon et en Guinée Equatoriale, nulle au Congo et au Tchad. Il est à craindre dans l'avenir un effet de « syndrome hollandais » dans la plupart des économies de la région si les autorités de la CEMAC ne prennent pas les mesures adéquates pour le contrer. Une diversification de la production par l'usage des recettes pétrolières au développement des secteurs à la traîne est une solution pérenne."
"Les résultats de l'estimation de la pauvreté monétaire sur la période 2002 et 2006 au Sénégal montrent une baisse significative de la pauvreté de 10% (voir chapitre 3). Cependant, selon l'Enquête sur la perception de la pauvreté au Sénégal (EPPS) réalisée en 2006, et portant sur le même échantillon que celui de l'enquête suivi de la pauvreté au Sénégal (ESPS - 2006), près de deux tiers des ménages estiment que la pauvreté a augmenté dans leur communauté durant cette même période. Elle révèle un écart entre l'incidence de la pauvreté calculée selon un seuil de pauvreté monétaire et la perception que les populations ont eux-mêmes de l'évolution de leur condition de vie. Les ménages estiment être privés d'un certains nombre de besoins nécessaires à une vie descente. Ceci étant l'approche monétaire de la pauvreté s'avère insuffisante. Pour cerner les multiples aspects de la pauvreté au Sénégal, une analyse multidimensionnelle est alors nécessaire pour établir une mesure exhaustive de ce phénomène à partir d'une estimation des degrés de privation de certains besoins essentiels des ménages qui constitue l'objet de ce travil. L'approche par la théorie des ensembles flous est une parfaite voie pour une telle analyse."
"Le contexte de la forte instabilité financière globale des années 1990 et 2000 caractérisée par la recrudescence des crises financières, en général de caractère systémique et contagieux, engendrant d'importants coûts socio-économiques et financiers pour l'ensemble d'acteurs publics et privés des pays en crise, ainsi que pour le système financier international via les mécanismes de contagion, a suscité de nombreux débats sur les moyens susceptibles de prévenir de futures crises financières. Les systèmes d'indicateurs d'alerte, dont l'objet est de détecter les faiblesses d'une économie à travers une batterie d'indicateurs et ainsi d'informer les Autorités du pays et/ou les organisations financières internationales de la probabilité d'une crise afin qu'elles prennent de mesures économiques nécessaires dans le but de la prévenir ou au moins de diminuer ses effets sur l'économie, se trouvent donc au coeur des recherches portant sur la prévention de futures crises. Cette thèse, faisant partie de ces travaux et fondée sur les leçons de la littérature théorique et empirique, vise à construire un système d'alerte afin de déterminer les principaux facteurs de l'occurrence des crises turques d'avril 1994, de février 2001, de mai 2006 et d'octobre 2008 à travers un modèle de type logit multivarié. A travers les résultats d'estimation du modèle économétrique, sont également proposées des politiques économiques appropriées à adopter afin de pouvoir éviter de futures crises et atteindre un développement économique soutenable en Turquie."
"Cette thèse approfondit l'analyse des déterminants de la concentration urbaine et de leur effet à différents stades de développement. De manière générale, l'analyse empirique porte sur les pays en développement examinés sur la période 1950-2000. Après une description des tendances des inégalités urbaines dans ces pays (évolution de l'urbanisation, de la primatie et de la hiérarchie urbaines), l'analyse en 3 groupes de niveau de revenu par tête confirme l'existence de trajectoires particulières des taux d'urbanisation et de la primatie urbaine qui diffèrent significativement selon l'étape de développement considérée. Certains facteurs peuvent encourager une accélération de l'urbanisation et sa concentration dans les villes principales lors des premières étapes du développement. Il s'agit notamment de l'exode rural et du faible niveau d'ouverture économique. D'autres facteurs peuvent freiner voire infléchir la progression du degré de primatie urbaine lors des stades plus avancés de développement, qui semble suivre une courbe en cloche. C'est le cas d'une forte ouverture internationale, d'un faible degré de protection commerciale, d'un réseau routier dense, des coûts de transport interrégionaux faibles et des spécialisations productives plus orientées vers des industries technologiques. Ces phénomènes d'évolution relativement différente des catégories d'agglomérations consécutive à l'évolution des structures industrielles et productives locales, au cours du développement, sont particulièrement examinés dans le cas du Maroc. Ce dernier, pris globalement, se caractérise par un niveau de développement correspondant à l'étape 3, avec un revenu par tête de l'ordre de 2927 $ par tête (en parité de pouvoir d'achat) en 1980. Progressivement à partir de cette date, un certain redéploiement sectoriel et géographique de l'appareil productif tend à s'opérer. La baisse de la primatie du Grand Casablanca est la résultante d'un double mouvement contraire: la délocalisation d'un certain nombre d'activités reposant sur l'exploitation d'économies d'échelle et de bas salaire poussant à la déconcentration urbaine non compensée par la localisation d'activités plus intensives en technologie et de services à la recherche d'économies d'agglomération renforçant la concentration urbaine."
"Cette thèse propose une analyse comparative de la richesse, de la créativité, de la connectivité et de l’attractivité résidentielle des territoires. Dans un premier chapitre, la hiérarchie des richesses régionales dans l’Union Européenne sur la période 2000-2015 est analysée, en se focalisant sur la dynamique de la distribution des PIB par habitant des 276 régions européennes identifiées au niveau NUTS 2. À l’aide de modèles économétriques, ce chapitre montre que les mécanismes structurels européens ne conduisent pas à un processus de convergence des PIB par habitant. Les mécanismes de rattrapage se sont avérés inefficaces et on assiste à une polarisation de performances qu’on caractérise comme l’Europe à deux vitesses. Le second chapitre, sur la période 2006-2013, étudie l’impact direct et indirect des industries créatives et culturelles et de la connectivité sur la richesse des régions européennes. Il montre que lorsque l’on tient compte des effets d’autocorrélation spatiale, l’impact de ces industries sur la richesse des régions européennes est plus contrasté et peut, dans certains cas, creuser les disparités. Le troisième chapitre se concentre sur les migrations résidentielles en France métropolitaine. En analysant les migrations résidentielles entre les zones d’emploi françaises, il constate que les jeunes (15-29 ans) migrent davantage que le reste de la population. Plus l’individu est diplômé, célibataire et locataire, plus il a de chance de migrer. Dans un même temps, les caractéristiques des territoires de localisation influent sur les décisions de migration des individus. Les jeunes préfèrent emménager dans des territoires connectés, où l’emploi créatif est présent et le taux de chômage est faible."
"Le numérique, la carte et le territoire Depuis les années 2000 et l'explosion de la bulle Internet, les technologies de l'information et de la communication (TIC) sont au centre de la société. Etats, organismes publics ou privés, médias n'ont de cesse d'utiliser le terme « d'économie numérique » pour définir la nouvelle économie, l'économie digitale, le e-commerce et l'ensemble des nouvelles technologies de l'information et de la communication (NTIC). Cette notion, dont l'utilisation devient fréquente, tient à sa participation de plus en plus importante dans la croissance économique."
"La concentration de plus en plus forte d’individus en ville incarne des menaces en matière sociétale (sécurité, fractures sociales, etc.) et environnementale. Les territoires doivent faire face aux risques et préoccupations liés à la dégradation de la qualité de l’air, l’affaiblissement des ressources énergétiques ou encore le réchauffement climatique, en essayant de limiter les impacts sur la santé publique, l’alimentation, la gestion des déchets, etc. Afin de réguler ces risques et aboutir à une meilleure gestion urbaine, il devient nécessaire de chercher des solutions au travers de modes de vie nouveaux, plus durables et en réduisant les inégalités. Face à ces défis, liés au développement économique, à l’optimisation des ressources et au bien-être des habitants, le numérique apparaît comme un outil dont il faut se saisir pour penser et développer la ville de demain. Il représente de formidables opportunités pour construire ou reconstruire des villes et des territoires « plus intelligents », et réinventer une nouvelle urbanité. Cette note présente les principaux concepts de la Ville intelligente, en évoque les principaux acteurs et axes de développement, avec quelques exemples locaux, et présente quelques cas de villes d’ores et déjà intelligentes."
"Le nombre de personnes exerçant une activité culturelle et créative, ainsi que le nombre d'établissements de ce secteur ont doublé au cours de la dernière décennie illustrant ainsi son dynamisme. Ce développement économique est d'autant plus important que ces industries sont considérées comme un levier du développement économique des villes. Cette étude propose d'établir un portrait des industries culturelles et créatives (ICC) dans le Var mais aussi de déterminer les différents catalyseurs du développement de ces activités."
"Cet article propose une comparaison de la croissance et de l’évolution des hiérarchies urbaines entre la région Provence-Alpes-Côte d’Azur et la France métropolitaine entre 1962 et 2013. Il montre que la région Provence-Alpes-Côte d’Azur enregistre une évolution différente de celle de l’ensemble du pays : tandis que la France renforce son système urbain primatial, la région PACA observe une baisse du poids démographique des plus grandes aires urbaines aux profits des petites et moyennes villes. Ce travail s’appuie sur le recensement historique de la population sur des aires urbaines publié par l’Institut National de la Statistique et des Études Économiques. Il utilise un ensemble d’outils statistiques et économétriques tels que la loi rang-taille, les tests de stationnarité en données de panel et les processus de Markov."
"Cet article propose d’étudier la hiérarchie de la richesse régionale dans l’Union Européenne et son évolution entre 2000 et 2015, en se focalisant sur la dynamique de la distribution des PIB par habitant des 276 régions européennes, identifiées au niveau NUTS 2. Cet article mobilise une méthodologie utilisée dans l’analyse des hiérarchies urbaines mais qui semble tout à fait adaptée à l’étude des inégalités régionales en Europe. Il montre que les mécanismes structurels européens ne conduisent pas à un processus de convergence des PIB par habitant. Les dynamiques enregistrées entre 2000 et 2008 ont permis aux régions en retard d’enregistrer des taux de croissance plus élevés sans pour autant que cela ne conduise à une véritable convergence, la mobilité ascendante s’estompant au-delà d’un certain seuil de PIB. Dans la période qui a suivi la crise de 2008, ces mécanismes de rattrapage se sont avérés inefficaces et on assiste à une polarisation de performances que certains chercheurs et hommes politiques caractérisent comme l’Europe à deux vitesses."
"Cet article étudie le lien entre le poids des activités créatives et culturelles dans les territoires et l'attractivité de la population. L'analyse de la décision de migrer s'appuie sur des données du Recensement de la population mises à disposition par l'Institut National de la Statistique et des Études Économiques (Insee) en France avec des informations personnelles sur l'individu comme l'âge, l'état matrimonial, le diplôme, l'accès à la propriété, etc. L'article se concentre sur les tendances de migration dans les zones d'emploi entre 2012 et 2017 en distinguant les jeunes âgés de 15 à 29 ans de la population de 30 ans et plus, et sur la façon dont les caractéristiques territoriales (économiques, sociales ou environnementales) des lieux de départ et de destination agissent sur les flux d'entrants et de sortants. A partir de modèles de type Probit et Heckman, il apparaît que les jeunes (actifs ou étudiants) privilégient leur choix de localisation dans les zones où le poids des industries créatives et culturelles est important."
"En utilisant des données régionales d’Eurostat sur la période 2006-2013 et en mobilisant un modèle empirique et un panel d’outils économétriques, cet article propose d’étudier l’impact direct et indirect des industries culturelles et créatives sur la richesse des régions européennes. Bien que de nombreux travaux existent dans ce domaine, il n’y a pas, à notre connaissance, d’étude qui propose une analyse économétrique du rôle de ces industries dans la richesse économique des 226 régions européennes. Ce travail montre que l’emploi dans les industries culturelles et créatives n’est guère impacté par la crise économique de 2008. Il montre également que ces industries captent en partie l’effet capital humain régional. Il montre enfin que lorsque l’on tient compte des effets d’autocorrélation spatiale, l’impact de ces industries sur la richesse des régions européennes est plus contrasté et peut, dans certains cas, creuser les disparités."
"Depuis deux décennies, le tourisme s'est affirmé comme une activité majeure dans les échanges mondiaux. De nombreux paramètres en révèlent l'importance : les flux touristiques représenteraient actuellement 11% du PIB mondial, avec des taux de croissance particulièrement élevés dans les pays en développement et en transition. L'importance du tourisme en tant qu'industrie créatrice d'emplois, de devises et de croissance dans les pays en développement n'a été reconnue que récemment. Néanmoins, malgré ses retombées stimulantes sur la croissance économique, l'expérience, corroborée par de nombreux travaux théoriques et empiriques, montre que la spécialisation touristique n'est pas exempte de risques et d'effets pervers. Certes, le tourisme exerce des effets positifs sur la balance des paiements, sur l'emploi et sur la production, du moins au niveau macroéconomique. Mais il peut par ailleurs exercer des effets lourdement négatifs sur l'environnement et compromettre les perspectives d'un développement durable. En outre, la spécialisation touristique peut souvent entraver le développement régional, en conduisant à des processus de désîndustrialisation et de recul des activités productives. Dans plusieurs pays, les retombées ambiguës ou incertaines de la croissance touristique ont souvent été observées : 1 ) la prédominance monopolistique des tour-opérateurs qui vendent des images stéréotypées, à travers des produits homogènes et sans diversification de l'offre ; 2) une croissance touristique géographiquement concentrée, fortement consommatrice de ressources et destructrice des écosystèmes ; 3) des processus d'exclusion sociale à l'égard de populations locales, souvent marginalisées ou confinées dans des activités à bas salaires, Notre recherche utilise différents instruments conceptuels, majoritairement empruntés à l'économie internationale et à l'ëconométrie. Les résultats de nos travaux s'articulent autour de deux axes constitutifs de deux parties distinctes :1)Déterminants et impacts du tourisme international sur la croissance, Dans cette première partie, sur la base principalement d'études empirico-statistiques en termes de données de panel, nous évaluons la signification de la spécialisation touristique par référence notamment au concept d'avantage comparatif Nous présentons deux séries de travaux de nature supply side et demand side, en nous référant préalablement aux substrats des théories du commerce international, dans le contexte d'un essor mondial de l'économie des services. A cet égard, nos recherches accordent une attention prioritaire à l'Asie du Sud-Est 2) Croissance touristique, exploitation des ressources et développement durable. Le tourisme est souvent perçu comme un facteur efficient de développement local, dans la mesure où il exerce des effets de liaison et de multiplication. Cependant, ce point de vue optimiste doit être tempéré, compte tenu de l'existence de nombreux effets de déperdition au détriment des tissus productifs et de retombées sur l'environnement. Dans la lignée de nombreux travaux, en empruntant au concept marshallien de << district industriel » et sur la base de modélisations en termes d'équilibre général, nous démontrons que la spécialisation touristique peut engendrer, au moins au niveau local, des situations de « croissance appauvrissante », avec une aggravation du sous-emploi structurel, un recul des productions et un épuisement des ressources naturelles. En conclusion, nos recherches, sans contester Futilité socio-économique du tourisme, nous amènent à formuler des diagnostics nuancés et des pronostics de prudence sur un phénomène dont l'importance s'affirmera encore davantage dans les pays en développement et en transition au cours des prochaines décennies."
"Le projet Satelit vise à consolider le positionnement des universités du Maghreb dans leur écosystème économique et territorial. Il est structuré sur l’axe « sud-nord » et il se base sur un partenariat entre six universités de la côte sud de la Méditerranée (d’ouest en est : Rabat, Fès, Béjaia, Constantine, Sousse, Sfax) et six de la côte nord (Cadiz, Murcia, Aix-Marseille, Toulon, Turin, Gênes). La participation du Céreq dans ce programme a pour objet d’observer les liens entre formation et emploi dans les pays du sud et plus particulièrement la transition de la thèse à l’emploi. Ce programme s’intéresse à la formation et l’insertion professionnelle des personnels hautement qualifiés issus des écoles doctorales de ces universités. L'hypothèse de départ est que l’intégration des docteurs en entreprise favoriserait le transfert de savoirs du monde académique vers les entreprises et donc les innovations."
La tarification incitative comme outil de gestion des déchets municipaux (DMA) s’est très peu développée dans les collectivités locales françaises malgré ses avantages théoriques sur la prévention des déchets et l’augmentation du recyclage. Nous analysons les déterminants de la décision des collectivités locales françaises relative au choix de la tarification incitative en étudiant notamment l’effet de la variation du coût total net des DMA sur la probabilité d’observer la tarification incitative. Nous estimons également l’effet de la tarification incitative sur le coût total net des DMA. Nos résultats empiriques soulignent l’importance de l’arbitrage coûts-bénéfices et de la diffusion de la tarification incitative dans le voisinage sur le comportement des collectivités locales. Nous montrons comment la réglementation a biaisé l’arbitrage coûts-bénéfices et en conséquence ralentit l’adoption de la tarification incitative.
"Ces recherche visent à démontrer la légitimité puis à définir le mode opératoire d'un Prêteur international en dernier ressort (PIDR) face aux crises d'illiquidité bancaire. Dans le contexte de l'existence d'un risque systémique, d'une incapacité de la régulation macroprudentielle à réduire la fréquence des crises, et du rôle central des banques dans leur déclenchement et leur déroulement, on cherche à savoir si un gestionnaire des crises pourrait améliorer aussi leur prévention. On formalise un cadre analytique novateur, puis on procède à des simulation sur logiciel Gauss, démontrant l'intérêt d'un PIDR, en termes de risque moral, de risque systémique, de capacités d'endettement international et de coûts des interventions, dès lors qu'il dispose d'informations fiables sur les banques. Cette légitimation théorique d'une Institution supranationale n'implique pas forcément l'existence d'un mode opératoire. Dans une seconde partie, celui-ci est donc précisé, grâce à la littérature relative à la finance catalyse, à l'ambiguïté constructive, et à la théorie classique du prêt en dernier ressort adaptée à l'échelle internationale. Cette amélioration des interventions, en rupture avec le risque moral, implique, d'un point de vue institutionnel, la triple nécessité de produire un rating à vocation incitative des pays et des banques, d'obtenir leur adhésion en définissant un cadre mutuellement avantageux, et de centraliser l'information détenue par les superviseurs régionaux, via une homogénéisation préalable. On formalise ce cadre de résolution des crises en accord avec leur prévention, appelé régime de sélectivité, et basé sur un principe de conditionnalité ex ante aux deux niveaux macro et microéconomiques. Après discuter le contenu de ces conditionnalités, on conclue en proposant des pistes de réforme l'Architecture financière internationale."
"L'objet de cette thèse est de proposer un mode opératoire de gestion des crises de dette souveraine basé sur le mécanisme sous-jacent aux Clauses d'Actions Collectives (CAC), l'action à la majorité qualifiée. Premièrement, nous rendons compte de l'évolution de la structure d'endettement des pays émergents, au sens financier du terme, aussi bien au niveau de la composition de la dette souveraine qu'au niveau de la répartition de ses créanciers. Nous concluons qu'elle est source et reflet de leurs vulnérabilités. Puis, nous montrons que la mutation de la structure d'endettement des pays émergents modifie la nature et l'ampleur des coûts associés à la survenue d'un défaut de paiement. Les coûts ne sont plus supportés uniquement par les Etats et leurs créanciers, mais également par le système monétaire et financier international. Secondement, il est rappelé les principaux enjeux auxquels doit répondre une procédure de restructuration de dette souveraine et en quoi le choix des CAC est pertinent pour répondre à cette problématique. Enfin, nous présentons les principales conclusions relatives à l'étude des CAC, qui nous permettent de présenter un mode de révision du seuil de vote de la clause d'action à la majorité qualifiée. Ainsi, nous disposons d'un mode opératoire ex ante, vecteur de diminution des asymétries informationnelles participant à une amélioration de la transparence. La coordination des acteurs est renforcée, ce qui permet d'abaisser le coût de résolution des défauts de paiement souverains."
"Les modes de fonctionnement des établissements bancaires ont profondément évolué ces dernières années, sous l'effet de la libéralisation financière et des innovations technologiques. Le développement de nouveaux instruments financiers et la globalisation du marché des capitaux ont généré de nouveaux risques et contraint les autorités prudentielles à faire évoluer les exigences réglementaires pour garantir la résilience du système financier. Avec l'entrée en vigueur, en décembre 2006, de la réforme ""Bâle II"", les banques ont dû adapter leurs outils de sélection et de mesure du risque de crédit et prendre en compte les risques opérationnels dans le calcul d'exigences de fonds propres. Cette thèse s'intéresse au passage des Accords de Bâle I à Bâle II et explique les évolutions prudentielles qui fondent cette réforme. Le passage du ratio Cooke au ratio Mac Donough, est présenté et analysé pour illustrer la ""montée en puissance"" de l'autocontrôle et démontrer que contrôle interne apparaît désormais comme la pierre angulaire du nouveau dispositif. Une étude du dispositif de contrôle interne en vigueur en France, notamment via le réglement n° 97-02 modifié du Comité de la Réglementation Bancaire et Financière, est présentée. Puis, à partir d'une étude qualitative menée au sein d'une banque coopérative française, nous proposons des grilles de lecture des contrôles clés, en terme de gouvernance et par typologie de risques."
"L'objet de cet article est l'analyse de l'aléa moral caractéristique des relations entre la Russie et le FMI. On s'appuie sur un modèle original de ""jeu de faux semblants"" comportant un prêteur multilatéral dont la fonction d'utilité dépend de la stabilité du Système Financier International, et un emprunteur souverain dont la dette fait peser des menaces sur la stabilité du système dont est garant le prêteur, de telle manière qu'il peut être optimal de prêter tout en sachant que les sommes correspondantes ne seront pas remboursées."
"Initié en 2003 par le Ministère de la Transition écologique français dans le cadre du dispositif d’investissement locatif « Robien », le zonage A/B/C réalisé à l’échelle nationale permet de catégoriser les communes en fonction de la tension du marché de l’immobilier. Si cet indicateur unique met en lumière les zones les plus tendues du territoire, il reste lacunaire en ce qui concerne les zones C dites « détendues ». En effet, en les assimilant à un « reste du territoire » uniforme qui semble vidé de toutes activités, cette cartographie ne rend pas compte de la diversité des formes et des fonctions de ces territoires situés en dehors des grands centres urbains et qui restent souvent dépréciés. L’objectif de cet article est de mettre en lumière les dynamiques territoriales des zones détendues en élaborant une typologie de ces territoires à travers le prisme de l’attractivité territoriale à l’aide d’une classification ascendante hiérarchique."
"L'Université du Sud Toulon Var s'est engagée depuis un an dans le déploiement au Vietnam d'un diplôme de Master professionnel en Intelligence Economique et territoriale. Les études d'opportunité préalables au lancement de ce projet ont permis de faire ressortir le caractère innovant de cette initiative puisqu'il n'existe pas de formations équivalentes à l'IE au Vietnam. Des investigations plus fines nous ont conduit à une conclusion plus nuancée. L'IE existe au Vietnam mais constitue davantage un "" socle culturel implicite "". Dans un premier temps, nous comparerons un corpus de documents français et vietnamien dans le domaine de l'IE pour proposer une première approche comparée du système d'IE français et Vietnamien. Cette analyse fera ressortir le caractère émergent du concept d'IE au Vietnam. Dépassant ces observations, nous nous interrogerons ensuite sur les déterminants de l'Intelligence économique en montrant qu'il existe au Vietnam des facteurs engageant culturellement les acteurs vers des logiques d'Intelligence économique."
"Notre objet est d'établir, concernant le financement extérieur d'emprunteurs souverains confrontés à un rationnement du crédit, les bases d'un cadre d'analyse moins restrictif que celui des modèles de dette souveraine. Dans ces modèles, le défaut ne dépend, en effet, que de la décision du seul emprunteur, en conséquence de la maximisation de son utilité intertemporelle. Il s'agit, ici, de dégager de la littérature récente sur les crises financières les déterminants d'un défaut ne résultant pas directement d'un tel choix. Les principaux éléments concernent les caractéristiques de l'emprunteur, c'est-à-dire les paramètres de solvabilité, et de structuration ainsi que de fonctionnement du système de financement domestique. Enfin, le défaut est envisagé comme une des conséquences du comportement d'aléa moral des prêteurs."
"Résumé Avant même le début de la crise, la volatilité infra-quotidienne des marchés d'actions augmente très fortement, dès l'entrée en vigueur des directives Reg NMS ("" Regulation National Market System "", Etats-Unis, à partir d'octobre 2006) et MIF (ou MIFID, "" Markets in Financial Instruments Directive "", Union Européenne, à partir de novembre 2007), promouvant le principe de libre concurrence sur les marchés d'actions. Parallèlement, on observe une très forte progression des volumes d'ordres, en lien avec la fragmentation des marchés et la forte progression de la part des transactions à haute fréquence résultant de ces chocs réglementaires. Pourtant, les volumes de transactions sur les marchés d'actions reculent. L'objet de cette contribution est d'apprécier, à partir d'un modèle théorique, les effets de ces chocs réglementaires sur la volatilité des marchés d'actions et les conséquences macroéconomiques qui en résultent, au travers de la réduction de la liquidité de financement (marchés primaires)."
"Nous proposons un modèle théorique simple comportant, d'une part, une économie caractérisée par un modèle d'intermédiation donné, défini par l'importance des portefeuilles de crédits aux ménages dans les bilans des banques et dans l'encours total de crédit ; et d'autre part, une banque centrale caractérisée par une règle de politique monétaire alliant politique de taux d'intérêts sous forme de règle de type Taylor augmentée, injections de liquidités, et achats de titres. Nous montrons que le modèle d'intermédiation détermine les conditions de transmission d'un choc sur un segment de marché déterminé. Il en résulte deux principaux éléments de conclusion. D'abord, l'importance accordée par les banques centrales à l'écart de production dans le cadre d'une règle de Taylor influe de fait sur la stabilité financière, pour un modèle d'intermédiation donné. Ensuite, la manière dont le risque de taux est partagé entre banques et emprunteurs détermine les modalités de la politique monétaire. S'il est supporté en proportion excessive par les emprunteurs, tout choc positif de taux directeur peut dégrader les conditions de financement au point de conduire la banque centrale à mobiliser l'éventail complet des instruments de politique monétaire."
"Nous étudions dans ce travail les changements de la structure de corrélation des séries de rendements des marchés d'actions dans le monde de 1960 à 2018, afin d'analyser de manière endogène quelles sont les périodes d'intégration financière internationale. Pour cela, nous considérons différents indicateurs décrivant les corrélations (évoluant dans le temps) et utilisons une méthode de débruitage de signaux multivariés constants par morceaux pour réaliser la segmentation."
"Les crises génératrices d'effets d'amplification conduisent les banques centrales à intervenir hors du périmètre conventionnel de la politique monétaire. Notre objet est de spécifier une règle de comportement intégrant cette mission. Notre modèle, fondé sur la prise en considération des conditions de financement de l'économie au travers des prix et des quantités, inclut l'existence d'un lien positif, en période de crise, entre hausse des spreads et demande excédentaire de financements. Cette prise en compte permet d'appréhender les modalités d'intervention au titre des "" politiques monétaires non conventionnelles "", et singulièrement de quantifier les achats de titres publics."
"Après deux années de relative stabilité, l'observation des spreads EURIBOR/OIS fait apparaître, entre juillet et écembre 2011, une augmentation de 80 points de base (spread EURIBOR 3m/OIS). Ce phénomène est propre à la zone euro. Egalement observable sur les spreads EURIBOR/repo (interbancaires), il présente une double spécificité. D'abord, l'élargissement des spreads ne résulte pas d'une élévation de l'EURIBOR par rapport aux taux OIS et repo, mais d'une stabilisation de l'EURIBOR avec diminution des taux utilisés comme base de calcul. Ensuite, la structure par terme des taux EURIBOR correspond à une courbe des taux normalement croissante, contrairement à la crise de 2007-2008. Simultanément, la BCE augmente le nombre et le volume de ses opérations de politique monétaire non conventionnelle. A partir d'une base spécifique d'évènements de politique monétaire et d'indicateurs financiers propre à la zone euro, nos tests économétriques font apparaître, pour le second semestre 2011, des résultats mitigés quant à ces actions de politique monétaire non conventionnelle. En effet, seules les annonces (mais non les réalisations) d'opérations de rachats de titres et d'injections de liquidité calment les tensions interbancaires. Paradoxalement, la politique de taux semble au contraire être à l'origine d'un renforcement des tensions."
"Notre thèse traite des comportements d'aléa moral caractéristiques de la relation emprunteur souverain - prêteurs privés - Institutions financières internationales, et plus particulièrement de la question de l'optimalité de l'intervention du Fonds Monétaire International, lorsqu'il existe un risque de système. Cette problématique renvoie, dans la littérature, aux modèles de dette souveraine et de renégociations, aux modèles de comportement des Institutions financières internationales, et aux modèles de crises financières. Sur cette base, nous proposons deux modèles d'octroi de prêts multilatéraux. Le premier, de type Principal-Agent, comporte une conditionnalité ex post (la conditionnalité actuelle, où la vérification des conditionnalités est postérieure aux transferts, par opposition à une conditionnalité ex ante, où elle serait antérieure). Sa résolution conduit à conclure que, dans un contexte de risque systémique, si le coût de la crise excède celui du renflouement, le FMI devient un prêteur en dernier ressort obligé. Le transfert permet généralement d'éviter la crise, mais l'intervention n'est pas globalement optimale. L'objet du second modèle, de conditionnalité ex ante, est de déterminer si ce mode d'intervention permettrait de limiter les comportements d'aléa moral des prêteurs privés et des emprunteurs. Le transfert effectué par le FMI est, ici, lié à une note préalablement attribuée. Les prêteurs privés sont divisés en deux catégories : les ""spéculateurs"" et les ""investisseurs"". Selon nos hypothèses, le régime de conditionnalité ex ante permet de renouer avec une contrainte d'incitation, renforcée par le lien entre investissements durables et système de notation. L'efficience de l'intervention se trouve améliorée en termes d'allocation optimale des ressources multilatérales et de prévention des crises. L'importance accordée par le Fonds Monétaire International à la stabilisation à court terme du système de financement international entraîne cependant la persistance d'un aléa moral de l'emprunteur."
"L'état des lieux du processus de globalisation financière à l'orée de la décennie 2010 fait apparaître que la situation de l'espace européen est assez peu favorable, pour trois raisons principales. D'abord, alors même que la globalisation financière, en fait, n'est pas un phénomène véritablement global puisqu'elle ne concerne qu'un petit nombre d'économies avancées et émergentes, la crise récente a bouleversé la hiérarchie historique des risques relatifs aux placements internationaux. Les économies émergentes ont au cours des cinq dernières années un statut inattendu de valeurs refuge. Ensuite, au sein des économies globalisées, le financement de l'économie repose désormais majoritairement sur les marchés et non plus sur les activités traditionnelles des banques. Il en résulte, particulièrement dans le cas de l'Union Européenne, une course à la redistribution des bénéfices et une faiblesse structurelle de l'investissement des entreprises qui contribuent à l'installation durable d'un environnement de faible croissance. Enfin, les mesures récentes de dérèglementation des activités financières (MIF) ont conduit à une convergence des caractéristiques des marchés financiers de l'espace européen vers celles caractéristiques des marchés financiers des Etats-Unis, notamment en matière de concurrence sur les marchés de titres. Il apparaît donc que l'Union Européenne est dans une situation fragile. En termes d'entrées de capitaux elle n'a ni le statut de valeur refuge des pays émergents, relativement isolés de la conjoncture internationale, ni l'avantage du dollar que détiennent les Etats-Unis ; la structure du financement de sa croissance est extrêmement contraignante ; et ses marchés financiers, en pleine mutation, génèrent de l'incertitude et des risques mal maîtrisés."
"Notre objet est de déterminer si une conditionnalité de type ex ante, où l'évaluation précède le prêt, limite les comportements d'aléa moral liés à l'intervention du FMI. Le modèle comprend deux catégories d'investissements privés : durables, et spéculatifs. L'évaluation par le FMI donne lieu à une note, qui conditionne le prêt, mais aussi les investissements durables. Les investissements spéculatifs sont liés à logique d'aléa moral pur. Sous nos hypothèses, la conditionnalité ex ante est préférable à la conditionnalité ex post en termes d'allocation optimale des ressources multilatérales et de prévention des crises, même si l'importance accordée à la stabilisation à court terme du système de financement international entraîne la persistance d'un aléa moral de l'emprunteur."
"Le Lexique d'économie est conçu pour être un outil de formation, d'apprentissage et de révision des notions fondamentales en sciences économiques et de gestion, tout en s'ouvrant aux autres sciences sociales connexes, conformément aux programmes des classes préparatoires et des premiers cycles universitaires. Il se caractérise par : - la simplicité et la rigueur scientifique des définitions avec une identification des champs lexicaux ; - la présentation détaillée des différentes théories, écoles, doctrines et courants économiques ; - la prise en compte des modifications institutionnelles, aussi bien dans le cadre national et européen qu'international ; - la présentation de graphiques et tableaux pour faciliter la compréhension des notions exposées."
"Through a new lecture of many articles concerning the appearance of industrial districts in peripheral countries, this paper aims to analyse Mauritius industrial trajectory, during the last twenty years. The Mauritian productive system, specialised in textile and garments, was born thanks to the creation of an export process zone which allowed competitiveness through low costs of production. Neverthe-less, during the nineties this system evolved through complexification towards an industrial district’s productive organization whose competitiveness is based mainly on innovation."
"Nous nous interrogeons, dans le cadre de la NEG, sur la validité empirique d'un certain nombre de grands facteurs explicatifs de la répartition spatiale des activités (le coût du travail local, les coûts de la concurrence, localisation de la demande finale, le coût de transport etc). Nos résultats, trouvés à l'aide de différentes simulations dans un contexte de processus d'intégration régionale au sein des PVD, suggèrent que les firmes semblent réaliser un arbitrage entre s'agglomérer pour bénéficier de rendements croissants dus à l'effet taille du marché et se disperser pour bénéficier des coûts du travail plus faibles. Nous modélisons, en particulier, l'avantage des régions côtières par rapport aux régions intérieures dans le commerce avec le reste du monde. Ce constat est particulièrement vrai dans le bassin méditerranéen qui est caractérisé par une littoralisation excessive de l'activité économique et démographique. Il apparaît qu'au-delà des politiques d'ouverture internationale mises en évidence par Krugman et Elizondo (1996), la dotation régionale en terme d'infrastructures de transport a un impact certain sur le degré et l'efficacité de la concentration spatiale au sein des PVD. Le second grand enseignement de cette étude est que la tendance au redéploiement des activités des grands pôles urbains vers les zones de moindre densité se confirme avec la considération des forces centrifuges. En effet, cette extension influence directement la fonction de production des firmes et conduit à une répartition plus égale de l'activité économique entre deux régions. De ce fait, les possibilités d'équilibres de long terme sont plus larges avec la prise en compte des coûts de congestion."
"La progression rapide des niveaux d'éducation en Tunisie s'est traduite par un accroissement de l'off re de compétences qui, comme en témoigne le taux de chômage élevé des jeunes diplômés de l'enseignement supérieur, n'a pas rencontré une demande aussi dynamique. L'objectif de cette étude est d'expliquer le rôle joué par les évolutions de la demande de travail émanant de la sphère publique. Dans quelles mesures les choix de formation sont-ils in fluencés par le secteur (public versus privé) dans lequel les individus espèrent décrocher un emploi ? Ces préférences sont-elles justi fiées par des écarts en termes de rémunération entre le secteur public et privé ? Nous exploitons une base de données issue de l'enquête sur l'insertion professionnelle des diplômés de l'enseignement supérieur réalisé par le Ministère de l'Emploi et de l'Insertion Professionnelle des Jeunes en Tunisie auprès d'un échantillon de 4763 diplômés de 2004 en vue de suivre leur devenir professionnel après une période de 18 mois à compter du mois d'obtention du diplôme. Nous estimons un modèle d'équations de salaire à double régime (public - privé) avec contrôle des biais de sélection endogène qui caractérise l'accès à un emploi rémunéré et le choix entre le secteur public ou privé. Les résultats montrent que l'écart salarial entre emplois du secteur public et du secteur privé est quasi-exclusivement lié aux caractéristiques de l'insertion des maitrisards. Sur les autres diplômés, nous observons, à diplôme équivalent, des distributions des salaires très proches qui ne rendent pas assez attractif le secteur privé."
"Les échanges dans la zone euro-méditerranéenne ont été caractérisés par une intensification des flux commerciaux au début des années 2000. A cette même période, la question migratoire a été mise en avant en raison d’une explosion de l’immigration clandestine dans les pays du sud de l’Europe. L’objectif de cette thèse est d’étudier certaines conséquences économiques actuelles de ces migrations dans le cas de France. Dans cette optique nous abordons les questions liées à la relation commerce-migrations, à la politique migratoire, et à la localisation spatiale des immigrés en France. En utilisant des données récentes et en intégrant des paramètres jusque là peu utilisés, nous mettons en évidence des caractéristiques particulières de l’immigration en France. Nous apportons également des réponses quant a` la contribution des immigrés a` la production nationale, de même qu’a` l’accroissement du commerce bilatéral avec les pays d’origine. Les implications en termes de politiques économiques se situent d’une part dans la mise en œuvre d’une facilitation des opportunités d’aaires entre la France et les pays d’origine. Cela s’impose au regard de l’impact positif de l’immigration sur le commerce bilatéral. D’autre part, les difficultés relevées au niveau des politiques migratoires appellent a` une nécessaire harmonie de ces dernières, au plan européen, pour une gestion plus efficace. Cela passe encore par une implication des pays d’origine afin de faciliter l’immigration légale, et limiter les entrées clandestines. Enfin, la localisation spatiale des immigrés indique que leur concentration dans les pôles régionaux a un impact négatif sur leur contribution `a la production. La réponse peut se situer dans une analyse détaillée de la structure et du processus d’intégration des populations immigrés"
"L’aspect peu volatil observé durant les différentes crises financières et l’importance des externalités relatives à la simulation des économies hôtes attribuent aux IDE une dimension considérable surtout pour les pays en développement (PED). Dès lors, l'attractivité du territoire est devenue la priorité principale des politiques des PED vers une ouverture sur les marchés internationaux. Ce qui explique la grande vague de réformes structurelles et de stratégies attractives qui a marqué ces économies durant ces deux dernières décennies. La contribution de cette thèse repose sur l'analyse des déterminants des IDE au niveau macroéconomique et mésoéconomique. La première étude consiste à estimer l’attractivité des territoires de la région du Moyen Orient et de l’Afrique du Nord (MENA). Nous avons appliqué les nouveaux apports de l’économétrique spatiale pour la spécification des flux bilatéraux de huit pays MENA en provenance de 16 pays d’OCDE sur la période 1985-2010. L'objectif est d'examiner la spécificité de cette région en évaluant conjointement la contribution spatiale interrégionale pour expliquer la nature des IDE et le rôle de l’autocorrélation spatiale intra-régionale à promouvoir l’investissement dans les territoires étudiés. La deuxième étude procède à l’analyse typique du climat d’investissement marocain objet de la recherche. Il s’agit de faire une comparaison sectorielle en identifiant la significativité des déterminants des IDE dans le secteur primaire, le secteur secondaire et le secteur tertiaire. Le travail consiste en premier lieu d’évaluer l’impact des variables macroéconomiques sur les flux des IDE dans neuf secteurs. En deuxième lieu, des variables mesurant la qualité institutionnelle sont intégrées individuellement afin d’approximer les différentes élasticités du climat juridique et administratif sur l’attractivité des IDE."
"L’objectif de la thèse est d’étudier les cycles économiques en Tunisie en se centrant principalement sur les questions suivantes : quel est leur degré de synchronisation avec les fluctuations économiques des pays développés et quels sont les mécanismes de transmission de ces fluctuations sur l’économie tunisienne ? Pour répondre à cette problématique, le travail est divisé en trois chapitres. Le premier chapitre cherche à retracer les cycles économiques tunisien comparés aux cycles économiques mondiaux, représentés par quatre pays développés (PDE) : la France, l'Italie, l'Allemagne (qui sont les principaux partenaires commerciaux de la Tunisie) et les Etats-Unis (du fait de leur poids dans l'économie mondiale). Dans le deuxième chapitre, le travail introduit l’extraction des composantes cycliques d’autres variables macro-économiques internes (globales et sectorielles) et externes à l’économie tunisienne, afin d’identifier les sources des chocs et les canaux à travers ces chocs sont transmis. Le dernier chapitre a pour objet une validation économétrique des résultats descriptifs des faits stylisés données au premier et au deuxième chapitre, et de savoir à quel degré la nouvelle synthèse néoclassique peut être appliquée à l'économie tunisienne. Les résultats des faits stylisés montrent que le cycle économique tunisien est sensible aux trois catégories de variables étudiées, réelles, monétaires et financières, confirmant ainsi la théorie de la nouvelle synthèse néoclassique. L'application des modèles, VAR structurel et modèle dynamique à composante inobservable, valide les résultats des faits stylisés. En effet, les estimations du modèle VAR structurel montrent que les trois chocs qui contribuent le plus à la variance du PIB tunisien sont les chocs d'offre, les chocs monétaires et les chocs extérieurs. Concernant l'évaluation de la synchronisation du cycle tunisien avec ceux des PDE à partir du modèle à facteur inobservable de Stock et Watson, les résultats montrent un rôle significatif du facteur commun sur le cycle tunisien. Pour les déterminants de la synchronisation du cycle tunisien avec ceux des PDE, la transmission des fluctuations des pays partenaires commerciaux paraît se faire à travers les demandes intérieures, l'indice des prix des matières premières et le taux du marché monétaire en zone euro. Les exportations et les importations tunisiennes n'ont montré un rôle significatif qu'avec le premier partenaire commercial, la France. Pour les Etats-Unis, les fluctuations sont transmises à travers deux variables ""européennes"", le taux du marché monétaire en zone euro et les envois de fonds des immigrés, suggérant le rôle intermédiaire des pays partenaires commerciaux européens dans la transmission des fluctuations américaines et internationales à l'économie tunisienne."
"La doctrine du libre-échange a favorisé la création d’un cadre multilatéral en 1947 avec l’accord général sur les tarifs douaniers et le commerce (GATT) puis avec l’Organisation Mondiale du Commerce (OMC) en 1995. Les négociations commerciales multilatérales sous l’égide du GATT puis de l’OMC, aboutissent à une réduction considérable des droits de douane au niveau mondial. Or, devant la diminution progressive des droits de douane, un fort accroissement des barrières dites non tarifaires (BNT) apparaît.En effet, l’expansion du commerce international et l’extension des règles commerciales multilatérales à de nouveaux domaines, autrefois protégés, tels que l’agriculture, les services et la propriété intellectuelle ont amené beaucoup de pays à faire un usage plus intensif, voire « abusif » des barrières non tarifaires, une issue pour contourner la règle « libre-échangiste ».Ces barrières non tarifaires viennent ainsi compléter, voire remplacer les droits de douane et peuvent réduire, voire annuler la valeur des consolidations tarifaires. De plus, les BNT sont deux fois plus restrictives que les droits de douane, CNUCED, (2005) et elles limitent nettement plus l’accès aux marchés que les droits tarifaires, CNUCED, (2012). Egalement, les BNT peuvent prendre différentes formes, elles n’ont pas le même degré de restrictivité et ne s’appliquent pas de la même méthode. Elles varient en fonction des pays et des produits et correspondent à différents objectifs.La présente thèse s’inscrit dans le cadre des travaux de quantification des barrières non tarifaires et vise l’évaluation du degré de restrictivité des BNT tarifaires appliquées sur les importations des pays sud-méditerranéens (MED) en provenance des pays de l’Union Européenne (UE) dans le cadre de l’intégration euro-méditerranéenne. Ce travail de recherche introduit deux modèles économétriques : le premier modèle est une équation d’ « importations », il évalue l’ampleur des barrières non tarifaires appliquées sur les importations (corrigées des tarifs) des pays MED en provenance du partenaire UE et ceci à travers le calcul des équivalents tarifaires ad-valorem. Le second modèle est une équation gravitationnelle, il estime le rôle des BNT et les coûts de commerce dans les échanges UE-MED pour aboutir à une analyse plus fine de l’impact des BNT et d’autres barrières à l’échange à savoir : les droits de douane ; les coûts de transport ; la performance logistique ; les facteurs de la proximité culturelle et les facteurs institutionnels sur le commerce euro-méditerranéen."
"La relation entre capital humain et employabilité bien que clairement démontré d'un point de vue théorique n'est cependant pas toujours vérifiée de façon empirique. On peut ainsi constater un paradoxe de l’éducation dans les pays MENA où le taux de chômage augmente avec le diplôme et où les taux d'activité et plus particulièrement les taux d'activité des femmes sont faibles. Nous cherchons donc, à travers trois études empiriques, à mettre en évidence les éléments pouvant contrarier cette relation entre accumulation du capital humain et employabilité. Nous portons tout d'abord notre attention sur la Tunisie et le Maroc où le taux de chômage des diplômés du supérieur est particulièrement important. Nous menons dans un premier temps une analyse macroéconomique concernant ces pays avant de mener une analyse microéconomique en nous focalisant sur la région de Marrakech-Tensift-Al Haouz où nous analysons la relation entre le niveau d’éducation et l’accès à un emploi rémunéré. Notre seconde étude se positionne sur le marché du travail français où nous analysons l’accès à l’emploi ainsi que le différentiel salarial afin de déterminer si l’existence de « discriminations » peut contrarier la relation entre capital humain et employabilité. Enfin, notre troisième étude complète notre précédente approche concernant l’accès à l’emploi en prenant en compte des parcours scolaires des jeunes et plus seulement de plus haut diplôme obtenu."
"Cette thèse s’intéresse à l’étude de la relation entre l’ouverture financière et politique et la croissance économique dans les pays en voie de développement. En effet, réformes politiques et libéralisation financière étaient le mot d’ordre des instances financières internationales qui conditionnaient l’octroi des aides financières à l’application de certaines réformes en faveur de la démocratie, du respect des droits de l’Homme et de l’intégration à la sphère financière mondiale. Notre investigation empirique porte sur un échantillon de 108 pays en voie de développement entre 1984 et 2008 et fait appel aux techniques d’estimation de panel statiques et dynamiques et les nouveaux tests de causalité en panel hétérogène. Nos résultats, démontrent que la libéralisation financière en plus de son impact direct sur la croissance, agit positivement sur l’investissement, le commerce extérieur ainsi que la stabilité macroéconomique à travers une réduction de l’inflation. L’intégration financière favorise, aussi, le développement du secteur financier et du capital humain. Ailleurs, la démocratie, même si elle n’a pas d’impact direct significatif sur la croissance, semble influer positivement sur celle-ci de manière indirecte à travers le canal du commerce extérieur mais aussi du développement financier et la promotion du capital humain. Ailleurs, l’instabilité politique affecte négativement le développement économique de manière directe mais aussi de manière indirecte en diminuant les investissements et le commerce avec l’extérieur et en augmentant l’inflation. Finalement, nos résultats suggèrent l’existence d’une relation de causalité bidirectionnelle entre l’intégration financière et la démocratie. Cependant, la relation entre ouverture financière et stabilité politique est plus spécifique et dépend des caractéristiques régionales des pays."
"L’instabilité et le manque de régulation à l’origine des crises financières devenues cycliques ont été des facteurs propices à un questionnement sur l’éthique de la finance. Cette thèse se propose dans un premier temps de s’interroger sur l’épistémologie de la « science financière » et ses attributs normatifs. Cette interrogation nous a permis de mettre en avant les imbrications logiques qu’il existait entre démarche positiviste et posture normative avant de proposer une cartographie du référentiel éthique qui façonne le discours financier. Ce travail théorique a préfiguré les questionnements empiriques développés, dans le second volet de cette thèse, autour de la confrontation de deux référentiels éthiques et financiers distincts : l’Investissement Socialement Responsable (ISR) et de l’Investissement shariah¬-compatible (ISC). Nos travaux identifient leurs éléments de différenciation et les passerelles possibles entre ces deux déclinaisons de la finance éthique. Dans la première étude, après avoir pris en compte le profil stochastique de 24 indices domestiques, nous mesurons et identifions l’origine de leur performance respective. Les résultats confirment la meilleure résilience des indices ISC durant la crise des subprimes tout en soulignant l’influence du niveau de développement et d’intégration des marchés boursiers. La seconde étude empirique explore le lien de causalité entre les critères ISR et ISC en investiguant la relation entre la performance sociale d’une entreprise (PSE) et sa structure financière. Les résultats obtenus à partir d’un échantillon de 1745 entreprises américaines indiquent que seules les petites entreprises controversées (non-engagées dans une démarche RSE) ont plus systématiquement recours au financement par dette et sont donc plus susceptibles d’être exclues des portefeuilles ISC. La dernière étude mesure, à travers une démarche expérimentale, l’impact financier de la combinaison des critères ISR et ISC. Contrairement aux prédictions suggérées par la théorie du portefeuille, les résultats n'indiquent aucun effet négatif sur la performance dû à l'application conjointe de filtres islamiques et ESG."
"La réalisation et le maintien de la crédibilité de la politique monétaire, évaluée par l'écart entre les résultats et les annonces officielles de politique (Gilles [1992]), est devenue une tâche cruciale pour l'Autorité monétaire, lorsque, à partir des années 1980, il a été abordée, dans la littérature économique, la question du central banking (Gilles & Bastidon [2014], Ferguson et Schularick [2008]). Bien que les avantages de la crédibilité soient évidents, ses déterminants le sont moins. En effet, les dernières décennies sont marquées par de profondes mutations dans la gouvernance des Banques Centrales. En particulier, la délégation de la politique monétaire à une Banque Centrale indépendante vis-à-vis des pouvoirs publics est devenue un des principaux déterminants de la crédibilité dans les économies avancées (Goodfriend [2012]; Bordo & Orphanides [2013]; Persson & Tabellini, [1993]). Cependant, pour les Pays en développement, en revanche, le débat sur la nécessité et la faisabilité des mécanismes d'engagement n’est pas tranché, eu égards des caractéristiques spécifiques (Kugman & al [1992], Assoumou-Ella & Bastidon [2015]). En utilisant un modèle simple et une fonction de perte de la banque centrale similaire à celles de Ball [1999] ou Cavoli [2008], nous comparons deux régimes de change différents afin de déterminer lequel des cas est le plus susceptible d'inciter les gouvernements à intensifier la lutte contre la corruption, tout en maintenant l’objectif de stabilité des prix. Un régime d’ancrage crédible conduit à une taxation élevée et un faible niveau de corruption et d’inflation, mais également à un niveau de croissance faible. Un régime monétaire indépendant sans ancrage, en revanche, conduit généralement à un niveau de corruption plus élevée. Cependant, lorsque l’indépendance de la banque centrale est assez forte, le régime monétaire indépendant sans ancrage peut également conduire à moins de corruption, plus de production et de dépenses publiques, bien qu’avec une inflation plus élevée qu’un régime monétaire avec ancrage. Ces résultats semblent indiquer que dans le cas des Pays en développement, l’indépendance de la banque centrale associée à l’ancrage du taux de change ne serait ni une condition nécessaire, ni une condition suffisante à la stabilité des prix."
"Cette thèse analyse les différents aspects de la filière cotonnière au Mali en se basant sur ses atouts et limites à travers une démarche théorique, appliquée et de terrain. Cette filière est issue d’un long processus amorcé au début du 20ème siècle. Les gouvernements successifs après la souveraineté du pays en 1960 ont réservé une place de choix au développement des activités ayant un rapport direct ou indirect avec le coton. L’objectif était et reste d’assurer la participation du pays aux échanges internationaux à travers un produit pour lequel, l’économie bénéficie d’externalités positives en termes d’entrée de devises et de création d’emplois. Notre recherche nous a permis d’identifier les atouts du pays que les acteurs doivent améliorer et les défis endogènes qu’ils doivent relever, afin de pérenniser le développement de ce secteur qualifié de « space maker », au cœur d’une économie encore largement dominée par l’agriculture. La pertinence d’une spécialisation dans la production cotonnière est avérée à travers un modèle économétrique en « données de panel », qui permet de prendre en compte l’aspect temporel et spatial des structures de production cotonnière du pays en les restituant dans le contexte régional des pays d’Afrique de l’Ouest. Les résultats des différentes estimations classent le Mali en bonne position (leader, co-leader ou deuxième). Ce résultat doit conforter les acteurs du coton (États, Compagnie Malienne de Développement des Textiles, Producteurs) et les partenaires techniques et financiers à s’investir davantage pour améliorer la compétitivité de la filière, malgré que le pays soit en situation de preneur de prix « price taker », dans un environnement où certaines grandes puissances économiques (États-Unis, Chine, Espagne) produisent le même produit, en utilisant des moyens parfois critiquables, qui pénalisent la situation des producteurs dans les pays non développés.La crise qui a secoué la filière lors de la décennie 2000 a substitué l’or au coton comme première recette d’exportation (Instat-Mali 2012, Mainguy et al., 2013). Est-ce un argument suffisant pour délaisser la filière cotonnière ? Nous ne le croyons pas après avoir constaté lors de nos trois enquêtes de terrain son dynamisme et ses retombées sur les autres secteurs d’activités. En outre, l’impact de la dite filière sur l’amélioration des conditions socioéconomiques des populations demeure essentiel de sorte que, malgré ses défis, elle reste un levier majeur de la croissance économique du pays."
"Cette thèse aborde la question de la gouvernance locale dans le secteur de l’éducation au Mali. Elle propose une analyse économique des mécanismes par lesquels les autorités publiques impliquent les acteurs locaux (collectivités territoriales, communautés, services décentralisés et déconcentrés de l’État, ONG…) dans la gestion et la prestation de services publics (éducation). Elle est structurée en trois chapitres, avec comme cadre d’analyse l’économie de l’éducation, l’économie du développement et l’économie institutionnelle. Le premier chapitre donne un aperçu du système éducatif malien à travers quelques indicateurs clés (taux brut de scolarisation, taux net de scolarisation, etc.) et caractéristiques majeures dont les inégalités d’accès. Il mesure également le poids du secteur de l’éducation dans l’économie malienne en termes de dépenses publiques et analyse quantitativement et qualitativement l’offre éducative avant d’identifier les facteurs qui influencent la demande d’éducation des familles.Le deuxième chapitre clarifie le concept de la gouvernance locale et son évolution notamment dans le secteur de l’éducation. Il analyse la gestion du système éducatif malien entre réforme et transfert de compétences et de responsabilités ainsi que les mécanismes de coordination et de coopération entre tous les acteurs. Il décrypte les logiques d’approbation de la gouvernance de l’éducation au niveau local grâce à une étude de terrain menée auprès des acteurs locaux dans la région de Kayes, de Koulikoro, de Ségou, de Sikasso et le district de Bamako.Le troisième chapitre analyse les enjeux et les défis de la gouvernance locale comme stratégie de développement éducatif. Il apporte une contribution à la problématique de la gouvernance de l’éducation par les acteurs locaux, tant en matière d’offre éducative que de scolarisation des élèves au cours de la période 2004-2011. Ensuite, à travers une application économétrique des données de panel, il évalue les effets de l’amélioration de l’offre scolaire sur l’évolution des effectifs. Enfin, ce chapitre évalue le développement de l’éducation au niveau local (région) à travers l’indice de développement éducatif (IDE).Au terme de notre réflexion, nous considérons que la gouvernance locale est un atout pour améliorer certains indicateurs éducatifs, notamment le taux d’accès, le taux de scolarisation, le taux de rétention scolaire, la couverture scolaire du pays etc. Pour autant, à elle seule, la gouvernance locale ne saurait satisfaire ou relever tous les défis éducatifs, tant dans leur dimension quantitative que dans leur dimension qualitative."
"Ce travail analyse sous différentes approches la pauvreté au Sénégal en s’appuyant notamment sur les données fournies par les deux dernières enquêtes auprès des ménages (ESAM 2-2002 et ESPS-2006) réalisées par l’Agence Nationale de la Statistique et de la Démographie en partenariat avec la Banque mondiale.Dans l’analyse de la pauvreté monétaire, nous faisons apparaître des différences importantes en termes de seuils de pauvreté dans les régions avec aux extrêmes la région Dakar 923,55 F CFA (1,40 €) et Tambacounda 515,70 F CFA (0,78€), ce qui suggère le peu de pertinence quant à l’utilisation d’un seuil établi au seul niveau national. Sur la base de ces seuils, les indices de pauvreté issus de la formule générique de Foster, Greer et Thorbecke (FGT) dévoilent une baisse du taux de pauvreté entre 2002 et 2006 de 57,1% à 50,8%, soit de 6,9 point dans l’ensemble du pays et un écart à la ligne de pauvreté passant de 18% à 16,4%. Cette baisse est particulièrement observée dans les régions de Dakar, Diourbel, Kaolack, Saint-Louis et Thiès. Au niveau départemental, les taux de pauvreté montrent une concentration importante dans les zones rurales et l’existence de poches de pauvreté enclavées dans les zones urbaines. L’estimation d’un modèle économétrique spatial met en évidence les facteurs socioéconomiques susceptibles d’expliquer les différences interdépartementales de taux de pauvreté constatées en 2006, notamment le degré de développement économique des territoires (urbanisation, emploi) ainsi que les comportements des ménages liés au niveau d’infrastructures (d’éducation, de santé et de fécondité).Par ailleurs, nous proposons un modèle dichotomique à partir duquel il est possible de mettre en évidence les déterminants de la pauvreté monétaire des chefs de ménage. Les résultats montrent que les femmes chefs de ménage ne sont pas la couche la plus pauvre. De manière générale, les disparités de pauvreté manifestes entre milieux urbain et rural sont largement corrélées à des handicapes en matière de d’éducation et à l’inégal accès aux moyens d’information et de communication.Nous abordons une analyse multidimensionnelle de la pauvreté au Sénégal, à travers une estimation des degrés de privation de certains besoins essentiels des ménages. L’approche par la théorie des ensembles flous utilisée à cet effet suggère que la pauvreté a faiblement diminué : 1 % contre 7 % pour la pauvreté monétaire. Contrairement à l’approche monétaire, la baisse de la pauvreté non monétaire observée concerne d’autres régions comme Kolda et Ziguinchor et les régions de Diourbel et Kaolack connaissent une hausse. L’estimation des indices flous unidimensionnels a permis d’identifier les domaines dans lesquels les ménages affichent le degré de privation le plus important : la qualité du logement, le niveau d'instruction et les moyens d’information et de communication, au-delà du revenu.Les profils de pauvreté monétaire aussi bien que multidimensionnelle sont d’excellents outils pour cibler les groupes les plus nécessiteux de la population. En revanche, ces outils restent muets sur la perception de ces pauvres quant à leur propre situation socioéconomique. En ce sens, une analyse économétrique des facteurs déterminants de la pauvreté ressentie au Sénégal en 2006 fait apparaître l’importance de certaines dimensions non économiques (exclusion sociale, culturelle et manque de concertation des intéressés sur les politiques de développement et de lutte cotre la pauvreté)."
"L’objet de cette thèse est de comprendre en quoi l’instabilité financière est un phénomène inhérent au cycle et en quoi la relation entre la politique monétaire et le secteur bancaire joue un rôle central dans son explication. Nous mettons en évidence de nouveaux canaux de transmission de la politique monétaire, notamment les canaux de la prise de risque et du capital bancaire. Une idée forte de la thèse est que le crédit productif est évincé dans les deux phases du cycle au profit des actifs risqués dans la phase ascendante et des valeurs refuge en phase descendante. Notre démarche est alors de construire un modèle théorique qui rend compte de cette problématique et dont les hypothèses s’appuient à la fois sur une revue de la littérature abondante et l’observation de faits stylisés."
"Dans cette thèse, nous discutons la relation entre les exportations et l’IDE, et tentons de trouver une relation de long terme entre ces variables. Dans cette analyse, nous étudierons tout d’abord de manière empirique les déterminants des exportations et des l’IDE, à la fois au niveau micro et macro. Ceci nous permettra par la suite de détecter plus précisément la relation entre ces deux variables. Plus précisément, cette thèse comporte les points suivant : Au niveau micro (niveau de la firme), les multinationales sont susceptibles de mettre en œuvre les deux activités (exportations et IDE) pour servir les marchés étrangers, mais les choix stratégiques des multinationales permettent aussi de choisir entre exportations et IDE. Sur ce point, la productivité des entreprises multinationales ainsi que leurs autres caractéristiques ont un rôle crucial pour éclairer le mécanisme de choix entre les stratégies et la relation entre exportations et investissements. Ceci fera l’objet de la première partie qui proposera une application au cas français. Dans cette partie, nous distinguerons également les décisions stratégiques en fonction de la taille de l’entreprise (très grandes ou grandes entreprises françaises). Au niveau macro, nous chercherons à identifier les déterminants simultanés des exportations et des IDE. Pour se faire, un système gravitaire dynamique bivarié sera estimé afin d’éclairer le rôle de ces déterminants et la relation entre exportations et IDE. Ceci fera l’objet de la seconde partie, qui sera appliquée aux échanges entre la France et dix partenaires euro-méditerranéens. Le choix de ces pays s’appuie sur l’importance qu’ils revêtent dans les échanges français. Par ailleurs, l’absence de littérature appliquée à ces pays dans ce domaine constitue une motivation supplémentaire."
"Cette thèse propose une analyse axée à la fois sur les relations interportuaires et sur celles qui régissent les segments du commerce international maritime : les ports leurs avant et arrière-pays. Dans un premier chapitre, un bilan sur les hiérarchies et la croissance relative en Europe est dressé. A l’aide de deux méthodes complémentaires : chaines de Markov et les clubs de convergence, il montre qu’il existe une polarisation de la croissance au sein d’un nombre limité de grands ports et une forte fréquence pour les plus défavorisés. Ce résultat marque l’absence de convergence au sein des ports européens. Par ailleurs, le phénomène de « contestation portuaire périphérique » s’exercerait ici par les petits ports à l’égard de ceux de taille moyenne, ce qui contesterait fortement certains résultats de la littérature sur le sujet. Le second chapitre met en lumière l’importance de la relation entre le port et ses environnements terrestres et maritimes. Il se concentre sur l’aspect spatial qui caractérise ces relations et montre qu’il existe des disparités entre les régions portuaires. Alors que le Nord de l’Europe se caractérise par la complémentarité, les ports du Sud entretiennent des relations concurrentielles. Il montre aussi que, la taille du marché local et sa proximité des ports ne sont pas systématiquement, de bons déterminants du débit. L’accessibilité au marché à partir des régions portuaires périphériques est cependant, déterminante et peut s’expliquer par l’existence de niches de marchés au sein de ces zones. Le troisième chapitre se concentre sur la connectivité maritime par pays, ses déterminants ainsi qu’aux effets spatiaux pouvant l’influencer. L’analyse révèle qu’il existe des différences substantielles entre l’Europe du Nord et la Méditerranée. Elle, permet au-delà, des résultats de la littérature, d’identifier les relations dominantes dans chaque région. Bien que la complémentarité et la concurrence puissent coexister entre ports/pays voisins, cette étude montre que l’une peut très bien l’emporter sur l’autre."
"Cette thèse cherche à approfondir la nature et la forme des relations entre les inégalités éducatives et le développement. Elle s’inscrit dans le prolongement des analyses engagées sur les liens éducation/croissance et inégalités/croissance, en essayant d’apporter un éclairage complémentaire sur ces deux relations. Elle vise à retracer de manière stylisée l’évolution des inégalités éducatives particulièrement dans les pays en développement et à caractériser la non-linéarité de la relation à partir de l’estimation de modèles non-paramétriques et semi-paramétriques. Cette thèse est constituée de trois chapitres auxquels correspondent des objectifs, des bases de données et des méthodologies spécifiques. Dans un premier chapitre, nous proposons une nouvelle base mondiale sur les inégalités d’éducation. La majorité des travaux sur la relation entre capital humain et développement économique ont principalement appréhendé la mesure du capital humain à travers des mesures de l’éducation en utilisant notamment la moyenne d’années de scolarisation (stock du capital humain). Notre base de données, qui présente une mesure alternative du capital humain, tend à améliorer sensiblement le mode de calcul des inégalités de l’éducation. Elle exploite toute la richesse des données désagrégées, corrige les pondérations inappropriées et affine certaines hypothèses réductrices sur les durées des cycles d’enseignement et les niveaux d’éducation retenus. Nous avons aussi généralisé la formule proposée par Berthélemy (2006) sur l’indice de Gini de l’éducation. Le domaine de variation possible de cet indice est identifié graphiquement selon la moyenne d’années de scolarisation et les durées cumulées des cycles d’enseignement. Nous mettons en évidence, dans le cadre du chapitre II, l’existence d’une relation non linéaire entre les inégalités dans l’éducation et le développement économique en utilisant des modèles non-paramétriques et semi-paramétriques qui n’exigent pas de formes fonctionnelles prédéfinies à l’avance. Plusieurs phases sont ainsi mises en évidence : les trois premières sont repérées seulement par rapport aux niveaux de développement ; deux autres sont identifiées à la fois par des seuils de développement et d’inégalité d’éducation ; une sixième et dernière phase est définie par rapport au seul niveau d’inégalité d’éducation. Nous montrons que c’est dans la troisième et cinquième phases que la réduction de l’inégalité d’éducation présente l’impact le plus bénéfique sur le développement économique.Au-delà du schéma général mis en évidence sur le plan transnational dans les chapitres I et II, nous explorons dans le chapitre III la nature de cette relation au plan régional dans le cas du Maroc, pour lequel nous disposons de données aux niveaux communal et provincial. La non-linéarité de la relation est aussi confirmée. La troisième phase repérée au chapitre II est subdivisée, dans le cas des provinces marocaines, en deux sous phases qui présentent un impact différencié selon un seuil de développement et d’inégalité d’éducation."
"Cette thèse porte sur le patriotisme économique à travers le cas de 50 grandes firmes industrielles françaises. Elle entend d'abord analyser les contours du concept - assez négligé en sciences économiques - de patriotisme économique par le lien historique entre Nation et Entreprises puis par un examen de la pensée des « grands » auteurs de la question (Friedrich List en particulier). Elle vise ensuite empiriquement à mesurer l'inclination patriotique de 50 grandes firmes industrielles françaises sur la base d'une série de données recueillies dans leur document de référence déposé auprès de l'Autorité des Marchés Financiers (AMF) entre 2006 et 2016: répartition géographique des effectifs, ventilation du chiffre d'affaires, nombre de filiales étrangères, impôt sur les résultats, attribution par pays (ou continent) des actifs immobilisés. Il en ressort notamment une réduction importante en l'espace de 10 ans seulement des effectifs en France {-17%) et une hausse concomitante des effectifs dans les pays émergents (+29%). A l'aune de ces résultats, le présent travail fera apparaître les principaux déterminants du patriotisme : actionnariat, secteur d'activité, rentabilité, rémunération du personnel… Enfin, cette thèse compare ces résultats découverts au sein des firmes françaises avec ceux des firmes européennes concurrentes. Les mêmes données ont été consignées auprès d'une soixantaine de multinationales allemandes, britanniques, suédoises ou encore suisses. Lesquelles, au cours de la période (2006-2016), parviennent à augmenter de 14% leurs effectifs domestiques. Un défaut de patriotisme français apparaît donc clairement au sujet du patriotisme économique. Les derniers développements de cette thèse en analyseront les raisons sur la base de résultats empiriques mais aussi d'observations venues d'autres sciences sociales."
"Cette thèse élucide certains questionnements qui pourraient être soulevés au sujet de l’économie cambodgienne en prenant le commerce, l’investissement direct étranger et le tourisme comme sujets d’interrogation de notre recherche. Ces trois secteurs sont considérés comme secteurs clés du développement économique et source de bien-être et de prospérité économique du pays. Nous utilisons principalement les modèles empiriques pour étudier les problématiques évoquées. Nous profitons également des développements récents des modèles théoriques pour justifier la construction des modèles empiriques et expliquer les résultats d’estimations. L’examen des structures d’exportation révèle que le Cambodge dispose de forts avantages comparatifs dans les produits agricoles, les produits de textiles et de chaussures pour les exportations vers l’ASEAN et vers les marchés mondiaux. Les estimations obtenues sur la base du modèle de gravité montrent que les investissements directs étrangers, la logistique, et la qualité de la réglementation sont des facteurs qui favorisent les exportations, tandis que les droits de douane et la distance avec les pays importateurs constituent des obstacles aux exportations internationales. L’étude sur les déterminants de l’investissement direct étranger utilisant une analyse statique et dynamique des données de panel révèle que les facteurs d’attractivité locaux tels que la taille du marché ou la qualité de la gouvernance n’influencent pas les entrées des investissements directs étrangers au Cambodge. En revanche, des facteurs tels que les échanges commerciaux et l’adhésion à l’Organisation Mondiale du Commerce ont attiré davantage d’investissements étrangers. Nous avons également mis en lumière des effets complémentaires entre le commerce et le tourisme dans le sens où le tourisme favorise le commerce et réciproquement. D’après les analyses développées à travers ces quatre chapitres qui constituent notre recherche, cette thèse a révélé les effets pertinents et complémentaires des trois principaux secteurs de l’économie du Cambodge. Une augmentation des exportations du pays encourage davantage les investissements directs étrangers et le développement du tourisme au Cambodge. Les effets réciproques sont également confirmés par le fait que les investissements directs étrangers et le tourisme accroissent les exportations."
"L’objet de cette thèse est l’étude de l’évolution des villes et le développement économique en Algérie, en faisant une première étude sur les villes des pays du Maghreb qui présentent une similitude dans leur évolution et connaissent un processus accéléré de leur urbanisation dû au phénomène de l’exode rural. Ce travail est composé de trois chapitres.Le premier chapitre propose d’étudier le développement des hiérarchies urbaines et de déterminer la nature de la croissance urbaine pour les trois pays de cette région du monde qui a subi des changements démographiques, politiques, économiques et sociaux importants depuis la seconde moitié du 20ème siècle. On s’appuie dans ce travail sur une base de données qui regroupe la population des villes urbaines des pays du Maghreb de plus de 5000 habitants pour tous les recensements effectués depuis les années 60. Nous analyserons ensuite les résultats trouvés à l’aide d’outils et modèles économétriques utilisés souvent par les chercheurs dans le domaine de l’urbanisation.L’Algérie, qui constitue notre cas d’étude dans le deuxième chapitre, présente à son indépendance en 1962, une économie dépendante, désarticulée et orientée autour de l’intérêt de la minorité coloniale et du capitalise métropolitain, sa population pauvre et quasiment analphabète vit sur la bande Nord du pays. Les projets industriels postindépendance lancés par les gouvernements successifs n’ont pas eu de résultats probants. En effet, plus d’un demi-siècle plus tard, l’Algérie est toujours fortement dépendante de la rente des hydrocarbures et son économie n’a pas été diversifiée. La ville d’Alger, objet de notre troisième chapitre, est née au 10ème siècle et devient la capitale de la Régence entre le 16ème et 19ème siècle. Durant la période coloniale 1830-1962, la ville se développe, s’occidentalise, elle devient la capitale coloniale hors de l’hexagone et à l’indépendance, dès le départ des européens, un rush sur les biens vacants est observé, l’exode entamé durant la guerre s’accélère du fait que le nouvel État n’a pas eu une politique urbaine, il reconduit la législation coloniale puis opte en 1974 pour une législation de type socialiste et libérale à partir de 1990. Les tentatives de maitriser l’urbanisation par des institutions, des études, et des découpages n’ont pas donné les résultats probants, la ville a évolué spontanément."
"L’objectif de ce travail est d’analyser les déterminants des inégalités de performances scolaires de 56 392 élèves en fin de cycle primaire dans 2 603 établissements scolaires situés dans 647 districts des 12 pays d’Afrique australe et orientale (Afrique du Sud, Botswana, Kenya, Lesotho, Malawi, Mozambique, Namibie, Ouganda, Swaziland, Tanzanie, Zambie et Zimbabwe) à partir de la troi-sième enquête de 2007 du Consortium de suivi de la qualité de l’éducation en Afrique orientale et australe (SACMEQ III). Le travail s’articule autour de trois chapitres. Il s’agit d’étudier, dans le premier chapitre, l’influence des caractéristiques individuelles de l’élève et de l’école sur les per-formances scolaires, ainsi que le rôle important des caractéristiques régionales. Dans le deuxième chapitre, nous cherchons à analyser comment l’inspection des établissements scolaires et leur ac-cessibilité par rapport au domicile des élèves agissent sur l’eÿcience scolaire. Le dernier chapitre propose d’étudier les facteurs socio-économiques et les conditions de scolarité déterminant les per-formances et les inégalités scolaires entre les filles et les garçons, compte tenu de leurs origines socio-économiques. Pour ce faire, nous avons utilisé di˙érentes approches économétriques, à savoir une modélisation multiniveau dans le premier chapitre, un modèle de frontière non paramétrique dans le chapitre deux et un modèle d’économétrie spatiale dans le chapitre trois. Les résultats montrent que les acquis des élèves, l’eÿcience des établissements et les inégalités scolaires entre les filles et les garçons sont très hétérogènes en Afrique australe et orientale. Les résultats révèlent que les élèves qui enregistrent des scores relativement élevés sont situés dans les régions urbaines riches et ayant accès aux moyens de transport. De profondes inégalités scolaires existent du fait de la carence de transports, d’infrastructures routières, éducatives et de santé particulièrement pour les districts situés en milieu rural et dans les quartiers pauvres des grandes villes. Il est également montré que les variables touchant les missions d’inspection des écoles, l’utilisation de la langue d’instruction à la maison et la sécurité dans le milieu scolaire jouent un rôle important dans la réussite des élèves. Les résultats obtenus permettent une meilleure compréhension du système sco-laire dans ces pays."
"Face à l’instabilité des cours agricoles et à ses conséquences notamment pour les pays en développement, la première partie de cette thèse est consacrée à la présentation des déterminants des cours des matières premières alimentaires, incluant les évolutions récentes en matière d’offre, en tenant compte des conséquences du réchauffement climatique, et de demande, considérant notamment les biocarburants. Il est également question de présenter la financiarisation en cours des économies, et les doutes qui planent sur le rôle que peuvent avoir la spéculation sur les marchés à terme ou encore la mise en œuvre des politiques monétaires, sur les cours au comptant observés sur les marchés physiques des produits agricoles. Suite aux réflexions et éléments de littérature avancés, la seconde partie procède de deux études empiriques. La première est axée sur l’impact de la spéculation sur les marchés financiers à terme sur le cours des sous-jacents (agricoles), alors que la seconde questionne le rôle des marchés monétaires, abordé à travers la capacité du banquier central à stabiliser les taux d’intérêt à court terme. Sur cette base, des conclusions mais également des pistes de recherche sont établies, du fait du prolongement en cours du processus de financiarisation des économies."
"La région MENA est aujourd’hui, au centre d’ambitieux enjeux économiques essentiellement en matière d’intégration régionale et de transformation structurelle. Toutefois, les Etats Arabes de cette région ont présenté des défaillances et une vulnérabilité importante dans le système économique et productif au lendemain des mouvements et des bouleversements politiques, économiques, sociales et populaires de très grande ampleur. Ces chocs apportent un lot de nouveaux défis à relever. L’objectif principal de cette thèse est d’étudier les nouvelles dynamiques du processus de transformation structurelle afin de proposer de nouvelles trajectoires de développement pour ces pays. Dans cette optique, nous abordons les questions liées aux déterminants et aux effets spatiaux de ce processus, à la relation entre d’une part la complexité économique et la pollution de l’air, et d’autre part, la complexité économique et les inégalités de genre en éducation. Pour cela, nous considérons un panel dynamique de 133 pays qui couvre une période longue et récente (1984 à 2014). En utilisant des données récentes et en intégrant des paramètres jusque-là peu utilisés, nous mettons en évidence des caractéristiques particulières du processus de complexification des systèmes productifs. D’un point de vue général, les résultats révèlent que les performances en matière de complexification des systèmes productifs sont très hétérogènes au sein des pays MENA et que leurs déterminants dépendent des caractéristiques des économies. Les fortes disparités observées s’expliquent, au-delà de l’effet significatif du revenu par habitant, par une carence dans le système institutionnel, éducatif en particulier dans l’accès à l’innovation, mais aussi à l’abondance des ressources naturelles ou encore à l’attractivité des investissements directs étrangers. Au-delà des caractéristiques individuelles des économies, l’analyse spatiale montre que des facteurs géographiques tels que le taux d’urbanisation, les accords commerciaux, et la localisation spatiale jouent un rôle très important dans le processus de transformation structurelle. Nous apportons également, grâce aux outils tirés de la mécanique classique, des réponses aux limites des modèles économiques traditionnels qui peinent à démontrer l’existence d’un processus d’accélération du développement économique."
"Le programme « PME Minières en Nouvelle-Calédonie, histoire, identités, enjeux » complète l’analyse du paysage minier calédonien en s’intéressant aux acteurs de l’extraction du nickel autres que les groupes métallurgistes, et analyse leurs relations avec les sociétés locales et leur rôle social, politique et économique dans la Nouvelle-Calédonie contemporaine."
"Le programme « PME Minières en Nouvelle-Calédonie, histoire, identités, enjeux » complète l’analyse du paysage minier calédonien en s’intéressant aux acteurs de l’extraction du nickel autres que les groupes métallurgistes, et analyse leurs relations avec les sociétés locales et leur rôle social, politique et économique dans la Nouvelle-Calédonie contemporaine."
"De nombreux auteurs se sont penchés, ces dernières années, sur le thème des hiérarchies urbaines et de leur évolution afin d'étudier et interpréter la régularité extraordinaire de la distribution des villes selon leur taille, connue, plus généralement, sous le nom de loi de Zipf. Tout en contribuant au débat méthodologique relatif à la détermination et au choix le plus efficace des méthodes d'estimation du coefficient de hiérarchisation d'une distribution rangtaille des villes, cet article tend à mettre en évidence la permanence de la validité de la loi de Zipf au sein des Balkans, une région qui a pourtant connu, durant ces trente dernières années une mutation politique, économique et institutionnelle majeure et dont les conséquences migratoires ont fortement affecté sa démographie."
"La présente thèse a pour objectif d'analyser le rôle des institutions sur la croissance économique, et notamment sur l'attractivité et l'effet des investissements directs étrangers (IDE). Pour cela, nous avons utilisé douze indicateurs de mesure de la qualité des institutions et testé leurs effets en retenant un échantillon important de pays développés, en voie de développement et en transition. Notre démarche se décline en trois chapitres. Le chapitre 1 est consacré à l'évaluation de l'effet direct des institutions sur la croissance économique en fonction du niveau de développement des pays étudiés. Les résultats montrent que la qualité des institutions a un effet plus important sur la croissance dans les pays à revenu intermédiaire, en particulier la stabilité politique et la lutte contre la corruption. Pour les pays à revenu élevé le facteur le plus déterminant est le respect des lois et des contrats. Le chapitre 2 analyse l'effet indirect des institutions sur la croissance à travers les IDE. Les résultats mettent en évidence l'existence des seuils institutionnels qui conditionnent l'effet des IDE sur la croissance selon la situation géographique et le niveau de développement des pays. Il ressort notamment que la lutte contre la corruption et l'amélioration de la démocratie sont les canaux travers lesquels les IDE favorisent la croissance dans les pays d'Afrique du Nord et du Moyen-Orient et d'Asie, la stabilité d gouvernement et le respect des contrats étant les plus importants dans les pays d'Europe et d'Amérique. Le chapitre 3 s'intéresse à l'effet de l'ouverture financière et du développement financier sur l'attractivité des IDE en mettant e évidence le rôle des institutions. Les résultats montrent que pour les pays qui ont libéralisé leur compte de capital, l'entrée d'IDE est plus importante lorsqu'ils disposent d'une bonne qualité institutionnelle. De la même manière, l'attractivité des IDE est favorisée par 1 passif liquide des banques et les crédits au secteur privé, mais il est moindre pour les pays dotés d'importantes ressources naturelle: Les résultats soulignent l'existence d'un seuil institutionnel à partir duquel le poids de la capitalisation boursière et des litres échangé en bourse favorisent l'attractivité des IDE. La confrontation des résultats des chapitres 2 et 3 permet d'avancer que le seuil institutionnel à travers lequel le développement financier améliore l'attractivité des IDE est plus élevé que le seuil à travers lequel les IDE impactent la croissance. Dans les pays e développement en particulier, la qualité des institutions constitue une contrainte plus sévère en termes d'attractivité qu'en termes d'effet des IDE sur la croissance."
"Durant ces dernières années,un ensemble de travaux originaux ont été consacrés à l'étude des hiérarchies urbaines et à l'organisation en réseau des villes. A l'issue de ces travaux, confirmant ou infirmant la validité de la loi de Zipf, certains chercheurs ont montré qu'il était nécessaire d'intégrer une approche dynamique visant à étudier la croissance, absolue et relative, des villes, afin de comprendre les changements de leurs positions au sein même d'un système urbain donné. Ces travaux débouchent sur une interrogation quant à la nature de la croissance urbaine, ""aléatoire"" ou ""endogène"",et la relation fondamentale entre la taille d'une ville et sa croissance. En s'appuyant sur l'étude de l'évolution du système urbain chinois entre 1984 et 2004, cet article apporte une contribution au débat qui oppose les approches de la croissance urbaine aléatoire à celles de la croissance urbaine endogène."
"Les travaux de S. Kuznets et son hypothèse de courbe en cloche sont à l’origine d’une littérature importante sur la relation entre croissance économique, disparités de revenu et urbanisation. Cet article examine la relation entre disparités régionales de revenu et dynamiques d’urbanisation en Chine durant la période 1978-2005 qui correspond à l’ouverture du pays à une économie de marché. L’objectif recherché est non seulement de tester la corrélation entre ces deux phénomènes, mais également de mettre en évidence les effets de contiguïté qui les caractérisent, à travers l’utilisation d’un certain nombre d’outils d’analyse spatiale exploratoire. Cet article aboutit à deux conclusions : en premier lieu, les effets de contiguïté sont plus importants dans la distribution provinciale des PIB par tête que dans celle des taux d’urbanisation ; en second lieu, le développement des régions côtières n’est pas uniforme et un clivage Nord-Sud apparaît. La côte nord de la Chine est caractérisée par une prolifération régionale des effets de croissance et une urbanisation diffuse, tandis que dans le Sud, on assiste à un effet de polarisation autour de la province de Guangdong."
"Les contributions réunies ici s'intéressent à ce que pourraient être les conditions d'un multiculturalisme réussi, dont deux dimensions sont ici privilégiées : celle de conflits religieux qui n'ont cessé d'être instrumentalisés au service de causes moins saintes que profanes ; celle des modalités d'intégration dans un espace que les institutions internationales ne sont manifestement pas parvenues à unifier, laissant aux organisations privées la charge de reprendre à leur compte cette responsabilité sociale dont les sociétés civiles portent la demande pressante et qui paraît bien détenir la clé de la soutenabilité d'un avenir commun."
"Si la crise grecque a montré le caractère potentiellement périssable de la construction monétaire européenne, il convient aujourd'hui de s'interroger sur les perspectives de la zone euro alors que les divergences entre pays sont nombreuses et que l'économie européenne reste morose. La décision d'adopter l'euro a été essentiellement motivée par des raisons politiques plutôt que par des déterminants économiques. Néanmoins, le facteur politique de la construction monétaire européenne n'implique pas que les facteurs économiques sont non pertinents. En fait, le facteur politique a certainement conduit à sous-estimer les implications économiques du passage à la monnaie unique. Ainsi, le processus d'UM n'a pas considérée comme centrale la question de l'hétérogénéité structurelle entre les pays membres. La vision dominante était alors que la convergence nominale devait conduire à des modifications structurelles des économies dans le sens de plus de flexibilité. Dans cette perspective, la crise des dettes souveraines (2010-2013) peut être interprétée comme le résultat d'un double problème : celui de l'incomplétude institutionnelle d'une part ; celui des divergences structurelles entre les pays membres d'autre part. Spolaore (2015) explique l'incomplétude de l'architecture institutionnelle européenne en considérant son échec initial. En effet, au début des années 1950, les initiateurs de l'intégration européenne ont défendu une approche ambitieuse fondée sur une intégration politique poussée et la mise en place d'une politique de défense commune. Les difficultés rencontrées pour atteindre ces objectifs ont conduit à une stratégie de fait axée sur une logique selon laquelle une approche gradualiste des réformes serait susceptible de conduire à une réaction en chaîne dans le sens de plus d'intégration. La position défendue dans cet article est la suivante : sans réformes de fond sur la gouvernance économique de la Zone Euro (ZE), celle-ci sera sans doute soumise à des tensions de plus en plus fortes mettant en péril sa pérennité Divergences macroéconomiques et fragmentation financière au sein de la zone euro Tout d'abord, la question des divergences macroéconomiques au sein de la ZE est doublement importante. D'une part, elle suggère que, contrairement à ce que certains économistes avaient pu penser, le fait d'être membre d'une UM ne conduit"
"Cette thèse propose d’étudier l’évolution des hiérarchies des villes et de la croissance urbaine en s’appuyant sur le cas du Maroc. Le Maroc a connu ces dernières décennies un processus d’urbanisation soutenu, tant dans les grandes villes que dans les petites et moyennes villes. D’où la nécessité de procéder à une structuration démographique urbaine primatiale qui exige une coordination entre certaines politiques économiques nationales et les politiques d’aménagement menées par les grandes métropoles afin de faire preuve d’une réelle efficacité. Une politique d’aménagement résolument tournée jusqu’ici vers la gestion de la pression urbaine dans les grandes métropoles. Ce qui a conduit par conséquent à une polarisation des activités dans quelques régions du territoire. Ainsi, cette thèse se structure autour de trois chapitres. Le premier chapitre examine la loi rang-taille et l’apport des économistes et des géographes dans ce processus. Le second chapitre analyse les trois approches théoriques qui traitent la question de la croissance urbaine notamment les théories de la croissance aléatoire, de la croissance déterministe et l’intersection de ces deux approches dites d’hybrides. Enfin, le dernier chapitre est basé sur une étude empirique à l’échelle régionale afin de recenser les déterminants de la croissance urbaine des régions marocaine. Le travail engagé dans cette thèse s’appuie sur des bases de données originales fournies par le Haut-Commissariat au Plan permettant de recenser la taille des agglomérations marocaines et utilise un ensemble d’instruments statistiques et économétriques. Les différents résultats obtenus s’inscrivent dans le prolongement de différentes études effectuées en sciences régionales. Ces résultats indiquent que les hiérarchies urbaines marocaines sont appelées à changer dans les décennies à venir, mais également que la croissance économique des régions du Maroc n’affecte pas immédiatement la croissance de la population urbaine."
fr_abstract_s
"Ce mémoire d’habilitation à diriger des recherches a pour objectif de présenter une synthèse détaillée de mes activités de recherche post-thèse, sur la période 2011-2019. Ces travaux ont été regroupés et présentés dans l’objectif général d’une meilleure caractérisation et compréhension des limitations à l’effort dans les maladies respiratoires chroniques (MRC). L’approche que je développe ici suggère une complémentarité des évaluations de la tolérance à l’effort global d’une part, et de la fonction musculaire périphérique d’autre part. L’intérêt de ces tests est illustré ici au travers de plusieurs études conduites chez des personnes atteintes de mucoviscidose, de bronchopneumopathie chronique obstructive (BPCO) et du syndrome d'apnée obstructive du sommeil. Collectivement, certains de nos résultats suggèrent que les tests d’effort globaux, qu’ils soient maximaux ou sous-maximaux, présentent tous leur propres limites et intérêts. Ces tests ne sont pas interchangeables mais plutôt complémentaires. Le choix et la fréquence d’utilisation de ces tests doivent se faire selon une approche individualisée, en fonction de la pathologie considérée, de sa sévérité, de certains évènements conditionnant l’évolution de la maladie, et des ressources du centre hospitalier. Ces tests globaux, utiles pour détecter une intolérance à l’effort, restent peu adaptés pour identifier certains mécanismes physiologiques sous-jacents. En particulier, les différentes anomalies neuromusculaires retrouvées dans les MRC justifient une évaluation des aptitudes physiques sur des efforts isolant la fonction neuromusculaire. Malgré la présence d’un rationnel élevé en faveur d’anomalies périphériques dans la mucoviscidose, nos travaux suggèrent une fonction contractile et métabolique normale au cours d’efforts fatigants localisés chez des personnes avec atteinte légère à modérée. Certains de nos travaux actuels visent maintenant à déterminer si la sévérité de la mutation CFTR pourrait jouer un rôle sur la présence d’anomalies musculaires dans cette maladie. Au-delà d’arguments périphériques, il existe un rationnel élevé en faveur d’anomalies corticospinales en lien avec la dysfonction musculaire dans les phénotypes les plus sévères de certaines MRC. Certains de nos travaux ont notamment montré que les anomalies corticospinales et cérébrovasculaires présentes au repos chez des individus apnéiques sévères persistaient au cours de l’exercice, pouvant contribuer à la réduction de force musculaire, à l’augmentation de la fatigabilité musculaire ou encore à une réduction des capacités maximales aérobies. La dernière partie de ce mémoire est dédiée à la présentation de différentes perspectives de recherche. Je propose un rationnel justifiant de s’intéresser aux limitations à l’effort des individus MRC âgés dans des contextes cognitivo-moteur exigeants. Nous testons actuellement l’hypothèse d’une fatigabilité musculaire particulièrement accrue chez des personnes BPCO en condition de double tâche cognitivo-motrice, en lien notamment avec des anomalies de la commande centrale. Les indicateurs neuromusculaires classiques sont cependant limités pour la compréhension d’un phénomène aussi complexe, sous l’influence de différentes interactions corticales, spinales et musculaires. Ainsi, nous développons actuellement une méthodologie afin de faciliter l’utilisation de différents indicateurs de complexité neuromusculaire, issus de la dynamique non-linéaire, en condition de fatigabilité musculaire. L’utilisation d’indicateurs adaptés à l’étude de la complexité du système neuromusculaire permettra une meilleure compréhension des limitations psychophysiologiques éprouvées par les individus MRC dans des contextes cognitivo-moteurs exigeants, pouvant favoriser à terme le développement de nouvelles modalités de prise en charge pour ces populations spécifiques."
"Contexte: Le Functional Reach Test (FRT) est une méthode d’évaluation clinique du risque de chute de la personne âgée ou handicapée. Cependant le FRT est un test complexe mettant en jeu les articulations des membres inférieurs et du tronc, et de précédentes études ont montré que différentes stratégies pouvaient être utilisées pour réaliser le test. Objectifs: Décrire les stratégies de réalisation du FRT utilisées par des personnes adultes et saines et évaluer l’influence de l’âge dans le choix de la stratégie. Méthode: Il s’agissait d’une étude pilote. Il était demandé à 29 personnes, non chuteuses (18 de moins de 50 ans, 11 de plus de 75 ans), de réaliser le FRT sur une plateforme de force dans un laboratoire d’analyse du mouvement. Dix-huit marqueurs réfléchissants étaient placés sur le corps. Les principales mesures prises en compte étaient le score au FRT, le déplacement du centre de pression (CdP) et les données cinétiques et cinématiques enregistrées au cours du test. Les deux groupes d’âge étaient comparés en utilisant le test non paramétrique de Mann-Whitney. Une analyse par cluster sur la population totale a permis de grouper les sujets en fonction de similarités fonctionnelles. Résultat: Les sujets les plus vieux produisaient un plus petit déplacement antéro-postérieur du CdP (p < 0,01), un plus grand déplacement vers l’arrière du bassin (p < 0,05) et moins de rotation de tronc (p = 0,024) durant le FRT que les sujets plus jeunes. L’analyse par cluster a permis de séparer la population en deux groupes qui différaient en âge, en score de FRT, en déplacement du bassin, en déplacement du CdP. Conclusion: Nos résultats suggèrent qu’au moment de la flexion du tronc, les sujets plus âgés utilisent une translation du bassin afin de limiter le déplacement du CdP pour prévenir un déséquilibre vers l’avant."
"Ce mémoire d’Habilitation à Diriger des Recherches comprend essentiellement une partie de présentation de mes activités de recherche agrémentée par trois projets à des stades différents d’avancement. Il comprend également une présentation de mes activités d’enseignement et de mes activités administratives. En matière de recherche, à la suite d’une formation en Activités Physiques Adaptées, j’ai poursuivi ma formation de deuxième cycle par une thèse également orientée vers le handicap. Mon activité de recherche (initialement sur l’analyse du mouvement humain pathologique et plus particulièrement de la locomotion) s’est progressivement orientée vers la recherche de signes infracliniques et de mouvements infratraumatiques susceptibles de contribuer au dépistage ou à la prévention de pathologies, dans le contexte d’un contrôle partagé complexe de la motricité. Cette problématique nécessite une approche multidisciplinaire et des expérimentations de longues durées (suivis longitudinaux) qui ne sont pas sans poser des problèmes pratiques et méthodologiques majeurs. Cependant, l’orientation donnée à mes travaux permettent à la fois de conserver le côté applicatif (prédiction/prévention) et de contribuer de façon progressivement croissante aux connaissances sur l’organisation de la motricité et son contrôle (et plus particulièrement de la locomotion humaine). Les principaux résultats en ce sens sont par exemple : - la mise en évidence de différentes stratégies de locomotion chez la personne âgée que nous essayons de rapprocher du stade initial de certaines pathologies, en parallèle d’un suivi de ces personnes âgées, - la possibilité de différencier des sujets ‘futurs’ chuteurs, des sujets à risques importants, des sujets non chuteurs, a partir de mesures réalisées dans les deux ans qui précèdent les premières chutes. De même, de différencier des sujets chuteurs de sujets non chuteurs sur la base unique de leurs données biomécaniques de posture et/ou de marche. - ou encore la mise en évidence de stratégies de marche différentes chez la femme ayant eu des enfants comparées à celles n’en n’ayant pas eu, dans une optique de prévention des troubles musculosquelettiques. Les projets 2 et 3 présentés dans ce manuscrit s’appuient largement sur ces travaux mais essayent également d’aller plus loin dans notre stratégie d’investigation. Ainsi par exemple, nous essayons de plus en plus de combiner (voir projets 1 à 3) ou de relier les informations quantitatives issues de l’analyse du mouvement, à celles plus qualitatives de la clinique, dans l’optique d’augmenter leurs pouvoirs discriminants. En matière d’enseignement, recruté comme MCF en 2000 à la FSMS de l’Université de Valenciennes, j’ai initié mes enseignements supérieurs en 1996 à l’UFR STAPS de Lille. Après avoir atteint mon objectif, qui était d’obtenir l’habilitation de la licence et du master APA pour la FSMS de l’UVHC, j’ai assuré la direction de ces deux formations pendant plusieurs années. Actuellement, je souhaite davantage diversifier l’offre d’enseignement dans le domaine des Activités Physiques Adaptées, notamment en mettant en place de nouveaux contenus spécifiques à visées prophylactiques pour le milieu de l’entreprise. Des prises de contacts avec des entreprises locales comme Toyota, Renault et Sevelnord montrent un intérêt certain de ces dernières et ouvrent également la voie à de possibles collaborations de recherches appliquées sur le domaine et à de nouveaux débouchés professionnels ciblés pour nos étudiants APA… Cette orientation contribue également à rapprocher le domaine de mes activités d’enseignement et de recherche. En matière administrative, depuis le DEA j’ai toujours pris part à la vie de l’université en m’investissant comme représentant étudiant puis enseignant dans certaines commissions universitaires au niveau du laboratoire, puis de la composante et enfin de l’université. En près de 10 ans, je me suis ainsi investi dans des fonctions de représentation collégiale au niveau de l’université, de mon laboratoire, de ma composante d’enseignement et de la formation en APA. De même, je me suis impliqué dans différentes actions d’animation de la vie du campus. Ainsi par exemple, mon investissement au niveau du VUC me paraît nécessaire du fait de ma conviction du rôle que je crois important d’un club sportif universitaire pour toute la communauté universitaire."
"Le but de cette revue de littérature narrative est d’identifier à travers les logiques internes de deux disciplines collectives paralympiques en fauteuil roulant manuel (FRM), l’apport et les limites des principaux dispositifs accessibles aux parasportifs en FRM. Dans le cadre de l’optimisation des performances, les parasportifs sont habituellement testés au laboratoire et/ou sur le terrain. Au laboratoire, les ergomètres à manivelles (EM), les ergomètres à rouleaux pour fauteuil (ERF) et le tapis roulant (TR) sont les plus utilisés. Les EMs ne permettent pas de simuler la gestuelle mécanique de la propulsion du FRM. Les ERFs permettent l’utilisation du FRM personnel mais, neutralisent les forces de résistance des roulettes du FRM. Le TR est plus réaliste mais neutralise les mouvements latéraux du FRM. La technologie embarquée est une évolution des outils de laboratoire. Ainsi, les roues instrumentées (RI) et des centrales inertielles (CI) sont adaptées pour les mesures en situations de terrain. Cependant, la masse des RI limite le comportement du FRM et les CI ne quantifient pas les forces développées sur les mains courantes. La simulation des forces exercées sur les mains courantes à partir des données des CI permettra en perspective le développement de capteurs de force miniaturisés."
"De par ses propriétés intrinsèques, l'eau est un milieu privilégié en rééducation. Cependant, pour en tirer pleinement bénéfice, le thérapeute doit connaître l'ensemble des paramètres concernant l'hydrothérapie, que ce soit la physique des fluides, l'aménagement des bassins, les effets physiologiques de l'immersion ou encore les techniques et charges de travail recommandées pour chaque déficience. La conception et l'aménagement des locaux et bassins doivent également faire l'objet d'une grande attention dans le cadre d'une politique de prévention, de gestion des risques d'accident et de surveillance physico-bactério-chimique de l'eau. Les activités aquatiques à visée thérapeutique (AAVT) conduisent à des bénéfices dans le cadre de la prise en charge d'un nombre toujours plus important de pathologies, notamment en rhumatologie, gériatrie, orthopédie, cardiologie, etc. Plus largement, elles sont aussi un moyen privilégié de prévention et d'entretien physique."
"Le niveau de performance atteint par les sportifs paralympiques à Rio 2016 montre toute l’étendue de l’évolution des méthodes d’entraînement et de l’optimisation du matériel utilisé. Pour réaliser la meilleure performance, les solutions adoptées répondent à des choix de réglages du fauteuil en tenant compte des caractéristiques individuelles et des capacités fonctionnelles résiduelles en fonction de la classe de handicap. L’objet de recherche de cette habilitation à diriger des recherches porte sur la technique de propulsion et les aspects ergonomiques des réglages du fauteuil roulant (en adéquation avec son utilisateur) afin d’améliorer la performance et de réduire les facteurs de risques de blessure liés à ce mode de propulsion. Nous nous appuierons sur un modèle conceptuel illustrant les principaux facteurs influençant l’ergonomie de la performance en Fauteuil Roulant de sport. Ce modèle conceptuel montre la nécessité d'une approche interdisciplinaire en mettant en évidence les facteurs biomécaniques et physiologiques associés à l’interaction FRM-utilisateur qui influent en définitive sur l'ergonomie des performances sportives en fauteuil roulant."
"Le but de cette revue de littérature narrative est d’identifier à travers les logiques internes de deux disciplines collectives paralympiques en fauteuil roulant manuel (FRM), l’apport et les limites des principaux dispositifs accessibles aux parasportifs en FRM. Dans le cadre de l’optimisation des performances, les parasportifs sont habituellement testés au laboratoire et/ou sur le terrain. Au laboratoire, les ergomètres à manivelles (EM), les ergomètres à rouleaux pour fauteuil (ERF) et le tapis roulant (TR) sont les plus utilisés. Les EMs ne permettent pas de simuler la gestuelle mécanique de la propulsion du FRM. Les ERFs permettent l’utilisation du FRM personnel mais, neutralisent les forces de résistance des roulettes du FRM. Le TR est plus réaliste mais neutralise les mouvements latéraux du FRM. La technologie embarquée est une évolution des outils de laboratoire. Ainsi, les roues instrumentées (RI) et des centrales inertielles (CI) sont adaptées pour les mesures en situations de terrain. Cependant, la masse des RI limite le comportement du FRM et les CI ne quantifient pas les forces développées sur les mains courantes. La simulation des forces exercées sur les mains courantes à partir des données des CI permettra en perspective le développement de capteurs de force miniaturisés."
"De par ses propriétés intrinsèques, l'eau est un milieu privilégié en rééducation. Cependant, pour en tirer pleinement bénéfice, le thérapeute doit connaître l'ensemble des paramètres concernant l'hydrothérapie, que ce soit la physique des fluides, l'aménagement des bassins, les effets physiologiques de l'immersion ou encore les techniques et charges de travail recommandées pour chaque déficience. La conception et l'aménagement des locaux et bassins doivent également faire l'objet d'une grande attention dans le cadre d'une politique de prévention, de gestion des risques d'accident et de surveillance physico-bactério-chimique de l'eau. Les activités aquatiques à visée thérapeutique (AAVT) conduisent à des bénéfices dans le cadre de la prise en charge d'un nombre toujours plus important de pathologies, notamment en rhumatologie, en gériatrie, en orthopédie ou en cardiologie. Plus largement, elles sont aussi un moyen privilégié de prévention et d'entretien physique."
"Objectifs: Cette étude vise à analyser les différences d’apprentissage existantes entre le mode de propulsion synchrone et asynchrone en fauteuil roulant manuel. Matériels et méthode: Vingt sujets valides novices (19,9 ± 1,3 ans ; 65,4 ± 8,8 kg ; 173,6 ± 6,5 cm) ont pris part à cette étude. Deux sprints, en conditions synchrone et asynchrone, avant et après douze heures de pratique sportive incluant l’apprentissage de ces deux modes de propulsion, ont été réalisés. Le taux d’augmentation de la force totale lors de la saisie de la main courante, la cadence, la force totale et tangentielle, la vitesse et la puissance ont été mesurés. Résultats: Des différences significatives avant et après les douze heures de pratique sont mises en évidence pour la vitesse (p < 0,0001), la puissance (p = 0,006), le taux d’augmentation de la force totale (p = 0,02) et la force tangentielle (p = 0,005). L’Anova réalisée montre également des différences significatives pour la force totale (p = 0,01), la vitesse (p = 0,0002), la puissance (p = 0,002) et la force tangentielle (p = 0,005) entre asynchrone et synchrone. L’analyse statistique ne montre pas d’interaction significative. Les paramètres mesurés tendent à augmenter de manière plus importante pour le mode asynchrone vs synchrone. Conclusion: L’apprentissage des deux modes de propulsion a eu des effets significatifs sur le principal paramètre de performance propulsive, la vitesse. Or, deux des paramètres biomécaniques liés aux risques de blessures, le taux d’augmentation de la force totale et la force totale, augmentent significativement pour les deux modes de propulsion. Il serait alors intéressant de proposer des séances de renforcement musculaire adaptées durant la phase d’apprentissage. De plus, les pourcentages d’évolution tendent à être plus importants en propulsion asynchrone vs synchrone. Ces résultats amènent à penser qu’il faudrait privilégier l’enseignement asynchrone de manière précoce durant l’apprentissage du basket en fauteuil roulant manuel."
"Douze ans après la loi du 11 février 2005, le premier objectif de ce travail est de rechercher s’il existe différents profils d’enseignants d’éducation physique et sportive en fonction de leurs perceptions de l’inclusion d’élèves en situation de handicap. Le second objectif est de définir plus précisément les perceptions de chaque profil d’enseignant, ainsi que de mieux comprendre les facteurs qui les influencent. Pour ce faire, notre travail se présente en deux étapes. Tout d’abord, 101 questionnaires, traités via une analyse de partitionnement des données et une analyse de la variance, qui ont permis de classer les enseignants en trois profils (exclusion fonctionnelle, intégration et inclusion). Ensuite, 12 entretiens (4 enseignants de chaque profil), traités via une analyse thématique, ont permis d’apprécier plus finement leurs perceptions respectives ainsi que de mettre en évidence certains mécanismes qui les sous-tendent."
fr_abstract_s
"Des échanges épistolaires à la diffusion en ligne globalisée, le chercheur, au-delà de communiquer ses résultats a toujours dû mettre en avant sa personne pour continuer ses recherches. Il semble que le chercheur doive désormais veiller davantage à sa marque de fabrique que constitue sa signature, ce qui l'entraîne dans des stratégies d'intelligence personnelle au sens d'intelligence économique ou territoriale. En effet, ce dernier travaille davantage en réseau au point que désormais nous pouvons parler de science réticulaire qui opère en liaison. Faut-il pour autant parler de « chercheur 2.0 » ?"
"A l'instar de l'entreprise, le territoire sera dans un proche avenir plus orienté dans un rapport de forces concurrentielles où la demande de l'habitant s'inscrira de plus en plus dans la recherche de sécurité. La notion de risque et son cortège de signes souligne De Saussure, (in Starobinski, 1971) focalisent le plus souvent ce type de message «à une sémiologie de l'alarme, de la rupture ou du danger ». C'est pourquoi, la capacité du territoire à anticiper sur les menaces qui vont l'atteindre, nécessite la mise en œuvre d'une évolution de sa culture. Celle-ci s'inscrit dans le paradigme de ""la complexité de la connaissance"" illustré dans les tomes successifs de la La Méthode d'Edgar Morin(2000) et son évolution s'appuie notamment sur les logiques d'intelligence territoriale."
"Les dispositifs numériques institutionnels déployés dans des formations présentielles ou à distance (plates-formes, campus virtuels, environnements virtuels de travail, etc.) sont aujourd’hui confrontés à la concurrence de dispositifs tels que les réseaux sociaux (Facebook, Twitter, etc.) ou de dispositifs collaboratifs ouverts, Google Docs par exemple. L’étude longitudinale portant sur des déclarations d’étudiants toulonnais de l’UFR Ingémédia montre un abandon de plus en plus fréquent des dispositifs institutionnels. Cet article analyse les modalités d’adoption, de cohabitation et de régulation des usages entre les sphères académiques et personnelles."
"La pensée, nous dit Morin, est un négoce entre certitude et incertitude et cette dernière est une préoccupation de plus en plus constante au sein du territoire. Celui-ci cherche en réponse dans un traitement inédit de l'information, une posture proactive d'anticipation des risques et ruptures. Or, le caractère habituel formel et explicite de l'information publique condamne celle-ci à constater le plus souvent l'événement au lieu de l'anticiper. Il faut alors avec Latour (1999) donner au risque une chevelure, c'est-à-dire l'éclairer dans le champ de l'information par une approche beaucoup plus sémiologique. De Saussure, Barthes ou Eco nous ont donné quel-ques clés de lecture du signe. Sur l'acquis empirique d'une expérimentation régionale, nous iden-tifions l'existence d'un construit sémiologique mutualisé propre au concept d'intelligence territo-riale."
"Cet article souligne les potentialités didactiques du micro-blogging, en tant que dispositif de médiation, pour accompagner le processus d'apprentissage à distance. Notre approche empirique est fondée sur une observation participante menée dans le cadre tutoral auprès d'étudiants de niveau II. L'apprenant est au coeur d'un dispositif pédagogique qui lui permet d'interagir avec ses pairs et avec le tuteur, pendant le cours ou bien en dehors du cadre académique, considérant l'apprentissage informel comme partie intégrante du processus de formation. A la lumière des théories socio-constructivistes, nous postulons que le partage et la diffusion informationnelle par le biais d'un dispositif de micro-blogging, créent de nouvelles modalités collaboratives et développent une culture de la participation au sein d'une communauté d'apprenants. Ces pratiques participent de la mise en oeuvre de processus d'apprentissage pertinents à travers les interactions sociales et la médiation des tuteurs dont nous exposons ici les modalités. Ce travail de recherche présente la phase exploratoire d'analyses qualitatives et quantitatives basées sur l'exploration des pratiques de micro-blogging en formation distante."
"Dans un contexte économique mondialisé, les prises de position d’achat et/ou de vente sur les marchés financiers obéissent à des logiques qui échappent parfois à la rationalité (bulle spéculative…). Les prévisionnistes et les analystes financiers mobilisent une boite à outil statistique pour connaître les tendances futures à partir de l’étude des tendances passées. Cette boite à outils repose sur l’hypothèse de normalité des lois statistiques sous jacentes ce qui autorise des logiques d’inférence statistique, de test, de corrélation... On a pu observer par le passé que les résultats de ces projections ont souvent été miss à défaut : la crise financière que nous traversons correspond par exemple à un choc difficilement prévisible même s’il fait l’objet d’une rationalisation a posteriori. Notre objectif, partant de ce constat, est de renouveler les approches traditionnelles des prévisionnistes et analystes financiers en mobilisant deux approches complémentaires : l’intelligence économique appliquée au domaine financier et l’utilisation de techniques modernes de gestion de l’imprévisible. Dans ce travail interdisciplinaire, notre approche s’inspire tout d’abord du concept d’image, de réputation d'une entreprise cible et de la démarche du cycle de renseignement issue de l’approche de l’intelligence économique. De plus, nous pouvons compléter notre démarche à travers les travaux de Nassim Nicolas Taleb. Nous mobilisons enfin le concept de force de situation (François Julien) pour renforcer la décision des investisseurs institutionnels en situation d’incertitude. Pour valider notre contribution théorique, nous avons choisi le Vietnam comme terrain de recherche. A partir d’une approche qualitative conduite auprès de gérants de portefeuilles Vietnamiens, nous avons pu connaître mieux leurs pratiques de prises de décisions, les critères d’évaluation d’investissement différents issus des analyses de matrices stratégiques, leur perception de la réputation et le rôle de l’intelligence financière dans leur processus d’investissement. Nous proposons alors une méthode qualitative reposant sur la réputation pour caractériser le degré de robustesse d’une organisation à des chocs et élaborons en outre un système de renseignement financier en prenant en compte la hiérarchie des critères d’évaluation d’investissement des gérants de portefeuilles Vietnamiens. Notre démarche est illustrée par l’étude de cas d'une entreprise aquacole Vietnamienne."
"Dans une recherche-action menée au sein de La Poste, nous problématisons le rôle d’une « communication organisante » dans la prévention des risques psychosociaux liés à un changement : « l’acculturation au numérique ». Dans cette thèse, nous revenons sur les différentes transformations opérées au sein de La Poste depuis sa création, et nous nous intéressons au rôle de la communication dans l’accompagnement de ces dernières, plus particulièrement avec le « Grand Dialogue » se présentant comme « une démarche inédite en faveur du bien-être au travail ». A l’issue de nos recherches, il apparaît qu’une majorité des collaborateurs n’est pas en difficulté dans ses usages du numérique, compte tenu des actions d’accompagnement à l’acculturation mises en place par la direction de la communication, grâce à ses différentes innovations. La communication des organisations se révèle donc être un facteur de protection des RPS de cette « acculturation au numérique », et vient par là même questionner son rôle en tant que contributrice de la citoyenneté. En effet, cet engouement des postiers pour les technologies numériques semble révéler que leur usage devient une nécessité pour tous et trouve une utilité au-delà de l’entreprise, dans leur vie quotidienne. Par ailleurs, la recherche-action vient mettre en lumière les enjeux mais aussi les limites de la prévention des RPS au sein de l’organisation, pour l’ensemble des collaborateurs, y compris pour les assistants sociaux. Ainsi, découvrons-nous comment ces collaborateurs atypiques mettent en place des stratégies d’usage des technologies numériques pour palier à une organisation et à des pratiques communicationnelles jusque-là frustrantes."
"Cette thèse interroge ce en quoi le musée constitue une clef de développement de la fabrique & de la communication de l'histoire. Nous observons que l'historien-ne se confronte à des dilemmes communicationnels: retranscrire l'insaisissable passé au monde présent ; résoudre le paradoxe historiographique de la mise en relation du réel & du discours. Notre proposition consiste à transposer au musée ces problématiques communicationnelles historiennes. Voici le troisième dilemme : faire de l'histoire renvoie à l’écriture tandis que dans la société postmoderne où nous vivons, les images ont de plus en plus remplacé les textes écrits comme forme culturelle dominante.Le musée pose la question de la représentation du passé. Nous présentons un travail de modélisation muséale de l'opération historiographique. Les mouvements de production & de communication de l'histoire au musée nous orientent vers les principes de l'histoire publique & vers des questions esthétiques. Nous explorons l'hypothèse conceptuelle d'une esthétique de l'histoire, dispositif sensible, social & agissant dont le musée d'histoire participe. Après des démarches exploratoires d'observation participante, nous entreprenions une enquête qualitative. Suite à cette campagne de témoignages, nous avons développé sous LaTeX un programme de moissonnage thématique des données. Les résultats abordent : 1) les dispositifs, fonctionnements & vies des musées visités ; 2) l'histoire des musées d'histoire ; 3) une stylistique de l'histoire ; 4) l'idée d'esthétique de l'histoire. La principale originalité de cette thèse vient des jalons qu'elle pose à une réflexion sur le concept nouveau d'esthétique de l'histoire."
"Il s'agira dans cet article d'éclairer un champ créatif émergeant : les « audio-­‐games » ou « vidéo-­‐ludiques sonores », se situant au carrefour du jeu vidéo et de l'informatique musicale. Aujourd'hui, une multitude de petites applications, qui proposent des expériences audio-­‐visuelles ludiques où la dimension sonore est prépondérante, sont disponibles sur des consoles de jeux, des ordinateurs, des téléphones portables. Ces expériences représentent un nouvel espace où la notion de « jouabilité » venue du jeu vidéo et appliquée à la création musicale favorise de nouveaux ponts entre les deux disciplines. Ce champ créatif, en proposant la manipulation de représentations que nous appelons « a-­‐musicologiques » 1 (c'est-­‐à-­‐dire utilisant des symboles ne faisant pas référence à la musicologie classique pour manipuler et produire du sonore) interrogent de manière nouvelle la représentation du sonore et des structures musicales et produit ainsi de nouveaux gestes instrumentaux et compositionnels qu'il s'agit d'étudier. D'autre part, ces objets s'inscrivent dans une montée en puissance d'une nouvelle figure de l'amateur, notion travaillée, concernant la photographie, par des auteurs comme Vilèm Flusser (Flusser, 1 996). Après avoir défini les caractéristiques et les limites de ce champ et repéré quelques-­‐unes de ses filiations historiques (cinéma abstrait, théorie du jeu en musique, partition graphique et d'actions…), nous étudierons quelques exemples de jeux sonores et proposerons des pistes de recherche pour l'élaboration d'outils d'analyse de ces nouveaux objets."
"Avec l'avènement de la « société de l'information », les entreprises se sont essayées, dès les années 90, à réduire à de l'information les nombreux savoirs qui étaient à la source de leurs richesses. Cela conduisait à la prolifération de bases de connaissance pour la plupart aujourd'hui inexploitables. Depuis, les organisations se sont aperçues que la connaissance était quelque chose de vivant, d'évolution complexe et néguentropique ; qu'il convenait de ne pas considérer uniquement la partie formulée et encodable d'un acquis. La dichotomie existante entre savoir reconnu et savoir tacite, étu-diée par Polanyi déjà en 1969, est amplifiée dans les nouveaux usages des TICE. Nonaka (2006) parle de savoir explicite et tacite, d'autres de savoir social et d'intellectualité diffuse. Sur la base de la mise en œuvre d'une communauté d'échange au sein d'un groupe d'étudiants, nous repérerons les limites des principes avancés par Wenger en faveur des transmissions de savoirs tacites vers des savoirs ex-plicites. Nous en proposerons une mise en application au sein d'une communauté d'échange électroni-que et en rapporterons quelques éléments d'une démarche complémentaire."
"Le sourire versus KRISIS, source de composition d'un nouveau mode de communication non verbal interculturel Il est établi que nous traversons une crise. Dans une dynamique d'adaptation, de nombreuses organisations plurielles luttent donc afin de communiquer- d'établir au mieux une relation avec autrui- pour émettre des messages porteurs de positivisme (Comte) dans l'intention de contrer la situation subie. Notre hypothèse se fonde sur une observation qui témoigne d'une technique de restructuration des messages par les professionnels de la communication. En effet, pour lutter contre les mouvances actuelles, les entreprises publiques et/ou privées mettent beaucoup plus en avant le sourire dans tous leurs messages de communication externes comme pour rassurer et/ou pour inciter à la résilience. Faisant leurs,"" Le bonheur est dans la possession des êtres ou des choses que l'on aime. On doit vivre pour réaliser le bonheur de ces êtres"" (Ernest Pallascio-Morin), les organisations de toutes cultures adressent à la population des sourires sans compter pour mieux les guider. Qu'ils soient consommateurs ou pas, les spectateurs enregistrent donc inconsciemment ces nouveaux messages qui vont de textes simples "" Souriez... ""aux "" smiley "", en passant par les logos souriants et/ou subliminaux (Amazon) et même par des oeuvres d'Art (Monument au sourire, WinterAgnès, New York, 2008). Ce phénomène communicationnel résulte d'une crise. Des symboles de plaisir et/ou de satisfaction s'affichent de plus en plus fréquemment sur de multiples supports car "" un sourire ne coûte rien et produit beaucoup"". Mais ce mouvement de résilience, axé sur une nouvelle forme de communication, mise en place sur de nombreux territoires ne comporterait-il pas un risque de modification de l'environnement culturel et sociétal?"
"Cet article issu d'une recherche menée pendant l'année 2002-2003, vise à une réflexion critique concernant l'histoire, l'organisation et la communication de la technopole de Sophia-Antipolis, au regard du phénomène complexe dit de « glocalisation ». Nous montrons comment, à ses origines la technopole qui devait concourir à la fertilisation croisée, fonctionne comme une utopie, et comment elle aboutit finalement à n'être qu'un non-lieu privilégié de la mondialisation réservée aux élites vivant dans des clos sécurisés paradisiaques en marge du monde. La comparaison entre ce que la technopole était supposée être, au regard de ce qu'elle est réellement, permet de comprendre sa réussite, mais également les limites définies par son mode de construction."
"Nous présentons ici l’effort produit par un ensemble de collectivités et acteurs du Nord-Pas de Calais pour faire face à la question de la transition énergétique, dans le cadre particulier d’une mission confiée à J. Rifkin. Nous montrons la manière dont cette région, relance à cette occasion de nouvelles réflexions et engage un nouveau mouvement. Les processus de travail et de mobilisation des acteurs de la région sont décrits de manière synthétique. Cette mobilisation et la fabrique du consentement qui l’accompagne s’appuient autour du projet de la « Troisième Révolution Industrielle ». À partir des commentaires formulés par les groupes de travail et les collectivités territoriales sur le Master Plan, on voit comment ceux-ci débattent, ajustent, enrichissent et en modifient les orientations. La feuille de route finale se veut être une proposition très ambitieuse de transformation d’un territoire à ses différents niveaux d’échelle."
"Ce travail est un trait d'union entre les sciences de l'information et de la communication. Une robuste méthodologie et des outils performants d'analyses bibliométriques sont utilisés pour des études scientométriques et médiamétriques. Pour cela, nous avons étudié la production scientifique d'une organisation publique de recherche et développement, l'Entreprise Brésilienne de Recherche Agronomique (Embrapa), les compétences de ses chercheurs et enfin nous avons évalué la performance de cette organisation et ses 40 centres de recherche dans les médias. Les résultats indiquent que la fonction d'analyse d'informations internes et externes aux organisations est aujourd'hui un instrument important et stratégique pour améliorer les décisions et les politiques organisationnelles et contribuer au développement de la société. L'étude des compétences de R&D démontre le potentiel d'intelligence qui peut être générée avec des informations déjà présentes dans les organisations, mais dispersées en plusieurs départements administratifs. Il est le témoin de la nécessité d'organiser des environnements informatiques spécifiques pour la fonction d'analyse dans les organisations, intégrant bases de données et informations d'origines diverses. Avec les méthodologies de médiamétrie les fonctions d'audit du travail de communication organisationnelle, d'évaluation du résultat de campagnes et de la production de dépêches pour la presse gagnent en efficience et l'élaboration de suggestions de reportages peut mieux correspondre aux préférences des éditeurs. Ceci ne serait pas possible sans l'acquisition de l'intelligence sur les médias pour déterminer les tendances, les préférences des divers véhicules et le comportement de leurs éditeurs. La fonction de clipping, d'accompagner ce qui sort dans la presse est présente dans les grandes organisations. L'apport de ce travail consiste à intégrer la fonction analyse d'une base Clipping. A partir de tels corpus et munis des méthodologies et outils décrits plus loin, il est possible de réaliser des études prospectives sur l'évolution historique d'un thème, d'un produit, d'un politicien ou d'une entreprise dans les médias et de faciliter des activités de veille de l'environnement, des concurrents ou de tout un secteur de l'économie."
"A l'instar des expérimentations européennes, ça et là en France, se mettent en place depuis 2003, des projets d'intelligence territoriale (cf régions de la Basse Normandie, de la Lorraine, Ile de la Réunion, région d'Aquitaine etc.) ; ceux-ci ont pour ambition de mettre à contribution dans une logique de développement durable, un traitement mutualisé de l'information qui dépasseront les huis clos sectoriels. Outre les institutions, la société civile, et les habitants du territoire, on constate que les entreprises et notamment les Pme/Pmi, sont des partenaires naturellement intéressés par cette démarche. Les processus de filière économique comme les organisations participantes gagnent ainsi quelques gradients d'anticipation des menaces et réaffirment le territoire comme un lieu ressource commun à défendre. Au-delà des systèmes de traitement de l'information fonctionnant parfois au sein de ces organisations ou filières, l'articulation des actions internes de capitalisation informative aux actions locales d'intelligence territoriale, permettent d'acquérir un effet levier d'une visibilité européenne sinon mondiale (Herbaux, 2007). Néanmoins, ces expériences connaissent des fortunes diverses dont l'abandon progressif du projet par les entreprises initiées est l'un des effets le plus couramment observé. En appui d'un apport théorique en trame de cette communication, nous rapportons les résultats d'une enquête de type Delphi réalisée en 2005 sur 53 entreprises du Nord-Pas de Calais engagées dans un processus d'intelligence territoriale depuis 2003 ; celle-ci constatait que 43 entreprises sur les 53 engagées n'avaient pas poursuivi leur projet interne de mutualisation des flux d'information et se contentaient par défaut, des résultats sectoriels d'une veille régionale publique. Au-delà d'une apparente démission d'un processus engagé, nous pouvions nous interroger sur cette apparente discrétion d'un volet de partenaires associés dans la gouvernance locale. Ce travail aboutissait néanmoins à un consensus des acteurs interrogés autour de quelques constats ; notamment dans la démarche dont ils ne reniaient pas la finalité mais dont les exigences obligeaient à une modification sensible de leur culture interne. Après les premières réponses convenues : « sécurisation du patrimoine informationnel, nouveaux choix en -investissement temps-, manque de moyens, priorités différentes etc. », un questionnement récursif et différencié chez les intéressés, établissait que l'abandon progressif des pratiques et engagements étaient en relation avec un ensemble de facteurs liés à l'humain dans ses aspects relationnels et cognitifs, privant ainsi le projet de ses ancrages fondateurs. Ce constat intervenait en écho d'une implication proposée par Girardot en 2004 sur le thème de la « gouvernance multi-niveaux ». Bien que l'aspect financier soit un des facteurs de pérennité d'un investissement régulier en temps-homme, ce critère apparaissait progressivement pour les acteurs enquêtés, comme accessoire du projet général au profit de plusieurs postures posées en pré-requis. Sur la base d'une synthèse réalisée sur les résultats de l'étude, nous proposons ainsi cinq facteurs clés de succès à promouvoir au sein des organisations pour le développement des logiques de mutualisation d'information. Pour ce faire, notre proposition de modèle appelé « CADIE » (Communication, Appui, Durée, Implication, Ecoute) propose quelques postures auxquelles les organisations doivent s'attacher pour un développement pérenne de leur intégration dans une démarche d'intelligence territoriale. Les limites de notre proposition souffrent de la faiblesse de l'échantillon à notre disposition et de la lisière régionale de notre recueil de données. Cette expérimentation dupliquée dans différents territoires de l'Europe, bénéficierait d'une onction multiculturelle et offrirait alors un modèle européen d'approche préliminaire des logiques d'intelligence territoriale."
"L'argument de l'accès à l'information scientifique et technique des années soixante – soixante-dix est reformulé de façon similaire pour les banques de données génomiques, puis de manière générale pour le Libre Accès, avec toutefois un changement de paradigme : de l'accès à la signalisation du document (la référence bibliographique) à l'accès au document lui-même, et plus profondément du document (objet) à son contenu (information)."
"Le libre accès à l'information et aux produits immatériels conçus par l'imagination a toujours été reconnu comme le fondement d'une société qui inclut, une société fondée sur des valeurs communes et ouvertes au changement. Les changements fondamentaux intervenus dan s l'utilisation de l'information ont un impact sur tous les aspects de la vie économique, sociale et éducative. La diffusion de l'information et les changements intervenus ont favorisé l'émergence d'une économie du savoir. Ce résultat global que les tic permettent et la demande de savoir qui en résulte est né de l'introduction rapide des technologies de l'information et de la communication. L'apprentissage et la demande de savoir deviennent les facteurs clé du succès individuel, organisationnel et national. La revue Isdm se veut un support en ligne ouvert de communication entre les spécialistes de divers domaines dans leur relation à l'information et à la décision."
"La question de l'identité est prééminente dans notre relation aux autres et cela est d'autant plus vrai que la mondialisation brouille les repères et met potentiellement tout individu de la planète en contact avec tous les autres. Cette question s'inscrit dans le mouvement de l'intelligence territoriale. L'article a pour but de montrer comment le sentiment d'identité méditerranéenne peut être explicité par les regards multiples que prône l'intelligence territoriale. L'introduction rappellera le mouvement de l'intelligence territoriale (It), son origine, ses principaux concepts et les développements actuels. La problématique envisagera Notre Méditerranée (Mare Nostrum) sous l'angle de l'intelligence territoriale; le défi de l'Union pour la Méditerranée servira de base à la démonstration de la dialectique du ""top down"" au ""bottom up"". Une étude de cas fournira un exemple de la complémentarité des chiffres et des sentiments. En conclusion on proposera l'intelligence au service d'un territoire: futur, passé, présent.... montrant l'importance de la démocratie participative et le rôle de l'information."
"Le discours francophone en sciences de l'information - communication se réfugie souvent dans la glose sur la complexité, le constructivisme ou la sémiotique. Un des intérêts des travaux récents est précisément d'être parti de la complexité comme un a priori catégorique et d'en avoir exploré les modalités de ses manifestations dans le champ territorial. Par exemple, le cas du "" projet d'extension portuaire de Bastia "" traité par Julien Angelini, conduit à la constatation que le "" débat public est l'émergence de l'intelligence territoriale. "" L'expression "" intelligence territoriale "", toute approximative et polysémique qu'elle puisse paraître révèle ainsi sa puissance. C'est l'association de l'intelligence, sous toutes ses formes, avec l'appartenance à un territoire, lui aussi sous toutes ses formes. La mise en pratique d'une approche en intelligence territoriale peut être caractérisée par "" une étude compréhensive, qualitative, factuelle, présentant des caractéristiques de l'observation participante et fondée essentiellement sur une collecte de traces tangibles de la production d'informations "" comme dans le projet d'extension portuaire de Bastia. Une autre étude sur la participation des citoyens bordelais aux micro-décisions d'implantation du tramway est encore une illustration de la mise en œuvre de la démocratie participative comme outil de communication dans un projet de développement territorial. Le présent papier se propose de confronter les apports conceptuels des sciences de l'information - communication avec des études de terrain pour situer les enjeux de l'intelligence territoriale et les questionnements qu'elle soulève au carrefour de la géographie, de l'anthropologie, de l'histoire, de l'économie, de la sociologie et de la politique. Dans tous ces champs du savoir on retrouve les problématiques classiques des sciences de l'information - communication : comment créer la relation, donc la confiance, faciliter l'accès aux données, faire circuler l'information, communiquer et interagir."
"Favoriser l'apprentissage social dans les communautés virtuelles est un défi plus complexe qu'il n'y parait. Tout d'abord, il concerne l'identification des processus de socialisation et d'apprentissage mis en œuvre dans les échanges, mais aussi les conditions qui favorisent ou bloquent ces processus. L'objet de cet article est d'aborder ces deux dimensions en soulignant d'un point de vue méthodologique les difficultés pour l'enseignant qui souhaite initier une communauté d'apprenants sur un thème ou une matière. Une étude ethnologique sur les processus de socialisation observés dans une communauté d'apprenants en ligne et à distance est présentée. Cette étude met en lumière la place des échanges socio- affectifs (politesse, remerciements, soutien) dans la construction identitaire et le rôle primordial des échanges socio-cognitifs (débats, entraide sur le contenu du cours) dans le contexte éducatif. Le passage du conflit vers l'entraide est notamment discuté pour fpratique."
"Aujourd'hui, l'Homme voit se transformer ses méthodes de travail grâce ou à cause, du développement ininterrompu des Technologies de l'Information et de la Communication. Ces dernières permettent ainsi, à l'ensemble des acteurs, travaillant dans des organisations éclatées, de transférer et échanger des données, de communiquer des informations et des savoirs, sans limite de temps ni d'espace. Dans les entreprises, les responsables ainsi que leurs collaborateurs, se voient ainsi entraînés à s'adapter à ces évolutions technologiques pour continuer à remplir leurs missions, atteindre leurs objectifs et créer de la valeur. Dans le même temps, le manager, à la fois sujet et objet de notre problématique, doit accompagner ces changements et s'adapter aux nouveaux modes de fonctionnement de ses collaborateurs, initiés par les besoins et contraintes de la communication à distance par l'intermédiaire des Technologies de l'Information et de la Communication. Cet état des lieux des entreprises communicantes, ouvre de nombreux champs de recherches autour des usages, des logiques d'appropriation, des représentations sociales, des processus de communication, de coactions, de coopération, de collaboration et d'intelligence collective, des méthodes et outils de direction et de motivation des femmes et des hommes à distance. Nous avons, depuis plus de dix années, observé ces transformations, en nous concentrant sur l'impact des Technologies de l'Information et de la Communication sur les comportements humains en entreprise. Les demandes des entreprises promotrices et confrontées aux conséquences de la distanciation dans les projets et les tâches au quotidien, les évolutions des comportements des nouvelles générations arrivant sur le marché du travail, les problématiques posées par les managers opérationnels ont été pour nous les déclencheurs de nos recherches sur le besoin de comprendre et d'accompagner tous les aspects de ces transformations comportementales et communicationnelles."
"Région, régionalisation, régionalisme sont des formes lexicales variées dont la floraison témoigne de l'importance que prend actuellement le territoire dans l'espace public européen. Le développement durable est un concept émergent depuis que la mondialisation a fait percevoir aux responsables politiques comme aux citoyens le danger et l'inanité d'une croissance débridée des activités humaines sur la planète Terre. L'intelligence territoriale est une notion introduite par les penseurs de toutes les disciplines des sciences humaines pour donner une cohérence conceptuelle aux tentatives de comprendre la complexité et encourager le développement territorial. L'objet de cette communication est de fournir aux participants à la conférence européenne de Caenti à Alba Lulia (Roumanie, 20-23 sept 2006) des fondements théoriques et des axes de recherche mettant en relation les concepts de région et de développement durable avec celui d'intelligence territoriale. Dans un monde que les technologies de l'information communication font rétrécir tout en faisant voir à la planète entière les désastres de l'impérialisme d'état et de la concurrence effrénée, le développement des régions en Europe est une chance pour voir émerger une autre gouvernance publique. Les régions considérées comme des foyers culturels et économiques rayonnants peuvent établir leur identité et régler leurs relations sur des logiques autres que les antagonismes à somme nulle, constitutifs de la pensée politique héritée des siècles précédents. Mais les régions sont-elles le meilleur échelon de cette mise en œuvre du développement durable à l'Européenne ? L'histoire et la raison nous conduisent à répondre : Oui, mais en coopération avec les autres échelons comme les départements, les pays, les villes."
La vision statique du territoire a vécu et le brouillage des territoires et des références spatiales qui en résulte pose problème à tout acteur local dans la mesure où il fonctionnait sur la délimitation territoriale de ses compétences. L'espace invisible prend un poids croissant et la dynamique territoriale contemporaine suppose une communication double : bottom up et signal down. La complexité qui résulte de l'intrication de l'espace physique et virtuel nécessite la captation puis l'utilisation d'une quantité plus grande d'information de qualité sur le territoire. Nous présentons dans cet article les caractéristiques de l'intelligence territoriale en tant que théorie et démarche ascendante d'intelligence collective.
"Dans cet article, nous observerons et étudierons les représentations Sophipolitaines sur Internet et nous les confronterons à une lecture du territoire"
Cette communication examine certains aspects des nouvelles écologies qui s'expriment à travers les espaces immersifs numériques
"Cette thèse adopte une approche intégrée de la gestion de l'information, de la gestion des connaissances, de l'intelligence compétitive et organisationnelle, tournée vers l'activité d'éducation corporative. Elle développe un cadre conceptuel et méthodologique en vue d'offrir de la formation dans les micro et petites entreprises (MPE). Son but est le développement de compétences spécifiques pour la gestion des entreprises à travers un nouveau modèle d'éducation corporative à distance. L'objectif est d'offrir de la formation aux MPE en s'appuyant sur la structure des Télécentres d'Information du Ministère du Développement, de l'Industrie et du Commerce Extérieur. Le texte examine plus particulièrement le rôle de l'apprentissage dans la société contemporaine; il prend en compte le contexte des MPE, la proposition d'éducation corporative et les possibilités d'application pour les MPE; il identifie les possibilités technologiques destinées à l'apprentissage; il identifie des modèles de référence pour l'éducation corporative et à distance pour les MPE; il fait des recherches sur la dynamique et les éléments essentiels pour le réseau Télécentres d'Information et d'Affaires; il développe les bases théoriques, méthodologiques et opérationnelles pour offrir de la formation aux MPE au Brésil; il présente un modèle de formation en entreprise faisant usage des Télécentres d'Information et d'Affaires ; et développe un prototype destiné au secteur de l'artisanat d'Amazonie. Sur le plan méthodologique, il présente une nouvelle approche pour augmenter la capacité d'obtention d'information et de connaissance des MPE. Les stratégies de formation font usage de l'appui de mécanismes d'insertion digitale et de communautés de pratiques, en adoptant les concepts d'apprentissage situé et de partenariats. L'application du modèle E‐TIN a eu lieu dans le télécentre du Sindicat de la Micro et Petite Industrie de l'État de Rondônia (Simpi). Elle a adopté comme partenaires l'Association Télécentre d'Informations et d'Affaires (ATN) pour le Learning Management System et l'Université de Brasília comme fournisseur de contenus. Le prototype a adopté la communauté de pratique Gestion de la Connaissance pour Microentreprises et la formation à distance Artisanat Amazonien. La recherche recommande la plate‐forme technologique la plus adaptée à la formation des adultes, en considérant aussi la standardisation existante entre les partenaires et l'évolution du web 2.0. En conclusion, la recherche comfirme l'émergence de nouveaux modèles éducationnels destinés aux adultes, identifie les communautés de pratique comme étant fondamentales dans le processus d'apprentissage, met en évidence l'importance de l'éducation corporative pour des actions de gestion de la connaissance et offre une solution en vue d'une vraie révolution de la connaissance dans les MPE au travers des Télécentres d'Information et d'Affaires. Parmi les possibilités d'approfondissement des investigations rejoignant cette recherche, on distingue celle de la stimulation des secteurs économiques stagnantes au travers de la formation en entreprise."
"La métacognition et plus spécifiquement la méta-mémoire provoquant un système d'auto-évaluation pourraient améliorer l'apprentissage à distance et permettre une évaluation des « métamorphoses cognitives » de l'apprenant, tout en réduisant l'opposition entre évaluation formative (par le contact humain) et évaluation sommative (le présentiel). Nous passerons en revue et proposerons ici des outils métacognitifs d'auto-évaluation qui pourraient être opérationnels et nous essaierons de transférer un modèle de questionnaire permettant d'évaluer la méta-mémoire des apprenants dans la formation à distance."
"Avec Mac Luhan (1977), nous pouvons avancer que la télématique a remplacé «la fusée» dans la figuration visionnaire de l'avenir. Les TIC (technologies de l'information et de la communication) sont au centre de ces temporalités par le caractère inédit de leurs effets médiologiques qui font du local, le lieu fugitif d'une communication et d'une information généralisées. «L'agir instrumental et l'agir communicationnel» d'Habermas trouve ici une illustration dans leur complémentarité. Cette évolution de la culture du local est en rapport avec la capacité d'un territoire à développer ses thèmes d'innovation et devenir ainsi, apprenant."
"Le reportage "" Diagnosing difference "", base d'une analyse de la question de diversité que recouvre les transidentités, du problème du diagnostic, et des réalités sociétales et individuelles qui en découlent."
"Révoltes, crises, rébellions, insurrections, révolutions, quel que soit le nom que l'on donne aux bouleversements qui sont apparus depuis deux ans autour de la Méditerranée, ces mouvements ont donné naissance dans le monde occidental à un espoir de "" printemps arabe "", avec référence plus ou moins implicite au Printemps des peuples de 1848 et au renouveau qu'a connu l'Europe centrale dans la suite des événement de 1989 (la chute du mur de Berlin étant le moment le plus spectaculaire de ce processus). Mais, après le printemps, les opinions publiques du Nord comme du Sud de la Méditerranée voient actuellement un automne de désillusions, sans même être passé par l'été. Comment faire renaître l'optimisme dans ces conditions ? Notre hypothèse est que le vocabulaire qui sous-tend notre interprétation du monde constitue un blocage à la communication entre les différents acteurs du jeu stratégique et de pouvoir et que cette absence de langage commun, ou de référentiel commun, est le ferment de l'incompréhension et du pessimisme qui peut en découler. Nous nous fonderons sur plusieurs faits qui nous paraissent former des verrous sémantiques à tirer pour parvenir à une situation de communication garante de la possibilité d'échapper à la malédiction de chocs dialectiques supposés être les moteurs de l'Histoire dans la lignée de Hegel."
"Les enquêtes sur les pratiques informationnelles en milieu universitaire tendent à reposer sur le modèle de la fracture numérique générationnelle, où le milieu universitaire est impacté de l'extérieur par les usages des digital natives, et à évaluer les pratiques des étudiants selon leur écart avec ce que les médiateurs estiment les ""bonnes pratiques"". Un mode d'évaluation non normatif mais basé sur l'efficience des stratégies de recherche permettrait de rendre mieux compte de leur diversité voire de leur inventivité. Une telle évaluation suppose une nouvelle compréhension de ces stratégies. A travers trois axes de questionnement (connaissance des ressources, appropriation critique des interfaces, réassomption de la subjectivité des pratiques) nous esquissons un nouveau paradigme d'une ""recherche floue"" qui associe la maîtrise des outils selon le critère de la relevance à la sérendipité en tant qu'elle suppose l'aptitude à accueillir l'inattendu"
"L'intelligence territoriale est une approche du développement durable fondée sur deux paradigmes majeurs des sciences sociales: le constructivisme sociotechnique et la théorie systémique comme point d'entrée dans le management de la complexité. Mais cela est souvent implicite et les auteurs ne se réfèrent pas aux sources de ces disciplines. Le but de cette communication est revisiter quelques concepts fondamentaux de la systémique en vue de la construction d'un cadre conceptuel cohérent, explicite et pragmatique, base d'une théorie générale de l'intelligence territoriale en cours d'émergence."
"L'enseignement à distance est désormais très répandu dans l'enseignement supérieur. Malgré l'évolution des nouvelles technologies, un des principaux problèmes liés à ces types d'approches pédagogiques reste la distance des acteurs. Nous faisons l'hypothèse que l'apparition récente de nouveaux dispositifs socio-techniques de type « Mondes 3D Virtuels » offrent par leurs dimensions ludiques et créatives de nouvelles perspectives sur le plan de la médiation sociale. Nous illustrons notre réflexion par l'étude de groupes d'étudiants en phase d'internalisation du monde « Second Life »."
"Le projet d'Union méditerranéenne lancé par le Président de la République française est clairement situé pour le moment dans une perspective stratégique et politique. Son annonce vient « d'en haut ». Cette démarche en fixe d'emblée les ambitions et les limites. La réflexion fondée sur le paradigme de l'intelligence territoriale peut servir à l'examiner sous un autre angle et contribuer à son succès. En effet, l'un des freins à la réussite de ce projet serait l'hostilité ou même l'indifférence des populations concernées. L'échec du processus de Barcelone est là pour en témoigner. Refaire la même chose 10 ans plus tard conduirait sans doute au même résultat. Le projet d'Union méditerranéenne est pour le moment une utopie. La démocratie participative à l'échelle d'une union de territoires doit s'affranchir des schémas classiques de confrontations territoriales pour s'insérer dans le mouvement de la Mondialité. Dans l'Europe actuelle, la notion de nation est en train d'évoluer et les régions prennent une place de plus en plus notable. Sur la planète Terre, les grandes régions sont celles du Nord par rapport à celles du Sud. Les mécanismes de la démocratie participative mettent alors en œuvre non seulement des formes de collaboration mais aussi des principes de raisonnement que nous plaçons sous le signe de l'Intelligence territoriale."
"Le liban est un pays multicommunautaire, ce qui a conduit, depuis 1920, à de nombreux conflits intercommunautaires, non religieux, entre des groupes qui s'arrachent le pouvoir. ces conflits opposaient souvent les chrétiens qui ont, pour la plupart, une vision pro-occidentale d'une part aux musulmans qui sont plus orientés vers les pays arabes d'autre part.toutefois, il semblerait que plusieurs phénomènes de l'histoire contemporaine du liban aient modifié cette dualité chrétienne-musulmane. l'assassinat de rafik hariri, leader sunnite, en février 2005 en plein coeur de beyrouth et la guerre israélienne sur le hezbollah, parti chiite par excellence en juillet-août 2006 auraient changé le visage du conflit, pour en faire un conflit sunnite-chiite. un conflit qui règnerait non seulement sur le liban mais sur toute la région du moyen-orient.ayant pour objectif d'étudier ce conflit sous l'angle de 'la communication politique et le confessionnalisme au liban', plus particulièrement le cas des élections législatives de 2009 ; nous étudierons essentiellement les discours politiques des deux partis représentant les communautés sunnite et chiite afin de pouvoir en ressortir les éventuelles tensions intercommunautaires après avoir établit le cadre théorique et le contexte de notre projet de recherche."
"L’entrée tardive du développement durable en France apporte son lot d’expériences basées sur un cadre législatif en constante évolution et la volonté politique d’orienter les territoires en fonction de leurs problèmes et potentialités. « Penser global, agir local » fait désormais figure de principe pour définir l’action des territoires insérés dans des logiques mondiales mais dont les ressources peuvent être exploitées par la reconnaissance de compétences attribuées aux acteurs locaux. Dès lors, une culture de la participation émerge progressivement au travers de procédures nouvelles vouées à se faire rejoindre l’ensemble des acteurs territoriaux autour de règles communes pour la construction du territoire et en faveur de la connaissance des dynamiques territoriales. Cependant, les multiples injonctions à la participation de la société civile supposent de faire évoluer le territoire dans sa culture, ce que nous proposons par une démarche d’intelligence territoriale. Ce paradigme de recherche suppose donc qu’en préalable à l’établissement d’un processus de communication, issu d’une médiation sociale (Ateliers 21, Conseils de quartier, CIQ, etc.) ou socio-technique (journal municipal, forum électronique, Chat, etc.), le territoire doit constituer son « capital formel » (Bertacchini, 2004) pour permettre aux acteurs locaux d’accepter des règles et procédures communes, d’échanger leurs compétences, de se mobiliser et se rejoindre autour du projet territorial. Or, la constitution du capital formel territorial suppose non seulement que la collectivité échange de l’information sur les dynamiques territoriales à l’oeuvre mais également qu’elle apporte une plus-value à cette information échangée, notamment par l’exploitation de l’ensemble des « ressources communicationnelles des TIC » (Habib & Baltz, 2008). Il s’agit par-là de fournir des connaissances, des outils nécessaires afin que le citoyen se forge une opinion éclairée et mettre l’accent sur l’apprentissage collectif (Manin in Sintomer et Talpin, 2011 ; Urfalino, 2005) de la logique du développement durable (Angot, 2013).Notre objet de recherche s’intéresse aux collectivités territoriales de la région Provence-Alpes-Côte-D’azur engagées dans un projet de développement territorial durable du type Agenda 21, Plan Climat Energie Territorial et label Action Globale Innovante pour la Région (AGIR). Ce choix nous permet d’approcher le développement durable sous l’angle de la participation des acteurs au travers de démarches participatives spécifiques (de l’information à la concertation), au regard du paradigme de l’intelligence territoriale et de notre domaine de recherche : les sciences de l’information et de la communication. Nous aborderons la question des usages numériques au sein des collectivités territoriales, la production de connaissances formulées dans des contenus et supports numériques, diffusées et échangées au sein des différentes arènes de la société civile."
"Grâce à une expérimentation, cette recherche étudie les effets non conscients du parrainage de deux programmes quasi identiques, mais de valences affectives différentes, sur l'attitude à l'égard d'une nouvelle marque. Les effets, enregistrés par tests indirects, sont indéniables pour le parrainage du programme gai : avec deux apparitions, celui-ci a un impact positif sur la marque. Le programme triste n'a eu aucun effet : l'intensité des émotions déclenchées aurait provoqué des pensées impertinentes empêchant de mémoriser la marque. Les résultats sont en accord avec le nouveau modèle de la mésattribution de la familiarité. Aucune trace d'un transfert sémantique automatique du programme sur l'image de la marque n'est observé."
"L'organisation élargie Education Nationale, et plus particulièrement l'école élémentaire, est ici appréhendée comme terrain d'étude pour le chercheur en sciences de l'information et de la communication (Sic). La problématique questionne l'appropriation des technologies de l'information et de la communication (Tic) au sein des établissements scolaires à travers une approche communicationnelle de l'organisation. Les strates d'outils et médias éducatifs intégrés dans les classes cohabitent sans complémentarité et constituent la panoplie de l'enseignant chef d'orchestre (Moeglin, 2005). Nous posons la question de la généralisation de leur usage car nous constatons qu'il ne suffit pas d'équiper les écoles pour voir ces technologies appropriées dans les pratiques de classe. Sur le terrain, nous construisons un projet d'usage « engageant » nommé le « P'tit journ@l » et pensons son « accompagnement » autour d'une double médiation : la médiation de la communication, la médiation technique et sociale. Nous élaborons le concept de « projet engageant » en convoquant le paradigme de la communication engageante (Bernard, Joule, 2004). En considérant l'intentionnalité des acteurs, le « projet engageant » représente un changement « par le bas » (Bernard, 1997) des pratiques pédagogiques, c'est-à-dire un changement qui ne soit pas prescrit par la hiérarchie. Il permet de dépasser les contraintes d'action d'un cadre très hiérarchisé pour favoriser la coopération, la co-formation et la communication entre acteurs considérés comme autonomes et responsables. Dans les situations communicationnelles d'accompagnement entre pairs, la construction du sens passe par la situation sociale d'interaction. Les actes de parole produisent un « illocutoire organisationnel » (Gramaccia, 2001) entre acteurs qui échangent selon des pratiques de communication ordinaires et quotidiennes. Au terme de notre étude empirique, nous montrons, par une approche heuristique, que l'appropriation des Tic dans la pratique de classe relève plus d'une forme circulaire de la communication que d'une forme linéaire portant les injonctions du management."
"L'article privilégie, lors d'une opération de commandite par le Conseil Général 06, l'étude de la corrélation entre zone d'aménagement économique et mode d'habitat sur la technopole de Sophia-Antipolis. Cette étude a été l'occasion d'expérimenter le décalage entre les représentations des concepteurs (essentiellement axées sur la notion de cadre de vie) et le mode de vie des habitants."
"L'objectif de celle recherche consiste à identifier les mécanismes de construction d'une nouvelle situation de communication au sein d'un environnement virtuel de type momie persistant (Second Life) dans un contexte situé d'enseignement à distance. Nous convoquons notamment la notion d'artefact communicationnel comme un lieu symbolique de représentations et de construction commune de réalité. Nous favorisons une analyse pragmatique de la communication à travers l'approche des quatre médiations, l'étude des interactivités fonctionnelles de l'ordre de la communication homme/homme, et l'étude du lien social construit au sein de ces dispositifs. Les hypothèses portent sur la perception d'une forme de matérialisation de la situation de communication, et sur le rôle social que peuvent jouer les avatars comme projections des utilisateurs. L'analyse des signaux analogiques et notamment de la gestuelle, ainsi que la proxémique liée à ces environnements spatiaux peuplés d'avatars ""caractérisés"" sont privilégiées sur le plan communicationnel. Nous démontrons qu'elles participent de rites sociaux et de la construction de l'identité du sujet et du groupe. Dans le même temps, la particularité de ces dispositifs fait que l'utilisateur peut adopter une autre vue que celui des yeux de son avatar (en l'occurrence un point de vue décentré de la situation). En conséquence il peut être en alternance spectateur et acteur de la situation de communication à laquelle il participe. Cette possibilité engendre de notre point de vue une rupture et une distanciation possible du sujet avec son objet (qui de manière singulière peut aussi engendrer paradoxes et inconforts pour l'utilisateur). Elle questionne sur le plan épistémologique la notion d'artefact technique et l'hypothèse d'une forme de relation circulaire entre le sujet et son objet. Enfin, cette nouvelle situation de communication relève aussi d'une approche systémique car de nombreux facteurs interviennent (contexte, perception, maîtrise, autorité...) qui peuvent la complexifier mais aussi lui donner sa légitimité d'objet théorique."
L'objectif du présent document est de proposer un cadre théorique et méthodologique en intelligence territoriale pour résoudre des problèmes de transition soci-écologique par reference aux sciences de la communication et l'information. Ce cadre est appliqué au cas de la gestion de l'eau en élargissant la notion de services publics à celle d'une gestion collective du cycle de l'eau dans une grande agglomération. La méthodologie est basée sur plusieurs étapes qui vont être testées dans un projet de recherche en cours d'évaluation. Les étapes importantes sont: - Établir une liste exhaustive des parties prenantes - Comprendre les intérêts des parties prenantes et des professionnels - Concevoir un dispositif coopératif web2.0 expérimental - Donner l'impulsion à l'engagement des parties prenantes - Évaluer l'efficacité globale La conclusion envisagera les conditions de généralisation de cette expérience.
"Du décret franc¸ais du 10 février 2010 aux travaux de réécriture lancés par le DSM et la CIM, les questions transidentitaires doivent être discutées, appréhendées sur le plan théorique aussi bien que politique. Le schisme entre les associations d'usagers, les trans' en demande et/ou en obligation de suivi, et les équipes hospitalières spécialisées dans la prise en charge de ce public, pour le cas de la France tout particulièrement, a une histoire intimement liée à un ordre présumé du social et de ses acteurs en termes de genre dans une vision binaire de la société. La demande de reconnaissance de savoirs, d'expertises du terrain transidentitaire s'est heurtée, dès le début des années 1980, au bouclier thérapeutique, que l'on peut définir comme l'argument ultime de l'instance médico-légale en charge du sujet transsexe, écartant les autres sujets/trajets trans', pour mettre à distance toute intervention du politique, des sciences humaines et sociales dans ce qui est devenu un paradigme théorique : le changement de sexe est d'abord un changement culturel de genre. Enfin, l'instance médicolégale s'efforce d'ignorer sa propre politisation et militance. Partagé entre la volonté de résister et de participer face/avec un protocole inadapté, inefficace et culturellement obsolète, le terrain a connu ses propres mouvements s'interrogeant, s'interpellant sur l'esprit de la méthode. La question trans' compte désormais autant de politiques que de groupes. La théorisation du fait trans', trouve sa légitimité non pour revenir seulement sur l'inégalité homme/femme, mais pour lutter du même élan contre toute forme d'inégalité et de stigmatisation. Le terrain transidentitaire s'est profondément transformé, les outils pour l'appréhender comme le paradigme théorique lui-même, se doivent une chose : évoluer."
"Notre étude aborde les relations entre la culture traditionnelle chinoise portée par le confucianisme, le bouddhisme, le taoïsme, et les trois piliers du développement durable que sont le social, l’économie et l’environnement. La culture traditionnelle chinoise s’attache principalement à l'harmonie dans les relations interpersonnelles,dans les relations entre l’homme et la nature. Le développement durable cherche à créer pour le futur un état d’harmonie entre les êtres humains et entre l’homme et la nature. Notre question est la suivante : la culture traditionnelle chinoise ne pourrait elle pas apporter sa contribution au développement durable dans sa façon de communiquer au monde ? Les fondements de la culture traditionnelle chinoise définissent l’harmonie à partir de règles de vie : le confucianisme favorise la communication interpersonnelle, la relation entre l’homme et le social ; le taoïsme met l’accent sur la communication entre la nature et l’homme ; le bouddhisme quant à lui privilégie la communication entre l’esprit et le corps de l’homme. Nous tenterons de montrer à partir d’analyses de discours scientifiques, politiques et d’une enquête en Chine et en France, qu’une meilleure compréhension pour l’occident de la culture chinoise pourrait apporter une contribution significative au projet du développement durable. La réconciliation entre la tradition et la modernité, la combinaison des cultures occidentales et orientales sont les axes majeurs de ce projet."
"L'article consiste en une étude pluri-méthodologique du paysage des actualités en ligne, dans le cadre d'un projet de recherche intitulé Internet Pluralisme et Redondance de l'Information. L'étude cherche à repérer, dans le flux de publication comme dans la circulation d'une nouvelle, les éléments de convergence et de reprise, ou au contraire de créativité et de singularisation des différents types de sites d'information (médias en ligne, infomédiaires, sites natifs de l'internet et blogs) à partir de la publication le 8 mars 2011 d'articles sur un sondage donnant Marine Le Pen en tête des intentions de vote pour l'élection présidentielle de 2012. L'article répond essentiellement à deux objectifs. Le premier est d'identifier le volume de production de l'information dédié au sujet dans les différents sites en tenant compte de la circulation de la nouvelle dans sa dimension diachronique. Le second est d'identifier certains standards dans la production des différents sites, par l'étude des sources et signatures, du traitement visuel ainsi que de l'usage des liens hypertextes. Les résultats dessinent des lignes de convergence et de divergence entre les différentes catégories de sites : stratégies de productivité et reprise normalisée de l'information vs singularité, re-traitement de l'information et valorisation d'un réseau relationnel."
"A la lumière de la revue de la littérature et des différentes opportunités empiriques qui se sont présentées, nous étions devant une problématique de recherche qui reflète l’actualité de l’usage de la web TV à des fins contributives, et plus précisément l’influence de certains éléments d’atmosphère du site de la web tv sur l’attitude des étudiants les conduisant à adopter un comportement d’approche spécifique à ce dispositif. Notre recherche nous a permis de proposer un modèle conceptuel qui s’est hissé en un cadre global d’appréhension du comportement des étudiants quant à la fréquentation des web tv universitaires étudiantes. A partir d’une approche qualitative et sur la base d’une expérimentation menée auprès d’un échantillon de 200 étudiants de l’USTV, nous avons pu détecter non seulement les éléments d’atmosphère les plus influents sur le comportement d’approche des étudiants mais également certaines caractéristiques individuelles dont l’impact est bien modérateur. Cette thèse contribue finalement à une meilleure compréhension du rôle et du fonctionnement de l’influence des stimuli environnementaux de la web tv sur le comportement de l’étudiant et présente en outre un ensemble de réflexion à adopter et d’action à entreprendre en vue de drainer les réponses comportementales souhaitées par les concepteurs."
"La prise en compte du handicap moteur dans l’apprentissage via les TIC reste un aspect difficile à traiter. En se basant sur nos observations participantes, nous proposons de mettre à la disposition des enfants handicapés moteurs une plate-forme constituée de jeux sérieux adaptés à leur spécificité pour augmenter leur motiviation et améliorer les conditions de leur apprentissage. Cette plate-forme sera constituée de technologies open source pour une meilleure collaboration entre enfants, éducateurs, famille, chercheurs et ingénieurs afin de la rendre la plus accessible et la plus performante possible."
"Cet article ouvre une réflexion sur les modalités de structuration de l’open (res)sources en travail social, c’est-à-dire vers un open data social. Nous verrons comment l’accès à l’information en tant que bien commun est une question ancienne soulevant un problème pratique et éthique. Puis nous présenterons l’éducommunication aux/par les médias comme nouvelle épistémologie et pédagogie qui ouvre des perspectives : les médias ou toutes modalités de communication éducative étant des biens communs au service de la citoyenneté. Enfin, à partir d’une recherche-action « NUSERUS », nous exposerons nos premiers résultats et leurs enjeux en termes scientifiques, éthiques et économiques"
"L’insertion des technologies numériques dans la délivrance de formation et de savoirs au sein des universités pose question depuis de nombreuses années. Nous suggérons alors l’évolution du statut de l’apprenant récepteur passif vers l’apprenant acteur de sa formation jusqu’à la notion de l’apprenant stratège. Il s’agit alors de soulever des questions quant aux renouvellements des apprentissages dans l’enseignement supérieur dans le cadre de processus innovants autour de dispositifs numériques. A partir de deux cas, nous évoquons un paradoxe entre une sensation d’attrait vers une autonomisation de l’apprentissage et les enjeux pour une organisation, ici universitaire, lorsqu’il s’agit de la mise en œuvre."
"Ce texte contribue à une étude menée depuis trois ans sur l'analyse des usages numériques de groupes d'étudiants en situation de projet à l'Unité Formation Recherche Ingémédia de l'Université de Toulon. Il décrit les environnements virtuels de travail adoptés par ces groupes d'étudiants et tente d'analyser les fonctions de communication et d'information mobilisées durant les projets. Les résultats proposés montrent que certaines nouvelles pratiques culturelles et sociales numériques partici-pent fortement de détournements d'usages au sein des dispositifs utilisés. Les auteurs concluent sur l'émergence de ce qu'ils qualifient de nouvelles formes en ligne ""d'être ensemble"" pour mieux ""faire ensemble""."
"Cette communication questionne les enjeux scientifiques et éthiques de l’étude de l’activité d’acteurs engagés dans une formation à distance. Deux expérimentations menées respectivement auprès d’apprenants et de tuteurs à distance au sein d’une même entreprise privée de formation à distance sont présentées conjointement. Les chercheurs s’intéressent précisément aux portées et limites de l’analyse distanciée de l’activité au moyen de méthodes de recueil de données et d’interventions nécessairement renouvelées compte-tenu de l’éloignement géographique des protagonistes de la formation à distance. Outre le respect des données personnelles, un questionnement déontologique s’impose sur le plan scientifique lorsque les méthodes sont revisitées pour être adaptées à un nouveau cadre analytique."
"Depuis une vingtaine d’années, la production nativement numérique et la numérisation croissante de données de tous ordres (textuelles, chiffrées, relationnelles, etc.) a considérablement facilité leur archivage, leur circulation, leur consultation et leur manipulation, dans des agencements dont la diversité n’a d’égale que celle des fins auxquelles ces données sont convoquées. Aujourd’hui, qu’il s’agisse de journalisme, de gestion de l’information, de communication, de culture, de littérature, des politiques publiques, du droit ou encore du marketing, les champs de pratiques socio-professionnelles sont confrontés à une masse considérable de données. Dans le domaine de la recherche en sciences humaines et sociales relative à ces champs, ces données sont porteuses de riches potentialités, mais leur ampleur et leur caractère parfois chaotique peuvent décourager le chercheur. Pourtant, des méthodes et outils spécifiques, parfois désignés sous le vocable « digital methods », ont été progressivement pensés et développés pour faciliter la (re)construction de sens à partir de volumes plus ou moins importants de données. Indexation, crawling ou web scraping, archivage et gestion de bases de données, statistique classique et analyse de données textuelles, analyse de contenu et traitement automatique des langues, graphes de réseaux et cartographies du web, ou encore infographie et modes de visualisation : ces « techniques intellectuelles », distinctes et assemblées, se trouvent à la croisée de divers champs de recherche et concernent très directement les sciences de l’information et de la communication pour explorer, expliquer, synthétiser ou simplement présenter des faits. Ces méthodes et outils d’agencement informationnel ou de redocumentarisation permettent une herméneutique des corpus et ont une réelle portée heuristique, que cette journée d’études ambitionne de présenter, détailler, croiser et interroger. En même temps, force est de constater que cette immixtion du numérique dans nos activités quotidiennes s’est produite subrepticement, sans débat ni discussion, dans une forme d’imposition par l’adhésion immédiate (et donc irréfléchie) d’usagers divers dont l’enthousiasme était motivé par nombre de promesses d’ordre sémiotique (l’intelligibilité du monde à moindre coût cognitif), socio-politique (l’émancipation des individus par la démocratie digitale), économique (dans un marché libertaire débarrassé des lourdeurs bureaucratiques). Portées par des idéologies et des mythologies ambivalentes (et souvent contradictoires), les potentialités socio-techniques du numérique se sont actualisées dans des sociétés hétérogènes, et sont elles-mêmes porteuses d’enjeux et de mécanismes parfois flous ou obscurs qu’il s’agira ici de mieux cerner: Dans le cadre de cette journée d’études centrée sur l’Analyse et la Visualisation de Données (AVD), les réflexions interrogeront essentiellement deux aspects : les outils, techniques, formats et processus de l’AVD d’une part, les enjeux économiques, socio-politiques et socio-sémiotiques de la mobilisation de ces techniques intellectuelles d’autre part."
"Si la technologie est toujours allée de pair avec l’histoire, elle s’est assurément imposée comme centrale depuis la seconde moitié du vingtième siècle, en transformant radicalement notre appréhension du son et des rapports aux composants visuels et au geste. Et ce, par de multiples aspects : apparition d’une nouvelle lutherie qui favorise de nouvelles esthétiques (musique concrète, électronique, spectrale, granulaire…) ; évolution de la représentation du sonore et du musical (partitions interactives) qui favorise des pensées compositionnelles novatrices ; rapprochement avec d’autres pratiques artistiques (image, danse, théâtre, installation, environnement…) dans une optique d’intermédiation et de poly-expressivité ; ou encore, transformation de la réception du sonore au travers de situations médiatisées et individualisées (radio, disque, internet, baladeur…). Cette rapide synthèse des transformations de notre rapport au son, induites par la technologie, fait apparaître celle-ci comme un milieu complexe produisant de nouveaux paradigmes en continuelles transformations. Il devient dès lors pertinent d’interroger les rapprochements et les influences croisées entre pensée sonore et pensée visuelle, tant selon une approche historique (rapports peinture/ musique, langage de l’image et du son dans le cinéma) qu’en référence à la convergence actuelle des différentes pratiques artistiques dans le numérique, tout en mettant en avant l’impact de ces nouvelles pratiques sur l’enseignement musical. Un langage multi-modal est-il vraiment en train de se constituer ? Si oui, dans quelles conditions ? Quel sera l’apport de ces nouveaux outils dans l’enseignement artistique (musical, mais aussi visuel et chorégraphique) ?"
"1) Les guerres et le Liban a. L'horreur Elle est sans limite. Elle est sans intérieur. A cette absence d'intérieur, la mort peut conduire. Mais ne suffit pas. Au-delà de la mort, il faut à Don Juan aller. Alors qu'il semble encore faire partie des vivants, pour rentrer dans l'espace illimité de l'horreur, espace désigné par Jacques Lacan de seconde mort, Don Juan serre la main du Commandeur. L'horreur n'a pas plus de limite extérieure que de limite intérieure. Elle ne cesse pour cette raison d'avoir besoin d'être circonvenue. Chaque guerre est ainsi limitée, désignée souvent de manière plurielle et toujours de façon inadéquate : La Grande Guerre ou la Première Guerre Mondiale ? Mais quel sens raisonnable recouvrent ces termes si ce n'est à désigner, quel qu'en soit le prix, un espace circonscrit, une temporalité définie. Hélène Puiseux dans les Figures de la guerre montre comment se règle ainsi entre « Représentations et sensibilités 1839-1996 » dans le discours en Occident, via différentes techniques, mises en scène, médias, bref différents dispositifs, la question de l'horreur. Sans conclure, l'auteure décrit comment « Les écrans que les représentations tendent à la guerre depuis deux siècles, au travers de ces décennies que les historiens appellent la période contemporaine, sont passés de l'héroïsation du conflit à l'héroïsation de la perte puis à la perte de l'héroïsation et à la perte du sens [PUISEUX, 1997, p. 243] ». Mais comment la guerre est-elle représentée au Liban ? Avec quels objectifs ? b. La contamination des guerres Par l'horreur, les guerres se contaminent. Elles constituent un réseau serré d'événements, d'accidents, de soulèvements, de printemps et d'hivers. Elles se répondent et s'interpellent, produisent leur lot de pertinence sur bien des plans : la situation géopolitique, les armements, les moyens de communication employés, les tactiques, l'implication des civils, etc. Elles produisent leur régularité et leur singularité. Se dessine une casuistique lancinante et sans intérêt en soi. Le dénombrement inévitable du nombre de morts se transforme inexorablement en une tentative de gradation indécente de l'horreur. A part bien sûr pour les marchands d'armes. Concernant la seconde guerre mondiale, je joue l'invention du radar, je joue l'invention des armes de destruction massive, je joue Alan Thuring contre Enigma. Les guerres du Liban apparaissent comme un envers des guerres mondiales occidentales et plus précisément de ce que ces guerres ont laissé pour solde de tout compte. Ces guerres dites mondiales ne l'étaient que parce des pays, des nations, des peuples colonisés se trouvaient embarqués dans les guerres de leurs colonisateurs. La délimitation « guerres du Liban » tombe en conséquence. Chacun peut envisager qu'il s'agit d'une guerre mondiale à l'envers qui s'est jouée à cet endroit. De l'horreur en ce qu'elle implique chaque Libanais dans une communauté contre d'autres communautés transformant son voisin de toujours en ennemi d'aujourd'hui avec"
L'objectif de cet article est de proposer une réflexion sur les mutations actuelles de l'industrie de la musique enregistrée à travers l'expérience d'un label de musique expérimentale (Trace-Label). Il s'agit pour celui-ci de s'adapter à une nouvelle situation marquée par la dématérialisation des supports sonores et d'étendre une activité liée au disque à une pratique artistique des réseaux et passer ainsi du disque au fichier numérique.
"Le renseignement militaire a développé un modèle empirique de transformation de l’information en connaissance, appelé cycle du renseignement. L’avènement d’un marché de l’information au début des années 90, conjugué à une crise de la communauté du renseignement, a créé les conditions d’un transfert de ce modèle vers les entreprises, lequel est aujourd’hui considéré comme le cœur du processus de veille. Enseigné dans les écoles, le cycle du renseignement est censé donner aux étudiants un outil universel, à la fois méthodologique, fonctionnel et organisationnel. Son appropriation par les entreprises permettrait la mise en œuvre d’une démarche d’intelligence économique et conduirait idéalement à une logique de flux partagés d’information et de connaissance en vue d’acquérir un avantage stratégique ou concurrentiel. En étudiant de plus près l’origine de ce cycle, on s’aperçoit toutefois que son transfert vers l’IE relève d’une lecture partielle qui pourrait expliquer la difficulté qu’éprouvent les organisations à le mettre en œuvre, en particulier lorsque leur taille est importante et leur structure complexe. Cet article a pour objectif de porter un regard critique sur ce modèle d’exploitation d’information, à partir d’une expérience de terrain, et de poser les bases d’une réflexion sur les processus de construction de connaissance opérationnelle. En ce sens, loin de se limiter au renseignement et à l’intelligence économique, il pourra également intéresser les praticiens de la veille et de l’intelligence marketing."
Nous nous proposons dans cette intervention de mettre en avant les spécificités qu’entretiennent les images et les sons dans les systèmes numériques interactifs. Nous regarderons plus particulièrement les jouets sonores (audio-game) en essayant de mettre en avant ce que nous pouvons appeler les « interfaces a-musicologiques » et les nouveaux effets perceptifs qu’elles induisent. Les jouets sonores (entre instruments de musique et jeux interactifs) ouvrent un champ créatif de nouvelles pratiques amateurs où le rapprochement des deux représentations via l’interaction laisse présager leur fusion dans un nouvel objet audiovisuel conçu comme un processus multi-modal complexe.
"Le temps où Clausewitz critiquait l’efficacité du renseignement semble révolu. Aujourd’hui nerf de la guerre dans un monde où les menaces sont disséminées et invisibles, le renseignement s’affranchit bien souvent de certaines normes éthiques pour faire prévaloir la raison d’État. Franck Bulinge et Charlotte Lepri expliquent qu’entre éthique et nécessité, le renseignement se cherche une légitimité, notamment auprès de ses opinions publiques. Les auteurs suggèrent qu’à défaut de soumettre le renseignement à une éthique, ce que l’activité limite de facto, un « contrôle démocratique » pourrait permettre de mieux encadrer les activités des services de renseignement."
"Cet opus de la collection «Cahiers de Logique et d'Epistémologie» est une contribution à l'histoire de la presse mathématique et plus particulièrement des journaux qui marquèrent ou accompagnèrent son émergence dans l'Europe du 19ème siècle. Bien que traitant de ce sujet sur un plan général, il est plus particulièrement question ici de périodiques ayant vu le jour dans quatre pays européens sur la période 1800-1900 : la France, l'Italie, le Portugal et l'Espagne. Le lecteur y trouvera des contributions originales sur leurs é ;diteurs, leurs politiques éditoriales, leurs contenus et les populations d'auteurs qui les alimentèrent. On vérifiera ainsi comment l'é ;mergence de ces journaux contribua à la spécialisation des mathé ;matiques, à ; l'instauration de la forme moderne des échanges et de l'émulation entre mathématiciens et indirectement à la mise en place des critères actuels de la reconnaissance de ces derniers par leurs pairs."
"Les écoles d’Arts et de spectacles vivants, telles l’École Supérieure Nationale de Danse de Marseille (ENSDM) ou l’École Supérieure d’Arts de Toulon rejoignent depuis peu l’architecture LMD. Elles participent aux projets pédagogiques et recherches de l’UFR Ingémédia et du laboratoire I3M. Le CNRR est aussi un partenaire important de nos formations. Enfin, les structures de diffusion (Théâtre Liberté, Opéra, Châteauvallon) manifestent un vif intérêt pour l’UFR Ingémédia. En quoi l’articulation pédagogie - recherche, entre le monde culturel, l’UFR Ingémédia et le laboratoire I3M favoriserait-elle de nouveaux projets pour l’Université du Sud Toulon-Var et ses partenaires ?"
"Cet article est le fruit d’une recherche effectuée en 2008 sous l’égide de l’Institut national des hautes études de sécurité et de justice (INHESJ). Elle portait sur les techniques de manipulation employées sur Internet par des mouvements radicaux, notamment islamistes, et leurs conséquences possibles sur les internautes. À partir des données théoriques de la littérature, nous avons élaboré une grille d’analyse permettant de mesurer le niveau de manipulation de divers supports-contenus disponibles sur Internet et le risque lié à l’effet d’exposition possible sur un public supposé vulnérable."
"Dynamiques de citoyenneté, d’éco-citoyenneté, de réseaux, de médias, de territoires, de culture et d’interculturalité. Pratiques journalistiques, innovations numériques, nouvelles formes de savoir, mouvements sociaux et divulgation des sciences... Autant de problématiques et de pratiques que l’approche communicationnelle relient. En croisant les regards théoriques et empiriques, cet ouvrage offre une contribution à la compréhension et à l’analyse des logiques d’actions et de construction des savoirs dans un espace méditerranéen en pleine mutation. Car, depuis près de trois décennies, par vagues successives, les relations scientifiques entre chercheurs en communication du nord et du sud de la Méditerranée ne cessent de s’intensifier. Il semble émerger sinon une identité méditerranéenne contemporaine, du moins des dynamiques et mouvements qui renvoient à un patrimoine commun issu d’une l’histoire millénaire et qui se traduit aujourd’hui dans les paradoxes d’une unité plurielle. Cet ouvrage apporte aussi la preuve que le champ de la communication favorise le dépassement des frontières institutionnelles et académiques, et nourrit une réflexion commune entre chercheurs de l’Université et du CNRS."
"L’évolution des enjeux stratégiques liés à l’information et à la connaissance implique une évolution des modèles disponibles dans le domaine du renseignement et de l’intelligence économique. Face à la complexité des situations, l’individu ne peut plus résoudre seul les problématiques décisionnelles. Le modèle du “mythique décideur” ne correspond plus à la réalité et peut s’avérer nuisible vis-à-vis des institutions et des entreprises. Nous poursuivons notre exploration du modèle d’intelligence économique centrée projet, dont nous pensons qu’il est une piste intéressante pour la mise en œuvre d’une intelligence collective susceptible de booster les performances des organisations. Cet article propose notamment d’étudier le modèle de la “war room” comme cadre opérationnel et pédagogique."
"Le passage de l’oralité à l’écrit et de l’indifférence à l’égard des outils de communication de savoir à leur intégration dans un projet éducatif a pris des siècles et il est passé par plusieurs épisodes. Malgré les différences qu’on peut constater quant aux rythmes et aux modalités de cette transformation pédagogique et communicationnelle entre l’Europe et le monde arabe, cette évolution de la réflexion sur les outils et médias éducatifs a fortement influencé la façon avec laquelle ces populations appréhendent aujourd’hui ces techniques de plus en plus numériques. En France, depuis des décennies, les plans numériques pour l’éducation se succèdent. En Tunisie, après la création en 2002 de l’université virtuelle de Tunis (UVT), on parle actuellement de l’intégration de tablettes numériques dans l’école publique. En dépit des différences que présentent ces deux pays quant aux contextes politiques, économiques, sociaux et culturels, nous avons noté des similitudes stratégiques qui caractérisent leurs démarches respectives pour le numérique éducatif. Nous pouvons remarquer, entre autres, une similitude dans l’installation volontariste des techniques pour apprendre accompagnée de discours annonçant plus de facilités, plus de fiabilité et plus de réussite. Dans le cadre de notre recherche, nous nous intéressons à la question de l’enseignement-apprentissage à distance, et aux outils éducatifs mis en œuvre pour ce faire. Nous ne prétendons pas faire une comparaison binaire entre un modèle d’utilisation des technologies dans le cadre de l’apprentissage qui serait français et un autre qui serait tunisien. Nous mettons en revanche en perspectives les deux expériences tout en insistant sur la démarche tunisienne. L’enseignement/apprentissage à distance est une situation de communication particulièrement délicate, d’une part par son inscription dans cette logique de contextes sociaux et économiques, mais aussi par l’éclatement de la notion de l’espace qui renforce le sentiment de l’isolement chez l’apprenant. Pour surmonter ce sentiment d’isolement et réussir son apprentissage à distance, plusieurs chercheurs proposent un processus d’autorégulation de l’apprentissage. Bien qu’il soit centré sur l’activité de l’apprenant lui-même, ce processus insiste sur l’importance de l’intervention des autres usagers du système éducatif (institution et corps pédagogique). Par notre recherche, nous avons voulu interroger cette question d’autorégulation de l’apprentissage dans le contexte tunisien de l’université virtuelle de Tunis. Nous avons alors cherché à comprendre le comportement autorégulé des apprenants tunisiens à la lumière du dispositif (humain et technique) de l’institution. Pour soulever ces questions nous avons opté pour une méthodologie multiple qui réunit observation participante, observation cachée, questionnaire, protocole géode et analyse de contenu. Cette recherche a abouti à trois résultats majeurs : la création de l’UVT est un projet avant tout politique destiné à véhiculer une image moderne de la Tunisie. En effet, à part l’expérience de l’institut supérieur de la formation continue crée en 1984, aucune réflexion autour d’une pédagogie adaptée à l’enseignement-apprentissage à distance dans le terrain tunisien n’a été entamée surtout que le dispositif UVT est basé sur les technologies numériques. De ce fait, le comportement autorégulé des apprenants oscille, selon le degré de contrôle pédagogique qu’exerce le dispositif UVT sur leur apprentissage, entre adaptation aux conditions formelles et création de conditions informelles contournant ainsi le dispositif de l’institution."
"La rationalité, associée au progrès technique, a pu laisser croire que le monde se désenchantait. Or, loin de désenchanter le monde, certains dispositifs techniques révèlent la dimension sacrée inhérente à toute activité humaine. En effet, paradoxalement, nous montrerons que l’interaction homme-machine produisant des environnements virtuels constitue une expérience du sacré."
"Cet article, issu d’une conférence dans le cadre du colloque ORG&CO à Rennes en mars 2016, présente une expérience et de fait un effet d’expertise sur les changements liés à la diffusion organisée ces dernières années d’une « culture numérique » censée modifier la praxis scolaire et devant construire l’école de demain. Un ensemble de travaux de recherche lié à différents contrats ou programmes au sein d’I3m Toulon est convoqué ici et sert d’indicateurs situés à plusieurs échelons : du local au régional jusqu’au niveau national et enfin retraduits dans une rencontre internationale. Il sera question d’un angle théorique fondé dans la théorie de systèmes sociaux pris en compte afin d’étudier la forme scolaire qui seront donc les appuis de cette réflexion. Nous tentons de montrer, de comprendre et de rendre compte d’apories, de contradictions et d’asymétries liées à l’intégration d’une « culture numérique » à l’école dont nous contestons la réalité. Afin d’envisager un autre cadre pour penser ces évolutions nous proposons un idéal fixé dans la notion « d’organisation apprenante » et faisons aussi un retour vers une approche scientifique et critique objectivée sur un tel contexte qui relève autant des SIC que, plus spécifiquement, de la communication des organisations."
"Il est demandé à des étudiants de témoigner de leurs démarches pour s'informer et s'orienter dans leurs choix de formation. Dans une logique expérimentale, ces témoignages sont présentés comme sollicités, à des fins d'enquête, soit par le rectorat, soit par un journal universitaire. Les témoignages sont analysés tant sous l'angle de leur contenu que sous celui de leur mise en scène discursive. Les résultats montrent que les étudiants qui témoignent ""pour le rectorat"" adoptent une attitude discursive de désengagement, se traduisant notamment par une atténuation de la valeur et de la portée de leur témoignage ; tandis que les étudiants qui témoignent ""pour un journal universitaire"" produisent un discours caractéristique du discours militant, caractérisé notamment par la mise en scène d'un énonciateur collectif."
"Pour évaluer les capacités fonctionnelles d'une personne handicapée et les compensations nécessaires à la commande d'un fauteuil roulant électrique, une plateforme technologique a été réalisée sur la base des conseils d'un ergothérapeute. Elle a été réalisée avec du matériel standard, comme il est souhaitable de le faire chaque fois que c'est possible pour réduire les coûts financiers. Différentes commandes ont été implantées et sont personnalisables en fonction des spécificités de chaque personne. Elles sont faciles à installer car elles sont modulaires et interchangeables. L'électronique embarquée est intégrée dans des circuits programmables de type FPGA. La programmation est faite en langage VHDL et est obtenue directement à partir de la modélisation des algorithmes par Réseaux de Petri Architecturaux pour obtenir une adéquation algorithme architecture optimisée."
Le soulèvement qu’a connu la Tunisie en 2011 ainsi que les attentats terroristes ont reçu une couverture médiatique intensive dans le monde et ont eu un effet négatif sur le secteur du tourisme. Cet article vise à analyser l’effet du terrorisme et de l’instabilité politique en Tunisie et dans la région ainsi que la dégradation de l’image du pays en tant que destination touristique attirante. Cette situation nécessite la mise en place d’un modèle global pour restaurer une image positive afin de reconquérir les touristes.
"Le concept de « motivation » se distingue par une multitude d’approches et d’études déterminant ses aspects et ses impacts dans le champ des sciences humaines et sociales. Il se présente comme un élément important pour mobiliser le facteur humain et stimuler son niveau de performance. En référence aux sciences de l’information et de la communication et aux sciences de l’éducation, la motivation, de type intrinsèque ou extrinsèque, se manifeste dans un environnement qui favorise, parmi d’autres, l’expression, l’interaction et l’engagement. Dans une vision d’apprentissage, les pratiques de formation évoluent introduisant de nouvelles techniques sophistiquées, plus efficaces, telles que l’usage de dispositifs immersifs de formation à distance. Dès lors, notre thèse se penche sur l’étude du phénomène de la motivation au sein d’un dispositif sociotechnique animé par des avatars. Ceci va montrer la force de l’immersion dans le monde virtuel et son impact sur la motivation de l’usager de ce système de communication. Il s’agit donc de traiter une problématique traduisant les ambitions de l’apprenant de réussir son concours de Ma thèse en 180 secondes en combinant les pratiques et l’environnement qui stimulent son comportement au cours de sa formation. Un apprentissage favorisant la motivation par l’usage des personnages virtuels reproduisant les mouvements et les émotions de l’usager est un nouveau modèle de médiation porteur dans les sciences de l’information et de la communication.Notre thèse est d’avancer un outil de formation à distance qui optimise la communication entre les apprenants et leurs enseignants et développe un cercle d’apprentissage basé sur la motivation. Notre choix méthodologique est fondé sur une approche compréhensive permettant d’élucider la nature des liens conçus entre le monde réel et le monde virtuel mettant en interaction un sujet et son avatar. Pour ce faire, une expérimentation est engagée sur notre terrain de recherche, balisée par des techniques d’observations et d’entretiens qualitatifs afin de comprendre la nature des liens forgés entre les acteurs du système immersif et de saisir les stimuli de la motivation de l’apprenant dans une mise en situation de formation à distance."
"Le xxe siècle a vu se transformer en profondeur les conditions de production, de diffusion et de réception du sonore. Indiscutablement, la captation du son par les techniques d’enregistrement et sa diffusion par la duplication d’un support de stockage ou la transmission par ondes hertziennes ont été des facteurs déterminants de ces profondes mutations. Dans une approche médiologique, et après avoir brossé un bref historique des bouleversements techniques concernant le son, nous nous pencherons sur deux conséquences importantes de cette suite ininterrompue d’innovations technologiques : le rapport mouvant du son à l’image et l’hybridation esthétique généralisée."
"La dernière décennie a été marquée par un élargissement des modalités de distribution des contenus cinématographiques et télévisuels des prescripteurs traditionnels au principe de mise à disposition (VOD, TVR, SVOD). Or, contre toute attente, ce dernier semble s’accompagner de nouvelles formes de logiques de programmation. Celles-ci organisent l’offre de plus en plus par le recours à l’outil marketing de l’événementialisation. Nous nous proposons ici d’analyser des usages inédits des événementiels : Festival-Direct-to (S)V0D, les sorties Day & date, l’Avant-première TV, la prescription orientée des sites VOD, … Ils s’inscrivent dans un processus de réintermédiation de la programmation en proposant de nouvelles formes d’éditorialisations illustrant la réarticulation des modèles socioéconomiques à l’ère numérique."
"Dans cet article on esquisse quelques changements possibles dans les configurations des modes de gouvernance de la santé, dans le cadre du développement des objets connectés, de l’épigénétique et de la molécularisation des processus d’individuation."
"Cet article s’intéresse à l’évolution de la pratique du jeu de rôle à l’ère du numérique, principalement en France. Internet, le Web 2.0, et les autres dispositifs numériques sont pratiquement omniprésents dans notre société moderne. Les jeux, notamment les jeux de rôle, ne sont pas épargnés par cette imprégnation numérique puisque l’expérience de jeu des rôlistes est bien souvent augmentée et améliorée à travers ces nouveaux dispositifs. Ce travail se propose d’observer les nouveaux comportements et usages des rôlistes français dans leur pratique du jeu de rôle à l’ère du numérique grâce à un questionnaire soumis à de nombreux sujets (rôlistes). D’autre part, avec l’intégration des nouvelles technologies dans les univers ludiques, se pose la question de l’intégrité de l’essence du jeu de rôle dont les frontières sont souvent redéfinies."
L'objet de cette communication est de proposer un projet de construction d'une Ontologie de l'Intelligence Territoriale. Il sera traité des raisons d'un tel projet et de la façon dont il peut être mené dans une approche contributive de type Web 2.0
Les moteurs de recherche s'appuient sur les habitudes de navigation des internautes et leur lieu de connexion afin d'affiner la pertinence des résultats. Les personnes en charge des campagnes de veille sur Internet pourraient influencer les résultats des requêtes : elles auraient été différentes selon la personne et le lieu.
"L'institution judiciaire peut être entrendue comme système symbolique constitué par des rites, une architecture, des costumes... qui sont ancrés dans le divin. ce système symbolic assure l'autorité de l'institution et de son discours: un discours dogmatique. Le passage de la tradition à la modernité puis à la post modernité caractérisé par la domination du principe de raison est susceptible d'affaiblir son discours. Cette rationalisation entraîne une concurrence avec le discours d'autres institutions et particulièrement avec le discours politique. Comme d'autres institutions, l'institution judiciaire est particulièrement confrontée à la problématique post-moderne du sens et de la représentation."
"Si la mondialisation est la dernière aventure culturelle de l'Homme, elle apparaît chez le citoyen dans une montée des incertitudes au sein du « local ». L'État montre ses limites et la décentralisation n'a pas encore convaincu chacun, de la capacité des Régions à le suppléer. Au delà de l'affrontement économique mondial, ce siècle témoigne d'une autre guerre : celle de l'information. Le pays devient alors pour l'habitant, un cocon protecteur des violences à venir. Quelle sera l'étendue des champs d'informations accessibles par l'acteur territorial ? Pourquoi le développement local est-il une organisation à construire par de l'information ? En quoi l'intelligence territoriale, par sa capacité à mutualiser le signe et l'indice, peut-elle devenir un outil d'anticipation des ruptures affectant le territoire ? Point de gouvernance sans conduire une nécessaire évolution de la culture des organisations qui fera de l'environnement local un territoire « apprenant ». Sans négliger quelques apports paradigmatiques transversaux, cet ouvrage témoigne de l'ancrage des problématiques d'intelligence territoriale dans les sciences de l'information et de la communication. Il donne les clés de filiation entre intelligence territoriale et intelligence économique dans une vision systémique de ce que l'on appelle l'intelligence informationnelle. Il sera l'un des points d'appui utile pour étudier et comprendre l'évolution des systèmes d'information dans la dynamique émergente offerte par l'intelligence territoriale."
Recension de la transyclopédie par ses auteurs
"Cette thèse envisage les effets occasionnés par la créativité collective contextualisée, plus particulièrement à l’échelle des groupes restreints, à partir d’une approche communicationnelle par les « constructions médiatrices ». Elle poursuit trois grands objectifs. Premièrement, elle s’inscrit dans une volonté d’éclaircissement de la notion de créativité de groupe en tant que phénomène (tel qu’abordé par Woodman, Sawyer et Griffin, 1993) et propose d’en faire la distinction par rapport à la méthode employée par le groupe de créativité (Demory, 1986; Aznar, 2011). Deuxièmement, elle cherche à clarifier les formes d’échanges et d’interactions au cours du processus de création-communication. Troisièmement, elle interroge les effets de ce phénomène collectif complexe de manière à mieux comprendre les conditions de déploiement de la créativité contextualisée.En prenant pour objet d’étude l’expérience vécue d’acteurs en situation par le biais d’un dispositif pédagogique, nous avons convié une communauté d’apprenants à vivre une expérience de création collective immersive et originale qui, par sa nature engageante et déroutante, était susceptible de heurter les habitudes et le cadre traditionnel d’apprentissage. Nous avons émis l’hypothèse que la tension créative entraine la réorganisation constante de l’activité des groupes en influençant les « constructions médiatrices » puis, que la nature de ce contexte cristallise la cohésion et l’engagement des groupes impliqués. Par une analyse de l’activité-créativité (la créactivité) de groupe, et plus particulièrement par l’observation des actions conjointes, des rapports d’interactions et de l’implication des membres, cette étude postule que la créativité collective contextualisée participe à l’émergence de formes d’interactions. Puis, que le recours aux moyens médiateurs (aux artefacts, à la division du travail et aux règles d’interactions) s’effectue dans un mouvement constant de réorganisation de l’activité par dépassements successifs des contradictions, par la formation de noeuds de médiations. Les constructions médiatrices sont ainsi le reflet du processus de transformation du sens des interactions sociales au cours de l’activité."
"En questionnant l’Architecture à travers différents exemples et périodes jusqu’à notre monde contemporain, nous obtenons beaucoup de réponses à des interrogations existentielles qui nous semblent énigmatiques. Le phénomène de communication ou d’expression par l’architecture, n’est pas indépendant de l’antagonisme qui a abouti au clash des civilisations. Par sa « Présence » l’architecture configure le monde de l’homme, répondant à ses besoins matériels mais aussi à ses aspirations, rêves et idéaux. Elle porte en elle ses « Idées » reflète sa « vision » et communique ses « Messages ». Elle seule peut l’enchanter par l’édification d’un monde nouveau. Mais elle peut aussi le détruire en agressant ses sens et en polluant ses esprits par les idées qu’elle reflète. L’œuvre architecturale marque aussi le lieu par sa présence, elle le poinçonne et devient « symbole » ou « image », repère, jalon dans un parcours international qui revêt l’aspect d’un défi, voire une nouvelle guerre de puissance au travers de bâtiments emblématiques. Cette architecture que nous appelons « Landmark » devient un paradigme essentiel de « présence » et de mutation. Sa mission essentielle est d’être au-delà de l’instrument, un précepte de communication toujours plus innovateur. L’architecture, par cette nouvelle dimension, devient un facteur inéluctable de stabilité et de continuation du monde. Un convoyeur de l’avenir non seulement un miroir du passé. C’est la problématique liée à une nouvelle dimension de l’Architecture, qui se conjugue avec la Quatrième dimension mais qui va au-delà, chercher la vraie raison de survivance du monde par l’architecture à travers les temps. Cette dimension liée à la communication, qui est présente chez les historiens et les sociologues mérite d’être fouillée plus en profondeur par nous architectes qui occultons parfois, certains aspects majeurs relatifs à la communication par la seule « présence » des bâtiments crées à bon escient par notre subconscient alerte et assidu."
"Cette troisième journée d’étude s’inscrit dans le cadre du projet de recherche PIND (Punk is not dead. Une histoire de la scène punk en France, 1976-2016), soutenu par l’UMR THALIM (CNRS/ ENS/ Paris 3). Elle vise à poser les bases d’une étude comparée des formes d’émergence, de diffusion et de transformation du punk dans les pays européens. Son objectif est triple : permettre de mieux situer la place de la scène punk française dans un ensemble culturel plus vaste, à l’échelle euro- péenne ; identifier les processus sociohistoriques et les formes de transferts culturels qui parti- cipent à l’invention des scènes punk en Europe; élaborer une première approche comparée des temporalités du punk en Europe."
"Dans le milieu des années 1990, l’arrivée d’Internet dans le secteur de la presse écrite devait renouveler la démocratie, proposer une nouvelle économie et développer des formes de journalisme multimédia. Les recherches issues des sciences de l’information et de la communication ont démontré que le secteur a, certes, connu des transformations mais peut-être pas celles attendues initialement. Cet article ne propose pas de revenir sur les acquis de ces recherches. Le point de vue développé ici est que le succès d’une innovation technologique consiste à mettre en signe des activités et des contenus, qui ont un sens socio-économique : médiation sociale et modèle d’affaires viables. C’est cette capacité à « designer » des formes et des activités nouvelles pour les usagers et à les rendre monétisables, qui nous semble centrale dans le processus d’appropriation d’une innovation technologique par des acteurs économiques."
"Cet article présente les raisons et les conditions de la réalisation d’une cartographie des formations de niveau masters en Sciences de l’Information et de la Communication en France. Depuis quelques années, la SFSIC (Société Française des Sciences de l’Information et de la Communication) en particulier au travers des travaux de sa commission formation, s’interroge sur la composition de l’offre de formation en masters en Sic et sa lisibilité. Référencer ces formations, les catégoriser doit permettre d’en analyser la cohérence d’une part et d’en renforcer la lisibilité d’autre part. Cette cartographie intéressera aussi bien les responsables de formations de niveau master que les futurs étudiants. Dans la pratique, ce travail s’est heurté à de nombreuses contraintes d’ordre méthodologique et technique : Comment identifier l’ensemble de ces formations, quelques informations retenir, comment définir les catégories de classement ? Devait-on reprendre les normes classificatoires du Ministère de l’Enseignement Supérieur ou définir des critères plus compatibles avec les réalités du terrain… Une fois l’identification des données réalisée, nous nous sommes posées la question de la mise en forme et en image de ces données et leur communication. Cet article fait état de l’avancement de nos travaux et présente une première version de cette cartographie des masters en SIC."
"Le banal objet manufacturé a pris une place de plus en plus importante sur la scène artistique depuis son irruption, via le geste de Marcel Duchamp en 1913. De 1913 à aujourd’hui, cet objet pose problème : 1/car il est à la fois objet quotidien et objet d’art ; 2/ car exposé, il est soumis à un dispositif communicationnel qui tout à la fois en montre le sens et en génère ; 3/ car plus l’objet se réduit plus le discours tissé autour est important. Le sens qu’il revêt au long du siècle change et une évolution se dessine : plus l’objet tend à s’abstraire, à se numériser plus il va revenir sous la forme du discours et de la communication nous donnant à voir quelque chose d’irréductible, ce que l’homme dépose en lui de fondamentalement humain. Envisagé sous les angles esthétique, anthropologique et institutionnel, l’objet, apparemment simple, nous montre son indéniable complexité. Il est fonctionnel, utile, décoratif parfois, objet d’art plus rarement et en lui se fixe quelque chose du sujet qui l’utilise, le regarde, le crée. L’objet, comme paradigme de la communication humaine, suit, induit, en tout cas entérine les progrès et les régressions de la société. Et l’objet d’art, dans les dispositifs d’exposition de plus en plus prégnants, interactifs et sophistiqués, est un révélateur essentiel pour une meilleure saisie du présent."
"Les quatre ouvrages à caractère philosophique d'Henri Poincaré (trois publiés de son vivant, un post mortem) ont eu un succès public très important pour son époque. L'aura du personnage y contribua, mais l'écho qu'en fit la presse joua aussi un rôle certain, même si c'était sur des récupérations idéologiques de son oeuvre dont il eut à se défendre. Nous montrerons donc comment La science et l'hypothèse, La valeur de la science, Science et méthode, et enfin Dernières pensées, furent reçus dans la presse grand public, et comment la pensée de Poincaré fut parfois détournée ou affublée de qualificatifs qui ne lui correspondaient pas ou que partiellement."
"À travers une recherche expérimentale de la réception, envisagée sous l'angle du traitement socio-cognitif du discours politique, on montre qu'une expressivité personnalisée, comparée à une expressivité impersonnalisée, favorise la mémorisation, et ce plus encore en association avec une argumentation linguistiquement marquée. Mais ce meilleur traitement de la surface linguistique n'est pas obligatoirement associé à la construction des inférences nécessaires pour la compréhension."
"Le « plissement numérique » du monde est en cours et ce processus affecte les socles anthropologiques de nos sociétés. Le tissage continu, des écritures, des flux et des données, des êtres et des objets, de leurs pratiques, bref l’écologie de ces relations, de ces nouveaux territoires, ne cesse de croître sous ces conditions. Ces transformations sont loin d’être consensuelles et elles mettent en tension les agencements socio-techniques, cognitifs, économiques, environnementaux, culturels, professionnels… Cet ouvrage a pour but d’appréhender des « débats du numérique » en les éclairant de regards issus de champs disciplinaires différents. Sont ici rassemblées des contributions de chercheurs et de praticiens analysant le passage à de nouveaux modes de gouvernance du web et des collectifs numériques, à de nouvelles économies politiques des savoirs et des « Data » : la régulation d’Internet et la complication croissante de ses infrastructures et des acteurs qui les produisent ; le renouvellement puissant des « Commons » et des intelligences collectives soutenu par le mouvement hétérogène de l’Open Data ; le web des données et la question politico-cognitive des écritures ; l’hegemon du Datamining au coeur du marketing ; la montée du Data journalisme. De même, le déploiement récent de dispositifs socio-numériques en organisation est examiné en appelant à une mise en débat des sémio-politiques qui caractérisent le management et les modes d’existences au travail. Est également interrogé, le monde militaire, sa quête insomniaque d’efficience et de performativité. Enfin, le monde de l’art est lui aussi ébranlé par la matière numérique comme nouvelle substance d’expression."
"Le processus d'urbanisation est protéiforme. Les transformations qui affectent les écologies urbaines, en particulier sous les conditions de l'extension du milieu numérique sont profondes. La place grandissante des infrastructures des réseaux numériques et leurs traductions contrastées en terme d'organisation politique et économique, l'explosion de l'Internet des objets, la prolifération des interfaces nomades, et la production compulsionnelle de « Data », tout cela travaille les modes de gouvernance, les modes d'existence ainsi que les processus de subjectivation qui les accompagnent. En prenant comme point d'appui la question des « smart cities », des « data cities », cet ouvrage examine tout d'abord les tensions qui parcourent les divers modèles politiques, organisationnels des villes et le rapport entre interfaces urbaines et techno-politiques d'infrastructure décentralisée. En réunissant différentes perspectives (de chercheurs, d'urbanistes, de responsables de projets) et à partir de nombreux exemples à travers le monde, sont interrogées les formes d'actualisation des intelligences et des modes d'existence dans la « ville numérique ». Est notamment examinée la convergence entre la MOBIlité du téléphone devenu ordinateur et l'ubiQUITE d'Internet devenu 2.0 et ses effets urbains. L'analyse concrète de cas spécifiques (Ville de Rennes et Région Nord-Pas de Calais) dessine une réflexion sur l'évolution des gouvernances urbaines où se négocie une véritable rupture de l'économie politique des territoires. De façon plus théorique, de nouvelles notions pour dire les états successifs et en tension de l'urbanisation sont énoncées et discutées. Ouvrant sur la ville émotionnelle comme milieu de l'expérience de soi, une perspective anthropologique de l'intime invite à penser la ville comme agencement désirant, milieu riche de production de subjectivités, à la traversée de la prolifération des interfaces et objets nomades. Enfin, éloigné des réflexions sur les formes « datacentriques » de l'Urbain, un regard aigu est porté sur les pratiques urbaines, prises dans une autre histoire, en Algérie."
"Peut-on tomber du sommet d'une colonne virtuelle ? La question peut paraître étrange. Or, immergé dans un environnement virtuel, un individu peut éprouver la sensation paradoxale d'être dans un lieu dans lequel il n'est pas réellement. Brouillant la frontière entre le réel et l'illusion, la réalité virtuelle permet d'expérimenter un type de présence au monde qui ne peut se réduire à une simple illusion mais qui, en même temps, ne peut être réelle. Ainsi, loin d'être seulement une prouesse technologique, la réalité virtuelle n'exige-t-elle pas de repenser notre rapport au monde, qu'il soit réel ou virtuel ?"
"Ce travail étudie l’impact de la cyberlangue, induite par le développement des technologies de l’InfoCom, sur les situations de communication médiatisée, jusqu’à la ritualisation des interactions sur les dispositifs sociotechniques.Ce travail mobilise un pluralisme théorique et méthodologique, une approche quantitative et typologique. Il s’appuie sur un terrain de données nativement numériques sur un forum de discussion en ligne, dans le domaine de la téléréalité. En appui sur les concepts de langue et langage, réduction de l’incertitude, cyberlangue, dispositif, communauté, réseau, rites et ritualités, et sur les théories de la conversation, du lien social, du lien rituel, de la «posture intentionnelle » et les rites d’interaction en ligne, une étude exploratoire permet d’identifier des procédés d’écriture élaborés et créatifs et d’établir une typologie enrichie des marqueurs de la cyberlangue.Dans une enquête, la confrontation des usages aux représentations permet de comprendre la perception de la cyberlangue par les internautes. Dans les micro-communautés identifiées par la mesure des interactions, la présence des interacteurs, le poids et la concentration des marqueurs cyberlangue et la polarité du discours interactionnel, l’étude de situations rituelles permet d’identifier une typologie de rites d’interaction associés à certains marqueurs cyberlangue et de corréler les potentialités de pacification de la cyberlangue aux intentionnalités de distanciation et de compensation des interacteurs, dans une dynamique de coconstruction pour le maintien du lien social."
Le renseignement peut être défini comme un processus de connaissances utiles aux décideurs politiques et militaires. Comment construit-on cette connaissance ? Le but de cet article est d’étudier le renseignement sous l’angle épistémologique.
"notre questionnement repose sur la difficulté de faire émerger durablement des compétences et des pratiques en matière de recueil et de traitement d'information, que ce soit chez les individus (étudiants, chercheurs, salariés) ou niveau des organisations (entreprises, collectivités, associations). A partir du concept d'intelligence économique que nous présentons comme un méta-modèle complexe, nous définissons un modèle progressif et adapté de transfert des connaissances que nous appliquons au sein de l'IUT et de l'Université de Toulon et du Var, dans le cadre du projet Epices (Etudes Prospectives en Intelligence Compétitive Economique et Stratégique). Cet article repose sur les travaux de recherche que nous avons exposés dans une thèse doctorale soutenue en décembre 2002."
"La production de dispositifs numériques de formation peut se comprendre à travers une économie de signes. La première économie tient dans les discours de modélisation d’objectifs à atteindre, la seconde dans la traduction de ces objectifs dans des formes pouvant reprendre des formes connues et en proposer de nouvelles, la troisième, enfin, dans l’externalisation par les consommateurs d’activités qu’ils auraient sinon à réaliser par eux-mêmes, voire qu’ils ne réaliseraient pas par manque de temps, de moyens, de compétences nécessaires ou de désir. Pour étayer ce propos, nous nous appuierons sur un programme de recherche en cours, de développement d’une plateforme de ressources pédagogiques par un consortium d’industriels, financé par le Ministère de l’Éducation Nationale. Ce terrain permettra de penser le numérique comme une sémiotisation du social."
"L’ObTIC (Observatoire des Technologies de l’Information et de la Communication de la Région Provence-Alpes-Côte d’Azur) a réalisé une étude traitant la question de la culture numérique chez les jeunes (16-24 ans) et de son rôle dans leurs trajectoires d’insertion sociale et professionnelle. Il s’agissait d’aller au-delà des données sur les équipements utilisés et les usages dominants des jeunes que l’on trouve habituellement dans ce genre d’enquête, pour chercher à apprécier la nature de leur(s) culture(s) numérique(s) et d’évaluer le rapport entre leur maîtrise numérique et leur faculté à s'insérer dans la sphère économique. Lors d’un séminaire qui s’est tenu à Marseille le 22 octobre 2012 de nombreux intervenants et participants à même d’éclairer la problématique du projet avaient été rassemblés dans une fructueuse démarche d’échanges et de conseil : http://emergences-numeriques.regionpaca.fr/observatoire-tic-etanimation/ obtic.html Préalablement à l’enquête, un premier travail a consisté à proposer une assise théorique à l’expression « culture numérique » et à la rendre opératoire d’un point de vue empirique. Notre définition s'appuie sur trois dimensions théoriques définies ci-après et qui ont constitué les trois axes d’élaboration de l’enquête. Celle-ci a reposé sur un questionnaire élaboré conjointement par des enseignants-chercheurs de Toulon et l’équipe de l’ObTIC et administré selon une méthodologie détaillée ci-après. L’étude est précédée d’un résumé synthétique qui présente les principaux résultats et enseignements."
"Les sciences cognitives ont apporté des éclairages décisifs dans plusieurs domaines relevant des sciences de la culture. Notamment la faculté de langage a été l’objet de développements spectaculaires apportant des arguments en faveur d’une vision naturalisée des artefacts linguistiques. C’est actuellement à l’écriture, le premier média complexe de l’humain, scellant l’union du langage verbal avec le système de signes graphiques, de se prêter à l’exercice de l’épistémologie biologisante et de confirmer l’existence de ponts entre la génétique du substrat cérébral, l’épigénétique, le mimétisme comportemental et finalement l’apprentissage culturel. La localisation cérébrale des aptitudes scripturales découverte récemment dans la zone dite d’Exner, plaide pour cette circularité corps/culture et permet d’envisager de nouveaux programmes de recherches portant sur l’ensemble des médias. La diffusion des médias multisensoriels numériques apporte une nouvelle donne à cette perspective. En effet, l’usage intensif de ces médias provoque un impact ultrarapide sur l’architecture fonctionnelle du cerveau humain. Cette situation inédite, où l’apprentissage d’un nouveau média et l’impact cognitif que son usage suscite, coïncident avec le processus de sa conception et de sa mise en circulation, confère aux concepteurs le rôle de « pro-généticiens »."
"Nous proposons un survol rapide de ce que pourrait être un ensemble (ouvert) de problématiques et recherches menées à partir du champ des SHS et visant les multiples dynamiques du plissement numérique du monde en cours dans le cadre de l’Internet des Objets. Nous indiquons dans le même temps quelques repères pointant l’étendue de l’évolution bio-technique et bio-politique par des moyens non-organiques, évolution qui est au coeur de l’artificialisation du monde ainsi que l’extension des processus de robotique et de l’automatisation. Nous montrons aussi les effets qu’à l’Internet des Objets sur nos milieux d’intelligence ainsi que son couplage de plus en plus massif avec la transformation du milieu urbain. Cette artificialisation des milieux urbains allant de pair avec le renforcement de la classe moyenne consumériste mondiale. La tendance à converger de manière de plus en plus explicite, d’une physique sociale et d’une biosocialité radicalement nouvelle, ouvre un champ vaste de réflexions et soulève de nombreux problèmes. L’Internet des Objets exprime la profondeur et le caractère incertain de l‘évolution anthropologique et collective qui s’actualise sous nos yeux."
"À partir de l'interrogation sur le potentiel du Web en tant que nouveau média électronique dans la promotion des comportements éco-responsables, et sur les stratégies de sensibilisation les plus efficaces possibles susceptibles d'être mises en œuvre dans le contexte du Web, ce travail propose de comparer l'efficacité des dispositifs Web persuasif et engageant grâce à la mesure des comportements effectifs des internautes exposés à ce type de dispositifs. Une expérimentation conduite dans une grande surface de mobilier et de décoration a consisté à soumettre les sujets à un des deux sites Web conçus pour inciter les internautes à réaliser un geste éco-citoyen (installation au domicile de la personne d'au moins une nouvelle ampoule à économie d'énergie). Les données, collectées par le biais d'un outil de Web analytics enregistrant les statistiques de navigation, et d'un mini-questionnaire téléphonique réalisé 14 jours après l'intervention, ont permis de corroborer l'hypothèse selon laquelle un dispositif Web exploitant les stratégies engageantes est plus efficace en termes d'effets sur les comportements attendus qu'un dispositif Web basé uniquement sur l'argumentation. L'ensemble de nos résultats empiriques ont permis de proposer un modèle pour un dispositif de sensibilisation sur le Web, modèle que l'on pourrait inscrire dans le champ de la communication engageante sur le Web et qui pourrait être exploité afin d'accroître l'impact comportemental des campagnes de sensibilisation sur le Web relatives à la promotion des comportements éco-responsables."
"La première triennale de Guangzhou, en Chine, au Guangdong Muséum of Art, le Musée d'Art Contemporain de Canton, nous offre un bel exemple de la globalisation de l'art contemporain."
"Il s'agit dans cet article d'appréhender les effets de la représentation médiatique de l'institution judiciaire par le documentaire. Si le documentaire poursuit un objet démocratique par la diversité des regards qu'il propose sur la sociabilité, la médiatisation de la scène judiciaire, emporte avec elle le risque d'atteinte à l'intégrité de la figure du juge. En effet, alors que certaines productions placent l'emphase sur les attributs symboliques de son autorité du juge (cérémonial, apparat etc.), d'autres cherchent, au contraire, à saisir la réalité de l'homme ou de la femme incarnée sous la robe et affaiblissent sa fonction sociale. Dès lors une véritable démarche éthique doit présider dans la représentation de la Justice sur d'autres scènes."
"Le phénomène 2.0 marque l'apparition d'un nouveau paradigme de communication dit ""many to many"". Le web 2.0 traduit alors le passage de l'interactivité à l'interaction et contribue ainsi à la construction de réseaux qui ne se basent plus sur l'échange d'informations mais sur le partage du savoir. Le concept s'installe dans tous les aspects de la vie sociale : éducation, management, recherche etc. qualifiés désormais d' « éducation 2.0 », de « management 2.0 », de « recherche 2.0 ». Les valeurs propres à ces champs sont : travail collaboratif, innovation, absence de hiérarchie au sein d'un environnement caractérisé par une abondance d'information dans un monde plat [Flat world]. L'intelligence compétitive évolue donc et bénéficie à la fois d'un nouvel état d'esprit et de nouveaux outils pour devenir l' « intelligence compétitive 2.0 ». Elle trouve alors une légitimité réaffirmée par sa pertinence dans l'appréhension de tous les secteurs d'activité sociale."
"On s'attache dans cet article à poser un certain nombre de réquisits pour une Narratique Générale, à en examiner certaines difficultés, en particulier d'ordre sémiotique, dans le cadre de la grande transformation numérique. Cette transformation numérique est regardée de manière simplifiée, à travers le prisme du ""devenir empire"" selon Negri et Hardt, sous les conditions du ""devenir entrepreuneurial"" et du ""devenir expérimental du politique"". On porte une attention particulière dans le cadre de la strate anthropologique ""internet"", à l'émergence du Data Mining comme narration impériale, comme grand récit des sociétés performatives. On évoquera ainsi la sainte et obsédante trinité: Performation / Prédiction / Préemption qui sert aux sociétés de veille enchassées dans le développement du capitalisme mondial intégré comme processus hétérogène et conflictuel. Enfin on s'intéressera à la prolifération des formes courtes à travers, entre autres, le déploiement de Twitter, mais pas uniquement, et à certains de ses effets politiques et socio-cognitifs."
"Le temps passé sur les réseaux sociaux augmente régulièrement (en 2012, 18% du temps online). Si le temps moyen des visites sur Twitter est encore faible (21min/mois) au regard de facebook (6, 75h/mois1) ou plus généralement du web, il faut observer une croissance de l'ordre de 300% de l'usage de Twitter2 et des pratiques croisées de plus en plus fréquentes avec des médias plus anciens comme la télévision. Avec ces pratiques "" généralistes "", des pratiques professionnelles se dessinent.... C'est dans ce cadre des pratiques croisées généralistes et professionnelles que nous observons l'usage de Twitter dans le contexte de la publication scientifique"
"Le présent texte questionne l'édition de recherche numérique comme mise en évidence et prise en charge du caractère processuel, instable de production des textes. Notre réflexion s'inscrit dans le mouvement qui conduit à développer des dispositifs et des outils collaboratifs d'édition, de filtrage, de repérage, d'agrégation pertinente, non seulement de documents, et de ressources, mais aussi de conservation partielle des traces du travail critique de (re)-lecture et de (re)-écriture. Ce mouvement s'est par exemple, popularisé à travers l'expérience de Wikipédia. Nous sommes à présent dans une phase d'extension et de différenciation de ces modèles."
"Les dispositifs images-sons numériques ont souvent été étudiés sous le biais de l’ouverture de l’oeuvre, des rapports transformés entre auteurs et spectateurs ou bien encore, sous l’angle des correspondances images/sons1. Nous avons choisi de développer une approche spécifique : il nous a semblé, en effet, que la question d’une nouvelle écriture numérique musicale méritait d’être posée, une écriture prenant en compte les spécificités du média digital (cinétique, dynamique, multi-modal, interactif et génératif) dans une véritable médiographie appliquée à la musique. Pour cela, nous sommes partis de la notion d’idéographie dynamique (I.D.) avancée par Pierre Levy (Levy, 1991) au début des années 1990 pour tenter de l’appliquer au musical. Après avoir repéré ce qui nous semblait être les caractéristiques principales de l’ I.D., nous avons cherché à repérer ces caractéristiques dans quelques dispositifs numériques actuels. Dans une méthode comparative, nous avons confronté ces dispositifs à l’analyse de partitions de compositeurs des années 1950, 60, 70 liés à l’esthétique de l’ouverture de l’oeuvre et qui ont développé de nombreuses pratiques de notations novatrices (avant tout partitions graphiques, conceptuelles verbales, d’actions etc.). Cet article a été écrit à quatre mains dans une collaboration étroite entre une musicologue et un spécialiste des NTIC."
"THEMATIQUE Les pratiques collaboratives, le numérique et la participation : comment ça marche ? La dynamique de projet collaborative dans une perspective pédagogique : l'apport des techniques de créativité La pédagogie par projet implique des modes d'apprentissage différents au sein duquel la relation communicationnelle classique entre l'enseignant et l'apprenant change de format (elle n'est plus pensée sur le mode émetteur-récepteur). D'autres modèles communicationnels accompagnent cette nouvelle forme de pédagogie introduisant un espace d'échange plus horizontal et d'interactions plus fortes entre les apprenants : ⇒ Ce type de projet bouscule les pratiques traditionnelles de l'enseignant"
"A l'instar des expérimentations européennes, ça et là en France, se mettent en place depuis 2003, des projets d'intelligence territoriale (cf régions de la Basse Normandie, de la Lorraine, Ile de la Réunion, région d'Aquitaine etc.) ; ceux-ci ont pour ambition de mettre à contribution dans une logique de développement durable, un traitement mutualisé de l'information qui dépasseront les huis clos sectoriels. Outre les institutions, la société civile, et les habitants du territoire, on constate que les entreprises et notamment les Pme/Pmi, sont des partenaires naturellement intéressés par cette démarche. Les processus de filière économique comme les organisations participantes gagnent ainsi quelques gradients d'anticipation des menaces et réaffirment le territoire comme un lieu ressource commun à défendre. Au-delà des systèmes de traitement de l'information fonctionnant parfois au sein de ces organisations ou filières, l'articulation des actions internes de capitalisation informative aux actions locales d'intelligence territoriale, permettent d'acquérir un effet levier d'une visibilité européenne sinon mondiale (Herbaux, 2007). Néanmoins, ces expériences connaissent des fortunes diverses dont l'abandon progressif du projet par les entreprises initiées est l'un des effets le plus couramment observé. En appui d'un apport théorique en trame de cette communication, nous rapportons les résultats d'une enquête de type Delphi réalisée en 2005 sur 53 entreprises du Nord-Pas de Calais engagées dans un processus d'intelligence territoriale depuis 2003 ; celle-ci constatait que 43 entreprises sur les 53 engagées n'avaient pas poursuivi leur projet interne de mutualisation des flux d'information et se contentaient par défaut, des résultats sectoriels d'une veille régionale publique. Au-delà d'une apparente démission d'un processus engagé, nous pouvions nous interroger sur cette apparente discrétion d'un volet de partenaires associés dans la gouvernance locale. Ce travail aboutissait néanmoins à un consensus des acteurs interrogés autour de quelques constats ; notamment dans la démarche dont ils ne reniaient pas la finalité mais dont les exigences obligeaient à une modification sensible de leur culture interne. Après les premières réponses convenues : « sécurisation du patrimoine informationnel, nouveaux choix en -investissement temps-, manque de moyens, priorités différentes etc. », un questionnement récursif et différencié chez les intéressés, établissait que l'abandon progressif des pratiques et engagements étaient en relation avec un ensemble de facteurs liés à l'humain dans ses aspects relationnels et cognitifs, privant ainsi le projet de ses ancrages fondateurs. Ce constat intervenait en écho d'une implication proposée par Girardot en 2004 sur le thème de la « gouvernance multi-niveaux ». Bien que l'aspect financier soit un des facteurs de pérennité d'un investissement régulier en temps-homme, ce critère apparaissait progressivement pour les acteurs enquêtés, comme accessoire du projet général au profit de plusieurs postures posées en pré-requis. Sur la base d'une synthèse réalisée sur les résultats de l'étude, nous proposons ainsi cinq facteurs clés de succès à promouvoir au sein des organisations pour le développement des logiques de mutualisation d'information. Pour ce faire, notre proposition de modèle appelé « CADIE » (Communication, Appui, Durée, Implication, Ecoute) propose quelques postures auxquelles les organisations doivent s'attacher pour un développement pérenne de leur intégration dans une démarche d'intelligence territoriale. Les limites de notre proposition souffrent de la faiblesse de l'échantillon à notre disposition et de la lisière régionale de notre recueil de données. Cette expérimentation dupliquée dans différents territoires de l'Europe, bénéficierait d'une onction multiculturelle et offrirait alors un modèle européen d'approche préliminaire des logiques d'intelligence territoriale."
"Le printemps est habituellement la saison où se tiennent les colloques Ticemed. Le douzième de la série, Ticemed12, préparé à l’Université Panteion d’Athènes pour les 7-9 avril 2020 a dû être annulé quelques jours avant son ouverture en raison de la pandémie de Covid-19. Des mois de travail desconférenciers, des revieweurs et des organisateurs1 risquaient d’être effacés purement et simplement. Or il est de tradition dans ces colloques de publier les textes des conférences dans des préactes avant même la tenue du colloque. Les bouleversements de l’année écoulée n’ont pas permis depublier dans ces délais, mais il a paru indispensable aux organisateurs et à la communauté Ticemed de réaliser l’équivalent de pré-actes et de les mettre à disposition du public sur le net. Cela est l’objet du présent ouvrage."
"La notion d’interculturalité organisationnelle émerge dans les entreprises, à l’heure où la mondialisation s’impose comme une évidence dans l’évolution des organisations. Il n’en demeure pas moins que les dimensions humaines et culturelles constituant l’interculturalité structurent ces entreprises désireuses de s’internationaliser. Avec la notion de culture d’entreprise, s’est affirmée l’importance du manager, mais aussi celles de relations interpersonnelles, dans la préservation et la promotion de l’image et de l’identité de l’entreprise. Aujourd’hui, s’y ajoute la dimension de l’interculturel ouvrant le débat sur le métissage surtout lors de fusions d’organisations internationales. C’est au moyen d’une démarche communicationnelle et anthropologique que cette recherche tente de redéfinir la notion d’interculturalité comme outil à part entière. Cette thèse examine la problématique suivante : L’interculturalité peut-elle être un dispositif de médiation des organisations en situation de fusion-acquisition ? Cette recherche repose sur l’analyse des données qualitatives révélées par le logiciel Alceste qui contribue à une approche herméneutique du contenu des discours managériaux et tentant plus généralement de rendre intelligible la communication interculturelle sur 138 journaux internes, soit 723 articles pendant 5 ans. Grâce aux analyses de contenus annuelles, cette recherche contribue à une meilleure compréhension des discours fédérateurs et interculturels dans les rapprochements organisationnels. Elle propose aux entreprises de mieux considérer les outils de communication des organisations comme des dispositifs de médiation interculturelle complémentaire au management."
"Notre point de départ est que l’organisation, – entreprises, collectivités, ONG, institution territoriale, etc.—, tend à se revendiquer écocitoyenne, ce qui sous-tend qu’elle s’arroge un rôle d’animateur de territoire, facteur d’innovation et de changement dans le domaine public, professionnel mais aussi privé (De Backer, 1998 ; Cohen-Bacrie, 2006). Dans un contexte de crises économiques répétitives depuis l’éclatement de la bulle Internet en 2000 avec comme points d’orgue la crise des « Subprimes » (2007) et actuellement la crise de la dette souveraine, les citoyens se considèrent comme les premiers impactés par ce contexte difficile et de fait, la grande majorité d’entre eux veulent repenser les modèles économiques et sociétaux actuels. Les aspects économiques et sociaux du développement durable sont donc naturellement mis en exergue. Ils rejoignent les aspects environnementaux déjà omniprésents, comme en témoigne la longue liste des « situations insoutenables » actualisée par J. Theys (2002). Ils s‘adossent à un pilier culturel encore émergent mais renforcé par les effets d’une mondialisation agissante. Car la mise en visibilité des problématiques du développement durable « par des voies d’entrée planétaires par trop totalisantes et globalisantes (…) (et) invite à de nécessaires et difficiles ré-enracinements dans le local » (T. Berryman, 2004 - 2005). Elle convie à penser le territoire et les nouvelles façons d’aborder l’espace et le temps, les patrimoines et les identités individuelles et collectives. Elle nécessite une mise en culture du développement durable basée sur une pédagogie de l’appartenance et de l’engagement, apte à résoudre de façon féconde les tensions entre identité et altérité, entre globalité et localité, deux couples de forces caractéristiques de la globalisation. (L. Sauvé, 2006-2007). Elle convoque une communication dans, par et à partir le territoire visant une mise en liaison ou en re-liaison destinée à résoudre les crises et à favoriser de nouvelles alliances sociales et territoriales (M. de Certeau, 1993). Dans ce contexte, Le développement durable appelle de nouvelles formes de communication et de nouvelles pratiques de médiation. Celles-ci assurent, dans l’espace public, la co-construction et l’appropriation singulière par leurs acteurs des informations qui constituent la culture collective caractéristique d’une identité, d’un groupe social ou d’un pays, à un certain moment de son histoire et de son projet. Car, si la communication exerce une fonction de médiation dans l’espace social, c’est qu’elle organise et structure les expressions des appartenances dont les acteurs sociaux se réclament dans cet espace. Les questions liées au développement durable et à l’écocitoyenneté (Roesch, 2003) apparaissent ainsi au cœur des questions de sociabilité et de médiation culturelle que couvre le champ de recherche des sciences de l’information et de la communication. Cet axe du laboratoire se focalise sur deux champs interconnectés : celui de la communication du développement durable des organisations dans toutes ses composantes : environnementale et économique mais aussi sociale et culturelle ; et celui de la communication écocitoyenne et de la médiation des patrimoines des territoires dits durables."
"Fondé sur un exemple de réalisation de dispositif informationnel, nous présentons un mode de navigation qui, par l'exploration des données factuelles et textuelles, sert la construction de connaissances en génomique."
"L'interface graphique de l’application numérique, soumise à l’injonction du geste de l’utilisateur, prolonge le corps de celui-ci. Quel rapport s’établit entre l’image qui émerge et celui qui la convoque ? Que reste-t-il du message du concepteur construit par un métarécit singulier ? La diversité de chaque exploration favorise-t-elle une interprétation individuelle ? Notre problématique porte sur la tension du concept de design entre sa stratégie de communication et la contingence de son émergence liée au métarécit de l’utilisateur en liberté surveillée. Pour répondre à ces interrogations, nous émettons l’hypothèse que l’interactivité génère un récit individualisé.Nous guidons notre analyse selon trois axes, celui du design tout d’abord, ensuite celui de l’individualisation, qui souligne la dimension sensible qu’apporte l’interactivité du design au moyen d’une délégation d’énonciation envers l’utilisateur, et enfin celui du récit, qui organise le lien narratif entre le message iconique et l’utilisateur. Trois variables sont retenues (esthétique, médiatique, sémantique) et croisées afin de parvenir à circonscrire la notion de design interactif.Notre recherche pratique s’appuie sur l’analyse du site web Communicate de la webagency londonienne Hi-ReS!. Cette étude de cas rend compte des intentions du concepteur et permet d’évaluer la tension entre le message polysémique émis et le message monosémique reçu. Une expérimentation d’oculométrie a été mise en place afin d’évaluer la portée de l’interactivité sur l’attitude et le ressenti des utilisateurs. Nous avançons alors la figure métaphorique du « dess@in » afin de modéliser le design interactif comme un corps malléable individualisé, système négocié entre la stratégie de communication du designer et la production singulière de l’utilisateur."
"Ce texte présente le mouvement appelé Libre Accès (Open Access). Après avoir replacé ce mouvement dans le contexte de la communication scientifique, l'auteur revient sur sa définition et les enjeux associés, en s'interrogeant notamment sur sa mise en œuvre et sur la nature des documents susceptibles d'être partagés. Le développement du mouvement du Libre Accès est observé à travers sa réception en France et l'exemple des Archives Ouvertes."
"Quand on aborde l'aspect sociopolitique et stratégique de la diffusion des technologies numériques, deux attitudes ressortent immédiatement. Soit on vante les bénéfices que l'on peut espérer d'un accès plus rapide et plus général à une information mondialisée, soit on constate le fossé qui se creuse entre ceux qui ont les moyens d'accéder à cette information et ceux qui ne les ont pas, et l'on parle de « fracture numérique ». La vision de la société de l'information comme un « cyberspace » au sens de P. Lévy n'est cependant qu'un aspect, important certes, mais partiel de la problématique de diffusion des technologies de l'information-communication (Tic). Délaissant les voies de l'économie et de la production de matériels numériques, l'objet de cette communication est de réfléchir sur le rôle symbolique de ces technologies dans la relation de l'Europe avec le continent africain. La thèse défendue est que les frontières sont contingentes et poreuses ; ce sont les cultures, les idées et les connaissances qui diffusent à travers ces pores ; le vecteur privilégié de cette diffusion est le dispositif de communication qui s'est construit sur les Tic plus que les voyages qui ont marqué la première époque de la relation intercontinentale. Plutôt que de frontières, on parlera de croisements, de foyers culturels et d'identités mouvantes et vivantes qui communiquent par le biais de la « traduction » dans l'acception de P. Ricœur."
"L'auteur constate d'abord que Google, la marque, le projet, l'utilisation -le « googling »- sont des faits sociaux avérés par les nombres -d'internautes, de requêtes, d'usages- et par les signes de l'adhésion -linguistiques, économiques, sociaux. Une analyse sociolinguistique des discours des responsables de Google et des usagers indique que le fait social « googling » se traduit par l'émergence d'une culture et d'une communauté mondiale qui la partage. Celle-ci est sous-tendue par le langage et aussi par les mythes qui ont été créés et largement entretenus par les propriétaires de la marque « Google Inc. ». La conclusion est que l'organisation actuelle du marché des services sur l'internet fait que Google Inc. est quasiment la seule institution à connaître la population des googleurs. Pour ne pas être soumis à ce monopole, si confortable soit-il, l'auteur propose de développer un programme de recherche sur les usages et usagers de Google."
"Les (n)tic ont envahi la classe et l'université depuis les opérations du type « plan informatique » en France comme dans les autres pays du monde, quel que soit leur niveau de développement. L'attention se focalise généralement sur ces changements dans l'institution. Mais un autre facteur plus flou et moins bien étudié est l'impact des (n)tic sur les jeunes générations qui sont nées depuis ces premières « nouvelles technologies » : les jeux vidéos, les simulations, les téléphones mobiles avec leur sms et mms, les photos numériques, les ordinateurs, l'internet et son flot de musiques et de jeux en réseau, de courriel et de surf. L'hypothèse défendue est que l'exposition intensive des tout jeunes aux objets des Tic est un des facteurs-clé à la fois de l'effet Flynn (l'augmentation générale du QI et de la demande de stimulation intellectuelle) et de la crise mondiale de l'éducation, et qu'une des voies de résolution de cette crise passe par l'appropriation de la culture du cyberspace par les systèmes éducatifs. Si la technologie a des effets pervers sur l'éducation et la culture, qu'au moins on cherche par un nouveau projet pédagogique à en exploiter les potentialités"
"L'expression « fracture numérique », en anglais digital divide, a été introduite à la fin des années 90 par similitude avec celle de « fracture sociale » qui avait cours dans le débat politique. Elle se voulait une alerte à l'encontre des chantres de l'internet qui pensaient que le développement des technologies de l'information communication allaient créer une société sans classe fondée sur une disponibilité universelle du savoir et une démocratie immédiate. Elle visait à rappeler que l'accès et les bénéfices de l'internet étaient pour le moment l'apanage des sociétés les plus riches et les plus avancées, et que la plus grande partie de la population mondiale n'y avaient pas accès. Cette population se trouvant non seulement parmi les classes défavorisées des nations occidentales mais encore et surtout dans le monde en développement. La fracture numérique est ainsi devenue un des slogans des mouvements anti-mondialisation. La présente communication vise à recenser quelques données objectives sur le « sous développement numérique » et à montrer que les indicateurs couramment utilisés par les tenants de la fracture numérique sont partiels et sous-estiment les potentiels actuel et futur de l'internet pour le développement durable. Mettre la technologie au service du développement durable est une posture éthique qui met ces préoccupations à l'échelle de la planète."
"Face aux défis de la rémunération des auteurs dans l'économie du web 2.0, la réponse de l'Etat réside principalement dans la protection d'un modèle classique dit « mécanique ». En réalité, ce sont les multiples intermédiaires, producteurs, distributeurs,... d'une véritable industrie, qui trouvent un intérêt à ralentir le changement de modèle économique. L'évolution des usages et l'appropriation par le plus grand nombre des applications d'échange et de partage, annonce le déclin de ce modèle classique au profit de la gratuité et de la publicité."
"Les Tic (Technologies de l'information et de la communication) bouleversent nos rapports interindividuels. Elles impactent évidemment l'élève, et plus ou moins rapidement les enseignants. Entre eux, s'ouvre le fossé de la familiarité avec les technologies. Mais d'autre part la confrontation de l'homme avec l'apprentissage est aussi ancienne que l'émergence de la vie en société et le développement du néocortex cervical. La thèse défendue ici est que l'enseignant est toujours un chercheur, qu'il en ait la reconnaissance institutionnelle ou non."
"Le Liban a fait face à 15 ans de guerre civile qui ont détruit non seulement son économie et son infrastructure, mais aussi son image de communication. En effet, dans cette ère de mondialisation où la communication joue un rôle essentiel dans le tissage de liens sociaux et où les médias ont une place importante dans l’expansion des différentes cultures, les libanais perdent cette habilité et classent l’autre selon sa confession et son opinion politique. Les médias, en particulier les chaines télévisées locales (fondées par les différents leaders confessionnels ou politico-confessionnels), n’aident pas l’opinion publique à mieux connaitre l’autre. La cible la plus fragile est les jeunes Libanais nés après la guerre civile mais fortement influencés par ce que leurs parents ont vécu pendant la guerre. Ils sont peu ou pas soutenus dans la réception de l’information télévisée, l’école et le foyer n’étant pas toujours les milieux les plus adaptés pour les aider à interpréter l’actualité sous un angle d’ouverture au monde et d’acceptation d’autrui. Cette crise majeure nous a poussé à signaler un fort besoin d’établir de nouvelles bases aux standards de communication à enseigner aux jeunes en particulier dans les cours d’histoire et d’éducation civique, dans le but de les préparer à un avenir où ils auront à mieux connaitre les autres communautés et à apprendre à les accepter afin de cohabiter ensemble."
"Les moteurs de recherche s'appuient, en autres, sur des algorithmes dits « User Centric » ainsi que de géolocalisation afin d'affiner la pertinence de leurs résultats : les habitudes de navigation des internautes et leur lieu de connexion sont pris en compte lors du classement de l'information. Les campagnes de veille sur Internet sont principalement menées avec l'aide des moteurs de recherche, parmi lesquels certains utilisent ces algorithmes. Les personnes en charge de ces recherches peuvent donc à priori influencer de leurs habitudes de navigation et de leur situation géographique les résultats des requêtes : elles auraient été différentes selon la personne et le lieu. Ces résultats aidant, par la suite, à la prise de décisions parfois stratégiques au sein des entreprises, nous pouvons penser que les veilleurs influent indirectement ces décisions. L'utilisation des méthodes de navigation anonyme pour des recherches d'informations sur Internet permettrait d'inhiber l'action de ces algorithmes tout en pérennisant l'utilisation de ces moteurs de recherche : en effet, les outils de navigation anonyme bloquent, entre autres, les cookies et les adresse IP des ordinateurs, empêchant ainsi aux moteurs de recherche de personnaliser le classement des résultats des requêtes."
"L'information disponible dans les bases de données bibliographiques est une information datée, validée par un processus long qui la rend peu innovante. Dans leur mode d'exploitation, les bases de données bibliographiques sont classiquement interrogées de manière booléenne. Le résultat d'une requête est donc un ensemble d'informations connues qui n'apporte en lui-même aucune nouveauté. Pourtant, en 1985, Don Swanson propose une méthode originale pour extraire de bases de donnés une information innovante. Son raisonnement est basé sur une exploitation systématique de la littérature biomédicale afin de dégager des connexions latentes entre différentes connaissances bien établies. Ses travaux montrent le potentiel insoupçonné des bases bibliographiques dans la révélation et la découverte de connaissances. Cet intérêt ne tient pas tant à la nature de l'information disponible qu'à la méthodologie utilisée. Cette méthodologie générale s'applique de façon privilégiée dans un environnement d'information validée et structurée ce qui est le cas de l'information bibliographique. Nous proposons de tester la robustesse de la théorie de Swanson en présentant les méthodes qu'elle a inspirées et qui conduisent toutes aux mêmes conclusions. Nous exposons ensuite, comment à partir de sources d'information biomédicales publiques, nous avons développé un système de découverte de connaissances basé sur la littérature."
"Le langage en tant que Tiers organisant la représentation du Monde est au centre des questionnements relatifs au sens et à la pensée. Les Sciences de l'information dans le cadre d'une approche sémiotique sont susceptibles d'apporter un regard pertinent sur des questions généralement réservées aux sciences du langage. Il s'agit de mettre en perspective les dimensions historiques, anthropologiques, culturelles, politiques et même ésotériques de ce médium en considérant que celles-ci ne sont pas neutres dans la production du sens. L'hébreu, langue ressuscitée constitue notre terrain d'étude."
"Au Nord comme au Sud, il est maintenant admis que la mondialisation ne pourra se passer d'une certaine reconnaissance du «local» et des responsabilités qu'il prendra dans son propre développement. Ce congrès intitulé « demain la ville » organisé par la Région Bruxelles Capitale peut en être une des illustrations. Cependant, même si les expériences des vingt dernières années ont donné corps à des approches de problématiques de développement, ces expérimentations localisées peinent encore à se déployer en une dynamique régionale et durable. On sait concevoir des outils d'analyse pour une gestion décentralisée, on sait élaborer des cadres réglementaires et législatifs, mais la difficulté est de parvenir à généraliser des dynamiques durables, à les ancrer dans les pratiques des habitants et des institutions. L'innovation territoriale endogène se nourrit des signaux à l'interne comme des flux d'informations en provenance de l'externe. Si l'horizon de temps permettait au Moyen Age, une assimilation progressive des rumeurs et informations, le contexte actuel en modifie considérablement la perspective. Pour ajuster une décision, il ne s'agit plus de recueillir un goutte à goutte d'informations mais de capter un « torrent » de données qu'il faut traiter en continu pour dépister à temps la menace et éventuellement saisir l'opportunité. .A l'instar de la grande entreprise, le territoire sera dans un proche avenir plus orienté dans un rapport de forces concurrentielles où le traitement de l'information sera essentiel. Si les mutations culturelles liées au développement des TIC (technologies de l'information et de la communication) sont encore dans le domaine des études, les nouvelles formes d'affrontement indirect résultant de cette évolution technologique sont encore très mal identifiées par les acteurs du territoire. En exemple, la montée en charge des délocalisations et de la mondialisation oblige le local à puiser dans les projets à dominante culturelle et touristique pour espérer capter quelques revenus d'une population de nomades aisés. L'originalité des projets s'épuise dans la réplication de parcs d'attraction et de «route culturelle». Leur essoufflement ne résidera pas dans un aspect uniquement technique mais dans un empiètement funeste des projets qui deviendront concurrents. Le territoire doit agir sur plusieurs fronts : garder ses acquis, si possible en les faisant évoluer tout en limitant ses risques et les possibles ruptures. Pour cela une évolution du traitement de l'information doit être envisagé ; il s'agit de promouvoir et d'accompagner une véritable mutualisation de l'information par le citoyen au sein d'un processus appelé intelligence territoriale."
"Cette communication prend appui sur des recherches menées au sein de communes de taille moyenne. Nos travaux font apparaître que la mise en place d'un processus d'intelligence économique territoriale (IET) nécessite une méthodologie particulière. Pour cette raison, nous proposons dans cette communication de faire émerger une nouvelle démarche qui permet d'optimiser la mise en place d'un dispositif d'intelligence économique territoriale dans une municipalité. La modélisation que nous allons dérouler est à la fois, une production actionnable pour les manageurs ainsi que de nouvelles connaissances pour les scientifiques. Il s'agit d'un continuum d'opération à réaliser dans un délai déterminé avec des moyens et des objectifs de qualité. Ce découpage en phases va fournir la méthodologie à suivre par les manageurs territoriaux pour installer le dispositif d'IET. Ce modèle de construction répond à leur question « comment faire ?»."
"La création des archives ouvertes est née en réaction au paradoxe de l'édition scientifique : alors que le chercheur offre ces résultats, la communauté scientifique trouve un certain nombre d'obstacles pour lire ces publications. Pourtant la majorité des chercheurs accèdent aujourd'hui à Internet et pourraient se saisir de cette opportunité pour partager plus facilement les résultats de leurs travaux. De plus, l'inflation des écrits et la nécessité d'une validation dans un temps plus court pour certains résultats (souvent expérimentaux) se heurtent aux limites du modèle traditionnel de publication."
"Cet article s'intéresse aux enjeux théoriques des nouvelles modalités communicationnelles offertes par des dispositifs de type "" mondes ou univers virtuels "". Dans ces dispositifs récents, il est en effet possible pour l'utilisateur d'une communauté virtuelle de naviguer et de communiquer à distance au sein d'un environnement métaphorique et spatial. L'utilisateur dispose alors d'objets 3D et d'une représentation de lui même qui prend la forme d'un personnage que l'on nomme couramment son "" avatar "". Il s'agit pour nous ici de réfléchir aux conséquences éthiques, sociales et communicationnelles de la matérialisation en trois dimensions de l'utilisateur dans un espace virtuel. Cet espace met alors en scène une situation de communication dans laquelle la représentation est omniprésente. Sur le plan épistémologique, nous situons cet objet de recherche au carrefour d'une approche de type nouvelle communication (Winkin, 2001), et de l'analyse sémio-pragmatique de médiations sensorimotrices et sociales (Peraya, 2007). Afin de préciser notre réflexion, nous livrons au lecteur les résultats d'une expérimentation menée dans une communauté virtuelle d'apprenants (Bonfils, 2007). Grâce à la manipulation de son avatar, nous montrons comment chaque membre de la communauté utilise la communication non verbale et tend à participer à la dynamique relationnelle et à la construction d'un lien social collectif. Nous démontrons que par l'intermédiaire d'une ritualisation progressive des mouvements et des interactions des avatars (Goffman, 1973 ; Lardellier, 2003), l'espace virtuel apparaît comme un lieu symbolique et proxémique de communication (Hall, 1971)."
"L'acronyme francophone Swapie (Sites Web d'Auto-Publication d'Information Ethique) évoque avec insistance : to swap (« permuter »). Ce verbe révèle tout le contexte et l'enjeu des Swapie : échanger l'information autrement et dans d'autres lieux. Changer les producteurs, les diffuseurs, les éditeurs... Mais aussi changer la manière dont s'organise la publication des différents genres (journalistiques, scientifiques, associatifs) avec une idée forte : le citoyen, le chercheur, le militant... peut (se) publier."
"Illustrant un exemple d'application de la cognition sociale à la communication externe des organisations : l'influence des campagnes de prévention de santé publique, l'article poursuit un double objectif scientifique et opérationnel. D'une part, il montre comment les récentes recherches en socio-cognition permettent, à travers de multiples modèles et méthodes, d'expliquer les effets et les processus psychologiques d'influence des campagnes de prévention. D'autre part, il met en évidence les implications opérationnelles de la cognition sociale pour les praticiens en matière d'aide à la décision, notamment pour optimiser l'efficacité des messages préventifs, des plans média ou des outils de mesure d'efficacité. Dans ce contexte, l'article envisage également les nouvelles perspectives scientifiques et opérationnelles ouvertes par le récent concept de communication engageante."
"Nous chercherons dans cette communication à proposer une démarche transdisciplinaire pour l'analyse du fait musical contemporain, qui s'inscrive dans les sciences de l'information et de la communication. Aussi, après avoir souligné la difficulté d'aborder le sonore et le musical par le seul biais de la musicologie classique, nous présenterons les approches sur lesquelles nous nous ap-puyons pour enrichir et multiplier les points de vue sur le fait musical (médiologie, sociologie de la culture, cultural studies, esthétique de l'art contemporain). Enfin, nous tenterons de montrer l'apport de la pragmatique et en particulier celui de la théorie de la pertinence (Sperber D, Wilson D., 1986) dans ce projet."
"Dans un début de XXI° siècle plein d'incertitudes, de menaces tout autant que de potentialités, les notions de territoire et de mondialisation semblent s'affronter. Les technologies de l'information et de la communication (Tic), qui ne sont plus toujours nouvelles, mais sont toujours renouvelées, sont censées permettre à la majeure partie des activités humaines de se dissocier de leur attache matérielle et spatiale ; c'est ce qu'on entend par la délocalisation au sens large, la virtualité, la mobilité des travailleurs « branchés », et les déclinaisons de tous les e-quelquechose : e-learning, e-business, e-finance, e-marketing, etc. Ce mouvement nous emmènerait dans un « cyberspace » dématérialisé, comme le dit P. Levy. Le territoire disparaîtrait. Cependant la question des territoires semble de plus en plus actuelle par le nombres de publications, de thèses, de sites web et d'interventions sur le net. Délaissant les voies de l'économie et de la géopolitique, l'objet de cette communication est de réfléchir sur le rôle symbolique des territoires dans la mondialisation. La thèse défendue est que les frontières sont contingentes et poreuses ; ce sont les cultures, les idées et les connaissances qui diffusent à travers ces pores ; le vecteur privilégié de cette diffusion est le dispositif de communication qui s'est construit sur les Tic autant que les voyages qui ont marqué la première époque de la relation intercontinentale. Plutôt que de frontières, on parlera de croisements, de foyers culturels et d'identités mouvantes et vivantes qui communiquent par le biais de la « traduction » dans l'acception de P. Ricœur. Le territoire a sa place dans la mondialité."
"Contrairement aux thèses du 3e cycle, les archives ouvertes contiennent peu de mémoires de niveau master. Le répertoire international OpenDOAR signale neuf archives ouvertes avec ce type de documents (sur plus de 1500 sites). En France, il existe trois sites pour les stocker, signaler et diffuser, avec plusieurs centaines de documents en texte intégral. C'est peu, aussi bien par rapport au nombre d'étudiants en master dans l'enseignement supérieur en France que par rapport au contenu des archives ouvertes dont ils représentent à peine 0,3%. Même si le niveau des mémoires est parfois inégal, leur intérêt est lié à leur caractère d'études de cas (expérience pratique), leurs synthèses documentaires (état de l'art) et leur réflexion et expérimentation méthodologique. Aujourd'hui, la dissémination est souvent limitée à un accès local aux documents imprimés. Leur contrôle bibliographique est souvent assez pauvre. Ainsi, il y a plusieurs arguments en faveur d'une politique de dépôt des mémoires de Master dans des archives ouvertes : - Une meilleure dissémination des mémoires, avec un accès immédiat et universel, et des métadonnées élémentaires. - Un renforcement du label de l'institution par une meilleure visibilité de la production et de l'activité de ses étudiants. - Une valorisation des formations et des étudiants, avec un impact positif sur l'insertion professionnelle. Le rapport aborde les aspects juridiques (droits d'auteur, confidentialité, validation par l'institution) et qualité (sélection des meilleurs documents), et il posera la question de l'intégration d'un tel site dans un dispositif national de travaux étudiants et dans l'infrastructure émergente des sciences humaines et sociales. Il inclut une étude comparative de dix sites Web."
"Ce livre propose la réédition augmentée de notes de bas de pages et d'une bibliographie de l'ouvrage que le mathématicien Paul Appell écrivit en 1925 pour rendre hommage à son ami d'enfance Henri Poincaré. Dans cette biographie à la fois intimiste et scientifique, il met ainsi en lumière des anecdotes méconnues sur la vie de Poincaré comme par exemple l'origine de son zéro au baccalauréat en mathématiques. Il analyse ensuite d'une manière très accessible son oeuvre scientifique (comprenant ses travaux en mathématiques, en mécanique, en astronomie, en physique mais également en philosophie) avec le regard avisé de celui qui resta tout au long de sa carrière dans le sillage de « l'illustre géomètre ». Il rappelle enfin les différents engagements de Poincaré en matière d'enseignement notamment mais aussi son rôle dans l'affaire Dreyfus."
"Le droit de réponse est la procédure qui permet à un individu mis en cause de faire connaître ses explications ou ses protestations dans les circonstances et dans les conditions mêmes qui ont provoqué sa désignation, par la publication d'une réponse. Il s'agit d'un véritable droit de communication paradoxalement exercé en dehors de toute référence à la liberté d'expression. Ce dispositif a pour objectif de rétablir l'équilibre dans un espace communicationnel."
"Le développement des technologies de l'information dans ce que l'on appelle la « société de l'information » est à l'origine de nouveaux usages, de nouveaux échanges qui imposent, de manière coercitive, la mise en place de nouveaux modèles économiques et juridiques. Les contraintes du droit d'auteur et surtout son immatérialité étaient alors mal comprises voire ignorées par le corps social. Récemment les parlementaires français posaient dans des circonstances exceptionnelles le principe de l'illégalité du téléchargement entre internautes. Il faudra conjuguer : juste et équitable rémunération des auteurs, prise en considération des contraintes juridiques internationales avec la garantie de l'accès à la connaissance et à l'information pour tous, conformément à l'engagement de Tunis. La question de la rémunération des auteurs apparaît donc comme un des aspects majeurs de la gouvernance de l'Internet."
"L'institution judiciaire est un dispositif de médiation sociale en tant que dispositif de représentation de la société, qui ne peut être saisie immédiatement. Il s'agit aussi d'un dispositif de représentation d'une réalité, à travers une mise en scène, une qualification juridique, des rituels... sous la forme d'une vérité dogmatique qui trouve son origine dans le sacré. La médiation de l'institution judiciaire est aujourd'hui contestée, une approche communicationnelle permettra d'analyser ce phénomène."
"Les nouveaux médias dans leur hétérogénéité permettent d'émanciper l'individu des représentations imposées par les médias traditionnels. Dans la société de l'information, l'individu, disposant d'une information « contradictoire » par la diversité des regards qui lui sont proposés, a la possibilité de construire sa propre représentation, sa propre vérité. Les problématiques de l'éthique de l'information doivent désormais être interrogées au regard du contexte de réception."
"Le but général de notre recherche est de présenter un dispositif de traitement d'un grand corpus de textes sous forme hypertextuelle, mis en ligne comme un site ""Web"" accessible à tous. L'apparente simplicité de cette description pourrait laisser penser qu'il ne s'agit là que d'""un autre site"" offrant des textes électroniques en libre accès sur le Web. Le premier objet d'étude est lié à l'analyse des nouveaux usages d'un corpus de littérature sous format électronique. En fait, la mise en place de ce site est conduite par une réflexion sur les dispositifs hypertextuels, dans le cadre de l'étude des activités cognitives impliquées dans la lecture et la compréhension, au travers de l'analyse de parcours de lecture et de la construction de sens par ces parcours. Notre analyse porte également sur les modalités de construction des bibliothèques numériques, combinant les aspects techniques et patrimoniaux."
"Illustrant un exemple de communication persuasive appliquée au marketing, l'article montre comment les recherches en communication de santé publique peuvent aider le marketing social à changer les comportements en matière de santé et accroître l'efficacité des campagnes. Suivant un double objectif scientifique et opérationnel, l'article envisage, après une présentation des récents modèles persuasifs, leurs apports au marketing social ainsi que des implications pratiques utiles aux praticiens."
"les droits d'accès à la culture des citoyens diminuent, les libertés fondamentales sont largement menacées mais les citoyens ne réagissent pas. La réduction de la dissonance cognitive et/ou l'augmentation de la confiance au fil de la fragilisation du lien social peuvent expliquer pour partie ce phénomène. Il reste qu'une morale laïque est à définir et à associer à la loi pour que la protection de la propriété intellectuelle ne se fasse pas au profit des grandes firmes et au détriment des auteurs et des citoyens."
"Face à l'idéologie de formations sans frontières, de (ré-)industrialisation de la filière de l'enseignement, de nouvelles pratiques pédagogiques liées aux plateformes de formation massive en ligne, l'appropriation et le déploiement des dispositifs numériques deviennent des enjeux contemporains de l'éducation et de l'apprentissage. Les textes de cet ouvrage cherchent à saisir et à rendre compte de cette réalité actuelle dans toute sa complexité : politique, économique, sociologique."
"Cette thèse questionne les Pôles de compétitivité, forme française de regroupements d’entreprises, lesquels sont considérés comme nécessaires pour favoriser l’innovation et la compétitivité des territoires et des entreprises (Silicon Valley, Sophia antipolis en sont les exemples les plus connus). Cependant à travers l’exemple Pôle Mer PACA, il semble que les acteurs se murent dans un présent perpétuel visible par la fédération autour de projets ponctuels sans forte pérennité ou capacité de fédération. Les dimensions formalisation de projet commun et inscription dans le territoire disparaissent au profit de la capacité opérationnelle du réseau.Notre problématique porte donc sur le sens de ces organisations avec une approche communicationnelle : l’organisation d’un pôle de compétitivité ne confère-t-elle pas une signification au territoire différente de celle que lui propose sa communication ? L’utilisation du terme « compétitivité » ne reflète-t-elle pas une certaine vision du monde qui se donne à voir et propose d’être réalisée ? Dans ce cadre précis, de quelle façon le patrimoine d’un territoire peut renforcer sa communication ?"
"L’article que nous proposons est articulé autour de la notion de classe : doit-on considérer la classe comme un ensemble statique composé d’individus possédant la même structure et le même comportement, ou opter pour une approche plus dynamique dans laquelle les contours de la classe sont définis à travers la prise en compte en situation des individus proches d’un prototype représentatif d’une catégorie donnée. Le choix d’une approche dynamique conduit les auteurs à revisiter deux théories « classiques », la théorie sémio-contextuelle et la théorie acteur-réseaux, en proposant une théorie des prototypes réseaux qui a pour objectif de faciliter l’appréhension des groupes d’acteurs dans un contexte de généralisation des communications numériques pour in fine une meilleure compréhension des situations."
"L'intelligence économique souffre aujourd'hui encore d'une image floue et d'une réputation sulfureuse. Cet ouvrage propose de revoir les principes fondamentaux de cette démarche managériale à travers une analyse systémique de ses multiples dimensions. Dans un langage clair, l'auteur propose un modèle simple et opérationnel pour mettre en place un dispositif adapté aux besoins de chaque entreprise, petite moyenne ou grande. Il contribue ainsi à démystifier un outil indispensable aux décideurs désireux de comprendre leur environnement et d'anticiper ses évolutions, dans le respect des principes de responsabilité sociale des entreprises."
"Les mondes virtuels sont issus des MMORPG (Massive Multi Online Role-Playing Gamers) qui sont eux-mêmes issus du monde du jeu vidéo. A ce titre, il existe une filiation ""ludique"" entre ces différents dispositifs. Les travaux de Steinkuehler suggèrent que les mécanismes de l'apprentissage générés par les jeux issus des mondes virtuels dépendent ""certes de la nature du jeu mais aussi des pratiques sociales qu'ils engendrent"" (Steinkuehler, 2006, p. 8). Dans cette continuité, nous avons démontré dans nos travaux que ces environnements immersifs permettaient de créer du lien social et donnaient lieu à des formes de rites numériques entre sphère privée et publique (Bonfils, 2007). Dans cet article nous nous intéressons aux controverses des environnements immersifs en questionnant la place du corps virtuel au sein de ces dispositifs qui pourraient se situer selon nous entre lieux de spectacle, de loisir et d'apprentissage médiatisé. Le cas des Serious Games sera évoqué à titre d'illustration."
"Le concept de « patrimoine culturel » intègre notre champ de recherche en sciences de l’information et de la communication reliant sa mise en valeur et sa protection à la médiation territoriale. Dans une vision de développement local, le patrimoine culturel naturel, matériel et immatériel, constitue un atout économique et touristique. Notre thèse étudie la mise en valeur patrimoine culturel dans les régions et zones rurales du Sud-tunisien. Il s’agit de traiter une problématique traduisant les soucis d’une population locale en termes de sauvegarde de son héritage patrimonial contre les formes de marginalisation et l’oubli. Les familles rurales souhaitent développer leur production locale fondée sur les ressources patrimoniales, les fabrications de produits du terroir et l’artisanat mais rencontrent des difficultés financières et institutionnelles pour soutenir leurs activités locales. Cela est dû à une absence de communication entre la population et les instances publiques pour collaborer autour de la problématique de la valorisation du patrimoine qui devient de plus en plus fragile. Notre thèse est d’avancer un élément central de soutien institutionnel pour médiatiser le dialogue entre les acteurs locaux. Il s’agit d’introduire les organisations non gouvernementales (O.N.G.) comme acteur de médiation pour renforcer des actions de concertation et impliquer encore plus les institutions publiques et la population locale dans la question de la valorisation du patrimoine culturel. Une recherche-action a été mise en œuvre pour explorer un nouveau terrain d’investigation, et comprendre les rôles et les responsabilités des acteurs réunis sur un territoire."
"Cette thèse questionne, d’un point de vue compréhensif et critique, la notion de traces numériques à l’heure du Big Data et de la relation entre les notions de la surveillance et la confiance. Le « Big Data » fait référence à la production massive de données qui représentent une manne précieuse de bénéfices. En effet, la quantité massive de données produites dans le monde atteint des volumes si importants qu’il est indéniablement impossible de les analyser par l’humain sans l’aide d’outils technologiques et statistiques adéquats. Parmi les secteurs concernés par cette révolution technologique et sociétale, le secteur aéroportuaire est aujourd’hui confronté à une importante transformation, nourrie par l’explosion des données au sein de sa structure. Les données générées, collectées et stockées au cours du parcours du passager sont désormais massives et leur gestion est un important levier pour la sécurité, l’amélioration de services et le confort du passager. Pour autant, les avantages attendus n’en soulèvent pas moins une grande question : où vont ces données ? Difficile d’y répondre. Et tant qu’on ne sait pas, comment peut-on faire confiance ? Ces réflexions sont mises en examen à l’aéroport de Larnaca à Chypre. Les différents angles d’approche ainsi que la diversité des acteurs ont nécessité la constitution d’un corpus multidimensionnel, issu d’une méthodologie mixte, afin d’avoir une approche compréhensive du sujet. Ce corpus comprend à la fois des entretiens, des questionnaires et des récits de vie des passagers et des professionnels du terrain. L’analyse qualitative et quantitative qui a suivi était basée sur un cadre précédemment élaboré afin de croiser les représentations des acteurs à propos de la surveillance et la confiance et mettre en évidence les différentes visions inhérentes à cette question."
"Définir le street art est aujourd’hui une question très complexe tant l’on range sous cette appellation des formes d’art (ou de non-art ?) qui n’ont finalement en commun que le fait d’avoir été aux origines produites in situ, « dans la rue ». Une lecture de la diversité de ces productions, des époques les plus reculées à l’explosion des collages, pochoirs et autres graffitis comme de leurs corollaires depuis les années 1960, justifiera l’ambiguïté de la question posée dans notre titre. Cette ambiguïté est accentuée par les deux mots « art » et « street » accolés dans une appellation générique floue qui nous force à nous interroger sur les concepts d’art, de low art, de high art, d’artification, de perte de sens du fait de la reconnaissance institutionnelle et marchande de pratiques tout d’abord considérées comme des actes délictueux et donc condamnables par la loi."
"Cet article s'attache à décrire le tout début du punk parisien, présente ses acteurs principaux : les groupes Strike Up et Loose Heart, Denis Quilliard (futur Jacno), le groupe Angel Face, Patrick Eudeline (futur chanteur d'Asphalt Jungle), et évoque l'environnement culturel de la France de cette époque (de 1972 à 1977). Il tente de restituer le sentiment d'urgence vécu des deux côtés de la Manche par cette génération de musiciens sensibles au message porté par les New York Dolls (1971-1977) et par la première émergence punk new-yorkaise qui suivit."
"Le but de cette communication est de proposer un modèle d'interaction des cultures fondé sur la notion de rayonnement par analogie avec le modèle quantique de la lumière. De même que la lumière peut être considérée dans la physique moderne soit comme un flux de particules, soit comme une ondulation, de même, à la dimension granulaire de l'information émise par une organisation dans ses flux de données, nous allons ajouter une dimension ondulatoire sous forme de rayonnement. Le dispositif socio technique de rayonnement est une forme particulière de Distic."
"La justice peut-elle se passer de mise en scène ? Et quels sont les effets de sa représentation sur d'autres scènes ? Au XXème siècle l'institution judiciaire est passée d'une forme classique à sa forme moderne. Désormais émancipée de son apparat traditionnel, elle découvre sa nouvelle « condition médiatique ». Devenue actrice de l'espace public, l'institution judiciaire est confrontée à la suprématie de la transparence et de la proximité au détriment de son système symbolique et culturel, intemporel. Ces valeurs postmodernes évoquent en effet les promesses de l'immédiateté d'une justice sans scène, dans laquelle les barrières symboliques s'effondrent. Ce phénomène est accentué par la médiatisation et par les promesses de l'image. Cette question de société est considérée dans cet ouvrage à travers le cadre conceptuel de la sémiotique politique au sein des sciences de l'information et de la communication. Celles-ci permettent ici, une fois de plus, de contribuer aux questionnements contemporains relatifs à la crise de la médiation et de la sociabilité."
"Notre communication porte sur l'enseignement à distance et en particulier les problématiques de communication au sein de dispositifs sociotechniques de type mondes persistants. Nous situons l'apport des Tice dans un contexte spécifique de « communautés virtuelles d'apprenants à distance» et mettons en relief l'importance de la médiation entre les acteurs du projet (Henri & Pudelko, 2006). Nous constatons l'utilité de dispositifs socio-techniques qui se révèlent très adaptés à la médiation des savoirs, à la médiatisation des contenus, et comme modèle d'échange et de stockage des informations, mais nous soulignons aussi leurs difficultés à se révéler de vrais lieux de socialisation (Lardellier, 2006). Nous présentons dans notre travail des éléments de réflexion sur un cadre théorique possible pour une étude pragmatique de la communication à partir de médiations sociales au sein de dispositifs socio-techniques de type monde persistant ou mondes virtuels."
"Comment expliquer la présence invasive des documents encadrant la circulation du bioart et leur influence sur la constitution, le fonctionnement et la réception des œuvres, alors que ce courant partage avec les arts vivants ou le body art une dimension live, incarnée et performative qui impliquerait une réception directe et non médiate de la part de l’audience ? Car si la documentation semble servir à compenser la rareté des expositions de bioart en assurant aux œuvres une visibilité médiatique, elle paraît aussi nécessaire sur leurs lieux d’accrochage pour garantir l’efficacité de leur réception directe, en paramétrant l’expérience sensible éprouvée par l’audience. En effet, d’abord produite par les artistes puis reprise par divers commentateurs, la documentation discursive et visuelle du bioart se compose d’une variété de déclarations, d’entrevues, de commentaires et de discours théoriques accompagnés d’illustrations des œuvres qui sont publiées dans les catalogues d’expositions, les revues spécialisées, la presse ou bien encore sur les sites Internet des artistes. Ainsi, si, à travers leurs récits, les artistes trouvent l’occasion d’en dire plus sur leurs créations, en dévoilant leurs recettes de fabrication et leurs démarches, les intentions qui les poussent à s’approprier les biotechnologies constituent l’argument principal prélevé par les discours d’experts qui visent à interpréter leurs sens au-delà de leurs qualités formelles, en valorisant leurs significations symboliques. Les images produites par les artistes sur leur travail illustrent aussi cette volonté de rattacher l’œuvre bioartistique à un sens caché. Pourtant, malgré la présence manifeste de ce réseau de significations véhiculé par la documentation autour des œuvres, c’est en fonction de leurs propriétés formelles intrinsèques que certains experts défendent l’intérêt d’en faire la perception sensible. En l’occurrence, selon le commissaire Jens Hauser (2008), le bioart devrait être expérimenté sur un mode phénoménologique de « co-corporéité » parce qu’il va « au-delà de la représentation » pour produire des « effets de présence » à travers la mise en scène performative de « bioartefacts » (Andrieu, 2008). Mais l’auteur ajoute que l’efficacité de cette expérience «co-corporelle» repose néanmoins sur une médiation verbale «liminale» placée entre le bioartefact et le spectateur pour l’informer des processus sous-jacents et imperceptibles que met en œuvre l’artiste. Du coup, bien que le bioart soit valorisé pour l’expérience esthétique inédite procurée par sa dimension biofactice, il paraîtrait que sans l’aide des documents, ses prestations resteraient sans effet sur l’audience. En partant de l’hypothèse qu’il existe un lien de dépendance entre les prestations bioartistiques et leur documentation pour structurer et garantir ainsi l’efficacité de leur réception comme œuvres d’art, cette thèse explore la dynamique qui s’instaure entre les médiations langagières et visuelles du bioart et les œuvres auxquelles elles réfèrent à travers des approches médiatiques et empiriques. Les résultats montreront que malgré sa sortie du paradigme mimétique et le processus de « rematérialisation » qu’il engage dans l’histoire de l’art (Hauser, 2008), le bioart détient un fonctionnement esthétique proche de celui de l’art dématérialisé parce que l’expérience qu’il génère ne dépend pas directement des propriétés esthétiques intrinsèques des prestations mais du réseau extrinsèque de significations qui leurs sont attribuées et par l’intermédiaire desquelles leurs compositions tangibles et symboliques en tant qu’œuvres d’art sont dévoilées au spectateur. Le fait que les prestations bioartistiques aient besoin du document pour véhiculer leur principe d’intelligibilité à l’audience remet donc en cause leur autonomie en tant qu’œuvres dotées de propriétés esthétiques intrinsèques et matérielles, et ébranle la croyance qui privilégie l’expérience phénoménologique de cette forme d’art."
"Depuis leur avènement au milieu des années 1990, les moteurs de recherche commerciaux ont pris une place de choix dans les habitudes des internautes. Le succès rencontré par ces outils n'a cessé de se renforcer et oblige à l'analyse. La position hégémonique du moteur de recherche Google dans le paysage français et occidental, son positionnement particulier et le changement de modèle qu'il a imposé dans la recherche d'information en ligne en font un dispositif qui mérite d'être interrogé."
"Les PME et PMI créent la majeure partie de la richesse nationale et des emplois, réalisent plus de la moitié des investissements productifs de notre pays. Si par leur taille, leur histoire, leur souplesse d’adaptation, elles sont souvent le creuset de l’innovation et sources d’informations convoitées par la concurrence, l’acteur au sein d’une organisation structurée en mode projet créatif comme l’utilisateur de ses produits, y tient une place principale dans le management de l’information et l’émergence de nouvelles idées. La place de la veille stratégique et de l’intelligence économique dans leurs processus de développement, de diversification, d’innovation et de prise de décision n’est plus à démontrer. Cependant si l’aspect défensif leur est devenu plus familier, l’aspect offensif se borne souvent à un rôle de collecte, de traitement de l’information et d’instauration de processus de veille. Le volet offensif de l’intelligence économique telle que nous l’envisageons dans notre propos a trait à la créativité en tant que support d’une intelligence compétitive et aux techniques publicitaires qui peuvent contribuer à définir, puis à intégrer un modèle offensif au sein des PME PMI. Un modèle qui ne serait pas l’apanage des seuls services de veille, mais à la portée de chaque acteur de l’entreprise afin qu’il puisse être source de diversification, d’innovation et de prise de décision. Nous nous référerons à la sociologie de l’acteur réseau ou sociologie de la traduction développée par Latour, Callon et Latour (1991 et suivantes), ainsi qu’aux propositions récentes que nous avons appelé de « reverse creativity » ou les usages nouveaux et utilisations nouvelles imaginées ou détournées de produits occupent une proposition pertinente. Nous prendrons appui sur la méthode de création de la connaissance Nonaka et Takeuchi (1997) qui soutient que le salarié est l’acteur principal de l’entreprise car il possède les connaissances, traite l’information et interagit avec son organisation pour y faire émerger de nouvelles propositions. Enfin, nous analyserons les techniques créatives publicitaires, et notamment la technique de la « disruption » que l’on doit au publicitaire Jean Marie Dru (1996 et 2003), et tenterons de démontrer qu’un modèle qui relierait le domaine de la « connaissance » à celui de la « créativité », servirait les objectifs tactiques et stratégiques de l’entreprise."
"Cette thèse explore l'influence de l'internationalisation de l'enseignement supérieur européen dans la construction d'une Université «contingente ». En partant de l'analyse de Richard Bagnall – qui identifie une «tendance des universités à mieux répondre aux exigences et aux attentes immédiates de ceux qu'elles servent, à mieux répondre aux préférences expresses de leurs marchés respectifs, à mieux répondre au désir collectif et individuel, à dépendre plus directement des contextes culturels dans lesquels elles s'insèrent», cette étude essaie de présenter une approche critique du rôle de l'internationalisation en tant que moteur et conséquence de cette situation de dépendance quelque peu nouvelle que semble vivre l'Université actuellement. Le cadre de Bagnall est élargi pour inclure l'examen de Bill Reading qui constate l'université en ruines (University in Ruins ) et la possibilité de développer une « nouvelle idée » de l'Université par le biais d'une approche «éthique » différente de l'internationalisation de l'enseignement supérieur. La recherche analyse les différentes définitions de l'internationalisation dans le contexte de l'enseignement supérieur. De plus, à travers le rapport de l'enquête menée en 2005 par l'AIU (2005 IAU Global Survey ), elle essaie de comprendre les différentes motivations (raisons d'être) présentées par les établissements d'enseignement supérieur (EES) pour développer des politiques/stratégies internationales. Ces différentes approches de l'internationalisation sont considérées dans le contexte de la mondialisation et de l'européanisation en ce qui concerne les politiques d'enseignement supérieur."
"L'article rappelle tous les témoignages selon lesquels il y a réellement une crise des systèmes éducatifs, depuis le primaire jusqu'au supérieur, et que cette crise est mondiale. Il met en relief l'un des facteurs de cette crise qui est l'exposition précoce, profonde et prolongée des enfants et des adolescents aux objets techniques numériques : jeux vidéos, télévision, téléphones portables, ordinateurs, internet. L'hypothèse défendue est que l'exposition intensive des tout jeunes aux objets des Tic est un des facteurs-clé à la fois de l'effet Flynn (l'augmentation générale du QI et de la demande de stimulation intellectuelle) et de la crise mondiale de l'éducation, et qu'une des voies de résolution de cette crise passe par l'appropriation de la culture du cyberspace par les systèmes éducatifs. Si la technologie a des effets pervers sur l'éducation et la culture, qu'au moins on cherche par un nouveau projet pédagogique à en exploiter les potentialités."
Une mode de l'incommunication serait-elle en passe de supplanter la mode de la communication qui domine les discours politique et scientifique depuis une cinquantaine d'années ? On pourrait le penser à la lecture de plusieurs publications récentes en France et à l'étranger dans le champ des Sciences de l'information communication. L'objet de cette communication est de définir ce que l'on entend par incommunication et de démontrer l'intérêt du détour par l'incommunication pour tenter de comprendre des situations de blocages sociopolitiques où précisément la communication achoppe. Le terrain de cette réflexion empirique est celui de notre monde méditerranéen.
"L'institution judiciaire apparaît, par le rôle du juge, dans une relation ternaire, comme médiat entre l'individu et la communauté, en tant que réalité inaccessible immédiatement mais saisissable « médiatement », elle « tient lieu de ». (Pierce, 1970). Cette prérogative de représentation confère au tiers, le juge, autorité et légitimité au regard d'une dogmatique réputée acceptée. Ce droit d'agir « à la place de » n'implique pas pour autant une tutelle, mais au contraire une indépendance : elle accomplit ce que la communauté ne peut accomplir elle-même. L'institution judiciaire porte alors à la fois le message d'une certaine communauté et son propre message, un message lié à la culture de la communauté et à l'organisation elle-même."
"Le développement des technologies de l'information dans ce que l'on appelle la « société de l'information » est à l'origine de nouveaux usages, de nouveaux échanges qui imposent, de manière coercitive, la mise en place de nouveaux modèles économiques et juridiques. Les contraintes du droit d'auteur et surtout son immatérialité étaient alors mal comprises voire ignorées par le corps social. Récemment les parlementaires français posaient dans des circonstances exceptionnelles le principe de l'illégalité du téléchargement entre internautes. Il faudra conjuguer : juste et équitable rémunération des auteurs, prise en considération des contraintes juridiques internationales avec la garantie de l'accès à la connaissance et à l'information pour tous, conformément à l'engagement de Tunis. La question de la rémunération des auteurs apparaît donc comme un des aspects majeurs de la gouvernance de l'Internet."
"Les publicitaires ont toujours mené une veille scientifique dans le but d'appliquer les modèles et les méthodes des sciences humaines et sociales pour améliorer leurs pratiques. L'objectif de l'article est double. Premièrement, à travers un bref historique, des années 1930 jusqu'à nos jours, nous montrons comment, au fil des décennies, les sciences humaines sont devenues de plus en plus complexes et comment se sont opérés les principaux transferts de connaissances. L'arrivée récente du paradigme des sciences cognitives et de la socio-cognition implicite a davantage complexifié le champ (e.g. le « neuromarketing ») et a rendu les applications plus rares. Nous montrons comment, à partir du paradigme de la socio-cognition implicite, nous avons construit un outil de mesure de l'influence publicitaire non consciente."
"Ce travail de recherche dont le thème est centré sur la communication et l’entrepreneuriat dans le cadre du développement durable avec comme terrain d’expérimentation le cas des PME de la ville d’Annaba en Algérie. La dimension centrale de cette recherche a pour objet l’identification des facteurs en termes de connaissances attitudes et comportements qui permettent à un dirigeant de PME de jouer un rôle de leadership dans une perspective de développement durable ainsi que des contraintes qui peuvent l’en empêcher. De manière plus spécifique, nous pensons que la communication responsable peut amener les entreprises à mieux s’adapter et répondre aux enjeux environnementaux d’une part mais également favoriser l’émergence d’un nouveau profil entrepreneurial de type éco transformationnel d’autre part. Les données collectées sur le terrain à partir de la technique de l’observation directe, d’entretiens et d’un questionnaire appliqué à un échantillon de 100 entrepreneurs, le tout approfondi par le test« Epi Info » ont confirmé nos présomptions.Un certain nombre d’orientations et de suggestions ont été émises à la chambre de commerce et d’industrie d’Annaba sous la forme d’un plan d’action qui préfigure et rend nécessaire la création d’un observatoire des PME au niveau national."
"A l'échelle du web, la massification des corpus et celle des accès ainsi que la concentration des acteurs (moteurs de recherche), témoigne d'une tectonique documentaire qui bouleverse nombre d'usages associés au document, du point de vue du traitement de corpus documentaires inédits dans leur forme (formats) et dans leur taille (volumétrie), ou de celui de la recherche et de l'accès pertinent et/ou raisonné auxdits documents. Des pratiques d'indexation sociale (folksonomies) émergent, en lien avec la préemption par un public non expert de techniques d'analyse et d'outils s'inscrivant habituellement dans l'héritage de la linguistique de corpus et plus globalement des sciences du document. Ces pratiques de re-documentarisation s'appuient sur une assise communautaire forte. Leur succès – à la fois public et technique – s'explique par un faible coût cognitif conjugué à une renégociation inédite des espaces documentaires du web. L'observation de motifs récurrents dans l'application de ces pratiques les situent dans la lignée des travaux définitoires du web socio-sémantique et pose également la question de la perméabilité de deux modèles. Avec le risque que le modèle bibliothéconomique d'accès raisonné aux connaissances soit battu en brèche par un modèle « marchand », subordonnant pour la première fois la problématique qualitative du « classement » des biens culturels à celle purement comptable du nombre de fois où ils sont « accédés »."
"Cet essai de modélisation du potentiel d’action locale d’un échelon territorial, qui a fait l’objet de publications partielles et d’applications de recherches doctorales sur différents territoires, nationaux et internationaux, s’est inséré dès le début dans le programme de recherches M.A.I.N.A.T.E (Management de l’Information appliquée au Territoire) initié en 1994, développé en 1997, poursuivi avec le Groupe GOING (Groupe d’investigations des nouvelles gouvernances), du programme TERRATER (Du territoire à la territorialité), du programme @MEDIATIC (Médias et Technologies d’intelligence collective) et bientôt dans COLLETER (Collectivités & Territoires). L’objet principal du programme que nous exposons dans cette contribution était de pouvoir mesurer le réservoir de capacité de développement local que possède ou pas un territoire, lorsqu’il décide de formuler une stratégie de développement. Ces différents programmes ont connu des développements tant d’un point de vue académique, avec des thèses de doctorat et des publications scientifiques, que d’un point de vue applicatif en relations avec plusieurs partenaires et territoires. Nos recherches visaient initialement les territoires des villes moyennes qui souhaitaient définir et mettre en œuvre un projet de développement par l’intégration des T.I.C. Ce programme de recherches a été dans un premier temps appliqué à deux territoires distincts que nous désignerons par A et B et reposait sur un modèle que nous avons nommé « Méta-modèle » en référence aux travaux de Schwartz (1992 et suivantes). Nous l’avons étendu depuis à des régions, départements, bassins, zones protégées etc."
"Ce projet de recherche appliquée a été initié afin d'optimiser la mesure et le pilotage de la performance e-Marketing dans un contexte évolutif lié aux mutations perpétuelles du Web (réseaux sociaux, Web 2.0 ...) et des modèles d‟affaires associés. Il vise à identifier des solutions organisationnelles et technologiques pour pallier aux difficultés rencontrées par l'équipe e-Marketing de l'agence interactive BleuRoy.com (groupe HighCo) tout en optimisant l'efficacité et la rentabilité des actions menées pour ses clients. Pour ce faire, plusieurs recherches ont été conduites. Dans un premier temps et afin de faire le point sur le sujet, nous avons étudié les différentes notions potentielles sur lesquelles le projet repose. Mais face aux nombreuses définitions éparses et démarches proposées, nous avons utilisé la méthode de méta-analyse afin de synthétiser les résultats. Durant cette étude, nous avons étudié le concept de Web Analytics 2.0 qui se veut être une composante de l'e-Marketing fournissant des techniques et des méthodes pour mesurer les différents vecteurs de communication en ligne. Celui-ci embarque partiellement la notion d'Intelligence Compétitive en se limitant aux aspects de Veille Concurrentielle pour expliquer les variations potentielles des indicateurs clés de performance reportés par les solutions de Web Analytics. Au moyen de la méthode observation participante, nous avons élaboré un état des lieux des techniques et des pratiques en dressant les différentes attentes du projet. A partir de l'ensemble, nous avons pu développer davantage l'Intelligence Compétitive pour l'e-Marketing à travers la proposition du modèle KIM (Knowledge Internet Marketing). L'ingénierie de projet nous a amené à concevoir la plateforme sur la base d'un logiciel Open Source de Web Analytics comportemental préalablement sélectionné au moyen de la méthode OSMM (Open Source Maturity Model) de Capgemini. Face à quelques difficultés pour appréhender les développements, nous avons intégré la méthode des Cas d'Utilisation en complément de l'étude fonctionnelle pour décrire, tester et documenter précisément les différents modules du dispositif d'Intelligence Compétitive à développer Suite à une fusion stratégique de l'entreprise, le projet voit son spectre fonctionnel évoluer et se doit d'appréhender de nouveaux métiers, de nouvelles fonctionnalités. La réorientation du projet est aujourd'hui envisagée notamment grâce à sa conceptualisation fondée sur la notion globale d'Intelligence Compétitive."
"Cette étude, dont le début s'initia avec la pratique en consultation stratégique il y a dix ans, aboutit dans la thèse qui veut démontrer que chaque organisation présente une originalité qui la rend singulière, et que cela dépend de la force des talents qui la composent. Il nous est alors démontré que les résultats que cette étude obtient dépendent de l'ensemble des profils professionnels de ces talents-là, associé à leur alignement selon la stratégie et les objectifs tracés pour les affaires dans leur secteur de performance et leur marché. Voilà pourquoi nous pouvons conclure que l'utilisation du concept d'équilibre de forces dans le domaine de l'Intelligence économique enrichit le dossier de tendances et d'importances pour les prises de décisions les plus réussies, celles qui apportent de meilleurs résultats pour les affaires concernées, y compris des sauts d'évolution. L'étude conclut aussi que, conformément aux résultats pratiques analysés, le niveau de développement durable de l'entreprise dans le temps sera plus consistant lorsque le balancement sur les foyers utilisant actuellement des actions qui valorisent le passé réaffirment le présent et construisent un avenir s'avèrant, lui aussi, également important. Pour comprendre cette perception concluante, nous est apportée une idée explicative et didactique qui facilite la vision des interactions internes et externes. De cette manière, l'organisation est décomposée en trois piliers (appelés foyers), chacun possédant différentes potentialités, leur ensemble pouvant impacter les uns sur les autres avec des intensités différentes, étant possible de comprendre lesquels sont les facteurs qui servent d'intermédiaires pour la force de chacun. Après les expériences pratiques ayant utilisé ce principe, il nous a été possible de comprendre aussi, que, chaque fois qu'une organisation atteint son bilan idéal de forces, elle obtient des sauts naturels d'évolution dans les résultats où le talent humain est démontré, et cela avec un niveau d'effort exécutif minimum. L'effort excessif est dû, dans ce cas, aux besoins de dépense d'énergie supplémentaire pour essayer de compenser le désalignement. Toute organisation qui a utilisé un très grand effort pour atteindre à ses objectifs, ne possède pas de bilan de forces dans le panneau de diagnostic appliqué. Dans 100% des cas étudiés, la stratégie possède poids de puissance de levier et l'étude expérimentale démontre qu'il est possible aux entreprises de prendre conscience de leur actuelle capacité de forces et de leur besoin en puissance de levier. Cela nous paraît évident, mais, comme les organisations dernièrement sont décrites de forme si «autonomes» et «vivantes», il devient important de souligner que, celui qui remarque est toujours l'être humain, c'est-à-dire la capacité stratégique est humaine, jamais organisationnelle. L'Intelligence économique ou compétitive est humaine et donc considérée dans cette étude du point de vue de son origine biologique, psychologique et socioculturelle et de sa façon d'opérer et d'apercevoir dans le monde. Tous les travaux issus d'autres domaines, tels que l'économie, la gestion ou la technologie de l'information, sont les bienvenus, ici, comme complémentaires au scénario économique, au système capitaliste et aux demandes de chaque affaire."
"Dans cet article nous parcourons librement un certain nombre d’aspects liés à la transformation de l’encyclopédisme après l’advenue de la numérisation du signe et du réseau internet. Nous nommons « encyclopédisme en éclats » l’ensemble ouvert «de la communauté des oeuvres, (des textes, des objets et des hybrides...) comme incomplétude en procès de production », le vaste système de relations internes des agencements collectifs d’énonciation, des équipements collectifs de subjectivation en quoi elle (la communauté) consiste, l’immense texture des écritures numériques, de l’algorithmie générale. En fin de compte l’immense marmite précambrienne à moins que nous ne soyons dans le cambrien, de Data, Metadata, Linked Data... qui en autorise le plissement numérique."
"Cet article propose une étude longitudinale et comparative des offres d'usage d'Internet susceptibles de renforcer la démocratie dans le système politique local de quelques villes françaises en y incluant les deux villes pionnières, Issy-les-Moulineaux et Parthenay que nous avions étudiées en 1997. Nous avons cherché les sites Internet municipaux qui proposaient des informations et/ou des outils pour le droit de référendum décisionnel local et le droit de pétition."
"Notre recherche a été établie dans le cadre du programme de recherche interdisciplinaire Langages, objets, territoires et hospitalités. Notre objectif est d’apprécier les procédés informationnels, communicationnels et de management pour la valorisation des régions de Nabeul et de Médenine sous l’égide du CGDR et de l’ODS, et d’estimer et d’évaluer la nature du lien entre le sentiment d’appartenance et la valorisation territoriale par le biais de ces dispositifs d’intelligence collective. L’ancrage de notre recherche dans les champs interdisciplinaires des sciences de gestion et des sciences de l’information et de la communication nous a permis d’appréhender notre objet de recherche dans une logique d’instrumentalisation de pratiques et de dispositifs de management du territoire et dans une logique de communication, de transmission de l’information, de capitalisation et de diffusion des connaissances. Toutes ces pratiques étant sélectionnées à travers le ressenti envers le territoire en termes d’identité, d’attachement et de solidarité.In fine cette thèse révèle les liens de causalité entre le sentiment d’appartenance et les dimensions de l’IET et certaines dimensions du KMT et de l’ITP. Nos propositions invitent au diagnostic, l’évaluation et la compréhension du pourquoi de l’abandon de certaines pratiques d’intelligence territoriale, l’adoption de nouvelles formes de communication publique et territoriale pour le renforcement du sentiment d’appartenance et pour assurer un meilleur partage et diffusion de l’information."
La promotion des industries créatives serait la solution économique et sociale pour conjuguer la nécessité de disposer de suffisamment de revenus pour vivre décemment et le besoin de ne pas être que consommateur. Au coeur du dispositif le capital intellectuel à monnayer et à partager à la fois. Communiquer honnêtement sur ce paradoxe qui implique une transformation sociale profonde n'est possible que dans une société où l'individuel ne l'emporte pas sur le collectif. La France n'est pas encore prête.
L'objet de cette communication est de proposer un projet de construction d'une Ontologie de l'Intelligence Territoriale. Il sera traité des raisons d'un tel projet et de la façon dont il peut être mené dans une approche contributive de type Web 2.0
La crise écologique comme toutes les crises est propice au recul de notre consentement éclairé.
"Cette thèse porte sur le tourisme durable et son impact sur le développement régional. Alors que les démarches de développement durable sont souvent envisagées dans une logique de développement descendant (top down), nous privilégions dans ce travail une approche ascendante (bottom up) qui mobilise les énergies de terrain et les transforme en intelligence collective. Cette approche se retrouve dans le concept d’intelligence territoriale. L’intelligence territoriale (Masselot 2008; Bertacchini 2004) pose l’hypothèse que le développement d’un territoire est entre les mains des acteurs territoriaux. Faire de l’intelligence territoriale, c’est comprendre le maillage entre différentes familles d’acteurs poursuivant chacun leur objectif propre. C’est aussi mobiliser les acteurs de terrain autour d’un projet partagé. Une des contributions de ce travail est de présenter une « boîte à outils » interdisciplinaire constituée d’outils méthodologiques complémentaires au service d’un état des lieux d’un système complexe, le territoire étudié. Les outils que nous avons retenus reposent tous sur la nature interactionniste des données et des acteurs territoriaux. La clé pour comprendre un territoire nous semble être la compréhension fine des interactions entre ses acteurs (institutionnels, publics, privés, associatifs, éducatifs…), l’interaction entre les variables qui le décrivent (environnementales, économiques, sociales, politiques, endogènes, exogènes...). Les outils méthodologiques que nous proposons de mobiliser appartiennent à des horizons scientifiques distincts. Ainsi parmi les 3 méthodes que nous avons mobilisées, la méthode MERI, d’origine Roumaine (Robu et Macoveanu, 2010) a été développée dans un contexte d’ingénierie environnementale. La méthode d’analyse structurelle, d’origine française(Godet 2001a,b,c), a été développée dans le domaine des sciences de gestion et de la prospective stratégique. Enfin la méthode d’analyse réseau, née en sociologie des organisations (Wasserman et Faust, 1994) est largement utilisée en sciences de l’information et de la communication (Boutin, 1999). Notre approche théorique s’appuie sur un travail de terrain qui a été conduit au sein de la microrégion de Sucéava (Roumanie). Cette région présente des avantages physiques et géographiques qui en font une des premières places touristiques de Roumanie. Ce travail de recherche conduit à aborder des concepts appartenant à des champs disciplinaires distincts. Assez naturellement, ce travail de thèse est donc interdisciplinaire : - le domaine applicatif du tourisme, est inscrit en sciences de gestion de même que la méthodologie d’analyse structurelle que nous avons retenue ;- les perspectives de développement régional s’inscrivent davantage en sciences économiques ;- l’intérêt manifesté par la durabilité conduit à intégrer une approche environnementale que l’on retrouve en génie de l’environnement et qui se retrouve dans la méthode MERI ;-enfin certains concepts (intelligence territoriale) et outils (analyse réseau) sont issus ou travaillés en sciences de l’information et de la communication. Cette interdisciplinarité constitue une richesse de ce travail et justifie le choix de cette cotutelle dans laquelle se trouvent présentes les dimensions environnementales et information-communication"
"Nous présentons dans ce texte, la démarche de modélisation de la performance que nous avons mise en oeuvre pour mettre en évidence les compatibilités communicationnelles des membres d'une équipe. Nous admettrons comme point de départ à cette démarche que la performance est étroitement liée aux caractéristiques des personnalités et aux compétences relationnelles des individus. Cette démarche avait pour finalité la création de deux briques technologiques d'un moteur d'aide à la décision pour un site web d'entreprise. La première brique permet de cibler les personnes qui ont des compétences communicationnelles (soft skills) liées à leur métier ; la seconde brique propose un moteur de prescription fonctionnelle pour la constitution d'équipes projet en fonction des attentes d'un donneur d'ordre. Le développement informatique de ces briques a nécessité la modélisation de fonctionnements et pratiques, articulée autour de niveaux de traduction et de codification des discours tenus par les professionnels sur leur métier et par les donneurs d'ordres sur leurs besoins. La mise en relation des deux niveaux de traduction donne les points d'interaction entre les individus qui favorisent la performance de l'équipe constituée. Le problème crucial soulevé ici par notre approche a été de traduire les données représentant des performances, des affinités, des personnalités et des compétences interpersonnelles afin qu'un codage informatique soit possible. Le codage en lui-même a été une des dernières étapes du développement des moteurs, mais sa mise en place a duré tout au long de la recherche et a particulièrement guidé le recueil des données (observations, questionnaires et entretiens, cf. Agostinelli et coll., 2015)."
"L’utilisation de l’informatique et de l’Internet par les personnes handicapées mentales accueillies en établissements spécialisés s’inscrit aujourd’hui dans les pratiques professionnelles du champ médico-social. Si cette communication médiatisée devient un relais de la prise en charge, elle équipe et valorise une professionnalité, mais participe aussi d’une évolution des représentations des professionnels sur les usagers de ces établissements."
"Notre thèse interroge la notion de communication virale dans les espaces socionumériques de manière générale et plus particulièrement lorsque ce phénomène s’applique aux vidéos publicitaires en ligne. Nos recherches ont révélé un manque évident de clarté et de cohérence au niveau de sa définition et de son acception (Beauvisage et al., 2011) qu’il a fallu corriger par un travail d’harmonisation avant d’envisager la suite de nos travaux. De plus, notre revue de littérature a mis en avant la complexité du phénomène viral et de son appréhension ; complexité nourrie par le nombre et la nature des facteurs à son origine (Beauvisage et al., 2011). Afin de mettre en avant un de ces facteurs, nous émettons pour première hypothèse que les variations de qualité de la définition (haute ou standard) d’une vidéo influencent l’appréciation de la vidéo (H1). Par corollaire, nous pensons que le partage de cette vidéo est affecté par la qualité de la définition (H2). Plus précisément, nous pensons qu’une même vidéo sera plus partagée si elle est visionnée en haute définition plutôt qu’en définition standard. Pour répondre à ces hypothèses nous avons opté pour une approche expérimentale."
no abstract
no abstract
no abstract
no abstract
"Aujourd’hui, il existe une pression de plus en plus forte en provenance des concepteurs des réseaux socionumériques professionnels, mais aussi des acteurs de l’éducation, comme ceux du monde professionnel, pour inciter les jeunes à mettre en œuvre une identité numérique professionnelle, considérant cette démarche comme un facteur déterminant dans la réussite de leur trajectoire professionnelle. Partant des résultats d’une enquête quantitative réalisée auprès d’un échantillon de jeunes âgés de 16 à 24 ans, nous proposons de mettre en évidence les usages informationnels et communicationnels accompagnant leurs choix en matière d’orientation et d’insertion professionnelles afin d’évaluer s’ils traduisent une stratégie d’élaboration d’une identité numérique professionnelle."
"Le scenario-based design est une pratique de conception qui vise à anticiper l’expérience vécue de l’utilisation d’un produit/service en cours de conception. Le terme est impropre et on devrait lui préférer celui de « récit d’anticipation des usages ». Le manque de rigueur terminologique renvoie à un autre problème : celui de l’absence de règles d’écriture qui permettent de faire cohabiter des faits sociaux, économiques et culturels avec un imaginaire technique. C’est ce à quoi s’attache cet article : théoriser la pratique des récits d’usage et proposer des règles de construction des récits dans le cadre de contextes d’innovation."
"En prenant appui sur l'exemple du groupe Sud-Africain Die Antwoord, nous analyserons-en croisant la socio-économie, la médiologie et l'esthétique-les nouvelles pratiques de création et de communication à l'ère digitale. Nous tenterons de donner un contrepoint aux récentes enquêtes quantitatives (Beuscart (2008), Bastard et alii (2012)) qui relativisent les prévisions de Chris Anderson sur la longue traîne."
"Avec le recul, très court, d’une dizaine d’années d’un processus de type essai-erreur engagé sur l’objet de notre article, l’intelligence territoriale, nous essaierons par cette contribution de proposer un cadrage de « l’intelligence territoriale » en tant que capacité d’intelligence collective mobilisable sur un territoire ou résultat d’une démarche collective. Nous fonderons notre propos sur l’acquis cumulé d’un parcours professionnel, tant en entreprise que dans la Recherche, en France et à l’exportation, et d’un ancrage théorique avec plusieurs expériences, Recherches, applications d’ordre pratique terminées, en cours et à venir dépassant en cela le cloisonnement « théorie-pratique » obsolète. Nous poserons et décrirons le contexte informationnel et organisationnel de notre contribution dans une 1re partie intitulée « Dispositifs et Organisation », conséquences issues du développement combiné du numérique et de la distance, de relation et des organisations, dans un contexte de compétition économique contemporaine. La 2e partie définira et présentera l’intelligence territoriale en tant que champ scientifique, ses contraintes d’observation, d’analyse et de pratiques pour le « chercheur-praticien » officiant ou se situant dans une Science de la Société. La 3e partie mettra l’accent, en l’illustrant, sur ce qui fait sens dans l’intelligence territoriale à savoir le transfert organisé des compétences entre les acteurs locaux. Ce que d’aucuns nomment la « transversalité » et qui appelle, à notre sens, une adaptation voire un changement nécessaire, parce que vital, des comportements. Nous illustrerons notre propos en prenant appui sur trois applications associées à des recherches doctorales. Nous présenterons dans cette 3e partie, les caractéristiques communes à ces deux recherches."
"Nous prendrons appui sur deux projets en cours, de taille et d'impacts différents, dans deux continents distincts (Amérique du Sud, Afrique du Nord) c'est-à-dire, dans des univers culturels et à des stades de développement différents pour discuter de l'approche retenue, systémique dans l'un, constructiviste dans l'autre projet. Ces projets, national pour l'un, local pour le second, ne concernent pas le même champ d'application, et nous essaierons de repérer quels sont les éléments théoriques, pratiques, organisationnels qui peuvent relever d'un modèle qualifiable d'intelligence territoriale et, posés, en perspective d'un développement durable."
"Ce troisième ouvrage ponctue une série -trilogie- Vers l'Intelligence Territoriale composée pour l'heure de *Territoire & Territorialités (2002), suivi de *Mesurer la distance, Pensez la durée, Mémorisez le virtuel (2004). Plus généralement, cette contribution se propose de présenter une rétrospective des recherches, travaux, publications qui ont jalonné ces dix dernières années. Depuis le Programme ‘Mainate' (Management de l'Information Appliquée au Territoire) initié en 1994 au sein du laboratoire LePont, la création du Groupe Going (groupe d'Investigations des Nouvelles Gouvernances), la défense d'une thèse puis d'une habilitation à Diriger des Recherches et maintes coopérations en France, Europe (le réseau REIT http://www.intelligence-territoriale.eu ) et la fédération du laboratoire I3m-Equipe d'Accueil 3820. Les travaux qui seront pour partie présentés ici s'inscrivent dans un axe spécifique du laboratoire I3m intitulé : intelligence informationnelle (http://i3m.univ-tln.fr ). Nous en présentons les caractéristiques."
"La recherche in silico. Ce vocable indique le début et l'ampleur d'un phénomène en biologie moléculaire: les recherches ne sont plus seulement in vivo ou in vitro, mais ont un recours de plus en plus essentiel aux analyses informatiques. Il souligne ainsi l'importance des technologies de l'information et de la communication (TIC) dans le développement de cette discipline et en désigne surtout deux champs spécifiques : la génomique et la bioinformatique. Nous présentons ici un modèle explicatif du cycle de l'information scientifique et technique (IST) dans cette communauté par le biais d'une observation participante effectuée au sein d'un laboratoire de génétique des microbes à l'INRA. Nous avons appréhendé l'appropriation de l'information numérique par les chercheurs à l'aide d'un modèle heuristique construit sur le cycle de vie du document (acquisition, recherche, archivage...) et de transformation de l'information (informations / documents / connaissances) intitulé cycle de l'information scientifique et technique"
"Lors d'une enquête qualitative que nous avons menée en France auprès de producteurs de campagnes de prévention du sida, ceux-ci ont notamment indiqué qu'ils considèrent comme efficaces des dispositifs faisant réaliser des "" mini-actes "" aux personnes ciblées, avant et après la réception d'arguments persuasifs. Comme ils ne basent pas leur opinion sur de la littérature scientifique, nous avons réalisé une expérimentation, en milieu ordinaire, sur 196 sujets "" tout venant "" pour étudier, à partir des théories de la communication persuasive et de l'engagement, la validité des représentations concernant les "" mini-actes "" produits via Internet. Les résultats montrent notamment que ces producteurs de campagnes font preuve d'une bonne "" intuition "" puisque les mini-actes contribuent à effectivement favoriser la prévention du sida."
"Cet article est issu d'un projet de recherche action. Ce projet entend placer l'enfant en difficulté d'apprentissage ou de vie (y compris les situations de handicap) au centre d'un réseau d'accompagnement. Les intentions de la recherche se réfèrent à une économie du lien, dont la visée est de remettre le bien être humain au centre des priorités, et à l'intelligence territoriale ascendante."
"A travers l'exemple de la communication persuasive de santé publique, l'objectif sera de s'interroger sur la relation entre réception communicationnelle et action. La communication montre, d'une part, en quoi les recherches psychosociales sur la réception de la communication de santé publique peuvent être heuristiques en SIC pour comprendre les logiques d'action. D'autre part, elle envisage les nouvelles perspectives ouvertes par le récent concept de communication engageante. Les implications pour les recherches en communication de santé publique, et plus largement en SIC, sont également envisagées."
"Le mode de vie des pays de la Caraïbe, des pays de la Méditerranée et des pays tropicaux inclut le besoin de s'accommoder du climat et de prendre un rythme plus lent que ne le prescrit la norme des pays du Nord. D'où une langueur qui va jusqu'à être stigmatisée dans le terme « paresse ». Par ailleurs on décèle dans les pays occidentalisés, industrialisés et urbanisés un besoin de rompre le rythme de vie harassant qui est le leur par un appel à ralentir et changer de mode de vie. Le présent article propose de se réapproprier le « mode de vie siesta » comme un héritage culturel et immatériel remontant aussi bien à l'antiquité méditerranéenne qu'aux traditions orientales, créoles et amérindiennes."
"Ce livre présente un portrait inédit du mathématicien français Henri Poincaré à partir de ce qu'en disaient les journaux de son temps. Un choix abondant de coupures de presse permet en effet une approche originale du personnage : on y découvre les faits les plus marquants de sa carrière mais aussi son rôle dans l'espace public, tant pour ses multiples compétences scientifiques et techniques que pour ses éclairages philosophiques. Doublement académicien, auteur d'ouvrages largement diffusés, son aura dépassa le seul cercle des érudits pour toucher le grand public dans les domaines les plus variés, société savante et presse généraliste ayant fait de lui une sorte de référent dans la plupart des champs de la connaissance et au-delà. Des anecdotes les plus insolites aux publications méconnues, en passant par les diverses polémiques dans lesquelles on l'entraîna souvent malgré lui, les journaux nous dévoilent un Poincaré inattendu, qui se prêta au jeu de cette dialectique entre espace savant et espace public, assumant ainsi de façon originale une forme de ""vulgarisation scientifique"" comme un rôle d'éclaireur."
"Selon une formule célèbre "" le journalisme, c'est la vie "" (J. Fauvet)... Au cœur de cette vie saturée par la multiplicité et marquée du sceau de la différence, la tâche du journaliste consiste à trier les faits susceptibles de nourrir l'actualité. La parole journalistique assume ainsi la responsabilité de proposer un cadre interprétatif au réel. En ce sens, le journaliste qualifie l'événement, c'est ce qui lui confère une place particulière dans tout espace de communication (D.Wolton). Cette manière de penser l'évènement, de le nommer, doit nécessairement faire l'objet d'un questionnement épistémologique. Ce questionnement s'impose d'évidence au regard d'une pratique journalistique en mutation s'appliquant à un espace méditerranéen lui-même caractérisé par sa complexité. Le monde méditerranéen est, en effet, circonscrit par l'affirmation paradoxale d'une identité commune et par l'existence de variations dans notre conception des identités. Cette complexité, liée à l'espace géographique, doit être confrontée au développement récent des réseaux. Ces derniers engendrent une redéfinition des frontières de l'espace public, une déterritorialisation possible du contexte de réception et favorisent l'apparition de nouvelles pratiques de production de l'information. L'avènement de la "" société de l'information "" a ainsi bouleversé les habitudes de la profession. Les possibilités de diffusion, de recueil et de traitement de l'information se sont élargies entraînant une mutation des modèles de production de l'information. Désormais, chacun a la possibilité d'occuper une place dans un espace communicationnel, d'être le témoin de son époque, au jour le jour, en diffusant, sans limite ou presque, les informations de son choix. Chacun peut alors s'affranchir des contraintes liées au journalisme traditionnel. Cette mutation implique un véritable partage du pouvoir de "" nommer "", et de "" faire "" l'événement. La prise en compte de cette nouvelle réalité impose ainsi une réflexion éthique et sociétale sur l'exercice de cette responsabilité. Cette analyse doit s'articuler sur la constitution d'un espace public étendu, aux dimensions d'un espace politique méditerranéen caractérisé par une certaine complexité. * Axe 1 : Le pouvoir de nommer et la construction de l'évènement dans l'espace euro méditerranéen. * Axe 2 : Nouveaux médias et logique "" dialogique "" en méditerranée. * Axe 3 : Mutation des modèles de production de l'information : perspectives pour une éthique renouvelée."
"L'analyse de dizaines de milliers d'articles, produits par environ deux cents sites en France, permet d'évaluer la diversité des informations sur le web. La variété éditoriale apparaît très élevée, avec plusieurs centaines de sujets abordés chaque jour. Mais elle connaît en même temps une distribution inégale : quelques sujets ultra-médiatisés s'imposent au détriment d'une myriade de sujets isolés dans les méandres du web. La contribution de chaque site à ce double mouvement montre une partition du champ de l'information web, entre un mainstream médiatique tiré par les infomédiaires (portails, agrégateurs) et les sites de médias traditionnels, et une originalité éditoriale plutôt amenée par les blogs et les sites natifs de l'internet."
"Nous montrons que des publicités sur internet non traitées en vision centrale sont tout de meme mémorisées et ont un impact sur l'attitude à l'égard de la marque, alors que les sujets n'ont pas conscience de les avoir vues. Ces effets tiennent 8 jours en mémoire si les messages ont été répétés 15 fois (vs 5 fois)."
"Le cycle de l‟information, de la collecte à la dissémination, est central en intelligence économique. D‟autre part, depuis quelques années, le web 2.0, le web inscriptible a modifié la face d‟internet. Nos travaux ont pour sujet l‟étude de l‟impact que ce fameux web 2.0 a sur le cycle en question et nous proposons des méthodes et outils afin de tirer parti de ce nouveau paradigme, et ce, pour chaque étape du cycle"
Il s'agit du prolongement des éditions précédentes du ‘Petit Guide’ et des expériences professionnelles antérieures.
"Cette recherche porte sur les enjeux symboliques, artistiques et représentationnels liés à la démocratisation des appareils connectés. Car l’écran, désormais envisagé dans sa mobilité, fait non seulement office de fenêtre ouverte sur le monde (Alberti), mais plus encore, il compose un vaste paysage où le cyberespace, monde virtuel aujourd’hui à portée de tous, se combine avec un réel mis à distance, fragmenté, puis recomposé. Ainsi en va-t-il de nos trajectoires se dessinant sur Google Maps, de cette ville qui apparaît comme un nouveau texte à déchiffrer et sur laquelle les appareils connectés juxtaposent de nouvelles cartes. Dès lors se dessine, dans la frénésie informative que nous proposent ces objets techniques d’un nouveau genre, une vision de la technique participant de ce que Michel de Certeau appelait une ’sensibilité partagée’. Cette sensibilité, acquise à mesure que les dispositifs informatiques se sont perfectionnés, tisse de nouveaux réseaux d'associations entre l'individu et son milieu. Par conséquent, là où l’histoire et la théorie de l’art ont mis au jour un certain nombre de questions relatives à la surface, au cadre, au plan, la cybersensibilité actuelle s'inscrit dans une histoire non seulement technique, mais également optique. La ville, et sa symbolique du territoire, se présente non pas comme décor mais se donne à voir tel un terrain de jeu (sandbox) propice à de nouvelles explorations. Considérant ce maillage inédit de perspectives, nous garderons de la cybersensibilité sa formidable aptitude à reconsidérer le milieu urbain ou numérique suivant de multiples points de vue qui sont autant de nouvelles tactiques de nouvelles approches."
"Cette thèse s’intéresse à l’émergence des théorisations accompagnant le développement du monde de la communication en entreprise et dans les organisations. Communication est entendu ici comme un domaine professionnel de métiers et d’activités économiques, avec parfois un mode d’organisation de services en entreprise, parfois un secteur économique d’agences. En faisant ce travail de retour « généalogique », cette thèse s’intéresse dès lors aussi aux formations et aux recherches, progressivement installées dans un contexte universitaire : l’émergence de la « communication organisationnelle. Cette analyse généalogique est ici menée de manière comparative dans deux pays-nations : le Brésil et la France. Revenir sur ces moments où se développent les Relations Publiques, le Journalisme d’Entreprise, la Communication d’Entreprise, la ou les « Communication (s) Organisationnelle(s), amène à s’intéresser aux Etats-Unis, lieu d’émergence de pratiques et d’organisations professionnelles ainsi que de théorisations qui ont fait date, ont été exportées, ont fait modèle.Il s’agit aussi de décrire l’émergence et le développement de la « communication organisationnelle » au Brésil (souvent dans le cadre de la « communication sociale » et de l’étude des Relations Publiques, avec l’apparition de l’appellation « communication organisationnelle » en 1985) et en France (un peu plus de vingt ans après la création en 1975 d’une nouvelle discipline « Sciences de l’information et de la communication »). L’analyse de l’institutionnalisation d’un champ académique de recherches montre, c’est un élément essentiel, que les cadres nationaux des disciplines universitaires conduisent à des théorisations différentes. Les traditions théoriques, non seulement divergent en termes d’appui sur des interdisciplinarités différentes mais aussi en termes de construction de rapports sociaux entre les universitaires eux-mêmes et les mondes qu’ils observent, auxquels souvent ils contribuent. Certains, par la formation à la recherche et la consultance sont plus proches des approches a mélioratives souvent qualifiées de « fonctionnalistes », d’autres sont dans une distance critique et doivent trouver les modes d’accès et de suivi des évolutions tant des organisations que de leurs communications. Une approche particulière, la « Communication intégrée », développée universitairement au Brésil et encore défendue aux USA et en France par des agences et des professionnels nous permet de montrer dans le dernier chapitre de cette thèse comment les effets différenciés de l’institutionnalisation, dans les deux pays, pour la « communication organisationnelle », aboutissent à deux théorisations divergentes portées par des acteurs aux traditions et enjeux sans commune mesure."
"Dans le cadre de cette recherche nous nous interrogeons sur la perception et la représentation actuelles de l'architecture des grands hôtels à vocation touristique en Tunisie. C’est plus particulièrement le travail d’un architecte, d'Olivier-Clément Cacoub, que nous interrogerons, compte tenu du nombre important d’établissements dont il a été le maître d’œuvre et qui ont marqué une période importante du développement touristique de la Tunisie. Notre recherche repose sur l'étude du patrimoine architectural de Sousse, notamment le style et l'iconographie de la façade des hôtels du XXe siècle ainsi que les aspects pérennisant du tourisme et de l'image touristique. Dès lors, nous nous poserons la question de savoir si le tourisme permet de préserver le patrimoine architectural à travers la création d'hôtels au XXe siècle. Comment l'image du patrimoine architectural peut-elle être valorisée par les façades d'hôtels ? Comment la représentation de l'architecture de l'hôtel a-t-elle été comprise par les architectes ? La mise en valeur d'une ville touristique dépendait largement de la préservation de son patrimoine architectural. Aujourd'hui, les architectes veulent changer le regard de la ville, lui donner un autre portrait. De là naîtrait l'idée de la conservation du patrimoine, de sa définition et au regard de ses traditions, le rôle que le patrimoine architectural joue dans la conception des hôtels. Nous essayerons ainsi, de montrer de quelle manière la représentation du patrimoine architectural est mise en œuvre dans les hôtels et comment elle participe à la valorisation d'un territoire."
"— L'approche théorique et méthodologique développée dans ce texte a pour objectif d'analyser ce qui se joue dans les perceptions et les interactions de sujets qui utilisent des environnements numériques dits immersifs. Après avoir présenté ces environnements et le contexte de la recherche, il problématise la relation du corps des sujets à l'espace physique et l'espace numérique. Elle interroge le rapport de ces sujets à la technique. Nous proposons de caractériser ce rapport par une expérience vécue qui joue sur le sensoriel et l'imaginaire des sujets. Il s'agit ensuite de mobiliser une méthodologie originale pour analyser les usages de ces environnements. L'approche énactive est utilisée pour saisir ce qui se joue au niveau des perceptions et des interactions dans ces expériences. Associée à une approche interactionniste et sémio-pragmatique, cette recherche interroge aussi les publics, les contextes et les modalités sociales de ces expériences vécues par les sujets. Par la conjugaison de ces approches, la contribution tente d'identifier les éléments qui permettraient de clarifier, sur le plan méthodologique, le rapport des sujets à leur environnement numérique, et notamment sur ce qui pourrait relever dans cette relation, d'un couplage fort ou, au contraire, d'une opposition à celui-ci."
"La culture informationnelle constitue une des compétences attribuées à l’employabilité des individus dans l’actuelle société de l’information, des connaissances et du savoir (SICS). En exigeant des compétences employables (CE), celle-ci prend à notre sens une nouvelle dimension (SICS&CE) faisant ainsi transiter le marché du travail du concept du « marché d’emploi » d’attributions fixes et de routines classiques à celui du « marché d’employabilité (MEMP) » sollicitant la flexibilité et les compétences adoptives et adaptatives aux changements socio-économiques. Désormais, la veille stratégique en général et précisément numérique, œuvre par son « Processus de production d’information et des connaissances » à développer la culture informationnelle de veille sur Internet (CIVI) pour améliorer les diverses compétences individuelles. Dans ce sens, cette thèse vient dans le cadre d’une analyse qualitative et une approche exploratoire interprétative, traiter la problématique du développement de la qualité des compétences définissant l’employabilité des apprenants, futurs diplômés universitaires. Elle cherche à savoir dans quelle mesure une veille pédagogique et stratégique universitaire peut améliorer la qualité de leurs compétences employables en culture CIVI, en adoptant dans son processus de production informationnelle, un marketing relationnel orienté-marché impliquant les apprenants universitaires en tant que « clients internes », aussi bien que les acteurs du marché d’employabilité en tant que « clients externes » ? En évoluant dans une perspective socio-constructiviste, des séries d’évaluations sur les compétences et les satisfactions des apprenants aussi bien que des modèles d’évaluation de leurs compétences en CIVI ont été élaborés dans le cadre d’une approche par compétence (APC) adoptant un marketing relationnel interne et externe. Cette approche se caractérise en premier lieu par une : « Orientation-Implication des Apprenants-Clients : OI-APCL » et en second lieu par une « Orientation-Implication du Marché d’employabilité : OI-MEMP », se basant sur un blog pédagogique en tant que Système informationnel et outil de veille Web.2.0, destiné à développer la compétence en culture informationnelle des apprenants. Ceci nous a conduit à constater dans le cadre de l’orientation vers une université d’employabilité, l’amélioration des taux des compétences par la baisse des groupes profanes en culture CIVI, passant de 88,74% à 41,94% et un taux de besoin de formation en culture CIVI passant de 89,50% à 46,50% conduisant à la satisfaction des apprenants et des acteurs d’employabilité impliqués dans notre travail et le recrutement de trois groupes des 20 binômes apprenants/diplômés concernés par notre travail, Ces résultats amènent à recommander l’adoption d’une « veille d’employabilité » pour le développement de la qualité des compétences individuelles dans une société SICS&CE sollicitant désormais en plus des connaissances et du savoir, l’actualisation par l’apprentissage à vie et la flexibilité d’adaptation aux changements et innovations, définissant ainsi l’employabilité des individus."
"On s'interroge sur l'avenir des écritures face aux transformations en cours dans les espaces-temps documentaires et des communautés d'oeuvre(s) en procès de production, constamment relancées, ouvertes, et ce dans le contexte numérique. On prendra la question du Web sémantique comme repère. Il s'agira de mettre en perspective les technologies intellectuelles et cognitives, à l'œuvre ou en train d'émerger au cœur des modes de production, de circulation des savoirs et des apprentissages. Ces technologies affectent les capacités associationnistes et analogiques des intelligences, selon des niveaux d'échelle variés et variables. Transversalités, pratiques cartographes, « informatique sémantique » et sémiotique non exclusivement linguistique seront au centre de notre réflexion."
"Lac intérieur pacifié –Mare Nostrum-ou terrain d'affrontements, la Méditerranée occupe les réflexions de l'Orient comme de l'Occident depuis des millénaires. Au moment d'une rencontre du Groupe de Recherche en Intelligence Territoriale en terre Maghrébine et au cours de laquelle vont être exposés des projets venant largement des bords de la Méditerranée, nous nous proposons de confronter ces projets avec les principes de l'intelligence territoriale. L'article est divisé en deux parties. La première est une synthèse des avancées présentes sur la notion d'intelligence territoriale, partant de ses origines et de ses conditions de développement pour aboutir à une définition qui reflète en quelque sorte l'état épistémologique actuel du mouvement de l'Intelligence Territoriale. La seconde propose de partir d'une sélection des communications au colloque Ouarzazate 2015 sur Le développement durable des territoires vulnérables. Le « prisme » de l'intelligence territoriale appliqué à cette sélection permettra de mettre en lumière la façon dont les acteurs de l'intelligence territoriale s'appuient sur les principes évoqués précédemment. La discussion se prolongera dans l'exploration des lacunes actuelles et l'ouverture vers un enrichissement des interactions transméditerranéennes. C'est la vision de l'intelligence à l'oeuvre qui pourra nous rendre optimistes, malgré tous les nuages qui persistent. Et l'on se posera aussi la question de savoir si l'intelligence territoriale peut infuser la géopolitique méditerranéenne."
"La France n’a pas encore de système de collecte d’informations sur les immigrants standardisé. Ainsi, l’état de santé des immigrants est, aujourd’hui encore, déterminé de manière irrégulière. La seule information de santé des immigrants disponible correspond au statut de santé déclaré par l’immigrant. Ceci est à la fois problématique et surprenant car la France est considérée comme une des nations au monde à avoir le meilleur système de santé malgré le manque de données concrètes sur l’état de santé des immigrants. Une question se pose alors : « comment les immigrants se fraient un chemin dans le système de soin français ? ».Les Philippins vivant dans le sud de la France sont la cible de cette étude. Cette étude utilise le sondage, le « Key Informant Interview » et l’observation directe pour rassembler des données afin de comprendre comment leur culture et leur langage maternel interfèrent avec le système de soin français.Dans cette étude, l’hypothèse émise était que le langage est une barrière pour les immigrants Philippins cherchant l’accès aux soins médicaux. De plus, la culture traditionnelle de santé joue aussi un rôle important dans la pratique de santé des Philippins, même si ces derniers sont en France depuis de nombreuses années. Ils ont réussi malgré tout, à s’adapter au système de soin français. Cependant, cette adaptation doit être clairement identifiée et décrite. A l’interface de la culture, du langage et du système de sin de santé du pays d’accueil, les immigrants, consciemment ou non, créent une culture de santé émergente, différente de leur culture d’origine, qui identifie mieux un bon ou un mauvais état de santé ainsi que la manière de l’exprimer.Cette étude révèle que malgré le temps passé dans le sud de la France, la culture de santé traditionnelle des Philippins joue toujours un rôle dans leur pratique de santé. Les problèmes de santé sont toujours une affaire de famille. Ainsi certains médicaments sont importés des Philippines. D’un autre côté, ils profitent aussi des avantages du système de soin de santé français. Ils rendent visite à leur docteur plus souvent, ils prennent religieusement leurs médicaments et ils profitent des nombreuses options de traitements disponibles en France. Ils peuvent profiter de tout cela parce qu’ils sont couverts par le système universel d’assurance maladie français.D’un autre côté, les immigrants Philippins restent des patients passifs. A cause de la langue qui reste encore une barrière, leur comportement vis à vis de médecin est fait de hauts et de bas. Les Philippins sont d’un naturel timide. Ainsi ils essayent toujours de ne pas avoir de longues conversations. Ceci reste un défi pour les professionnels de santé qui ne sont ni formés ni préparés à gérer des patients parlant un langage étranger."
"Pour soutenir le développement de zones arides, fragiles et menacées au Maghreb, les acteurs politiques et les décideurs économiques misent sur un développement touristique qui accorde une place centrale aux patrimoines locaux dont font partie les Gsours. Les actions publiques en ce sens visent aussi à impliquer les populations locales dans la valorisation de ces patrimoines. Toutes ces initiatives et projets tiennent encore peu compte d’un autre public instable et protéiforme : les touristes, un public particulier destinataire et vecteur de communication territoriale en construction. Nous proposons, dans cet article, une première approche de ces publics touristiques dans ces zones qualifiées de fragiles et menacées (le cas du Sud-Est tunisien), et analyserons cette communication interculturelle et institutionnelle dont il est question dans les territoires concernés."
"Cette recherche s’intéresse aux effets induits par les dispositifs numériques de partage social sur les pratiques de collaboration, de communication et de médiation d’étudiants, dans un contexte situé de formation à distance. L’objectif est d’appréhender le processus de collaboration à l’origine de l’apprenance collective qui s’illustre dans les communautés d’apprentissage, constituées en marge de l’institution académique. Notre approche empirique par systématique hypothético-déductive est une observation netnographique menée auprès d’individus inscrits en Bachelor et Mastère dans un centre de formation privé. Notre corpus est composé de 1405 messages recueillis sur les forums de la plateforme institutionnelle et sur Facebook ou Google+ pour les groupes communautaires à l’initiative des étudiants. Nous recourons à une catégorisation de l’activité d’apprentissage suivant quatre dimensions intrinsèques à l’intervention des usagers sur les forums. Les phénomènes observés sont analysés suivant trois variables dépendantes : la temporalité, la reconnaissance sociale et le pouvoir hiérarchique. Notre ancrage est la théorie critique des médias de l’école de Francfort. Nous faisons l’hypothèse que les dispositifs sociotechniques d’information et de communication participent de l’horizontalisation des usages estudiantins. Nos observations de terrain révèlent en effet que les apprenants préfèrent s’en remettre à leurs pairs plutôt que solliciter l’institution ou les tuteurs en ligne. S’ils privilégient leur disponibilité indéfectible, ils accordent une plus grande importance à leur réactivité. Nous pensons que le clivage qui oppose l’environnement académique à la sphère estudiantine est pour partie le fait de deux temporalités distinctes. L’une verticale, celle de l’environnement numérique de travail (ENT), l’autre plus horizontale, caractérisée par les échanges entre pairs au sein de l’environnement personnel d’apprentissage (EPA). L’asynchronisme qui résulte de ces deux dispositifs engendre des effets de détournement d’usage par lesquels les étudiants exportent les ressources institutionnelles vers leur communauté. Mais l’analyse des praxis communautaires révèle d’autres artefacts induits par les technologies numériques. Qu’il s’agisse de désintermédiation ou d’accélération temporelle, ces usagers attirés par le modèle a hiérarchique, se retrouvent à leur insu dans un processus de domination sociale. Nous soulignons les effets pernicieux liés à l’accélération temporelle particulièrement prégnante dans la génération d’étudiants observée."
no abstract
no abstract
"L’art contemporain africain a fait son entrée dans le système globalisé, d’abord par les manifestations artistiques, ensuite dans les maisons de vente. Depuis les années post-indépendances, une nouvelle vision de l’art contemporain africain a émergé avec l’avènement des discours postcoloniaux. Ces idées inédites ont entraîné une lecture différente de l’esthétique contemporaine africaine avec l’appui de certains commissaires d’exposition. Parallèlement, une vision de l’artiste « authentique » subsiste dans les manifestations et donne de l’artiste africain une image qui peut paraître quelques fois figée. Les artistes contemporains africains se retrouvent aujourd’hui dans une situation conflictuelle ; ils doivent soit se rapprocher des centres d’impulsion artistiques c’est-à-dire du marché de l’art occidental ou rester sur le continent africain et tenter par des initiatives diverses et variées d’intégrer un marché qui reste encore en occident. Les zones périphériques arrivent cependant peu à peu à se muter en centres d’impulsion mais restent pour le moment modestes.Avec l’avènement des nouvelles technologies, des opportunités s’offrent aux artistes des périphéries en termes d’usages mais aussi en tant que medium. Leur application pousse à la réflexion, et à une tentative de compréhension des enjeux qui se profilent pour l’art contemporain africain et des perspectives qu’il faudra entrevoir à travers le paradigme des postcoloniales studies et à travers l’avènement des technologies de l’information et des communication. Est-il possible de proposer de nouvelles approches pour promouvoir la diffusion de l’art contemporain africain à travers les dispositifs socio-techniques disponibles mais aussi à travers la gestion de l’information. ? Dans une première partie, cette thèse tente de montrer la diversité des arts de l’Afrique pour en comprendre la complexité aujourd’hui. Puis en analysant les liens qu’elle a eus avec l’occident, d’en comprendre l’histoire.Dans une seconde partie la place de l’identité de l’artiste africain mais aussi son positionnement puis les initiatives qui sont menées sur le continent permettront de mieux appréhender les enjeux et perspectives qui dans la troisième partie, permettront d’avoir un point de vue global sur ce qui régit aujourd’hui l’art contemporain africain."
"Qu’il s’agisse du temps, des espaces, des relations, des formes du travail, les machines numériques dessinent un devenir instable et un horizon ouvrant sur des recompositions en cours. Dans les industries de la connaissance, l’évolution paraît emprunter un détour homéostatique qui voit le traitement de l’activité se modifier sans que la structure ou l’institution évolue véritablement. Dans des organisations privées, c’est l’horizon de la collaboration, de la cognition au travail, qui est nettement traversé par des évolutions plus que sensibles qui mettent en exergue la notion d’organisation apprenante. Dans le secteur de la santé, on peut assister à un véritable hiatus entre l’évolution de l’activité et le maintien d’une forme organisée. Depuis les travaux des années 1990-2000 (Zarifian, Le Moigne, Alter) jusqu’à aujourd’hui (Leplat, Conein, J.R. Taylor, D’Almeida, Durampart et tant d’autres), des constats pointent à la fois la nature instable des changements conduits avec des apories entre activité, hiérarchisation et formes de structuration du travail. Les recherches soulignent aussi des tensions entre rationalisation et bien être commun (Boltanski, 2009). L’insertion des machines numériques comme levier d’orientation fixant un devenir plus plastique et flexible d’une organisation supposée s’apparenter à une souplesse neuronale (Malabou, 2004) est autant une affaire d’incantation que de ferments constatés d’un mouvement combinant de nouvelles temporalités plus ou moins maitrisées (Rosa, 2010) en relation avec l’action, la réaction et la performance des fondements de l’organisation."
"Cet article part du constat que, célébrée par plusieurs disciplines voisines, sociologie, philosophie et histoire de l'art en particulier, la pensée de Guy Debord (qui met au centre de sa réflexion l'image et la représentation) est peu citée en Sciences de l'Information et de la Communication. Pour tenter de lui redonner toute sa place, nous chercherons, dans un premier temps, à présenter les principaux éléments de sa théorie, ses limites et ses critiques. Nous essaierons ensuite de montrer la contemporanéité de sa pensée critique au regard des média numériques et enfin de redéfinir sa place dans le champ des Sciences de l'Information et de la Communication."
"Un demi‐siècle s’est écoulé depuis l’invention du texte infini. L’hypertexte, qui structure l’accès à l’information, n’est nullement dépassé dans la « société numérique ». Cet ouvrage présente les techniques hypertextuelles et dresse un panorama des nouvelles technologies liées à la production, aux formes et à la réception des objets hypertextuels. Pluridisciplinaire, ce texte en sciences de l’information-communication, sociologie, littérature, sciences du langage, etc., propose une réflexion sur les évolutions du livre, de la fiction, du récit interactif, de l’objet sonore, mais également des jeux vidéo, des mobiles, des réseaux sociaux ou des cours en ligne. Au‐delà d’un simple enjeu, il s’agit de proposer une éthique scientifique qui doit guider les recherches contemporaines."
"Notre action sur le comportement, en liaison avec les autres aspects souhaitables d’une révolution dans les moeurs, peut se définir sommairement par l’invention de jeux d’une essence nouvelle. Le but le plus général doit être d’élargir la part non médiocre de la vie, d’en diminuer, autant qu’il est possible, les moments nuls. On peut donc en parler comme d’une entreprise d’augmentation quantitative de la vie humaine, plus sérieuse que les procédés biologiques étudiés actuellement. Par là même, elle implique une augmentation qualitative dont les développements sont imprévisibles. Le jeu situationniste se distingue de la conception classique du jeu par la négation radicale des caractères ludiques de compétition, et de séparation de la vie courante. Par contre, le jeu situationniste n’apparaît pas distinct d’un choix moral, qui est la prise de parti pour ce qui assure le règne futur de la liberté et du jeu. Ceci est évidemment lié à la certitude de l’augmentation continuelle et rapide des loisirs, au niveau de forces productives où parvient notre époque. C’est également lié à la reconnaissance du fait que se livre sous nos yeux une bataille des loisirs, dont l’importance dans la lutte de classes n’a pas été suffisamment analysée."
Dans cet article on s'efforce de réfléchir à un cadre conceptuel permettant de penser le pluralisme linguistique-sémiotique au sein de nouveaux agencements collectifs d'énonciation et d'indiquer certaines voies pour le renouvellement des approches pragmatiques. Nous nous appuyons en grande partie sur le travail développé par Felix Guattari et espérons en montrer la fécondité.
On essaie d'aborder l'émergence de Twitter dans le cadre d'une réflexion plus générale sur certains aspects de la prolifération des formes courtes. Cette prolifération touche toutes les substances d'expression. Cette réflexion et ce travail en cours ne font que les évoquer.
"On s'attache dans cet article à poser un certain nombre de réquisits pour une Narratique Générale, à en examiner certaines difficultés, en particulier d'ordre sémiotique, dans le cadre de la grande transformation numérique. Cette transformation numérique est regardée de manière simplifiée, à travers le prisme du ""devenir empire"" selon Negri et Hardt, sous les conditions du ""devenir entrepreuneurial"" et du ""devenir expérimental du politique"". 1 On porte une attention particulière dans le cadre de la strate anthropologique ""internet"", à l'émergence du Data Mining comme narration impériale, comme grand récit des sociétés performatives. On évoquera ainsi la sainte et obsédante trinité: Performation / Prédiction / Préemption qui sert aux sociétés de veille enchassées dans le développement du capitalisme mondial intégré comme processus hétérogène et conflictuel. Enfin on s'intéressera à la prolifération des formes courtes à travers, entre autres, le déploiement de Twitter, mais pas uniquement, et à certains de ses effets politiques et socio-cognitifs."
"Connue sous l'ancien nom de Mogador, la ville d'Essaouira (Atlas atlantique), construite à la fin du XVIIIème siècle, est un site portuaire et fortifié, qui a vu son essor urbain se prolonger jusqu'au milieu du XXème siècle. Elle est entourée à l'origine, d'une forêt dense de genévriers de Phénicie (Juniperus Phoenicea) couvrant 14 000 ha de manière homogène, du trait de côte jusqu'aux plateaux intérieurs (à une quinzaine de kilomètres du littoral). Cette formation vient alors se mêler et céder le pas progressivement à des peuplements de thuyas (Tetraclinis articulata) et d'arganiers (Argania spinosa). L'édification de la ville et son développement ont nécessité d'importants besoins en bois de construction et de chauffage principalement. Les boisements de genévriers situés aux portes de la ville ont fait l'objet d'une exploitation massive et incontrôlée ne considérant pas l'arbre comme un élément vivant et donc ne respectant aucune règle sylvicole. La pression exercée sur le milieu végétal s'est accompagnée d'un pâturage traditionnel, extensif et soutenu. Cet élevage pratiqué par les populations riveraines est majoritairement caprin, mais aussi ovin, bovin et camelin. Conjointement au déboisement, il n'a fait que renforcer le déséquilibre créé en empêchant les jeunes repousses et les rejets de se développer (broutement et piétinement). Tant et si bien qu'au début du siècle, la forêt de genévriers a complètement disparu dans un rayon de 8 à 15 km, excepté quelques isolats résiduels. A sa place se sont installés des sables mobiles d'origine marine et terrestre apportés par l'alizé. La vitesse de déplacement de ces formations dunaires pouvant atteindre 40 à 150 m/an, l'accès à la ville est fréquemment coupé depuis le début du XIXème siècle. Seules les caravanes de chameaux permettaient de franchir ces obstacles qui renforçaient nettement l'isolement naturel d'Essaouira. Un plan de reboisement a été mis en place à partir de 1914. Les travaux effectués jusqu'en 1987ont portés leur fruits puisque les résultats obtenus se sont avérés nettement plus positifs que les espoirs qu'ils portaient. Aujourd'hui, 11 444 ha sont reboisés. Les dunes vives sont donc fixées et végétalisées. Aussi le genévrier de Phénicie recolonise progressivement les lisières intérieures des dunes par une remontée biologique spectaculaire. Cela dit, le nouvel équilibre installé reste plus précaire que ne l'était l'équilibre naturel. En effet, des réactivations locales sont perçues depuis peu et l'érosion éolienne est en passe de reprendre son action dévastatrice. En 1994 encore, il était surprenant de constater qu'à l'intérieur de la médina les ruelles étaient envahies de sables malgré la protection de hauts remparts. Sous l'effet du vent violent et continu, qui souffle presque toute l'année, la menace de reprise reste constante. Les aménagements réalisés ont un but de protection et non d'exploitation. Le milieu dunaire est très fragile et toute nouvelle atteinte à son couvert végétal impliquerait un déséquilibre supplémentaire. Elle renforcerait l'érosion de sols peu évolués mais en cours de pédogénèse, l'ensablement qui anéantit l'agriculture dans l'arrière-pays, isole la ville et favorise des conditions atmosphériques insoutenables pour la population. Les délits de coupe et le non-respect des mises en défens sont encore trop fréquents."
"La première partie de ce dossier fut entièrement consacrée à l'exposé des fondements théoriques sur lesquels nos travaux de recherche se sont appuyés depuis maintenant une dizaine d'années. Le préalable nous est vite apparu incontournable sachant que chaque chercheur mobilise dans le cadre de ses travaux un ensemble de méthodes, d'outils, de procédures qui ne sont pas sans effet sur les résultats obtenus. Il s'agit là d'un protocole de recherche que tout travail en sciences sociales doit clairement annoncer à l'instar des sciences dures. Honnêteté intellectuelle et légitimité scientifique obligent. A cet effet, une première section présenta la théorie des conventions comme le socle théorique de la démarche. En abordant la conduite humaine selon un angle sensiblement différent, elle enrichît l'analyse de notions sociologiques que le courant économique standard avait, à notre sens, trop rapidement écartées. Nous estimions en effet qu'elle pouvait constituer la meilleure réponse aux impasses dans lesquelles une approche strictement contractualiste aurait pu nous entraîner. En introduisant les phénomènes d'incertitude et la rationalité mimétique comme réponse, la théorie des conventions resitue le comportement de l'agent économique dans un espace normé. Notre compréhension du fonctionnement de l'organisation et des marchés en est ressortie renouvelée et enrichie. La section suivante a permis de présenter le monde social et les moteurs de l'action humaine tels que nous les concevons ; sans nier que l'individu use de sa capacité de calcul, celle-ci est largement amoindrie par le poids du collectif. Sa conduite est tout autant dictée par les structures et les comportements dominants que par sa raison profonde. Entre l'individualisme méthodologique dur et le holisme surdéterminant, nous avons alors qualifié notre positionnement intermédiaire d'individualisme méthodologique affaibli. Notre posture constructiviste annoncée, nous avons considéré dans une troisième section que si le réel n'existe pas indépendamment de celui qui l'observe, le chercheur peut toutefois tenter de le saisir sans verser dans un discours normatif : comprendre et rapporter ce qui est sans avancer ce qui doit être. Nous avons alors terminé en décrivant les techniques d'investigation adoptées pour valider nos énoncés théoriques. Entretiens et sondages ont ainsi confirmé la nature conventionnelle de l'acte comptable. Seconde partie Ces présupposés scientifiques clairement exposés, la seconde partie nous a amenés à proposer une relecture sociologique du système comptable. Conscients des insuffisances que présentent les approches standard, nous avons emprunté au courant conventionnaliste son mode de raisonnement. Conformément à notre engagement épistémologique, nous avons commencé la réflexion en montrant que l'objet comptable ne s'impose pas de lui-même. En fonction de ses besoins, de ses objectifs et de sa propre culture, chacun s'en construit une représentation spécifique. Toutefois, les fonctions informationnelle et redditionnelle de la comptabilité devaient faire émerger un modèle dont les divers utilisateurs ont du accepter les hypothèses fondatrices. Acceptation ne vaut toutefois pas approbation et le respect du principe d'unicité engendre des situations auxquelles le modèle comptable n'est pas adapté. La représentation des structures réticulaires en est une illustration. Nous avons alors ensuite pris le parti d'analyser les conventions qui fondent le système tel qu'il s'est imposé à tous. Fortement marqués par un point de vue particulier, les choix qui président à la conception et l'évolution des comptes contribuent à orienter discrètement le regard que tout utilisateur pose sur la firme. La théorie des conventions a permis alors d'en mieux saisir la structure, l'origine et la logique. Ce jeune courant s'est présenté à nous comme une alternative salutaire au paradigme contractualiste. Nous avons ainsi découvert que la convention comptable pouvait être approchée et étudiée comme une procédure collective identifiable par sa morphologie comme toute autre convention. Constituant un ordre surplombant les individus et les groupes, sa principale fonction est de procurer à l'individu un ensemble de balises afin d'assurer une convergence des pratiques comptables dans un espace normé. La théorie des conventions fournit de cette façon, une grille de lecture originale invitant l'observateur à reconsidérer la comptabilité et les acteurs qui l'animent. Celle-ci ne se décrète pas en vertu de règles immanentes, justes et intangibles, mais apparaît plutôt comme une composition collective faite de choix, d'influences et de compromis. Quant aux acteurs, leurs actes au sein de l'espace comptable sont compréhensibles pour peu qu'on ait préalablement mis à jour les conventions auxquelles ils font référence pour agir. Enfin, nous avons considéré que la régularité de la convention n'excluait en rien son évolution. Aussi, cette perspective dynamique de l'étude était particulièrement enthousiasmante pour le sujet qui nous préoccupe : le changement dans les comportements collectifs, notamment les pratiques comptables, ne relève pas du pur hasard. La transformation progressive et désordonnée des conventions comptables répond en fait à des influences et des courants profonds, s'appuyant sur des mécanismes collectifs et socialement construits. Nous avons tenté alors d'en découvrir la logique. L'évolution d'une règle comptable nécessite l'émergence en amont, d'une convention adverse propre à la mettre en doute. Cette « alternative » présente un discours dont la cohérence et la pertinence peuvent selon les conditions séduire la population des convenants. En ce sens, elle doit être considérée comme l'élément de motricité par excellence du processus d'évolution des conventions comptables. Aussi, l'analyse incontournable de ses sources a-t-elle révélé deux catégories de facteurs d'émergence : les facteurs exogènes et endogènes, selon que leurs déterminants sont liés ou étrangers aux origines de la convention en place."
"La ligne éditoriale de l'Illustration révèle un discours naturellement colonial. La condamnation salutaire de ce discours laisse cependant un doute quant à notre manière de regarder aujourd'hui un document datant du siècle dernier. Peut-on examiner un numéro de 1917 de l'Illustration sans se trouver immédiatement submergé par nos convictions actuelles ? Afin de répondre à ces questions, nous nous proposons, sous l'angle sémiotique, de comparer la première de couverture de l'Illustration du samedi 15 décembre 1917 et certaines des photographies issues de ce même numéro consacré au Maroc."
"Cet article se propose d'étudier le difficile rapport entre le scientifique et la diffusion et la reconnaissance de ses idées et théories. Nous montrerons à travers des exemples historiques et contemporains que la question de la censure a toujours été présente dans cette dialectique souvent délicate entre le monde des sciences et la société, pour des raisons idéologiques, politiques, ou même souvent internes à l'univers des sciences lui-même, et à ses règles parfois autoritaires de reconnaissance et de contrôle de la parole de ses pairs. Science censurée, certes, mais science censeur aussi : les révolutions scientifiques, qui bouleversent des paradigmes forts, ne se font pas sans douleur, et sans une censure pour cause de protectionnisme de la science par la science elle-même. Nous évoquerons les avancées en matière de liberté de publication dans le monde moderne grâce à l'émergence des nouvelles technologies de l'information et de la communication. L'open access se préoccupe de cette liberté, mais se pose alors la question de savoir si trop de liberté et d'information ne tue pas la liberté."
"Coordonné par Paul Rasse - La mondialisation est aujourd'hui une réalité bien installée, qui emporte hommes, sociétés et cultures. Mais avec quelles conséquences au juste pour la diversité culturelle ? Pour certains, la mondialisation conduirait à l'homogénéisation, à l'effacement des identités locales sous le poids des modèles imposés par les pays riches et par l'essor des moyens de communication. Pour d'autres, notre époque hypermoderne serait caractérisée par la fragmentation et la diversification des formes d'expression qui se développent par métissage, réaffirmation des cultures régionales ou réactions communautaristes. Un ouvrage clair qui fait le point sur la question de la diversité culturelle, les débats et les enjeux qu'elle suscite."
"Salle de formation Urfist de Marseille, auprès du CRFCB"
"L'Homme est un animal sociable (Aristote) et par l'essor de récentes technologies, le réseau informatique mondial (web) permet aujourd'hui aux êtres humains de développer de nouvelles formes de relations. Mais le terme de réseau se suffit-il en lui-même-dans la mesure où il constitue déjà un moyen d'interaction social important- ou ce système d'information hypermédia doit-il obligatoirement se préciser comme social -dans le sens où il met en contact des êtres vivants? Cette recherche entend démontrer que les nouvelles technologies se satisfont pour l'instant d'un sens réducteur et primaire de la communication non verbale encore difficilement accessibles au web alors que les expressions non verbales de la réalité physique sont toujours riches de significations et de sous-entendus."
"Résumé : Après avoir pris soin de redéfinir les concepts de trace, de patrimoine et de mémoire, nous nous intéresserons au processus de patrimonialisation, qui débute en aval par la sélection des traces et aboutit en amont à leur interprétation, jusqu'à ce qu'elles constituent une mémoire collective. Nous interrogerons tout particulièrement la dynamique des cultures populaires, par opposition aux cultures savantes. Enfin, nous efforcerons de mettre en évidences deux grandes tendances caractéristiques de la valorisation récente du patrimoine populaire."
"Depuis la nuit des temps, la beauté sublime le monde. Alors si la notion abstraite de la beauté recouvre le champs de nombreux domaines, pour quelles raisons le questionnement de l'esthétique des sciences n'émergerait-il pas de la communication scientifique... Cette aporie peut alors se poser: la communication scientifique peut-elle être esthétique, peut-elle être belle?"
"Quels sont les modèles et comportements qui sous-tendent la manière dont s'organise la recherche et l'accès aux informations (numériques) à une échelle qui est désormais celle de la planète ? Chacun d'entre nous est aujourd'hui confronté à de nouvelles machines à communiquer dans le cadre de sa recherche d'information (moteurs et annuaires de recherche). Ces nouvelles ""bibliothèques"" de l'internet reposent sur des logiques de classement des documents très éloignées de celles en vigueur dans nos bibliothèques classiques. Pour autant, ces nouvelles technologies intellectuelles permettent des découvertes informationnelles qui se transformeront, après un processus créatif, en connaissance. De nouvelles pratiques apparaissent, qui utilisent la sérendipité (hasard, fortuité) comme adjuvant explicite de la recherche. L'interaction constante entre ces nouvelles pratiques et ces nouvelles logiques d'organisation de l'information fait émerger et permet de caractériser de nouveaux contenus."
"Notre posture épistémologique relève d'une approche « anthropo-socio-sémio-technique », c'est-à-dire que notre objectif est d'étudier les médiations technologiques et éducatives comme des dispositif techno-sémio-pragmatique{Paraya, 1999) dans le paradigme de la complexité (Morin, 1990). Partant de l'idée à la suite de Vygwsky (1985), Nonnan (1993), Lévy (1997) el Peraya (1999) selon laquelle les outils ou artefacts cognitifs participent à l'élaboration de notre pensée et peuvent agir sur notre cognition, nous avons posé comme premières hypothèses qu'un cours au même contenu sémantique, médiatisé de 4 façons différentes avaient des effets différents sur la mémorisation. Pour vérifier cette première hypothèse, nous avons mis En place 4 cours au contenu identique, mais médiatisés de manières différentes : 1) oral 2) Powerpoint sans prise de notes 3) Powerpoint avec prise de notes 4) cours en 3D en imagerie virtuelle immersif et interactif. Noire deuxième hypothèse, étant de savoir si le type de médiatisation changeait la communication, nous avons complété notre première étude par une analyse qualitative par l'attribution d'entretiens eompréhensits en essayant de voir comment nos 90 étudiants avaient vécu la situation communicationneile. Enfin, nous avons mis en place un tableau des différentes composantes des 4 formes de médiatisation (selon le modèle de Peraya). Nous nous situons dans le cadre des études pédagogiques médiatisées (Peraya, 2004) et dans le champ des outils éducatifs de communication (Moeglin, 2005)."
l'article décrits quelques Apports majeurs de la psychologie Sociale et de la Pyschologie aux Sciences de l'information et de la communication
"Les rapports entre Justice et médias constituent une problématique particulièrement actuelle qui n'a auparavant jamais reçu l'hommage des Sciences de l'information et de la communication (Sic). L'institution judiciaire est donc ici considérée comme terrain d'étude pour une analyse communicationnelle à travers les concepts de dispositif et de médiation dans le cadre théorique des approches, anthropologique et sémiotique. L'interdisciplinarité des Sic permet en effet d'appréhender l'institution dans sa globalité, d'étudier les rapports qu'en tant qu'acteur du système communicationnel elle engendre avec l'individu comme sujet et avec la société. En effet, le discours de l'institution judiciaire est à la fois individuel et pragmatique, adressé aux parties, et social, en ce sens symbolique, et dirigé vers la société. Les deux dimensions de l'activité symbolique du sujet doivent être considérées : « sa dimension singulière, celle du psychisme et de l'individualité et sa dimension collective, celle de la sociabilité et du fait politique » (Lamizet, 1997 : p. 9-14). Dès lors, il s'agit d'analyser les conditions de formation du sens et de la représentation dans le cadre d'un phénomène institutionnel, de comprendre le discours et la position d'une institution dans l'espace public. L'approche sémiotique est adoptée au profit de l'étude des interactions, des rapports, des positions et des rôles des individus dans le cadre d'une activité communicationnelle en dépassant la signification des énoncés pour découvrir la dimension pragmatique qui émane de l'énonciation. Il s'agit en effet de découvrir l'ensemble des formes et des logiques par lesquelles la communication entre les hommes donne à la sociabilité la dimension effective d'une continuité d'échanges et de relations à travers la mise en scène de l'activité judiciaire. La littérature de plusieurs disciplines constitue donc le corpus de cette étude [anthropologie, sociologie, histoire, droit, science politique]. Nous utiliserons de même des documentaires afin de mettre en évidence la médiation organisée par la mise en scène [symbolique et esthétique] du dispositif judiciaire. L'institution judiciaire en tant que dispositif, organise elle-même une médiation symbolique qui structure les représentations. En effet, le concept de médiation évoque une séparation et un lien à la fois, c'est-à-dire la présence d'un « Tiers symbolique » qui permet des représentations partagées. Dès lors, l'institution en général, et l'institution judiciaire en particulier, constituent ce « Tiers » qui rend possible la construction d'un sens commun. Ainsi, la normativité est liée à un système culturel et symbolique, qui par les projections imaginaires dont il est à l'origine, dispose d'une dimension performative. L'autorité d'une institution réside donc dans ce qui structure les conditions de la communication : un modèle communicationnel impliquant une mise en scène. Décorum, apparat, rituels constituent les éléments d'un dispositif d'énonciation qui exerce un pouvoir en tant qu'il assigne rôles et positions dans un espace symbolique [espace juridictionnel, espace médiatique]. Dans le contexte actuel dit postmoderne, caractérisé par l'avènement du sujet et un fonctionnalisme détaché, les conditions de l'énonciation du discours institutionnel sont affaiblies. La mise en scène du « Tiers » souffre les contraintes de transparence et de proximité liées à la reconnaissance d'un nouveau statut pour le sujet. Ces valeurs postmodernes renvoient à une im médiateté dans laquelle les barrières symboliques s'effondrent. L'espace judiciaire est partagé. Le juge est déshabillé, personnifié, sorti de sa fonction symbolique. Le rapport du justiciable à son institution évolue : la décision prend progressivement la forme d'une co énonciation entre le juge et les parties. Ces dernières deviennent spect acteurs du fait de l'abolition de la coupure sémiotique qui caractérise habituellement la scène judiciaire. Par les effets de la médiatisation, le fait judiciaire devient évènement et entre dans la discussion de l'espace public. Le dire judiciaire perd son statut de discours de vérité et se trouve confronté aux discours politique, social, économique et scientifique. Dispositif judiciaire et dispositif médiatique apparaissent alors comme concurrents dans leurs finalités et dans l'exercice de leurs missions respectives. Ils sont chacun à l'origine de la création d'un espace symbolique comportant une dimension pragmatique. La superposition de ces espaces entraîne une confusion dans les rôles endossés : dans l'espace juridictionnel, les protagonistes deviennent acteurs par l'effet de la présence de la caméra ; dans l'espace médiatique, le citoyen spectateur et même consommateur est institutionnalisé comme Tiers. Positions et rôles sont confondus, seul le rôle du spectateur est conforté : il prend alors la position de juge. La mise en scène médiatique bouleverse ainsi la pragmatique des interactions sociales telles qu'elles résultent de l'influence du dispositif institutionnel. À travers cette analyse communicationnelle de l'institution judiciaire, la pertinence des perspectives offertes par les sciences de l'information et de la communication est une fois de plus démontrée. De nombreuses institutions sont confrontées à la problématique de l'absence de projet et à un affaiblissement de leur autorité remettant en question certains aspects fondamentaux du lien social. L'École, l'Université et de façon générale les institutions de la République, subissent les évolutions d'une modernité qui privilégie l'avènement du sujet au détriment de la construction d'un sens commun. Les concepts de médiation, de dispositif et de représentation permettent alors de poser la problématique sous l'angle original des Sic et de considérer le caractère performatif d'une mise en scène à la fois symbolique et esthétique. Il s'agit de penser le statut de la technique en prenant en considération les aspects symboliques, sémiotiques de la transmission de l'information. Les conclusions issues de cette étude dépassent donc leur objet et ont vocation à s'appliquer d'une manière générale à l'ensemble des organisations [religieuses, économiques, politiques, sociales...] dont la vocation est de faire adhérer à un projet commun."
L'enseignement à distance s'accompagne de la mise en place d'un tutorat afin de garantir une intégration sociale des étudiants et de prévenir le sentiment d'abandon et de solitude de ces derniers face au savoir. Le tutorat entre étudiant permet un accompagnement pédagogique mais surtout relationnel. Cet article présente le contexte de mise en place d'un tel dispositif et en analyse à travers la théorie de l'activité élargie les apports et limites.
"Le développement de pratiques informationnelles amateurs, plus ou moins structurées et pérennes, fait périodiquement ressurgir, sous des formes renouvelées, la question de la définition du champ journalistique et de sa construction en tant qu'espace professionnel. Les évolutions techniques, organisationnelles et sociales des médias conduisent également à interroger les catégories de "" professionnel "" et "" amateur "", et leurs relations. Dans le domaine de l'information sportive, l'évolution du sport de haut niveau et son poids dans l'économie des médias, d'une part, la généralisation de la communication en réseau, d'autre part, suscitent des interrogations nouvelles sur la place et le rôle du journaliste de sport. Comment, dans cette configuration, le journaliste de sport peut-il asseoir sa légitimité en tant qu'informateur, reporter et critique ? Si l'intronisation de consultants, anciens sportifs chevronnés, assignait déjà au journaliste une place incertaine entre le pôle de l'expertise et celui de l'expérience vécue, avec les réseaux sociaux, la concurrence s'est encore renforcée. L'analyse des dispositifs d'information sportive sur le Web lors d'événements sportifs majeurs tels que les Jeux Olympiques montre que le journaliste de sport tire d'abord sa légitimité de la place qui lui est institutionnellement assignée dans le dispositif de médiatisation."
"Il s'agit de la préface au livre de Thierry Chanier: Archives ouvertes et publication scientifique : comment mettre en place l'accès libre aux résultats de la recherche ? publié en 2004. Nous nous proposions alors, d'examiner la question éditoriale numérique (scientifique) de manière prospective et programmatique, en anticipant de manière rapide, sur les développements du mouvement qui était déjà amorcé du coté des ""humanities and computing"" vers les ""digital humanities""."
Peut-on manipuler pourvu que ce soit pour une bonne cause ?
"Paul Valéry dit un jour (1942): "" Ce qui est simple est toujours faux. Ce qui ne l'est pas est inutilisable. "". Nous considérons ici que l'interaction et l'interactivité sont des notions protéiformes qui recouvrent des processus complexes. Le recours à la systémique sociale et à la modélisation dans la complexité selon les principes énoncés par Edgard Morin devraient nous permettre de penser l'interaction sans tomber dans la réduction simplificatrice, qui est tant à la mode actuellement. Cette démarche conduira à proposer une théorisation dualiste de l'information communication, vectrice de l'interaction."
"Ces 30 dernières années ont vu la considération du salarié évoluer dans l'entreprise au fur et à mesure que le concept de culture d'entreprise a été défini et incorporé dans les organisations. Généralement une culture d'entreprise s'est conçue au fil de plusieurs années sur un terrain national et pratiquement uniquement par rapport à des données socioculturelles locales. L'organisation est composée d'êtres humains qui confèrent à l'action collective une logique cohérente la distinguant ainsi de tout autre. Cette logique se constitue et s'affirme dans le temps. C'est elle qui donne à l'entreprise une certaine continuité, permettant à chacun d'identifier cette entreprise et, dans certains cas, de s'identifier à elle. A l'heure de la mondialisation, il est primordial d'acquérir des marchés étrangers et, par conséquent, d'entrer en contact avec des cultures d'entreprises basées sur des valeurs fondamentalement différentes. Si l'on considère la culture d'entreprise comme le moteur des injonctions des relations intra-organisationnelles, il n'en demeure pas moins qu'à l'heure où les entreprises s'internationalisent dans le cadre d'une inéluctable mondialisation, le concept de culture d'entreprise se voit évoluer vers une interculturalité des rapports. Dès lors, qu'advient-il de la culture d'entreprise lorsque cette dernière fusionne avec une autre ? La question des différences culturelles est de plus en plus centrale tant dans la société, avec la réaffirmation des différences régionales ethniques et religieuses, mais aussi avec l'élargissement de l'Europe, que dans les entreprises engagées de plus en plus dans un processus d'internationalisation. Les entreprises se veulent multinationales ou transnationales, elles démultiplient les pratiques de fusions-acquisitions, de rachats, d'alliances, de délocalisation...mais pour être à la hauteur des attentes de performance, elles doivent prendre en compte dans leurs modes de gestion et dans les processus d'intégration, les différences culturelles : culture nationale ou régionale, de groupe, d'établissement, de métier, différences collectives auxquelles s'ajoutent les différences individuelles. Si l'on définit la culture comme univers de sens, on imagine les difficultés de compréhension rencontrées par les différents personnels de terrain face aux modes de gestion ou aux modes de management proposés par un groupe relevant d'un autre univers culturel. On peut alors se demander si l'interculturalité serait le point de croisement entre deux cultures organisationnelles ? et dans ce cas n'instrumentaliserait-elle pas obligatoirement la culture d'entreprise pour pouvoir gérer cette nouvelle identité ? C'est au moyen d'une démarche qualitative par le biais d'une analyse sémiologique et lexicale des journaux d'entreprise (médias de la communication interne), et par le biais de l'analyse de contenu des discours de grands dirigeants que nous tenterons de démontrer comment la culture d'entreprise devient, de fait, un dispositif socio-technique d'information et de communication de l'interculturalité."
la cause du développement durable justifie-t-elle la manipulation ?
Quel est le sens de la création de technopoles un peu partout dans le monde dans les années 60/70 ? S'agit-il seulement de favoriser un développement économique ? La technopole de Sophia Antipolis est de ce point de vue paradigmatique : un véritable discours sur le sens a accompagné sa création. Le présent article se propose de rendre compte des enjeux ce cette production de sens.
"Les représentations de la table et de convives à table sont nombreuses au cinéma. A partir d'une étude de deux films Le Festin de Babette et Delicatessen, l'article montre que, lors de décès, le banquet s'impose en tant que fonction pour digérer les morts."
"Vicary ne prétendait à rien d'autre qu'à un modèle d'influence quasi parfait en propageant l'idée d'une manipulation des consciences par les images subliminales au cinéma. Pour autant le modèle idéal de manipulation de Vicary peut-il constituer aujourd'hui un paradigme organisateur de l'influence à la télévision ? Rien n'est moins sûr ! Pourtant, au fil d'un examen des positions respectives des chercheurs en psychologie sociale et en psychologie cognitive croisé avec une analyse du discours télévisuel et singulièrement de la place de la publicité dans ce discours, l'auteur invite le lecteur à découvrir ou à redécouvrir a quel point l'expérience de Vicary constitue une véritable tentation pour les théories de la réception des médias et singulièrement de la télévision. Décidément, en ce domaine comme dans d'autres, les théories méritent d'être examinées en une approche méta-critique, c'est-à-dire à la lumière des conditions qui ont permis leur production. A défaut, il se pourrait bien qu'il faille attendre encore longtemps une théorie de la réception des médias."
"Quelles similitudes on peut établir entre les cités radieuses imaginées et parfois entreprises par les utopistes et les projets de technopoles; et notamment le projet de la technopole de Sophia Antipolis ? En effet, cette dernière à toutes les caractéristiques des grands projets utopistes, à la lumière de quoi nous dégageons un premier bilan, moins pour dire ce qu'elle est, que ce qu'elle n'est pas, en abîmes de ce qu'elle voulait être à ses origines."
"On n'aborde jamais sans polémique la question linguistique en Lorraine, surtout lorsqu'il s'agit d'une langue minorée i.e. la langue francique généralement appelée "" Platt "" ou "" Platt lorrain "". À l'image de cette zone transfrontalière qui peine à trouver son/une identité - la Grande Région : Sarre, Lorraine, Luxembourg, Wallonie, Palatinat - ce Platt qui la traverse n'a pas encore fait l'objet d'analyses systématiques, mais se révèle être une source inépuisable de controverses, qui tiennent à la fois à une méconnaissance du contexte transfrontalier et à des stéréotypes "" antigermaniques "" encore vivaces. La caractérisation de ce qu'est et de ce qui reste de cette langue minorée constituera le premier volet de notre réflexion ; le second sera consacré à la diffusion et à la réception auprès du public d'un ouvrage : Le Platt lorrain pour les nuls, dont l'intérêt suscité confirme un enthousiasme particulier qui dépasse la seule question linguistique et contribue aussi à une médiation des cultures."
"Le discours francophone en sciences de l'information-communication se réfugie souvent dans la glose sur la complexité, le constructivisme ou la sémiotique. Un des intérêts des travaux récents est précisément d'être parti de la complexité comme un a priori catégorique et d'en avoir exploré les modalités de ses manifestations dans le champ territorial. Par exemple, le cas du "" projet d'extension portuaire de Bastia "" traité par Julien Angelini, conduit à la constatation que le "" débat public est l'émergence de l'intelligence territoriale. "" L'expression "" intelligence territoriale "", toute approximative et polysémique qu'elle puisse paraître révèle ainsi sa puissance. C'est l'association de l'intelligence, sous toutes ses formes, avec l'appartenance à un territoire, lui aussi sous toutes ses formes. La mise en pratique d'une approche en intelligence territoriale peut être caractérisée par "" une étude compréhensive, qualitative, factuelle, présentant des caractéristiques de l'observation participante et fondée essentiellement sur une collecte de traces tangibles de la production d'informations "" comme dans le projet d'extension portuaire de Bastia. Une autre étude sur la participation des citoyens bordelais aux micro-décisions d'implantation du tramway est encore une illustration de la mise en œuvre de la démocratie participative comme outil de communication dans un projet de développement territorial. Le présent papier se propose de confronter les apports conceptuels des sciences de l'information-communication avec des études de terrain pour situer les enjeux de l'intelligence territoriale et les questionnements qu'elle soulève au carrefour de la géographie, de l'anthropologie, de l'histoire, de l'économie, de la sociologie et de la politique. Dans tous ces champs du savoir, on retrouve les problématiques classiques des sciences de l'information-communication : comment créer la relation, donc la confiance, faciliter l'accès aux données, faire circuler l'information, communiquer et interagir."
"L'idée générale de la thèse est de proposer une modélisation d'un système d'information adaptable à tout type de projet mettant en présence des acteurs de territoires séparés, de cultures plurielles et de contingences structurelles et conjoncturelles différentes. La finalité sera de mettre en place un véritable « code de communication » autour d'un « projet de coopération » appropriable par l'ensemble des acteurs en intégrant leurs spécificités et celles de leurs environnements. Plus largement, cette thèse est un cadre de recherche permanent à capitalisation qui doit amener à enrichir « l'art projet » notamment en termes de valorisation des flux d'informations et de connaissances. Les deux dernières décennies ont été caractérisées par l‘émergence d'un environnement dont la complexité s'accroît de manière vertigineuse au vu du magma des flux d'informations à traiter. L'impact sur les entreprises, et plus généralement les organisations, a fondamentalement changé leur management et en particulier dans tous les processus « projet ». Au-delà des concepts de globalisation, de mondialisation, de mutation, nombre d'organisations vont chercher à créer de la valeur dans la mise en place de « projet de coopération ». Ces projets présentent des typologies bien particulières où l'information et la connaissance sont aussi bien des matières premières que des produits finis à leur réalisation. Dans cette optique, la notion de « l'environnement projet » devient de plus en plus prégnant d'autant que sa complexité se voient augmenter par une conjugaison de facteurs et d'acteurs qui ne sont pas uniquement issus de « l'environnement de proximité » du projet mais aussi d'un « environnement de connexion territoriale » qui redéfinit ainsi les « enjeux géospaciaux » du projet et qui pourrait se résumer à l'adage suivant : « penser global et agir local ». A travers cette lecture de l'environnement du projet, quel code de communication commun peut-on partager pour permettre de la mise en place d'une « architecture projet » en recherche d'efficience et qui doit amener un effet surgénérateur au projet, c'est-à-dire produire plus de richesses que de ressources consommées. Apporter une réponse à cette problématique, c'est avant tout faire émerger un modèle conceptuel de « proactivité managériale de l'information » autour de la notion de projet qui peut se décliner en différents éléments permettant de positionner précisément le projet dans son environnement en tenant compte « glocalement » de ses facteurs et de ses acteurs. Cette recherche est cadrée sur la genèse des composantes d'un « management proactif informationnel » au travers de retours d'expériences et résultats de construction d'un « modèle conceptuel informationnel projet » issu de mes travaux de recherches commencés en 1993."
"Ce travail de recherche s’inscrit dans le champ de la communication engageante. Un certain nombre de travaux antérieurs se sont intéressés à la pratique de la communication engageante dans un contexte de face à face. La contribution de ce travail porte sur l’étude de la communication engageante dans un contexte de communication médiatisée par ordinateur. Véritable travail interdisciplinaire, il se situe au creuset de la psychologie sociale, des sciences de l’information et de la communication et de l’informatique. La question de recherche consiste à s’interroger sur la transposition des conditions et techniques de l’engagement dans des environnements numériques. Pour répondre à cette question, un travail de terrain a été réalisé sur le thème de l’humanitaire sur le web. Les stratégies de communications en ligne des organisations humanitaires ont été étudiées de façon approfondie. Par la suite un travail de terrain a été articulé autour de plusieurs expérimentations conduites en face à face puis en ligne dans un contexte humanitaire. Le choix a été fait de privilégier la production expérimentale dans des conditions naturelles. Si l’efficacité de la communication engageante est incontestable dans le cadre de la communication en face à face, les résultats sont moins probants dans le cadre des expérimentations conduites dans les environnements numériques. S’amorce alors une analyse critique permettant de mieux comprendre les raisons de ces résultats."
"LES MUSEES ONT ETE, ENTRE AUTRES, DES ACTEURS IMPORTANTS DU DEVELOPPEMENT DES CD-ROM DITS CULTURELS DANS LES ANNEES 90, COMME PLUS RECEMMENT D’APPLICATIONS DITES E-ALBUMS SUR SMARTPHONES ET TABLETTES TACTILES. NOTRE ARTICLE MONTRERA D’UNE PART QUE LES INTERFACES DES E-ALBUMS CERTES SE NOURRISSENT DE L’IMAGINAIRE DE CONCEPTEURS ET DESIGNERS, EUX-MEMES USAGERS DE DISPOSITIFS NUMERIQUES QUI LES INSPIRENT AU RANG DESQUELS LES PRODUITS INTERACTIFS MULTIMEDIAS DES ANNEES 90 ONT LEUR PLACE. MAIS IL MONTRERA D’AUTRE PART L’IMPORTANCE DES USAGES DEVELOPPES DANS DES UNIVERS DE REFERENCE ANTERIEURS. EN EFFET, NOUS FAISONS L’HYPOTHESE QUE LES INTERFACES SONT DES OBJETS INTERMEDIAIRES ENTRE IMAGINAIRE, TECHNIQUE ET PRATIQUE, C’EST A DIRE : DE REPRESENTATION D’IMAGINAIRES, DE LEUR TRADUCTION SUR SUPPORT, ET DE MEDIATION ENTRE PRATIQUES ANCIENNES ET PRATIQUES NOUVELLES."
"La machine médiatique s'est emballée. Elle traque partout la violence scolaire, qui sert si bien ses objectifs de captation d'un public inquiet pour ses enfants. La ""crise"" de l'enseignement ne concerne pas que les Sciences de l'Education. Les Sciences de l'Information et de la Communication fournissent des concepts et des modèles pertinents pour penser l'éducation. Le présent article décrit une investigation empirique à l'inter-champ de ces deux disciplines, dans le but d'esquisser les conditions d'un enseignement possible dans le contexte spécifique d'un quartier défavorisé."
"La publication du livre de Hoggart The uses of Literacy en 1957 a lancé le mouvement des études culturelles anglo-saxonnes . En France, sa traduction, une bonne dizaine d'années plus tard, sous le titre La culture du pauvre , a bouleversé durablement le paysage, au demeurant assez désertique, des recherches sur les cultures populaires pour lesquelles il est devenu une référence incontournable. Le livre est présenté ici, en revenant sur les pistes de recherche qu'il ouvre et en expliquant pourquoi il demeure important."
"L'un des objets du documentaire réside dans sa qualité de « media citoyen » car il participe de la connaissance de l'autre et des institutions. Paradoxalement, la « mise en scène » médiatique de la Justice s'affirme parfois comme une atteinte portée à la fonction symbolique du juge et par conséquent à son autorité. En effet, le projet du documentaire et la médiation institutionnelle de la justice n'ont pas le même dessein. Par sa nature processuelle, la Justice s'oppose aux promesses du documentaire résidant dans la saisie d'un réel non médiaté ; et pourtant, les deux institutions démocratiques doivent cohabiter. Il s'agit donc dans cette étude de s'interroger : Doit-on autoriser les caméras à pénétrer dans l'enceinte sacrée de l'institution judiciaire ?"
"Préface La discipline des sciences de l'information et de la communication est très jeune par rapport aux disciplines déjà établies. Parce que la discipline mobilise des objets scientifiques d'autres disciplines et trouve des terrains d'application dans de multiples domaines, elle est forcément interdisciplinaire. Alors que certains concepts de la discipline commencent à se « stabiliser » et que la communauté scientifique de la discipline commence à se structurer, des besoins d'avoir des approches scientifiques partagées se font sentir. Par rapport à la formation, aussi bien en formation professionnelle qu'en formation à la recherche et par la recherche, il devient primordial d'avoir des références exposant les concepts fondamentaux, théories et résultats de la discipline, afin de déterminer leur origine, logique, valeur et leur portée. Ainsi, les chercheurs et les enseignants de la discipline pourront éviter le repli sur soi par le rejet des références aux objets scientifiques des autres disciplines. Cet ouvrage portant sur l'épistémologie et méthodologies de recherche en sciences de l'information et de la communication trouvera sans doute son utilité auprès des étudiants, aussi bien en formation professionnelle qu'en formation à la recherche, auprès des enseignants de la discipline comme outil pédagogique, et auprès des enseignants et chercheurs d'autres disciplines pour la compréhension de la discipline des sciences de l'information et de la communication. Professeur Amos DAVID Equipe SITE, LORIA-Nancy 2,"
"En 1982, le professeur René Küss plaide à la télévision pour un protocole appelant à distinguer entre "" vrais "" et "" faux "" trans' : de"" vrais trans' "" ne causent aucun trouble dans le genre ; on leur accorde une aide exceptionnelle (l'opération) par laquelle - hommes devenus femmes ou femmes devenues hommes - ils rentrent dans l'ordre du genre et de l'identité. Telle est l'une des premières expressions de ce que nous proposons d'appeler le "" bouclier thérapeutique "", formule qui paraît convenir à refléter l'ambiguïté de la position ainsi défendue. Les trois décennies qui suivent voient s'affronter les affirmations transidentaires et l'idéologie dominante des "" traitants "". S'inscrivant dans la dynamique des Gender Studies, les trans' hors protocole engagent un large mouvement de revendication sociétale, politique et philosophique, tandis que les "" traitants "" défendent leur statut et leur expertise de médecins et de "" professionnels "". Appareil de légitimation d'un ordre ancien, le "" bouclier thérapeutique "" ne serait-il plus aujourd'hui pour ses partisans que le dernier vestige d'une ère marquée par l'effritement d'un deuxième bouclier, juridique celui-ci, garantissant que la libre disposition de l'état civil reste une exception ? Sur ces questions qui interrogent profondément les représentations que nos sociétés se font d'elles-mêmes, peut-être le temps est-il venu de libérer la route tracée par la recherche en sciences sociales et humaines, en l'ouvrant en particulier aux nouveaux paradigmes amorcés par les Études de Genre."
"Toute tentative de transformation du système délibératif nécessite au préalable la définition claire et précise de l'idéal poursuivi à la fois par ceux qui cherchent à instaurer des rencontres délibératives, par ceux qui élaborent des modèles de rencontres et par ceux qui en étudient l'efficacité. S'agit-il de renforcer la démocratie en tendant vers plus de débat public ou au contraire de renforcer le système représentatif en sollicitant l'expression du peuple sans pour autant céder à ce dernier une part de gouvernance stratégique ? Il se pourrait que l'élite d'aujourd'hui, dans sa grande majorité, comme hier à Athènes cinq siècles avant J.-C., désapprouve la démocratie."
"La distinction entre savoirs formels et savoirs informels, ou encore, entre savoirs scientifiques et techniques et savoir faire profanes, devrait nous permettre de préciser certains enjeux de la muséologie contemporaine et de la communication scientifique publique. Nous commencerons par analyser ces termes antagoniques à partir des apports de la sociologie du travail. Dans un second temps, nous verrons comment est-ce qu'ils peuvent être utilisés pour préciser les objectifs de la médiation dans les musées et dessiner des pistes d'évolution possible. La distinction entre savoirs formels et savoirs informels, ou encore, entre savoirs scientifiques et techniques et savoir faire profanes, devrait nous permettre de préciser certains enjeux de la muséologie contemporaine et de la communication scientifique publique. Nous commencerons par analyser ces termes antagoniques à partir des apports de la sociologie du travail. Dans un second temps, nous verrons comment est-ce qu'ils peuvent être utilisés pour préciser les objectifs de la médiation dans les musées et dessiner des pistes d'évolution possible."
"Dans l'ombre, le silence des médias et le mépris de l'intelligentsia, le festival “Off” d'Avignon est devenu un espace culturel exceptionnel, par la diversité des spectacles et le nombre des spectateurs, mais surtout, parce qu'il s'y invente un théâtre différent, révélateur de profonds bouleversements qui agitent le champ de la culture. Le festival “Off” est devenu un espace public, ce moment suspendu de grâce, d'intelligence collective où une dynamique sociale se met en place qui transforme les rapports à la culture et où s'invente un théâtre différent qui articule une création foisonnante avec un public curieux, critique et passionné. Pour en éclairer la dynamique, Paul Rasse et une équipe de chercheurs universitaires ont mené durant trois ans une enquête auprès des troupes, du public et de la presse. Il montre en quoi le “Off”, loin de n'être que l'antichambre du “In”, est révélateur d'une dynamique que l'on retrouve ailleurs, et qui témoigner d'une aspiration du public à affirmer sa maturité, en initiant de nouveaux rapports à l'art et à la création."
"La mise en scène des techniques dans les grands musées du XIXème ou du début du XXème ne s'est pas faite au hasard. Elle poursuivait un projet idéologique visant à affirmer le pouvoir naissant des ingénieurs et leur conception cartésienne de la technique. Elle s'est faite aux dépens d'une approche sociale et critique, plus sensible, plus accessible qui aurait pu concerner davantage l'ensemble de la population engagée dans la grande aventure que fut l'industrialisation forcée des pays occidentaux"
"Nous utilisons le paradigme d'espace public pour développer un point de vue historique sur les questions de vulgarisation et de communication scientifique, pour en cerner les enjeux, puis ouvrir des pistes d'évolution possible aux musées et autres lieux de communication scientifique et technique."
"Lorsque Pina Bausch quitta ce monde, Wenders aurait pu renoncer à son projet de tourner avec elle. Sur l'insistance des danseurs et de la compagnie, il ne le fit pas. Wenders portait ce projet avec Pina depuis si longtemps. Il a fallu que la mort intervienne au moment de sa réalisation. Si le film n'accompagnerait plus Pina Bausch dans son travail comme c'était prévu, au moins les danseurs allaient quitter la scène et se mettre à danser au milieu de la ville. À trente ans d'intervalle, il y a un autre film où Wenders a été confronté à la mort du personnage principal jouant, cette fois-là, son propre rôle. Il s'agit de Nick's Film - Lightning Over Water (1980). Dans cette fiction sur et avec Nicolas Ray, celui-ci est mourant. Wenders met en quelque sorte en scène la mort du cinéaste. Or, dans ces deux films, on trouve une scène à peu près semblable : une séance de projection privée où les protagonistes principaux apparaissent, une projection dans la projection en quelque sorte. Le procédé est le même, la plastique est approchante. Ne faut-il pas voir dans cette ressemblance une forme de survivance cinéphilique qui nous renvoie à la notion de Nachleben formulée en son temps par Warburg et revisitée par Didi-Huberman ?"
"Nous focalisons notre propos sur les valeurs sémantiques circonstanciées que toute exposition qui, traite de l'immigration comme d'un enjeu scientifique et comme d'un enjeu de société, ajoute à l'usage linguistique commun du terme immigration dans le champ muséal et par extension, dans le français contemporain. L'analyse de deux expositions, c'est-à-dire de deux contextes institutionnels, de deux styles expographiques et de deux discours de la connaissance, nous permet de développer notre thèse selon laquelle le monde des musées agit incontestablement dans le débat actuel sur "" immigration et appartenance nationale "" en France. Mais, il y a plus encore, car nous le verrons, Repères et D'Isère et du Maghreb sont révélatrices des sensibilités sociales et politiques qui travaillent aujourd'hui en France, l'émergence d'une muséologie et d'un patrimoine de l'immigration."
"La Côte d'Azur 1951-2011 : territoire anastomose, source de communication cinétique artistique La singularité de la Côte d'Azur, cosmopolite depuis deux siècles au moins, favorise les échanges, les assimilations et les confrontations artistiques à un niveau international. Grâce à sa réputation touristique et à sa dynamique de création (Nouveau Réalisme, Fluxus, Ecole de Nice...), les artistes continuent de venir s'y ressourcer et de développer des relations inter-multi-et transculturelles intenses. Ces corrélations aboutissent à un syncrétisme artistique, mêlant plusieurs systèmes de pensées expérimentales qui favorisent le développement de leurs propres identités et les incitent à une plus grande capacité d'expérimentation (La cédille qui sourit...). Ils sont accompagnés en cela par des écoles (Villa ARSON...), des institutions officielles (MAMAC/ FRAC/ CIRM...), des structures privées ouvertes à tous les regards (MAEGHT,...) qui les encouragent à croiser leurs productions dans un esprit de dépassement des frontières car une fois intégrée à la notion d'une culture universelle, l'altérité est désamorcée (ADAMOWICZ). Peut-on alors supposer ce territoire anastomose et cinétique ? En effet, nourris par les mélanges apportés dans cet espace, les artistes peuvent féconder leurs cultures sans se perdre (RASSE). Par leurs diverses contributions régulières hors de tous schémas préétablis, par cet apport de nutriments artistiques variés, sans cesse renouvelés et portés par un réseau (Botox, Southart,...), ils cultivent ainsi une énergie unique, pour produire du remarquable et inventer des modes de transmissions (MANGION) originaux et reproductibles reconnus par des collectifs, des associations (La Station, les Abattoirs...), des Galeries (Depardieu, Dojo, Sapone, Soardi...). Bénéficiaires de cette vitalité, d'autres artistes pourront à leur tour participer à la fabrication de cette puissance créatrice auto-entretenue par la vis viva (force vive) de ce lieu. Preuve en est qu'ici, les artistes s'enrichissent, développent des approches variées (PINAUD), parfois même se reconstruisent (St PHALLE) ou consacrent ce lieu pour l'éternité (LE CORBUSIER)."
"Nous nous proposons d'identifier les caractéristiques médiatiques d'un braconnage pratiqué lors du développement d'une innovation. Pour cela nous menons une étude de terrain sur le développement d'une innovation [FLICHY 2003], une bibliothèque numérique, par une équipe professionnelle issue d'une organisation [CROZIER 2000]. Impliqué dans l'action en tant que professionnel, notre observation participante [SOULE 2007], nous permet de constituer un corpus d'actions et de communications afférentes au développement de ce service. L'activité tracée relève de fortes interactions des acteurs observés avec leur environnement. Le braconnage [De CERTEAU 1990] est identifié comme générateur d'avancées. Enrôlant [AKRICH 1988] des utilisateurs- précurseurs [VON HIPPEL 1988], cette équipe détourne l'usage [PROULX 2005] d'éléments disponibles pour établir de nouveaux équilibres [DURAMPART 2008]. À partir de cette double échelle d'actions et de communications précédemment constituée, nous remarquons des caractéristiques propres à identifier le braconnage."
"À l'aide d'une expérimentation, nous montrons que des messages publicitaires sur l'internet apparaissant dans le champ visuel périphérique provoquent des effets favorables sur les jugements et les intentions d'achat des marques publicisées, alors que les récepteurs n'ont pas « conscience » qu'elles sont entrées dans leur champ visuel. Nous étudions également l'évolution des effets cognitifs et attitudinaux huit jours après l'exposition. Pour démontrer ces influences de manière rigoureuse, nous avons conçu une méthode de présentation contingente couplant une caméra filmant les mouvements oculaires et un système informatique faisant automatiquement disparaître les bannières publicitaires dès que le regard de l'internaute se déplace dans leur direction. Après avoir proposé une explication quant aux processus socio-cognitifs impliqués dans l'influence, nous ouvrons de nouvelles perspectives pour la recherche sur la réception de la communication médiatique."
"L'article pose le problème de l'évolution de la diversité culturelle à partir de la question de l'innovation scientifique et technique. Dans ses formes les plus apparentes la mondialisation génère une dynamique extraordinaire dont les résultats sont prodigieux, cependant, au regard du temps de l'humanité, elle masque des tendances plus inquiétantes, car les technologies contribuent inéluctablement à l'uniformisation des processus d'innovation, par la sélection des plus performantes d'entre-elles et par l'écrasement des autres alternatives possibles. La diversité des civilisations comme autant de berceaux indispensables à la découverte, l'invention, la création, tend à s'effacer."
Cette étude s’intéresse à l’usage de médias sociaux et à leur rôle dans les dynamiques relationnelles et les processus décisionnels au sein de groupes d’étudiants en contexte de projet pédagogique. Les résultats montrent que ces médias sociaux facilitent l’efficacité et l’immédiateté au sein des groupes et donnent toute sa place à la dimension relationnelle durant le processus de production et la réalisation des tâches de leur projet. L’étude s’attache à comprendre quels sont les processus décisionnels adoptés par les étudiants pour créer les meilleures conditions de dialogue et de partage d’idées possibles afin de favoriser l’adhésion de l’ensemble de membres de chaque groupe à leur projet.
"Le SCD de l’université Nice Sophia Antipolis a lancé en 2011 une enquête pour améliorer les services aux usagers avec deux objectifs principaux : un état des lieux des pratiques documentaires hors du SCD et des abonnements en cours, ainsi qu’une enquête de satisfaction pour se rapprocher des usagers afin d’être au plus proche de leurs besoins. Nous souhaitions, en connaissant mieux la cartographie des structures de recherche et l’offre documentaire périodique hors du SCD, réduire les coûts inutiles issus de la complexité de la situation scientifique de l’université et des éditeurs de revues scientifiques. À ces objectifs principaux de l’enquête s’ajoutaient deux missions : se renseigner sur la perception du SCD; avoir une action de sensibilisation, de médiation autour des modèles éditoriaux et économiques des éditeurs scientifiques. L’enquête qualitative et quantitative s’adressait à l’ensemble des laboratoires de l’UNS, étendue à quelques autres structures documentaires périphériques. Elle a donné lieu à soixante-cinq entretiens directifs et semi-directifs, soit quatre-vingts personnes de fonctions et statuts variés : chercheurs/chercheuses, documentalistes, administratifs, responsables ou correspondant·es de documentation, directrices/directeurs ou secrétaires. Le questionnaire thématique comprend quatre parties : les informations relatives aux interlocuteurs et interlocutrices, la politique documentaire et la documentation électronique du laboratoire, ses rapports avec le SCD, notamment sur les questions de documentation électronique, enfin, les usages de la documentation électronique hors des circuits de l’UNS."
"Les esthétiques industrielles et post-industrielles dépassent l’histoire économique. Retracer une histoire sensible de ce patrimoine passe par des rencontres avec celles et ceux qui les font vivre. Nous sommes partis à la rencontre d’une vallée industrielle dont les sites patrimoniaux participent au processus de réappropriation culturelle, la Loire nivernaise : des sites témoins de chaque période depuis le xviie siècle jusqu’aux réhabilitations variées des xxe et xxie siècles. Après une relation aux esthétiques et aux terminologies comparées, l’article présente les études de cas de huit sites de la vallée. Notre analyse s’intéresse à la recherche de cohérence entre les initiatives culturelles et artistiques et les destinations d’origine des sites, dans un lien à l’histoire des lieux à leur mise en valeur esthétique que nous modélisons sur des critères de cohérence historique, de rapport sensible et social, de conservation et de vitalité : une première réflexion autour de ce que nous appelons depuis « l’esthétique de l’histoire »."
"« Afin de promouvoir les travaux innovants dans le domaine du patrimoine industriel par leur méthodologie, leur objet, leur mise en oeuvre et/ou leur forme finale », le Cilac lance aux jeunes chercheurs et chercheuses un appel annuel. J’ai proposé une étude autour des réappropriations du patrimoine industriel par des projets artistiques et culturels. En observant les travaux de recherche-création autour de l’histoire industrielle, la cohérence entre les productions artistiques et leur rapport à l’histoire m’a permis d’avancer pour la première fois le concept d’« esthétique de l’histoire »."
"Nous rappellerons comment, en créant artificielle- ment de la différence par des faits de culture, et non pas de nature, les sociétés premières ont inventé des façons de dominer la sexualité et de générer du lien social solide, indéfectible, pour tisser les communautés. Puis nous nous demanderons en quoi cela nous permet de réfléchir à la famille contemporaine, générée sur la base du désir, cimentée par l’amour romantique et une sexualité épa- nouie, mais apparemment soumise à une obligation de fidélité inconditionnelle"
"Parmi les expositions inaugurales du MuCEM en 2013, « Le Bazar du genre, Féminin/Masculin en Méditerranée (commissariat : Denis Chevallier) » pose question. Encensé, épinglé, dénigré, ce média sensibilisait le public du nouveau musée national à une grande variété de ramifications concernant le genre. Parfois édifiante, parfois maladroite, pour la première fois, une exposition prenait à bras le corps cette problématique. En parcourant les salles, ce chapitre engage une réflexion de cette démarche d’exposition, en scrutant son potentiel. Notre analyse critique porte sur la manière d’aborder le genre, la sémantique du bazar, le rapport sensible entre le message et la collection, les valeurs sémantiques de la démonstration et des interrogations. Outre la mise en valeur des manques et des réussites, nous notons que les expôts disposés par thématiques représentent assez bien le territoire méditerranéen sous un regard assez large, faisant fi des carcans chronologiques au risque de confondre, pour se concentrer sur l’immense variété des styles, en tension entre discours et métadiscours, dans un style narratif hybride inspiré des pratiques muséographiques et médiatiques audiovisuel, un engagement modéré. Sans aller jusqu’à parler d’une exposition bâtie sur les études de genre, la visite se situe au carrefour d’une anthropologie vulgarisée assez riche et d’une structuration discursive nébuleuse qui laisse au public le dernier mot."
"Dans ce premier ouvrage en français sur la plateforme Twitter, nous nous sommes intéressés à la pratique documentaire et médiatique de la curation de contenus en ligne : récolte, tri, conservation, signalisation, augmentation et partage d’informations jugées pertinentes dans le but d’encourager la réflexion et l’action. Après une définition contextuelle de la curation et une étude netnographique (enquête et observation on line et off line) sur un échantillon de curatrices et curateurs actifs sur cette plateforme, nous nous sommes appliqués à déceler et décrire les modèles communicationnels et documentaires observés. L’accès à l’information mais aussi le capital social de la curation implique une participation à une ingénierie du social entre les filtres humains et le partage du produit de la curation dans une forme d’intelligence collective ouverte à l’échange documenté autour d’une thématique. À l’issue de notre enquête, nous discernons trois modèles de curation sociale à différents niveaux de pérennisation, de plus-value, d’intérêts partagés et de potentiel d’action : le premier modèle, simple, comprend la chaine documentaire classique au service d’un écosystème numérique, de la veille jusqu’au partage, le deuxième intègre des outils spécifiques dans une dynamique de partage réactif et de maîtrise durable des données, le troisième plus achevé travaille à améliorer leur sélection, leur préservation, leur contextualisation et leur accessibilité."
"Les études centrées sur l'enseignement à distance et plus précisément sur les projets collaboratifs à distance, ne font pas explicitement référence à la notion de confiance, qui, selon nous, semble occuper une position importante dans la gestion des interactions. L'utilisation même des TIC dans ce domaine permet de bénéficier d'une multitude de ressources et induit des questions de crédibilité informationnelle et humaine et donc de confiance dans les situations à forte complexité, comme l'enseignement à distance. L'intérêt de cet article est de repositionner le travail collaboratif à distance dans le champ de la confiance , de recentrer le débat sur l'exploration des relations entre les membres d'équipes virtuelles et d'ouvrir celui-ci à l'idée d'éthique."
"Pour analyser les mutations du monde entraînées par l'essor des moyens de communication l'anthropologie à deux atouts. Le premier, en tant que biographe des sociétés disparues et panoptique des savoirs réunis sur ces sociétés, elle a accumulé pas mal de matériaux, d'objets, d'informations sur la diversité des cultures perdues, à partir desquels elle peut s'interroger et mettre en abîme nos mondes contemporains aplanis par la globalisation. Le second atout est sa méthode, d'immersion ethnographique, d'observation patiente, de compréhension des microsociétés étudiées par imprégnation, et en même temps, de mise en perspective des connaissances recueillies, en les resituant dans un projet anthropologique global plus vaste, celui d'une histoire de l'humanité, qui débute avec des communautés dispersées de par le monde, isolées les unes des autres, et qui, fascinées par leurs différences, ont commencé à se rencontrer, jusqu'à former cette civilisation planétaire."
"Dans l'ombre, le silence des médias et le mépris de l'intelligentsia, le festival “Off” d'Avignon est devenu un espace culturel exceptionnel, par la diversité des spectacles et le nombre des spectateurs, mais surtout, parce qu'il s'y invente un théâtre différent, révélateur de profonds bouleversements qui agitent le champs de la culture. En l'absence de la presse et des critiques, le jugement du public est essentiel pour la notoriété du spectacle. Au-delà des artifices déployés par les troupes pour se distinguer des autres, l'intuition du festivalier et le bouche à oreilles font le succès ou l'échec des pièces. Et le spectateur expérimente le plaisir de découvrir, d'échanger, d'expliquer et de défendre ses positions, dans la proximité des salles, des files d'attentes, des rues et des terrasses de café. Dans l'enquête, dont est issu cet article, nous avons choisi d'approfondir les données déjà disponibles et d'interroger moins de personnes (300), mais avec un questionnaire qualitatif, long, faisant une large place à des questions ouvertes pour que les personnes interviewées puissent exprimer plus librement leur point de vue..."
"Notre communication porte sur l'usage des technologies numériques comme aide à la décision managériale. En d'autres termes, nous nous questionnons sur la place des technologies de l'information et de la communication dans le développement des innovations managériales. Nous présentons ici les fondements et les méthodes utilisés dans l'analyse exploratoire préalable à la réalisation d'un moteur d'affinités qui doit associer des professionnels rationnellement compatibles pour constituer une équipe susceptible de répondre aux attentes des chefs d'entreprise en matière de développement de projets pour le web. Cette expérimentation est conduite par le Laboratoire des Sciences de l'Information et des Systèmes 1 et de l'entreprise Nodalys 2 dans le cadre d'une réponse à candidature aux projets Pacalabs 3 dont l'objectif est de promouvoir l'innovation numérique et ses usages à travers l'expérimentation en Provence-Alpes-Côte d'Azur. Mots-clefs : moteur d'affinités, innovation managériale, équipes projet"
"Dans ce texte nous présentons les grandes lignes du mode de fonctionnement d'une équipe de chercheurs impliqué dans la conception, le développement et l'appropriation d'innovations technologiques. Nous exposons particulièrement le processus de pilotage de la recherche et les méthodes qui ont permis d'envisager l'innovation comme le résultat des interactions entre les acteurs de l'innovation, entre la médiation des connaissances scientifiques et des pratiques professionnelles, entre les intentions des commanditaires et les besoins des usagers. La recherche devient à la fois, la référence qui structure et l'opportunité qui fait découvrir. Abstract In this paper we present the outline of the operating mode of a team of researchers involved in the design, development and appropriation of technological innovations. We particularly expose the process of steering research and methods that were used to consider innovation as the result of interactions between innovation actors, mediation between scientific knowledge and professional practices between intentions sponsors and needs of users. Research becomes both the reference structure and the opportunity to discover."
"Dans un contexte d'accroissement de leurs effectifs et de transformation du profil des étudiants, les Universités ont vécu le plan Université 2000 initié dans la période 1990-1995. Certains élus s'emparèrent de l'opportunité offerte par U-2000 de recevoir une implantation universitaire sur leur territoire . En 2005, le schéma L.M.D ou 3-5-8 (Licence-Master-Doctorat) a été adopté par la totalité des universités et les procédures de Validation des Acquis de l'Expérience & Professionnels sont connues des publics concernés. L'offre de formation universitaire est soumise à concurrence inter-universitaire et entre les Grandes Ecoles et les universités et contribue à rejoindre l'objectif de Barcelone (1995) de la création d'un espace européen de la connaissance. Les universités ont pris la mesure de ces enjeux et certaines déploient des dispositifs d'Enseignement à Distance. Le déploiement territorial suggéré par le programme U.2000 va se voir contrarié par une recentralisation ‘physique' de l'offre de formation des universités confrontées à une réduction budgétaire. Mais, les règles de cette mise en concurrence ne s'exercent pas toutes de la même manière pour tous les acteurs concernés. Dès lors, quelle peut-être demain la place, le positionnement, la structuration et le contenu de l'offre de formation offerte par la filière SIC ?"
"La demande sociale de concertation et de participation aux décisions publiques est à l'ordre du jour et pèse de plus en plus comme référence dans l'organisation, le fonctionnement et les enjeux de l'activité politique. La montée en puissance d'un idéal délibératif qui met en lumière les dimensions de pouvoir et d'échange, touche de plein fouet l'ensemble d'une société en tension démocratique. A l'aune de cet impératif délibératif (Blondiaux et Sintomer, 2002), l'idée d'une communication différente apparaît et des initiatives, majoritairement nées de la notion de la nouvelle gouvernance issue de la Conférence des Nations unies de RIO, se multiplient, en s'appropriant de nouvelles règles de communication fondées essentiellement sur la promotion du dialogue et la participation des citoyens à la discussion de la chose publique. D'innovantes modalités communicationnelles prolifèrent. L'enjeu est d'instaurer de nouvelles normes et de favoriser l'apparition d'acteurs permettant une meilleure intermédiation sociale. De ce point de vue, la procédure de débat public orchestrée par la Commission nationale du débat public (CNDP), autorité administrative indépendante, permet d'appréhender concrètement une facette de ces nouvelles relations entre les citoyens et la puissance publique. Ce modèle de débat public constitue un exercice récent dans le registre de la culture politique. Une nouvelle génération de débat public (Dacheux, 2005) apparaît transformant le sens commun des acteurs et produisant une culture du débat public conduisant à de nouvelles formes de citoyenneté promue par la société de la connaissance. Ces nouvelles formes de participation tentent, d'une part de rétablir le lien et la confiance entre citoyens et décideurs tant au niveau local, national que transnational et d'autre part à développer l'articulation des échelles de citoyenneté. Autant de conditions favorables à l'émergence de l'assemblage des échelles de la gouvernance (Gaudin, 2002) qui permettent de procéder à la modernisation de l'État (administration, institutions) mais aussi à sa transformation dans ses rapports avec la société.En incitant les citoyens français à la discussion dans le cadre d'un projet sociotechnique international portant sur la recherche nucléaire, le débat public "" ITER en Provence "" fournit de façon circonstancielle un terrain propice à l'analyse des enjeux de la diffusion de ces dispositifs normatifs de communication innovants qui jouent un rôle fondamental dans le partage de normes, de règles hybrides, de nouvelles pratiques irriguant l'espace public contemporain. De fait, les fonctions de médiation (Lamizet, 1997) de cette procédure du débat public vont permettre d'articuler des référentiels et des images cognitives neuves afin de réguler les relations. Il s'agit ici de mettre en évidence la dimension dialogique de la procédure du débat public ITER en Provence comme lieu privilégié d'interactions, de représentations, de sens et de normes visant tant à l'adhésion à la décision sur le projet en débat qu'à l'appropriation d'un changement de logiques entre l'État et les citoyens. Le débat public tel qu'institutionnalisé en France par la loi est un ensemble de processus et techniques de communication (Mucchielli, 2006), de transmission d'informations aux citoyens, de réaction des citoyens, mais aussi de rétroaction des citoyens sur l'État qui cherche à réguler les activités sociales. Ainsi, un des constats pouvant être tiré de ce qui se joue dans cet espace par le biais de la procédure du débat public, c'est l'existence d'un contrat social. En effet, il s'y opère une contractualisation publique entre les acteurs relative au projet ITER, et d'une manière plus générale, c'est l'appropriation de nouvelles normes qui est révélée par le débat public. Il s'agit d'un référentiel de règles reconfigurant l'organisation du système social visant à ordonner les nouvelles relations entre le citoyen et les institutions, l'administration, etc., d'un changement des représentations des relations entre l'État et la société et d'un changement de code communicationnel, relationnel entre les administrés et l'administration. En l'espèce, l'État se modernise et ouvre la relation à plus de dialogue et d'écoute, particulièrement dans son rapport avec la société civile ; il s'agit ici de l'aspect relationnel et communicationnel de ce rapport. C'est l'affaire de la communication. Mais, qu'en est-il concrètement ? En quoi ces dispositifs participatifs permettent-ils d'identifier qu'un système de normes porteuses de valeurs et d'actions est en marche ? Quels sont les impacts concrets de ce dispositif en termes d'influence sur les pratiques et le sens commun des acteurs ? Quelles en sont les portées à l'échelle de l'espace public ? Quel est l'enjeu du débat ?Objet éminemment social, avec son contexte, ses enjeux et ses acteurs, la procédure de débat public illustre parfaitement la complexité même de notre monde contemporain et ouvre la réflexion sur la nature et les effets de ces nouveaux dispositifs hybrides, permettant de prendre la mesure de ce qui les caractérise. L'analyse des processus normatifs de la procédure du débat public ITER permet ainsi de repérer les changements dans les conditions de légitimité des actions publiques (Pharo, 1990). L'étude du débat public nécessite donc une méthodologie favorisant l'accès direct au terrain et à ses acteurs : l'observation participante organisée en fonction d'un protocole expérimental fondé sur trois contraintes : description, contextualisation et comparaison. Cette observation a consisté à assister aux réunions publiques organisées par la CPDP en région PACA et à Paris. Par ailleurs, l'analyse de la tenue du débat public ITER a été faite à partir d'une approche interactionniste permettant, sous l'angle de la métaphore théâtrale et du paradigme interprétatif proposé notamment par Goffman, d'approcher le débat public sous ses divers aspects. Une approche interdisciplinaire se justifie donc pleinement : d'une part en raison de la spécificité de l'objet au regard des enjeux décisifs qu'il contient pour l'analyse des objectifs économique et de pouvoir, liés à la place centrale que représentent l'information, la connaissance et la communication, dans le débat ; d'autre part, en raison de l'enjeu communicationnel du débat public dont la "" portée "" et les "" effets "" sur l'espace public permettent de comprendre que des processus d'information-communication institués par les pouvoirs publics, jouent un rôle majeur dans l'évolution des représentations et des pratiques sociopolitiques collectives et en outre, que ce processus de médiation publique entraîne avec lui une série de repositionnements des modes organisationnels et de régulation des activités humaines (Rumpala, 2010)."
"Le thème de cette essai est d’étudier comment les personnes qui travaillent dans une entreprise ou dans une administration, génèrent et utilisent ce qu’il est convenu d'appeler « l’information ». En particulier, comment cette information est partagée–communiquée– entre les membres de l’organisation pour déboucher sur une forme d’action collective. Les prémices sont que l’information n’existe pas en soi, mais est le résultat d’un processus complexe de mise en contexte et d’émergence dans un système dit « de cognition ». Le modèle choisi pour décrire ce processus est un modèle dialectique. On définit la dialectique comme un processus dynamique d’évolution à partir d’éléments antagonistes, qui fait émerger de nouvelles propriétés. Différents outils méthodologiques sont proposés pour répondre à l’injonction d’Edgard Morin : penser la complexité, sans mutiler ni manipuler, mais en restant dans le domaine du possible pour un manageur d’organismes institutionnels. L’essai s’articule en trois parties. La première partie consiste en une remise en perspective, au travers du principe des propriétés émergentes, de plusieurs concepts apparemment familiers : complexité, système information, organisation, action, évaluation, etc.. La deuxième partie propose un modèle fondé sur une typologie de l’action organisationnelle d’une part, et une classification des processus de génération de la connaissance et de la conviction que l’on dénomme système de cognition, d’autre part. La troisième partie tire de ce modèle les éléments d’une doctrine de l’action et du management adaptée à la perspective dialectique informationnelle proposée."
"L'objectif de cette note d'habilitation à diriger des recherches est à la fois de dresser une synthèse critique de mes recherches, de discuter des perspectives théoriques, méthodologiques –voire épistémologiques– qu'elles ouvrent et d'expliquer comment je souhaite les poursuivre, tant au niveau scientifique qu'institutionnel. S'insérant dans une tradition de recherches des SIC, mes travaux portent sur les interactions production–dispositif–réception médiatiques situées au sein d'un large ensemble de contextes. Mon objectif est d'étudier la communication médiatique à la lumière d'une approche croisant, premièrement, une perspective pragmatique, tenant compte de la contextualisation socio-économique et socio-politique des phénomènes ; deuxièmement, une perspective interactionniste associée à la théorie du contrat de communication développée par le courant français de la psychologie sociale de la communication et, troisièmement, les récentes théories de la communication médiatique persuasive, de la socio-cognition et des attitudes provenant, en partie, des recherches américaines. Pour réaliser ce projet scientifique dans une perspective communicationnelle heuristique, je mobilise des ressources théoriques issues de la sémiotique, de l'esthétique, des sciences cognitives, des recherches en marketing et en communication des organisations. Convoquer ces multiples ressources, dans une problématique de sciences de la communication, conduit à attribuer à mes travaux quelques caractéristiques originales. La principale originalité réside, sans doute, dans les pluralités épistémologique, théorique et méthodologique. La pluralité méthodologique consiste en la mobilisation de méthodologies de recherches relevant de l'approche expérimentale et de l'approche herméneutique. Ces pluralités sont discutées sur le plan des épistémologies normative et analytique. Dans la deuxième partie de la note, j'ai synthétisé mes travaux portant sur les interactions dispositif médiatique-sujets sociaux qui s'opèrent au cours de la réception. Mis en perspective, ils contribuent à mieux connaître la réception et l'influence du dispositif communicationnel sur les cognitions, les représentations sociales, les attitudes et –dans une moindre mesure– sur les comportements, en tenant compte des émotions, des motivations et de l'intentionnalité des récepteurs. Une grande partie de mes travaux empiriques et expérimentaux étudient les traitements socio-cognitifs et socio-affectifs des systèmes sémiotiques, esthétiques et linguistiques propres aux dispositifs de communication publicitaire, politique et de santé publique. Partant du principe que le dispositif de communication médiatique est produit par des sujets sociaux membres d'organisation, la troisième partie de la note résume mes recherches portant sur les interactions sujets sociaux-dispositif qui s'opèrent au cours de la production médiatique. Elles permettent de mieux connaître, d'une part, les processus de décision ainsi que les savoirs, représentations sociales et théories implicites qui sont mobilisés dans la production de la communication médiatique persuasive et, d'autre part, la validité scientifique des principales représentations des producteurs."
"Face à un environnement international de plus en plus ouvert et à une concurrence élargie, la prise en compte des différences culturelles et de l'interaction entre les cultures devient un enjeu essentiel pour les organisations, tant pour satisfaire la diversité de la demande que pour intégrer des équipes de travail multiculturelles. Locale, nationale ou internationale, l'entreprise a connu par l'avènement des techniques et technologies un rapprochement des frontières et donc nécessairement des différentes cultures qui composent notre monde. De fait, ces rapprochements doivent aussi composer avec de nouvelles appréhensions dans toute l'acception du terme et notamment celle de "" l'Autre "". En effet, l'aspect culturel pourrait se présenter comme l'un des plus importants d'une éventuelle stabilisation en relation avec la restructuration des organisations, renvoyant logiquement et nécessairement à une rencontre des cultures. Lier la question de l'interculturalité à la stratégie de développement joue un rôle majeur dans la pérennité et l'évolution des entreprises. L'actuel contexte organisationnel nous amène à penser que le concept d'interculturalité, pourrait prendre une place prépondérante dans le monde de l'économie et de la finance. Instrumentalisé, "" l'interculturel "" pourrait s'avérer un outil puissant de communication pour les relations professionnelles recomposées dans un mouvement d'intégration des différences. Apparemment, aucun outil de communication ne recèle cette puissance. Le management stratégique et la communication processuelle donnent le ton de cette recherche qui a pour but d'apporter un éclairage sur l'analyse de la phase d'intégration post-fusion de deux compagnies aériennes en se focalisant sur l'un de ses outils essentiels de communication : son journal interne et les discours tenus dans ce dernier aux salariés de la part du nouveau groupe issu de la fusion. Depuis la fin des années 1980, les processus d'identification organisationnelle sont reconnus pour leur influence dans le fonctionnement organisationnel et plus particulièrement dans l'engagement des salariés. Or, l'engagement des salariés est déterminant dans la réussite de la coopération et donc dans la performance d'une fusion. Notre objet de recherche sera plus le discours de l'interculturel médiaté par le journal interne. Nous prendrons en considération sa place prépondérante dans les outils de communication, le considérant comme le dispositif socio-technique d'Information et de Communication (DISTIC) qui permet la diffusion du message managérial à la totalité des salariés d'une entreprise et a fortiori des entreprises en situation de fusion-acquisition. Ce message deviendrait vecteur de l'intentionnalité interculturelle managériale. Problématique : Dans le cadre des rapprochements organisationnels, la question du respect mutuel des cultures d'appartenance et en même temps de la création d'une culture commune pose le défi de "" l'interculture d'entreprise "" et ainsi, l'acceptation de la création de nouveaux symboles, d'une nouvelle sacralisation, d'une nouvelle culture hybride. Ce premier constat nous amène donc à formuler la problématique suivante : Comment l'association de deux cultures d'entreprises a-t-elle créé une nouvelle forme de sacralisation dans nouvelle entité organisationnelle ? Méthodologie : La méthode d'analyse textuelle choisie est celle issue du logiciel ALCESTE (Analyse des Lexèmes Co-occurrents dans les Enoncés Simplifiés d'un Texte). Ce logiciel d'analyse textuelle nous a permis d'analyser les discours managériaux contenus dans un journal interne, "" tabloïd "", édité à 70000 exemplaires, en français et en anglais, en version papier et numérique, sous la forme d'un bi-mensuel, durant cinq années (soit 138 journaux de 2003 à 2008), suivant une fusion "" hybride "" de deux grandes entreprises."
"La démocratie par définition ne saurait être représentative. Aucun système politique démocratique ne saurait légitimer la représentation autrement que par le défaut d'un autre moyen technique de réaliser la démocratie. Le principe de notre contrat social et de la République est le suivant : « gouvernement du peuple, par le peuple et pour le peuple » (article 2 de la constitution de 1958). L'existence d'un système représentatif n'est pourtant pas pour tous, y compris ceux qui revendiquent leur qualité de démocrates, un pis-aller."
"On ne disconviendra pas que les réseaux sont aujourd'hui omniprésents. Mais on précisera qu'ils le sont avant tout dans nos discours : réseaux informatiques, socio-numériques, relationnels, professionnels, entreprises en réseau, réseaux économiques, financiers, liens et réseaux sociaux..., tout n'est que réseau, réductible à la notion de réseau. Son avènement le confirme, la science des réseaux ouvre à ces universaux – physiques, biolo-giques, sociologiques… – que sont les réseaux, tissés des interactions qui y sont à l'oeuvre. Bien avant le numérique, Saint-Simon, pionnier d'une philosophie des réseaux, plus tard les courants interactionnistes, ou même Georg Simmel et son interactionnisme social, auraient-ils eu raison quand ils insistaient sur la dimension interactionnelle de l'action, et par là réticulaire de la société ? Pour certains, la société ne serait-elle alors que réseau ? Ou bien encore, tous les réseaux ne seraient-ils pas sociaux ? Non pas qu'ils soient le social, mais parce qu'ils le construisent ? Là où d'autres objecteront que les réseaux – quels réseaux ? – ne font ni ne façonnent en rien le social… Dès lors, parce qu'une telle approche épistémologique et transdisciplinaire ne l'exclut pas, l'avènement desdits « réseaux sociaux » – plus exactement réseaux socio-numériques – pourra être intéressant, parmi d'autres questionnements. Dossier coordonné par Jean-Thierry JULIA Acteur • Connaissance • École • Hodologie • Hybridation • Interaction • Lien • Objet technique • Pli • Réseau • Réseaux sociaux • Réseaux socio-numériques • Savoir • Science des réseaux Prix : 21 E w 3. sc so c. un iv-t ls e2 .fr / Revue publiée avec le concours du CNRS, du Centre national du livre"
"La personnalisation d'informations est une tendance devenue importante du web. Elle suppose la construction de représentations des lecteurs et/ou des comportements de lecture, construits sur la base de clics, que les serveurs arrivent à capter : données comportementales, contextuelles (sur quoi le clic a porté) et déclaratives (qui est derrière le clic). Cet article propose, à partir de données recueillies dans le cadre d'une recherche/action, d'interroger les enjeux de cette personnalisation pour les entreprises de presse écrite sur le web, et surtout les limites en termes d'usages et de stratégie économique."
"Nous nous attachons à poser quelques repères permettant de traiter la perception, à partir de la variation immanente et continue, dont elle est l'expression et l'exprimé. « Turbulences de la perception », donc."
"Le marché mondial de produits pour la santé animale a brassé près de 15 milliards de dollars en 2005, notamment dans le segment d'animaux de compagnie, et il est disputé par de grandes entreprises multinationales qui sont généralement les filiales de grandes organisations exerçant leurs activités sur le marché de la santé humaine, qui investissent lourdement en recherche et développement et en actions de marketing. Dans ce contexte, la société Vallée se détache dans le marché brésilien, car elle est au nombre des cinq plus grandes entreprises du secteur. La stratégie de l'entreprise, définie au début des années 90, de spécialisation dans le marché bovin et dans la prévention de ses maladies, lui vaut aujourd'hui le leadership sur le marché de vaccins et de préparations injectables. Cette position privilégiée et l'évolution de son chiffre d'affaires supérieur à la moyenne du marché durant de nombreuses années, a amené l'entreprise à se préoccuper des mouvements de la concurrence et de l'ensembie de l'environnement concurrentiel, ce qui exigeait la mise en place d'un système efficace de veille de l'environnement concurrentiel. Afin de se prémunir contre les mouvements de la concurrence et de détecter de nouvelles opportunités de croissance, un modèle de système d'intelligence compétitive a été mis au point, à partir de 1999. L'un des soucis de Vallée, au long des années, a toujours été de mesurer ses résultats, et ce, grâce à l'intelligence compétitive. En l'absence d'un modèle universellement accepté pour cette évaluation, l'entreprise a adopté diverses initiatives pour ce faire. Nous proposons dans ce travail deux nouvelles manières de déterminer la valeur de l'intelligence compétitive pour les organisalions, l'une directe, l'autre indirecte."
"La présente thèse s’inscrit dans le domaine des Sciences de l’Information et de la Communication. La résurgence et le poids croissant de la Religion dans l’espace public contemporain ainsi que l’importance de son rôle dans l’interprétation de faits actuels d'ordres divers nous ont conduits à considérer la médiatisation du phénomène religieux. Nous cherchons, essentiellement à étudier les rapports entre le journalisme et le fait religieux dans le cas portugais. Nous avons circonscrit notre étude à une des principales religions monothéistes : le Christianisme, dans ses trois principales branches - l’Église Catholique Romaine, le Protestantisme et l’Église Orthodoxe. Nous avons également fait le choix de limiter notre recherche à la presse écrite de référence.Dans cette optique, nous proposons dans un premier temps une démarche de contextualisation théorique de notre approche. Dans un deuxième temps, par une étude basée sur deux méthodes qui se complémentent : analyse de contenu et entretiens semi-directifs, ont été mis en évidence les enjeux et les visions des principaux acteurs (journalistes et représentants des confessions chrétiennes) impliqués dans le processus de production de l’information religieuse. En fin, nous réfléchissons sur les défis auxquels cette information, comme contenu spécialisé est confrontée dans le contexte numérique, notamment à travers le cas des blogs spécialisés en religion."
"Comment former au mieux un professeur de Français Langue Etrangère (FLE) avec une compétence en Communication Interculturelle dans un pays où les échanges interculturels naturels sont réduits par rapport à d’autres pays plus francophones? C’est une problématique sur le point d’être aborder en Colombie. En effet, il existe des recherches autour de l’interculturel, dans des contextes comptant d’innumérables immigrants et des mélanges de différentes cultures provenant de divers pays. Cependant, dans notre recherche, la problématique est née du manque de ces interactions au quotidien. La question se pose donc dans l´absence de l’exigence du contexte d’établir un échange interculturel exolingue qui donne comme conséquence la faiblesse d’une compétence en Communication Interculturelle. La recherche s´intéresse donc à mettre en évidence l’importance d’une dynamique de la Communication Interculturelle dans l´apprentissage d´une langue étrangère, le français, en dehors de son contexte familier (le bassin francophone) et utilise comme outils de recollection d’information l’observation, les rencontres interculturelles synchrones et asynchrones, les enquêtes, les questionnaires ou encore les rapports réflexifs, qui plus tard seront interprétés à travers le logiciel Atlas Ti. Comme résultat de la recherche, l’auteur propose une méthode d’apprentissage de la langue étrangère, à l’intérieur de l’approche interculturelle, en tenant compte des caractéristiques de la ville où le processus d´apprentissage a lieu ainsi que les caractéristiques des enseignants de FLE en formation, afin de développer leur compétence en Communication Interculturelle."
"A l'origine physiques, les échelons territoriaux intègrent progressivement les TIC. Ces dernières brouillent les découpages administratifs et favorisent l'émergence de territoires virtuels. Ainsi, la "" société de l'information "" se construit. S'il est primordial que les territoires intègrent ces technologies de l'information et de la communication, il est tout aussi nécessaire qu'ils la nourrissent, au risque de se trouver marginalisés sur un plan national comme international. Le processus d'intelligence territoriale que l'on peut qualifier de démarche d'information et de communication territoriales trouve ici sa pleine justification dans l'aide apportée à la constitution du capital formel d'un échelon territorial. A notre sens le capital formel d'un échelon territorial est le préambule à toute politique de développement, qu'il s'agisse de politique de mutation territoriale, de reconversion, ou d'innovation. Nous voyons bien ainsi, que les aspects portent en effet tout autant sur un volet infrastructures, réseaux de télécommunications à haut débit (tuyaux) que sur les supports et le contenu des documents numérisés créés grâce à ces outils. De tels enjeux intéressent tous les secteurs de la société, de l'éducation à l'économie en passant par la santé ; du monde de l'administration à celui de l'entreprise, en passant par le particulier."
"Deux points de vue s'opposent à propos de la mondialisation, les uns y voient l'uniformisation des cultures, les autres, son métissage et sa diversification. on comprend que face aux reconstructions réactionnaires d'un passé ré-enchanté, que confrontés à la montée des intégrismes archaïques, inquisitoires et sanglants, les anthropologues se fassent les chantres du respect de la différence, de la tolérance (leclerc, 2000, p. 475), du métissage (Jucquois, 2003 réf. : "" bricolage ""). ils martèlent inlassablement que les cultures ont toujours été en mouvement (Cuche, 1998, p. 64 et suivantes) et les civilisations ouvertes à l'influence des autres cultures, qu'elles disposent de ressources insoupçonnées pour se recomposer, se recycler, se réapproprier les apports de l'extérieur (Warnier 1999, Journet 2002). Affirmer et réaffirmer ces évidences est salutaire, mais ne nous fait guère progresser sur l'analyse du monde à venir. nous défendrons que les mutations en cours : * n'en sont qu'à leurs prémices, car la vitesse de croissance de la masse des échanges matériels et symboliques est exponentielle. * menacent la diversité des cultures, autant en raison de leur standardisation que pour l'explosion des formes d'altérité en parcelles de différences, qui se combinent entre elles dans un cosmopolitisme généralisé, fertile mais épuisant, sans cesse en recomposition, alors qu'il faut du temps, de l'espace, de l'isolement, pour donner aux cultures le souffle nécessaire à leur structuration, à leur cohésion interne et à leur diversification. La culture mondiale devient un maelstro¨m, où dominent les productions des pays les plus puissants et l'intérêt des plus riches, tandis que le corps social anomique se fragmente. Face aux catégories sociales aisées toutes en fluidité et en métissage, s'accumulent les populations "" immobiles "", abandonnées, aux prises avec des fondamentalismes eux aussi universels (rasse 2008) en même temps la culture mondiale n'est plus seulement celle des médias, mais aussi celle des réseaux. "" Une culture de tous vers tous "" pour reprendre l'expression de lipovesky (2010) est en train de bouleverser les règles du jeux, comme en témoigne les révoltes des indignés. Mais pour éviter de sombrer dans une "" cyberutopie "" qui draperait la technologie de vertus émancipatrices intrinsèques[1], il convient d'éviter de sur valoriser tel ou tel élément (les médias, l'internet, le téléphone, les manifestations, ou les nouvelles formes de sociabilité...) dont on pourrait isoler les effets mécanique. l'approche anthropologique des dispositifs sociotechniques de la communication (DiStiC) s'efforce : * d'une part de focaliser l'attention sur ce qui change, non pour le regretter, par déploration ou nostalgie, mais pour prendre la mesure de ce qui est en train d'arriver, pour interroger la dynamique des mutations et les conséquences qui s'annoncent * d'autre part d'analyser, sur la durée, les processus de reclassement et l'ensemble des interactions complexes entre les médias et les réseaux, les spectateurs et les sujets. nous nous proposons donc : 1. de reprendre, d'actualiser et de défendre ce point de vue, en partant de recherches récentes menées sur ces questions. 2. de montrer comment les théories opposant le métissages des cultures à leur disparition, prend actes des mutations en cours pour se centrer sur les formes et la dynamique d'une cultures mondiales."
"S'il convient de défendre la diversité des cultures comme une richesse indépassable, dans le contexte actuel de leur affaiblissement, de quels moyens disposons nous ? Les pistes paraissent parfois bien dérisoires (mais nous n'avons que celles-là). Elles sont encore à inventer et à développer. Certaines d'entre elles concernent les conditions de conservation du patrimoine matériel et immatériel que constituent les cultures traditionnelles héritées des terroirs, dans la perspective d'en faire un élément fort du développement local. Nous nous efforcerons ici d'en dégager quelques principes."
"L'anticipation des ruptures affectant le territoire est l'un des thèmes privilégiés qui se nourrit de recherches-actions et d'expérimentations au sein du local, plus particulièrement depuis une dizaine d'années dans la Région du Nord-Pas de Calais et la Région PACA. Par une action concertée de mutualisation de l'information et de mise en perspective des signaux, le territoire peut se dégager d'une posture principalement réactive au profit d'une attitude pro-active face à ses futures ruptures (épidémies, inondations, aménagements, emplois etc.). Or, celle-ci est un fait agissant sur le local dans un brouhaha exponentiel d'informations et de signaux qui obscurcissent l'horizon du projet territorial. La volatilité des projets et les changements brutaux des repères fondamentaux au sein du lieu de vie, crée un lieu de doutes, voire de vide, où tout un chacun s'interroge sur la nature de l'héritage symbolique qu'il veut défendre. Les décideurs du local, élus ou responsables d'organisation ont quelque peine, pour alimenter une réflexion politique, économique ou sociale, à traiter ce flux exponentiel d'informations. Chacun cherche dans le conseil de l'autre, l'avertissement salutaire qui préviendra la tempête en s'essayant dans le même temps à jouer les devins d'un avenir incertain."
"Depuis 5000 ans au moins, la parfumerie a suscité la création de vastes réseaux de communication et leur maintien, envers et contre toutes les difficultés, techniques, économiques ou politiques. Paradoxalement, elle est restée, dans son essence même, très extérieure à la révolution des moyens de communication qui a bouleversé ce siècle. Ce paradoxe nous permettra de faire une relecture anthropologique de l'histoire des parfums et d'aborder quelques-uns des problèmes de communication sur lesquels butte la parfumerie moderne."
"Dans une démarche constructive, les auteurs proposent la création d'une archive ouverte en InfoCom. Après un rappel sur l'origine, les mouvements et les possibilités offertes par les archives ouvertes (Open Archive), nous voulons dresser avec l'ensemble de la communauté InfoCom les contours d'un projet intellectuel favorisant le développement scientifique de notre communauté."
"Cette communication présente la conception et la mise en œuvre d'une archive ouverte de publications scientifiques dans le champ des sciences de l'information et de la communication francophone. Dans un premier temps, il s'agit de comprendre le sens de la dialectique homme/machine associée au développement de ces archives ouvertes dans le cadre de la communication scientifique, la genèse du mouvement et les différentes orientations actuelles sont rappelées. Plus spécifiquement, les auteurs analysent les enjeux liés au développement de ce type de dispositif pour les sciences de l'information et de la communication dans l'espace francophone. Prenant en considération les spécificités de cette communauté, les auteurs expliquent ensuite leurs choix concernant l'architecture de cette archive, la conception de l'interface-usager, l'organisation et le fonctionnement de l'archive. Enfin les premières statistiques d'usages d'@rchiveSIC (http//archivesic.ccsd.cnrs.fr) sont exposées ainsi que les différents scénarios envisageables pour atteindre une masse critique suffisante donnant sens à cette innovation de services."
"Ce travail s'inscrit dans une veine de recherche en intelligence économique. Un mot clé est souvent associé à l'activité d'intelligence économique au quotidien: c'est celui de réseau. Le réseau est présent à toutes les étapes du cycle du renseignement. Dans ce travail, le concept de réseau va nous intéresser dans une perspective interculturelle. Nous allons plus précisément nous attacher au concept chinois de Guanxi, orientation de l'esprit qui conduit à mobiliser les réseaux de relations dès lors qu'on se trouve face à un problème décisionnel et ce à tous les niveaux de la vie sociale.Pour comprendre les mécanismes du Guanxi, il est nécessaire d'en décrire les mécanismes en utilisant des concepts nécessaires à sa compréhension. La complexité du travail vient du fait que les concepts sous-jacents ne sont pas traduisibles en français autrement que par des périphrases et qu'ils se définissent de façon emboitée. Impossible de comprendre la notion de Guanxi sans s'immerger dans le système social chinois. Afin d'éclairer le lecteur, il nous a paru également utile de mettre en parallèle le concept de Guanxi de concepts voisins qui ont pu être utilisés dans d'autres sociétés. Le Guanxi joue sans doute un rôle de facilitateur dans le développement de l'entreprise chinoise. Il aide les entrepreneurs chinois à réussir. Quel rôle le Guanxi joue-t-il dans la réussite de l'entrepreneur français en Chine? Le Guanxi est-il aussi un élément favorable au développement des entreprises à capitaux étrangers (français) en Chine?Pour répondre à ces questions, nous sommes partis en Chine rencontrer des entrepreneurs français, écouter leur expérience personnelle ainsi que leur pratique professionnelle.La seconde partie rend compte de ce travail de terrain et montre toute la difficulté d'une compréhension fine du concept de Guanxi par ces chefs d'entreprise alors même qu'ils évoluent dans un environnement asiatique."
"RESUME: Les auteurs situent leur article dans un contexte d'innovation sociétale, qui place les usagers au centre de la démarche projet sous l'angle de la thématique énergétique. S'intéressant à une typologie des démarches de co-conception existantes, ils proposent un retour d'expérience du projet participatif Ecofamilies, qui consistait pour des familles participantes, à co-concevoir et tester une solution technologique de suivi des consommations énergétiques domestiques, sur le plan des Interactions Homme-Machine, en adéquation avec leurs besoins. Les auteurs interrogeront les apports, les contraintes et les limites de l'engagement d'une démarche de co-conception selon la double perspective de l'évolution des usagers vers leur positionnement en tant qu'acteurs d'un projet collaboratif et les processus d'intelligence collective qui s'y déploient."
"Le monde de la recherche d'information est actuellement en train de vivre une période d'intense bouleversement : position hégémonique du moteur Google, question délicate de l'interpénétration des sphères publiques et privées, redocumentarisation du monde, montée en puissance de la logique publicitaire et sa cohabitation avec le modèle « régulé » de la simple application d'algorithmes. De nouvelles modalités d'accès apparaissent, telle celle de la sérendipité que nous interrogeons, après l'avoir resituée dans l'héritage de la bibliométrie, au regard des modèles théoriques de la recherche d'information, pour isoler le rôle d'adjuvant indispensable qu'elle occupe désormais. Son instrumentalisation par les moteurs, sa perception liée au niveau d'acculturation socio-technique des usagers, la diversité de ses instanciations, pose la question de l'opacité des algorithmes et de la nécessaire ouverture d'un débat autour d'un écosystème non plus simplement documentaire mais politique."
"Nous aimons croire les nouvelles générations « mutantes », capables de traiter plus d'informations à la fois. Non seulement c'est inexact, mais ce type de fonctionnement mental aggrave les erreurs de jugement."
Cet article étudie la mise en scène de certains rites d’interaction sur un forum de discussion internet : fr.soc.environnement. À partir d’une méthodologie nethnospective sont analysées certaines formes de déférence (rites d’évitement et de présentation) ainsi que l’usage de civilités. Ces analyses sont menées dans des perspectives synchroniques et diachroniques. Les résultats obtenus valident certaines hypothèses i.e. la présence d’actes de déférence sur l’espace numérique étudié. Ils montrent aussi une très nette chute de l’usage de civilités au fil du temps. Se pose alors le problème de la non-stabilisation des rites d’interaction en ligne ainsi que des normes sociales d’entraide.
"Cet article traite de biais cognitifs qui pourraient affecter le jugement des internautes lors de la lecture d’une liste de réponses, suite à une requête dans un moteur de recherche. L’hypothèse est faite que des effets d’ordre i.e. de primauté et/ou de récence pourraient être observés dans de tels contextes. Les auteurs choisissent de la tester en réalisant une expérimentation en milieu contrôlé. Ils choisissent alors de s’intéresser plus particulièrement au domaine des techniques de sevrage tabagique et affinent leur questionnement ainsi : A la suite d’une requête dans un moteur de recherche, la place d’une médication dans une liste détermine-t-elle l’idée que se fait une population d’étudiants de sa pertinence ? En comparant trois groupes différents, les auteurs démontrent un effet de primauté et l’absence d’effet de récence. De plus, ils mettent en relief cinq variables modératrices : le sexe de l’individu, le fait qu’il soit fumeur ou non, le fait qu’il ait eu primitivement un avis ou non à propos des méthodes de sevrage tabagique, le fait qu’il soit ou pas concerné par des problèmes de santé liés au tabagisme, sa vitesse de lecture sur l’interface Web. Les auteurs concluent en plaidant pour une éducation à la culture informationnelle. Pour eux, dans le cas présenté, celle-ci s’avérerait pertinente tant d’un point de vue médical, qu’en termes de santé publique, que d’un point de vue socio-économique."
"Cet article s'intéresse à étudier l'éventuel pouvoir d'influence que détien-draient des informations soutenues par des statistiques. Les auteurs cherchent à explorer cette proposition à partir de différentes hypothèses en comparant, sur des sujets étudiants, l'effet relatif produit par des formulations plurielles contenant ou non des données statistiques. De ce travail émerge une nouvelle question a priori contre-intuitive : cette génération pourrait être plus sensible aux informations qui lui sont présentées de manière plus nuancée."
"L'objectif est de présenter un outil qui permet la création automatique de réseaux à partir de données bibliométriques. On entend par réseau une représentation figurative des différentes composantes d'un ensemble et des relations qui les unissent. Une telle représentation peut s'appliquer aux champs codes, auteurs, mots clés de notices bibliométriques extraites de bases de données brevets, scientifiques, techniques, technico-économiques. Les réseaux, une fois constitués mettent en évidence la notion de cluster de mots clé ou de collaboratoires d'auteurs. L'intérêt porté aux réseaux en bibliométrie est actuellement limité par une technique de construction souvent manuelle qui rend l'opération fastidieuse. Dans ces conditions, un algorithme débouchant sur la représentation automatique d'un réseau satisfaisant certaines contraintes esthétiques présente un certain intérêt."
"L'analyse en terme de réseau a fait l'objet d'un certain nombre de recherches, dans le domaine des sciences humaines et sociales. Les principaux résultats sont présentés dans l'ouvrage de Wasserman (1994). L'objet de ce travail est de porter à la connaissance de la communauté académique une pratique originale de traitement de l'information qui utilise un graphe appelé réseau pour représenter une information complexe de façon synthétique. Cette technique est tout d'abord positionnée par rapport aux << analyses de données classiques >> et fait ensuite l'objet d'une application dans le cas particulier du traitement de données bibliométriques."
"L'Internet est de nos jours une nouvelle source d'information incontournable. Par contre, qui n'a pas été déjà confronté au phénomène de sur-information lors de la recherche sur un moteur ou de désorientation lors de la navigation sur Internet. Ces problèmes sont persistants sur le réseau Internet, du fait même de sa construction, et sont à l'origine de pertes de temps considérables. Comment dans ce foisonnement de données juger l'importance des pages Internet et des sites les contenant ? Comment rationaliser la gestion de ces ressources et améliorer la performance de leur exploitation ? La communication aborde dans un premier temps l'ensemble des différentes approches actuellement développées pour aider l'utilisateur à mieux gérer l'information présentes sur Internet. Ces techniques ont principalement pour principe l'évaluation de pages ou des sites Internet. Nous évoquons les réflexions qui actuellement ont cours pour l'évaluation des ressources Internet. Nous verrons que les principaux critères proposés font bien souvent appel à une évaluation purement intellectuelle. Il parait indispensable de concevoir des outils de traitement automatique pour l'aide à l'évaluation de ces nouvelles ressources. Nous passons rapidement en revue les différentes techniques d'évaluation actuellement disponible en essayant de les classer par catégorie : analyse du contenu lexical des pages Web : classement par pertinence (moteurs de recherche ou robots tels que DigOut4U, Search'97) ; simple classement (Search'97, Nomino) ; agrégation statistique suivie d'une cartographie soit parcellaire (Sampler) soit intégrale (WebSom, Neuronav+, SemioMap, Umap, TKS, Tétralogie) ; analyse du contenu multimédia des pages Web ; analyse de la dimension hypertextuelle des pages Web : analyse de l'espace des relations entre pages ; analyse du phénomène de référencement entre sites, la ""sitation"". Nous positionnons alors notre approche, l'analyse des réseaux de ""sitations"", par rapport à ces techniques. Une description de l'ensemble des indicateurs et cartographie réseaux mise au point sera exposée. Pour conclure, nous présentons une étude de cas mettant en application cette méthode d'analyse réseaux des ""sitations"". Cette expérimentation a été menée sur un corpus de données extrait de l'Internet provenant des sites traitant de la bibliométrie-scientométrie-infométrie. Ces données ont été collectées et mises en forme grâce au robot Auresys, puis ensuite traités par le logiciel Dataview pour finalement être exploitées par le logiciel Matrisme. La démarche complète de cette étude ainsi que les principaux résultats sont exposés. Cette méthodologie d'analyse de l'espace des cybernautes a déjà fait l'objet d'expérimentation sur de précédents jeux de données. figure 1 : réseau des sitations réciproques entre les sites de l'administration française en 1996"
Ce travail transpose l'analyse réseaux au domaine de la bibliométrie. Le point de départ de l'analyse est un corpus composé de notices bibliographiques téléchargées d'une base de données. Lorsque la taille du corpus augmente l'expert a du mal à se forger une vue d'ensemble de la réalité qu'il cherche à appréhender. La construction automatique du tissu de relations entre les unités bibliographiques devient alors indispensable à l'analyse d'une grande masse de données. L'exemple choisi permet de représenter les collaborations entre auteurs travaillant dans le domaine de la bibliométrie dans le monde.
"La norme étant celle du plus grand nombre, les personnes en situation de handicap se retrouvent en situation d'isolement communicationnel. Le lieu de la parole libératrice et agrégative se cherche à tâtons. La problématique du lieu, de l'espace sans risque, se pose donc pour permettre à certains de se retrouver et de se raconter, ensemble, sans risque de contact mixte et de discrédit. Nous pensons qu'Internet peut être ce lieu. Notre regard va se porter sur un dispositif internet dédié aux personnes et proches d'individus ayant été, ou étant, atteints du syndrome de Guillain Barré : que reste-t-il après que les troubles pris en charge par la médecine ne soient repartis ? Quel peut-être le rôle, le cas échéant, d'un témoignage dépose sur un espace web public ? En quoi peut-il fournir un cadre de résilience assistée ?"
"La problématique de l'ident ification de signaux faibles est centrale en Intelligence économique. Le web, par son caractère informel, apparaît comme une source d'information à privilégier. Le problème qui se pose alors est celui de la collecte et du traitement d'une information massive peu structurée. Cette communication présente une approche non booléenne hybride de révélation de connaissances innovantes combinant une recherche informelle sur le web et une recherche très structurée dans un thésaurus."
"Les Tic sont présentes à tous les niveaux du cycle de l'intelligence économique au sein des entreprises, permettant en cela d'améliorer considérablement l'efficacité d'un tel système. Mais la méconnaissance des principes de fonctionnement des Tic, notamment ceux ayant un rapport direct avec l'utilisation d'internet, peut entraîner de graves préjudices en termes de sécurisation du processus. Cet article propose une modélisation du processus de l'intelligence économique conduit au travers des Tic et présente les conséquences d'une mauvaise prise en considération de la dimension sécuritaire."
"Nous allons étudier le classement, au niveau international, des sites web des établissements universitaires. De nombreux chercheurs ont travaillé sur des indicateurs webométriques des sites web universitaires. Nous allons nous pencher sur le classement mis en œuvre par le laboratoire espagnol Cindoc (http://www.webometrics.info). Ce laboratoire établit un classement annuel des universités mondiales à partir d'indicateurs infométriques de leur présence sur internet. Nous allons décrire les indicateurs élémentaires qui entrent dans le calcul de ce classement. Notre question de recherche consiste à nous interroger sur la validité de ce genre de classement à l'échelle internationale. En effet, malgré la volonté d'objectivisation et d'universalisation de ces indicateurs, ceux-cidépendent étroitement de paramètres culturels forts. Une approche interculturelle (Hofstede) peut permettre de comprendre des différences culturelles fortes et remettre en cause une volonté globalisante de présenter un indicateur qui permettrait de mesurer la performance d'une institution universitaire."
"Les thésaurus manipulent des concepts qui peuvent être considérés, avec un regard externe, comme un échantillonnage, ou sous-ensemble, de la langue dans laquelle ils fonctionnent. Comme leur langue mère, ils véhiculent des valeurs culturelles et sémiotiques. Rameau (Répertoire d'autorité-matière encyclopédique et alphabétique unifié) est le thésaurus communément utilisé en français. Il est élaboré depuis 1980 et enrichi au fur et à mesure des besoins des indexations. La Chine a également un thésaurus propre : "" Zhongguo Fenlei Zhuti Cibiao "". Etabli en 1980, sa dernière mise à jour date de l'année 2005. Il est utilisé comme liste d'autorité matière par la bibliothèque nationale et les bibliothèques. Il est conseillé particulièrement aux chercheurs de se référer à ce thésaurus pour le choix de mots clés lors d'une recherche d'information. L'objet de la présente recherche est de comparer par des méthodes scientométriques les contenus de deux thésaurus nationaux différents, dans le champ de l'intelligence économique afin d'éclairer les différences entre la France et la Chine dans l'approche de l'intelligence économique, et d'observer les diversités culturelles que cette analyse révèle."
Ce texte revient sur une spécificité française : l'association des sciences de l'information et des sciences la communication. Les auteurs analyse la nature de cette association à travers différentes appréhensions de cette conjonction.
"L'engagement est un sujet très actuel. Tantôt on le met en avant, tantôt on déplore son absence. Il semblerait cependant que l'on ait plus souvent à regretter le désengagement voire même le manque d'engagement. Mais de quoi parle-t-on ? Existe-t-il plusieurs formes d'engagement ? Dans ce cas, s'affrontent-t-elles ou sont-elles complémentaires ? Comment mesurer, évaluer l'engagement ? La communication peut-elle le favoriser ? Qui serait alors concerné ? Un petit nombre d'individus ou de grands groupes ? Voici livrées quelques-unes des interrogations qui nous ont menés à conduire la recherche décrite ci-dessous. Aussi, après avoir contextualisé notre contribution, dans une première partie, nous présenterons l'engagement sous un angle théorique, nous verrons comment la communication peut conduire à un engagement en actes, et comment s'y prendre pour amener de grands groupes à être engagés. Nous serons alors conduits à transposer aux environnements numériques une conception de l'engagement primitivement étudiée dans le cadre de communications en face-à-face. Ensuite, nous verrons comment peut être déployée une procédure de communication engageante, au travers d'un exemple concret. Il s'agissait d'expérimenter cette procédure et de la mettre au service d'une démarche éco-citoyenne, en cherchant à favoriser la pratique du covoiturage au sein d'une population d'étudiants. Enfin, nous ouvrirons notre propos sur des réflexions méthodologiques en proposant implicitement des voies de recherche à venir."
"La dynamique d'un territoire est fonction de l'articulation qui existe entre ses membres, de la capacité de ses acteurs institutionnels, de l'entreprise et de l'éducation à se mobiliser et à travailler en synergie. Différentes mesures de cette dynamique collaborative sont envisageables. Dans ce texte, nous nous sommes attachés aux relations hypertextuelles existantes entre les sites web des acteurs du territoire présents sur le site de Sophia Antipolis."
"Le cadre de ce travail est l'étude exploratoire des nouvelles formes de langue induites par le développement des technologies de l'information et de la communication et leur impact sur les situations de communication médiatisée. Ce travail propose une lecture informationniste de la cyberlangue, une analyse infométrique par une approche typologique des néographies, marqueurs de la cyberlangue et par la mesure du poids de ces marqueurs. Une approche pluridisciplinaire permet de construire une typologie des formes de la cyberlangue, de regrouper ces formes en familles et d'en estimer le poids sur un corpus suivi sur une période longue. La confrontation des usages aux représentations permet d'identifier les néographies liées à une émotion ou un sentiment. En perspective : une approche dynamique du processus de diffusion des néographies, l'analyse des réseaux et l'identification des micro-communautés."
"La "" veille d'image "" consiste, pour une organisation, à identifier le plus en amont possible des signaux faibles négatifs de type rumeur ou attaque informationnelle. Notre travail est centré sur la "" veille d'image sur Internet "". Cette notion, qui a fait l'objet de peu de travaux académiques, est surtout abordée par des consultants. Certains d'entre eux forcent le trait du risque encouru pour vendre leurs services. L'objectif de ce papier est de proposer un discours plus nuancé sur la portée "" virale "" de l'Internet. Cette étude repose sur deux études de cas qui montrent que les risques d'attaques contre l'image d'une organisation ne seraient pas aussi grands qu'annoncés par les vendeurs de logiciels de surveillance. Ce travail met en évidence l'existence d'une logique d'autorégulation qui agit comme une "" main invisible "" pour contrebalancer les critiques exprimées."
"Cet article s'intéresse aux phénomènes d'interaction entre les acteurs culturels des pays du Maghreb dans un contexte de mondialisation. Par analogie aux liens sociaux et culturels qui existent et unissent les pays du bassin méditerranéen, il est possible d'identifier sur le Web un certain nombre de sites Web d'acteurs culturels des pays du Maghreb et de s'intéresser aux liens hypertextuels entre ces sites Web. Dans ce contexte, notre question de recherche peut être formulée de la façon suivante : En quoi la meilleure compréhension du maillage hypertextuel entre les sites Web des acteurs culturels maghrébins dans la sphère virtuelle est elle de nature à éclairer les interactions qui existent entre ces mêmes acteurs dans le monde réel ?"
"Les techniques d’analyse du contenu d’un corpus de textes sont multiples. Le traitement exposé ici est basé sur une approche hybride intégrant l’analyse de réseaux, employée en sciences sociales ou en bibliométrie, et la technique de segmentation utilisée en analyse statistique textuelle. L’application de cette méthode dans le cadre d’étude d’analyse sensorielle est présentée. Ces études ont pour objet l’interprétation d’un corpus de commentaires libres proposés par des consommateurs soumis à des tests de produits agro-alimentaires. Ces commentaires étant saisis sous formes de textes électroniques, la mise en œuvre d’outils informatiques spécifiques a permis l’analyse de réseaux des segments présents dans ces commentaires. La première phase du traitement de ces commentaires est leur postcodage : correction orthographique ; réduction du vocabulaire par lemmatisation et synonymie ; marquage des termes ou locutions selon leur appartenance à des classes (arôme/odeur, hédonique, perception, saveur, texture, aspect, intensité des sensations) ; découpage du texte en segments. La seconde phase passe par le dénombrement des segments et de leurs associations, construction d’un tableau exprimant ces données. La dernière phase du traitement est la représentation de ce tableau sous la forme d’un réseau. L’outil informatique qui génère ce réseau permet le renvoi vers les commentaires contenant les noeuds du réseau ainsi qu’une navigation hypertexte."
"L'absence de quelque chose est parfois aussi, voir plus signifiante, que sa présence. Le réseau latent repose sur l'exploitation des vides. L'approche par les réseaux latents s'intéresse à l'identification de traces qui n'existent pas alors qu'elles devraient exister. Cette approche permet d'identifier des non associations remarquables. Par exemple deux auteurs n'ont jamais publié ensemble alors que leur recherche est voisine. Peut être y a t il une incompatibilité entre ces personnes qui ne peuvent travailler ensemble ? Peut être la non association préfigure-t-elle une association future, une association émergente? Dans ce travail nous allons décrire la méthode utilisée pour révéler les réseaux latents. Cette méthodologie sera ensuite déployée de manière expérimentale sur un corpus documentaire composé des publications scientifiques des membres d'un laboratoire de recherche. On fera alors ressortir l'intérêt stratégique des résultats obtenus pour la gouvernance d'un laboratoire, pour un ministère de tutelle et plus généralement l'intérêt de l'approche des réseaux latents pour l'Intelligence Economique."
"Cette proposition analyse les logiques de communication qui accompagnent l'émergence de changement de pratiques entrepreneuriales au sein d'organisations marchandes fondées sur une approche éthique et socialement responsable, tant dans la production que dans la distribution de leurs produits. La problématique questionne l'émergence et le développement de ces organisations en France : comment une intentionnalité première d'un entrepreneur devient une réalité organisationnelle. Notre objet d'étude sera le cas de l'entreprise Alter Eco dont les fondements reposent sur le développement et la promotion de produits de grande consommation issus du commerce équitable."
"L'Internet est de nos jours une nouvelle source d'information incontournable. Par contre, qui n'a pas été déjà confronté au phénomène de sur-information lors de la recherche sur un moteur ou de désorientation lors de la navigation sur Internet. Ces problèmes sont persistants sur le réseau Internet, du fait même de sa construction, et sont à l'origine de pertes de temps considérables. Comment dans ce foisonnement de données juger l'importance des pages Internet et des sites les contenant ? Comment rationaliser la gestion de ces ressources et améliorer la performance de leur exploitation ? La communication abordera dans un premier temps l'ensemble des différentes approches actuellement développées pour aider l'utilisateur à mieux gérer l'information présentes sur Internet. Ces techniques ont principalement pour principe l'évaluation de pages ou des sites Internet. Nous évoquerons les réflexions qui actuellement ont cours pour l'évaluation des ressources Internet [1]. Nous verrons que les principaux critères proposés font bien souvent appel à une évaluation purement intellectuelle. Malgré tout, il parait indispensable de concevoir des outils de traitement automatique pour l'aide à l'évaluation de ces nouvelles ressources. Nous passerons rapidement en revue les différentes techniques d'évaluation actuellement disponible en essayant de les classer par catégorie : - analyse du contenu lexical des pages Web : classement par pertinence (moteurs de recherche ou robots tels que DigOut4U, Search'97) ; simple classement (Search'97, Nomino) ; agrégation statistique suivie d'une cartographie soit parcellaire (Sampler) soit intégrale (WebSom, Neuronav+, SemioMap, Umap, TKS, Tétralogie) - analyse du contenu multimédia des pages Web [2] - analyse de la dimension hypertextuelle des pages Web : analyse de l'espace des relations entre pages [3]; analyse du phénomène de référencement entre sites, la "" sitation "" [4] Nous positionnerons alors notre approche, l'analyse des réseaux de "" sitations "", par rapport à ces techniques. Une description de l'ensemble des indicateurs et cartographie réseaux mise au point sera exposée."
"Un certain nombre de logiciels, connus sous le nom d'analyseurs de .Log proposent un ensemble d'indicateurs statistiques d'usage d'un site web. Cette étude a pour objectif de renouveler l'approche traditionnelle des analyseurs de .Log en utilisant l'analyse en terme de réseau. Cette nouvelle démarche est présentée en s'appuyant sur l'exemple de l'audit du serveur du CRRM réalisé en Décembre 1996 à partir de 2869 connexions. Dans un premier temps, nous positionnerons la méthode réseau par rapport aux méthodes existantes. Dans un second temps, nous développerons un bref aperçu de la richesse des analyses qui peuvent être conduites par cette méthode."
"L'Université du Sud Toulon Var s'est engagée depuis un an dans le déploiement au Vietnam d'un diplôme de Master professionnel en Intelligence Economique et territoriale. Les études d'opportunité préalables au lancement de ce projet ont permis de faire ressortir le caractère innovant de cette initiative puisqu'il n'existe pas de formations équivalentes à l'IE au Vietnam. Des investigations plus fines nous ont conduit à une conclusion plus nuancée. L'IE existe au Vietnam mais constitue davantage un "" socle culturel implicite "". Dans un premier temps, nous comparerons un corpus de documents français et vietnamien dans le domaine de l'IE pour proposer une première approche comparée du système d'IE français et Vietnamien. Cette analyse fera ressortir le caractère émergent du concept d'IE au Vietnam. Dépassant ces observations, nous nous interrogerons ensuite sur les déterminants de l'Intelligence économique en montrant qu'il existe au Vietnam des facteurs engageant culturellement les acteurs vers des logiques d'Intelligence économique."
"L'administration comprend différents acteurs , chacun a sa structure, sa mission. Ceci est le résultat d'un processus historique. Cela se traduit par un ensemble de structures qui se superposent et s'entremêlent parfois qualifié de mille feuille administratif. Nous allons nous concentrer sur la représentation des relations mutuelles entre les administrations qui offrent uncontenu en ligne. Nous allons construire puis analyser le réseau des interactions hypertextuelles entre un corpus de sites web publics présents dans la région Paca (France) en 2005."
"La problématique de l'identification de signaux faibles est centrale en Intelligence économique. Le web, de part son caractère informel, apparaît comme une source d'information à privilégier. Le problème qui se pose alors est celui de la collecte et du traitement d'une information massive et peu structurée. Cette communication présente une approche non booléenne hybride de révélation de connaissances innovante. L'approche utilisée combine une recherche informelle sur le web et une recherche très structurée dans un thésaurus."
"Se basant sur l'idée que le Web permet de créer des liens entre des acteurs distants, cet article a pour but d'étudier les interactions existant sur un forum de discussion. Afin d'atteindre cet objectif, on prendre comme cas le forum de babnet.net, un forum partagé par 759 membres et sur lequel on peut compter, jusqu'à la fin du mois de février 2008, 11444 messages. Notre recherche sera focalisée, plus précisément, sur le forum intitulé "" foi, divinités et croyances "" traitant 26 sujets avec un nombre de messages élevé à 645."
Cet article s'intéresse à la datation d'une page web et à son évolution depuis sa création. L'étude de ces variables temporelles débouche sur des indicateurs macroscopiques. Cet article fait l'objet d'une validation expérimentale dans le domaine de la veille territoriale à partir de l'étude de la présence sur le web de 10 villes françaises
"Le système de bourse de compétences mis en place au sein de l'IUP Ingémédia permet à différents acteurs d'échanger des compétences et des savoirs, chacun pouvant tour à tour se positionner en tant qu'offreur ou demandeur de compétences. Tous les échanges validés entre les acteurs sont tracés et référencés, autorisant ainsi leur traçabilité et exploitation future pour produire des indicateurs. Cette communication s'intéresse précisément au traitement des données en vue de créer pour chaque apprenant un bilan personnel de compétences. Ce document de synthèse, remis à chaque apprenant en fin de parcours, permet de caractériser le niveau d'acquisition de chacune des compétences identifiées et d'apprécier les capacités de management de l'acteur dans son parcours de formation. Ce bilan de compétences est réalisé à partir d'une modélisation des interactions au sein de la bourse de compétences directement transposée de l'analyse des réseaux sociaux."
"Les langages d'indexation sont des outils d'aide à la recherche d'information. Ils comportent des concepts (ou vedettes) liés les uns aux autres par des relations sémantiques hiérarchiques ou d'association. Dans cette communication, nous procédons à l'analyse macroscopique des interactions entre concepts du domaine de l'information communication. L'objectif est de considérer un corpus de mots clés français descripteurs du domaine des sciences de l'information et de la communication et de voir de quelle façon ce domaine est représenté dans le langage d'indexation Rameau. La classification du monde de l'information Communication renvoyé par Rameau est elle le reflet de la réalité de la discipline ou une vue sur le monde ? Cette représentation est elle fidèle à la réalité perçue par les chercheurs de la communauté des sciences de l'information et de la communication ? Pour répondre à ces questions, nous proposons de mettre en oeuvre une démarche quantitative de recueil et de traitement automatiques de données relationnelles."
"Nous rappelerons dans un premier temps les caractéritiques d'un territoire et préciserons le plan des relations qui forment la territorialité. Dans une seconde partie, nous nous intéresserons à la dimension normative pour ce qui caractérise les objets virtuels. Nous conduirons une démarche de veille territoriale appliquée à la technopole de Sophia Antipolis."
"L'information web ne comporte pas toujours de référent spatial et temporel. Rares sont les pages web qui portent une mention explicite de leur date de création, de leur date de mise à jour, de l'adresse de leur concepteur. Or, la validation d'une information quelle qu'elle soit (information scientifique, technique, économique, stratégique) passe par l'ancrage de cette information dans le temps et l'espace. Partant de ce constat, l'objectif de cette communication est d'estimer la date et le lieu de création d'une page web et de montrer l'intérêt que présente ces notions pour l'analyse cybermétrique. Cet article fait l'objet d'une validation expérimentale dans le domaine de la veille territoriale à partir de l'étude de la présence sur le web de 2 villes tunisiennes. Ces validations expérimentales seront analysées au regard d'études similaires réalisées à partir de corpus de villes françaises."
"La commercialisation sur Internet bouleverse les techniques commerciales traditionnelles. Pour optimiser l'audience de son site, voire ses ventes sur ce canal, l'entreprise a besoin d'informations statistiques pertinentes. Le but de notre recherche est de renouveler l'approche des analyseurs de fichiers .Log, outils disponibles actuellement, en montrant les apports de l'analyse réseau."
L'information contenue dans un forum de discussion est la meilleure et la pire des choses : meilleure en ce qu'elle porte le germe d'informations qui se situent encore au niveau des signaux faibles. Pire en ce qu'elle consiste souvent en des informations dont il est difficile de mesurer le degré de fiabilité et de validité. Cet article propose la mise en place d'une chaîne de traitement aujourd'hui semi automatique débouchant sur des cartographies relationnelles. Nous montrerons l'intéret que présente l'analyse relationnelle dans la mesure de la centralité des acteurs du forum et la fiabilisation du corpus de données.
"L'objet de cette recherche appliquée est de découvrir les aspirations et les pratiques relatives à la consommation d'énergie. Ces attentes en matière de service énergétique ont été appréhendées à travers la méthode des chaînages cognitifs et déterminées grace à une technique originale de traitement des données : l'analyse réseau. La conclusion dresse le bilan de cette étude, ses apports et limites respectives et dégage un certain nombre de voies de recherche futures."
"De nombreux travaux se sont intéressés à la représentation des SIC en France. Ces travaux ont mobilisés, pour leur partie expérimentale, des corpus scientométriques qui ont fait l'objet de techniques bibliométriques ou qualitatives. Dans ce travail, l'objectif est d'étudier la manière dont notre discipine est appréhendée à travers le langage d'indexation Rameau. Nous entendons par ce travail répondre à plusieurs questions : - Quelle interprétation du monde des sciences de l'information et de la communication présente le langage d'indexation Rameau ? - Comment cette vision s'est elle affinée au fil du temps ? - Cette vision est elle fidèle à la représentation qu'en ont les chercheurs de cette communauté ?"
"Dans le cadre d'une mission qui nous a été confiée par le Conseil Régional et la Préfecture de la région Paca, nous avons été chargé d'étudier le web public régional [4]. Cette étude nous a donné l'occasion d'identifier plus de 500 sites web à divers niveaux (communal, intercommunal, départemental, régional) et d'explorer les interactions hypertextuelles existant entre ces sites web. Nous avons pu montrer que les interactions hypertextuelles entre les communes de la région Paca disposant d'un site web se superposent au territoire physique. Le web aurait pu permettre des liens distants entre des communes éloignées. Il n'en est rien et on observe une prégnance forte des relations de proximité [1]. Dans cette étude, nous proposons d'analyser une seconde relation pouvant exister entre réel et virtuel à travers la couleur dominante de la page d'accueil des sites web des 250 communes de Paca disposant d'un site web. Notre question de recherche est la suivante : "" y a-t-il des déterminants du choix de la couleur dominante d'un site web de commune ?"". Pour répondre à cette question, nous identifions différents indicateurs du territoire ce qui nous conduit à formuler un certain nombre de questions intermédiaires : * Indicateurs géographiques : la couleur dominante d'un site web est elle fonction de la proximité de la commune avec une zone maritime, montagneuse ? * Indicateurs politiques : la couleur politique de la commune a-t-elle une incidence sur la couleur dominante du site web de ladite commune ? * Indicateurs démographiques : la taille de la commune a-t-elle une incidence sur le choix de couleurs particulières de son site ? * Indicateurs économiques : le nombre d'entreprises installées, les données fiscales récupérées peuvent elles expliquer le choix de certaines couleurs de site web ? * Indicateurs touristiques : le poids du tourisme dans l'activité économique de la commune est il de nature à expliquer le choix de certaines couleurs ? * Indicateurs temporels : la date à laquelle la mouture la plus récente du site a été déposée a-telle une influence en terme de couleur ? Notre travail a consisté à collecter des informations de nature très diverses sur chacune de ces dimensions et à rechercher, à l'aide de techniques d'analyse de données, les facteurs ou combinaison de facteurs permettant de rendre compte de la couleur des pages d'accueil des sites web étudiés. Ce travail suggère une question plus générale abordée par les géographes du territoire qui est celle du lien entre le réel et le virtuel ([2], [3]). Y a t il des déterminants réels permettant d'expliquer la couleur des pages web de communes ou l'information sur les couleurs obéit elle à des motivations subjectives du constructeur du site, à des préférences individuelles ou à une quelconque mimétisme web ?"
"Dans un environnement turbulent et agressif, les organisations humaines sont soumises à des évènements extérieurs susceptibles parfois de les déstabiliser et de les faire disparaître. Ce contexte explique la multiplication de travaux surtout anglo-saxons qui étudient la résilience sous l’angle des organisations. La résilience se définit alors comme la capacité de l’organisation étudiée à faire face à un choc extérieur. Cet article propose un état de l’art du concept de résilience et étudie l’intérêt de la transposition du concept au domaine d’une communauté territoriale. Une étude de cas nous conduira à appliquer le concept de résilience à la nation libanaise."
"Le Web est un "" é-cosystème "" [5] documentaire composé de pages web en interaction. Les moteurs de recherche majeurs restituent, suite à la requête d'un internaute, une information organisée sous forme d'une liste de pages disjointes de laquelle il est difficile de dégager une vision d'ensemble. Différentes familles d'outils ont été créés pour répondre à ce problème : les outils de classification en font partie. Après un état de l'art des outils de classification automatique de corpus web, cette communication présente une méthode de classification originale qui fait ensuite l'objet d'une validation expérimentale."
"La recherche d'information sur internet est une démarche qui met l'internaute dans une situation de devoir faire face à une quantité massive de données. Ces données sont parfois contradictoires. Elles sont bien souvent non validées : le travail de l'internaute n'en est que plus important. Comment réagit l'internaute face à une telle "" surcharge informationnelle "" ? Par quel processus parvient il à répondre à son besoin d'information en mobilisant les données qu'il a à sa disposition ? Des travaux en psychologie se sont intéressés au processus de décision en environnement complexe et incertain. Surtout appliqués dans le domaine de la gestion, ils apportent un éclairage intéressant à la notion de "" rationalité limitée de la décision "". Nous proposons de transposer ces travaux pour une meilleure compréhension de la construction de connaissances par l'internaute suite à une requête adressée à un moteur de recherche. Nous affinerons la notion de "" biais cognitifs "" qui correspond à des mécanismes de protection inconscients qui vont permettre à l'internaute de gérer le problème de surexposition à l'information de façon plus confortable. * Dans un premier temps, nous présenterons de manière conceptuelle et appliquée la notion de biais cognitif et l'intérêt de cette notion dans le domaine de la recherche d'information sur internet. * Dans un second temps, nous proposerons une vision renouvelée de la pertinence ainsi que quelques pistes permettant une meilleure prise en compte de ces mécanismes dans les algorithmes de pertinence des moteurs de recherche."
"Les entreprises doivent apprendre à faire face à des flux informationnels croissants ce qui les conduit à de nouvelles approches du management de l'information. Au-delà de la veille dite passive, toute stratégie gagnante repose dorénavant sur la vision proactive de l'environnement informationnel qui lie étroitement l'information et l'action. Dans ce contexte, les stratégies d'influence présentent un intérêt particulier en tant qu'outils offensifs de l'intelligence informationnelle. De façon duale, il est tout aussi intéressant de décortiquer les mécanismes des stratégies d'influence afin de mieux pouvoir s'en prémunir. La stratégie d'influence peut être définie comme : "" la combinaison d'un ensemble de modes d'actions, exercés de manière directe ou indirecte, ouverte ou couverte, vis-à-vis de personnes, de collectivités, d'organisations et/ou d'Etats, en vue d'acquérir un meilleur crédit, de prendre de l'ascendant et finalement d'orienter les décisions dans le sens souhaité "". Nous distinguons deux types d'actions d'influence : spontanée et rationnelle. ⇒ L'influence spontanée est une technique qui consiste à déclencher des comportements ou créer des jugements chez l'acteur influencé en exploitant certains raccourcis du cerveau. On trouve de nombreux exemples de ce type d'influence en marketing. Ce type d'action fournit des résultats immédiats via un processus d'influence qui n'est pas perçu par le sujet en tant que tel. ⇒ L'influence rationnelle, appelée aussi influence par l'information, est une stratégie qui a généralement pour objet d'amener l'acteur influencé à intégrer dans son processus de décision des informations qui sont déposées sur son chemin par l'acteur influenceur. Les procédés de ce type d'action d'influence sont multiples : argumentation, suggestion, déception, désinformation, contreinformation, stabilisation etc. Les stratégies d'influence sont le fait de nombreuses catégories d'acteurs et s'appliquent à ces mêmes catégories d'acteurs : institutions, entreprises, personnes etc. Ces stratégies sont souvent menées dans la discrétion et il est difficile de les mettre à jour. Dans cette validation expérimentale, nous avons choisi de nous intéresser aux stratégies d'influence conduites sur Internet par le lobby antinucléaire en France. En effet, des organismes et associations antinucléaires ont comme mission institutionnelle d'exercer une influence sur les populations, les gouvernants etc. Cette institutionnalisation de l'action d'influence devrait la rendre plus facilement traçable. Après avoir situé le phénomène de l'influence dans le processus de la prise de décision et présenté les principales formes des actions d'influence rencontrées dans la littérature, nous proposerons notre classification des types d'actions d'influence et illustrerons la stratégie d'influence du lobby antinucléaire en analysant ses différentes techniques utilisées."
"Le point de départ de ce projet est une commande du Conseil Régional Paca consistant à dresser un état des lieux du web public régional. Le web public régional est composé de sites web émanant de l'état, des collectivités territoriales, des communes ou des intercommunalités. D'ordinaire, les analyses effectuées pour répondre à ce type de questionnement mobilisent des approches qualitatives et quantitatives : * Les approches qualitatives conduisent à des monographies par site web * Les approches quantitatives débouchent sur des indicateurs caractérisant les objets d'études. Dans un cas comme dans l'autre, il s'agit souvent de considérer les sites web isolément. Dans l'espace virtuel, ces sites web interagissent de différentes manières. Certains sites web peuvent trouver parfois leur légitimité une fois resitués aux confluents de plusieurs autres sites. Ce travail nous a conduit à privilégier une forme d'interaction entre sites se traduisant par l'existence d'un lien hypertexte (Clément), (Belisle, 1998), entre deux sites web. Ainsi, l'analyse que nous mettons en oeuvre, débouche sur la construction de cartographies appelées réseaux (UQAM, 1996) représentant les interactions entre les sites web publics régionaux et sur un certain nombre d'indicateurs associés. Cette logique relationnelle s'avère féconde et permet d'apporter des réponses à des problématiques variées : * Quel rôle jouent les acteurs publics régionaux dans la valorisation de leur territoire ? * Quelle est la position des acteurs publics régionaux dans la topologie du graphe ? (La représentation cartographique laisse-t-elle apparaître les acteurs publics régionaux en tant qu'acteurs centraux desquels vont se rattacher d'autres acteurs, ou bien révèle-t-elle des acteurs périphériques tendant à s'annexer à certains pôles principaux?). * De quelle manière s'opère l'interaction entre la logique publique et la logique marchande ? * Quel est le niveau d'interaction entre les sites des différents échelons territoriaux : communes, intercommunalités, département, région ? * L'interaction entre les objets étudiés reflète-t-elle une quelconque proximité géographique entre les territoires considérés ? * Quelle place est-elle réservée aux sites qui se consacrent aux services publics, et qui émanent des intérêts d'autres sources que celles institutionnelles publiques ?"
"Lorsqu'on construit des indicateurs cybermétriques, on utilise parfois les fonctionnalités avancées des moteurs de recherche en faisant faire à ces derniers des opérations pour lesquelles ils n'ont pas été préparés. Ainsi, nous nous sommes intéressés à la représentation des interactions entre les acteurs du web public en région paca. Ce travail nous servira de fil rouge et de validation expérimentale dans cette communication. Pour construire le réseau des liens hypertextes entre les sites à étudier, on a mobilisé deux méthodes alternatives : - Dans un premier temps, on a utilisé le moteur de recherche Google et sa commande avancée link:. La commande link permet de connaître les liens entrants sur une page ou un site web donné. On obtient à l'issue d'un processus de collecte, de traitement et de cartographie le réseau des interactions entre les sites publics de la région. - Pour mesurer la qualité de ce résultat, nous l'avons confronté à une autre méthode de collecte de données. Il s'agissait alors d'explorer les diverses pages d'un site web à la recherche des liens que ce site a vers des sites extérieurs. Cette nouvelle collecte d'information s'intéresse aux liens sortants et non plus aux liens entrants et s'affranchit du moteur de recherche Google. Cette méthode s'est révélée à l'usage beaucoup plus riche, l'utilisation de la commande link du moteur de recherche Google rendant compte de façon très incomplète de la réalité. L'objectif de cette communication est de confronter les résultats de ces deux méthodes. Cette communication débouche sur l'explicitation des hypothèses sous jacentes du moteur Google. Elle montre aussi tout le risque qu'il y a, pour une personne non spécialiste, à utiliser les moteurs de recherche en les détournant de leur fonction initiale. Elle renforce également l'asymétrie du contexte, l'utilisateur lambda n'ayant accès qu'à une information biaisée et limitée qu'il ne pourra exploiter que de façon très prudente dans une perspective d'analyse cybermétrique."
"Cet article commence par traiter du lien souvent mis en avant entre Islam radical et salafisme. Ensuite, il cherche à éclairer certains modes de propagation de messages salafistes dans les médias, surtout sur internet. Devant le caractère problématique d’un prosélytisme salafiste sur les réseaux sociaux, une réflexion est amorcée concernant la mise au point d’une méthode visant la création d’un observatoire numérique interdisciplinaire, celui-ci s’appuyant notamment sur la notion de réseau latent. L’élaboration d’une cartographie dynamique du web salafiste pourrait permettre d’anticiper, de façon automatisée, certains mouvements potentiellement dangereux et servir de système d’alarme pour les services compétents. Il s’agit donc là d’une réflexion théorique mais avancée à des fins pratiques et portant sur un enjeu éminemment stratégique. Ensuite est discutée la façon dont une « minorité active » agit pour opérer une conversion idéologique, et engager d’autres individus. A partir d’une question déjà posée – islamisation de la radicalité ou radicalisation de l’Islam ? – s’ouvrent de nouvelles perspectives de recherches, notamment liées à l’errance d’individus livrés à eux-mêmes sur internet, sans compétences informationnelles, et exposés à des messages favorisant la formation d’un imaginaire délétère."
"Wikipédia est un projet d'encyclopédie collective construit à partie de contributeurs volontaires. Lorsqu'on interroge Wikipédia, on consulte la dernière mise à jour d'une page sur un sujet donné, mais des fonctionnalités avancées donnent à voir la trace des versions antérieures de la page. Il est ainsi possible de reconstituer le millefeuille des contributions, la dynamique par laquelle une page Wikipédia a été co-construite. Un certain nombre de travaux antérieurs ont étudié la façon dont, en temps réel, un ensemble de contributeurs retraçaient sous Wikipédia la mémoire d'une catastrophe, d'une crise, d'une révolution ; le décès d'une idole. Wikipédia apparaît comme un laboratoire d'observation de la façon dont la dynamique de coordination opère en temps réel. Dans cet article nous n'allons ps nous intéresser à une page Wikipédia qui relate un évènement d'actualité. Au contraire, nous allons étudier la page Wikipédia consacrée à Jésus de Nazareth. Cette page est le fruit d'une collaboration en ligne de large échelle. Nous allons privilégier l'étude de la dynamique de coordination autour de cette page en mobilisant une méthodologie d'analyse des traces laissées par les contributeurs."
"On s’installera plus volontiers à la table d’un restaurant fréquenté que d’un restaurant vide. Les travaux sur la preuve sociale (voir Cialdini, 2001) peuvent faire l’objet d’une transposition dans le domaine des technologies numériques. On accordera plus de crédit à un forum de discussion actif et nourri qu’à un forum déserté. Compte tenu de ce lien entre quantité et qualité, il est intéressant de s’interroger sur la façon par laquelle l’usager va se forger une représentation de la quantité d’information circulant dans un dispositif numérique. Cette appréciation est-elle fidèle à la réalité, biaisée, surestimée, sous-estimée? Afin de répondre à ces questions, nous mettons en œuvre un travail de terrain dans lequel nous demandons à des internautes issus d’un échantillon de convenance d’évaluer un nombre d’intervenants différents d’un forum de discussion. Les réponses sont analysées et les résultats discutés."
"Longtemps relégué dans le secret du pouvoir politique et militaire, le renseignement a surgi dans le champ scientifique à travers les travaux sur la veille et l’information documentaire (Dou et al., 1990), la bibliométrie (Paoli et al., 1992) et finalement son avatar, l’intelligence économique (Martre, 1994). Face au changement de paradigme que constitue la fin de la guerre froide et la démocratisation des technologies de l’information et de la communication, il devient possible aujourd’hui de faire du renseignement un objet de recherche en sciences humaines et sociales. Le but de cet article est d’introduire le renseignement comme un objet de recherche pluridisciplinaire en sciences humaines et sociales et, dans la mesure où il est avant tout une activité de traitement d’information, de montrer qu’il trouve une place naturelle dans le champ des sciences de l’information et de la communication (SIC)."
"Cet article s'intéresse à un message du Pape François destiné à la 48ème journée mondiale des communications sociales. La position de l'Eglise catholique à propos de l'internet y est exprimée : ce réseau est qualifié de ""don de Dieu"". Celui-ci doit, pour le Pape François, permettre au message chrétien de franchir les frontières pour attendre les personnes les plus éloignées de l'Eglise. Aussi, l'autoréférentialité de l'Eglise est stigmatisée. Cette notion semble pertinente à étudier, en sciences de l'information et de la communication. Les auteurs apportent une contribution qui s'ancre dans une approche infométrique et webométrique pour déterminer si les sites Web de l'Eglise sont structurés de façon autoréférentielle. Pour ce faire, ils définissent une acception particulière de cette notion. Ensuite, ils déterminent deux corpus de sites Web de l'Eglise, pour comparer quantitativement et qualitativement leur autoréférentialité, à partir d'un recensement de liens sortants. En conclusion, les auteurs confirment le caractère autoréférentiel de l'internet clérical de l'Eglise. Celui-ci serait d'autant plus prononcé que l'on se ""rapproche"" des sites Web du Vatican. Cette conclusion est à modérer, eu égard au caractère exploratoire de la recherche présentée. Plus largement, une réflexion est initiée sur l'utilisation et les usages des nouveaux médias, au sein de l'Eglise catholique."
"Le domaine de la scientométrie s'est penché sur l'identification de réseaux de co-auteurs. Il s'agit alors de mesurer le degré de coopération de deux auteurs qui ont publié ensemble. Cet article explore l'association latente des deux auteurs. Par ""association latente "", nous désignons une collaboration entre deux chercheurs qui n'a pas encore eu lieu mais qui pourrait très probablement avoir lieu à l'avenir. Dans ce papier, nous nous efforcerons de révéler des couples d'auteurs qui n'ont jamais publié ensemble et qui ont des intérêts académiques similaires."
"Aujourd'hui, avec l'analyse automatique des bases de données, la construction d'indicateurs bibliométriques ou scientométriques est facilitée. Le problème est plus de vérifier l'exactitude de l'analyse globale, y compris l'échantillonnage des données. La cohérence globale de l'analyse dépend de la pertinence de tous les maillons de la chaîne. En utilisant des bases de données en ligne, une expérience a été conduite : les mêmes indicateurs ont été utilisés sur différents échantillons. Les résultats obtenus sont profondément différentes."
"L'analyse des co-publications scientifiques a fait l'objet de travaux nombreux en mobilisant l'analyse relationnelle à l'étude de corpus infométriques. Tous ces travaux valorisent l'existence d'une relation entre auteurs comme composant élémentaire du réseau d'association. Dans ce travail, notre objectif consiste à mettre en évidence l'absence remarquable de collaboration entre auteurs. L'absence de relation peut se révéler beaucoup signifiante que sa présence. Le réseau latent repose sur l'exploitation des vides. L'approche par les réseaux latents s'intéresse à l'identification de collaborations potentielles. Cette approche permet de révéler des non associations remarquables. Une association latente entre deux auteurs correspond au fait que ces deux auteurs n'ont jamais collaboré ensemble alors que sur d'autres plans, ils ont entre eux une certaine proximité. Par exemple deux auteurs n'ont jamais publié ensemble alors que leurs recherches sont voisines. Peut être y a t-il une incompatibilité entre ces personnes qui ne peuvent travailler ensemble ? Peut être la non association révélée par l'analyse des réseaux latents préfigure-t-elle une association future, une association émergente qui viendrait renforcer ou tel ou tel champ de recherche ? Dans ce travail, nous décrivons la méthode infométrique utilisée pour révéler les réseaux latents avant de la déployer de manière expérimentale sur un corpus documentaire composé de près de 900 articles publiés dans l'archive ouverte d'Archivesic."
"La théorie des facettes sert de référentiel à cette recherche. Cette théorie a été développée dans le monde de l'information documentaire par Ranganathan (1933, 1967). L'auteur propose de décrire le contenu des documents selon une approche combinatoire et multidimensionnelle qui s'oppose aux classifications mono hiérarchiques en vigueur jusque là. Pour Ranganathan, le contenu de tout document touche à cinq aspects de la réalité : la Personnalité, la matière, l'énergie, l'espace et le Temps. A ces cinq perspectives s'ajoute une facette principale, représentant généralement le domaine de la connaissance auquel on peut rattacher un document particulier (Agriculture, Médecine, etc.). Nous proposons dans ce travail de réinterroger la théorie des facettes de Ranganathan dans une perspective de recherche d'information sur internet. Nous nous interrogerons sur la faisabilité et la mise en oeuvre de telles descriptions pour les documents en ligne. La caractérisation des facettes passe par un travail multidisciplinaire mettant en œuvre des emprunts et des transpositions dans le domaine de la fouille de données, de la psychologie sociale, de l'informatique, des mathématiques, des sciences du langage, de la communication."
The purpose of this paper is to present tools and methodologies that can be used in order to extract strategic information from Internet on a specific subject. The result of the analysis is presented through networks maps and visualize interactions of the main WEB sites on the analysed subject. The paper focuzes on a specific case study that is representing interactions between the main Web sites of the french administration that were present on the site of the << Documentation Française >> in September 1996.
"L'objectif de notre recherche est de présenter une nouvelle méthode d'analyse du trafic sur un site web et de valider un nouveau modèle de la répartition du trafic entre les pages du site. Cette modélisation est fondée sur l'analyse réseau et les chaînes de Markov. La première partie présente les différentes approches disponibles pour mesurer l'audience et le trafic sur un site web. Après avoir exposé la méthodologie mise en oeuvre, nous appliquons la modélisation envisagée au site Intranet de la SNCF et évaluons sa qualité en tant qu'outil de simulation. Enfin, nous développons les implications théoriques et managériales de notre recherche et exposons les voies de recherche futures."
"La publicité est principalement une communication commerciale (Dayan, 2005) – mais pas que – de masse et donc impersonnelle. Son but est de promouvoir, un produit, une entreprise, une marque, une cause, ou autre, qui est identifié dans le message. Elle est diffusée de façon unilatérale (de l'annonceur/émetteur vers le récepteur) par l'intermédiaire de médias et autres supports. Étant une communication persuasive, elle s'appuie sur un sens commun, variant selon les cultures et les groupes sociaux (Rouvrais-Charron, 2005). Dans le domaine du sport il faut distinguer deux types de publicités : celles qui font la promotion du sport et des produits complémentaires (équipements sportifs, etc.), et celles qui utilisent le sport pour la promotion d'un produit sans rapport. Parmi les sports, les sports de combat et arts martiaux en particulier exercent une fascination et une influence telle que la culture populaire reprend et parfois détourne leurs codes et leur pratique pour en véhiculer les valeurs, les mythes, les clichés, ou les performances de leurs pratiquants, même les plus illustres (Kato, 2007). Cette influence est d'autant plus remarquable qu'elle ne reflète pas leur pratique en termes de licenciés dans les diverses fédérations de sports. En France, le sport ayant le plus d'adhérents est le football, alors que le judo – premier art martial en termes d'adhérents – n'arrive qu'à la 4e place (le karaté – 2e art martial le plus pratiqué – n'arrivant que 16e). Cependant, ce sont les sports de combat et les arts martiaux qui sont les plus représentés dans la culture populaire. L'exemple le plus frappant de ce contraste entre nombre de pratiquants et représentation est l'utilisation de l'image du sumo ; combien existe-t-il de pratiquants de cette discipline par rapport au nombre de fois où l'on voit un rikishi (lutteur de sumo) dans une publicité ? Si les reprises répétées par le cinéma et l'industrie vidéo-ludique sont évidentes et font partie en quelques sortes de l'inconscient collectif, celles réalisées par la publicité le sont moins. Ces dernières multiplient les clins d'œil aux sports en général, et plus particulièrement aux sports de combat et arts martiaux afin de se réapproprier leurs valeurs et attributs supposés pour les appliquer"
"L'internet est un écosystème complexe dans lequel il est possible d'observer certaines régularités, certains mécanismes semblant obéir à des lois (Huberman). Ce fonctionnement en écosystème peut s'observer au niveau d'une micro communauté composée de membres partageant un centre d'intérêt commun. Ce qui va nous préoccuper ici est de représenter ce que va devenir un sous ensemble de l'écosystème lorsqu'une communauté web est soumise à un choc brutal. Nous formulons alors l'hypothèse selon laquelle le choc est de nature à modifier le fonctionnement de cet écosystème et d'introduire une polarisation des échanges autour de lieux privilégiés qui joueraient le rôle de totem, d'églises, de lieux de recueillement ou se construirait une histoire et une mémoire collective, ou se partageraient des émotions (Norris). Dans ce travail, nous avons choisi de réaliser une étude de cas qui nous conduira à étudier une communauté de fans suite au décès de leur idole : Michael Jackson. Nous mettrons en place de manière rétrospective un observatoire infométrique pour caractériser les échanges autour de la star avant et après sa mort (lieux d'échanges privilégiés, nombre de personnes échangeant, nature des échanges). Cette approche permettra de révéler une topologie particulière des échanges qui pourrait être un marqueur de résilience."
"Le monde de la recherche d'information est actuellement en train de vivre une période d'intense bouleversement : position hégémonique du moteur Google, question délicate de l'interpénétration des sphères publiques et privées, redocumentarisation du monde, montée en puissance de la logique publicitaire et sa cohabitation avec le modèle « régulé » de la simple application d'algorithmes. De nouvelles modalités d'accès apparaissent, telle celle de la sérendipité que nous interrogeons, après l'avoir resituée dans l'héritage de la bibliométrie, au regard des modèles théoriques de la recherche d'information, pour isoler le rôle d'adjuvant indispensable qu'elle occupe désormais. Son instrumentalisation par les moteurs, sa perception liée au niveau d'acculturation socio-technique des usagers, la diversité de ses instanciations, pose la question de l'opacité des algorithmes et de la nécessaire ouverture d'un débat autour d'un écosystème non plus simplement documentaire mais politique."
"L'approche française des sciences de l'information et de la communication est relativement ""exceptionnelle"" dans la mesure où ces deux champs forment une seule discipline contrairement aux pratiques étrangères, notamment amglo-saxones, où ces champs sont séparés. L'article traite du positionnement épistémologique, socioligque et culturel de cette exception française. Il s'appuie sur une analyse des thèmes des 850 thèses publiées en France depuis la création de la 71° section du Cnu. Il conclut que plutôt qu'une opposition il s'agit d'un continuum multidimensionnel entre information et communication."
"Avec le développement d'Internet et des connexions à haut débit, les modes d'accès à l'information se sont automatisés. La documentation numérique via le réseau a si bien supplanté les autres supports d'information qu'aujourd'hui lorsque l'on a besoin d'un renseignement, le réflexe premier est la connexion au Web. Ce réflexe déjà acquis par les professionnels de la recherche d'information et de la veille intervient désormais dans le cadre de stratégies de surveillance automatisées. Or, parallèlement à cette systématisation, l'accès automatique au Web est devenu de plus en plus complexe à cause du caractère hétérogène des éléments qui le composent. La mixité des formats, les contraintes liées aux modes d'accès ainsi que la multitude de pratiques rédactionnelles sont autant d'obstacles à son traitement. Ses principales exploitations que sont l'acquisition de documents, la surveillance stratégique et technologique et l'analyse des tendances souffrent de manière inégale. Il convient aujourd'hui de connaître les limites des outils que l'on achète ainsi que les caractéristiques des sources que l'on exploite afin de mettre en place une surveillance Web efficace."
"Cette communication vise à mettre en éclairage le principe de bourse de compétences déployée dans le cadre de l'Iup Ingémédia, à travers l'approche d'un dispositif de communication éducative médiatisée. Les modalités d'échanges mises en œuvre permettent d'émuler les interactions sociales et le travail collaboratif entre étudiants avec une valorisation de ceux-ci dans leurs modalités d'évaluation. La traçabilité des échanges offre par ailleurs la possibilité de révéler une cartographie des compétences déployées par l'analyse des interactions entre apprenants. Ce dispositif « situé » dans un projet pédagogique déterminé offre des potentialités d'adaptation et de déploiement à une échelle plus large et dans d'autres situations d'usages, notamment dans le monde de l'entreprise."
"Cet article traite de biais cognitifs qui pourraient affecter le jugement des internautes lors de la lecture d'une liste de réponses, suite à une requête dans un moteur de recherche. L'hypothèse est faite que des effets d'ordre i.e. de primauté et/ou de récence pourraient être observés dans de tels contextes. Les auteurs choisissent de la tester en réalisant une expérimentation en milieu contrôlé. Ils choisissent alors de s'intéresser plus particulièrement au domaine des techniques de sevrage tabagique et affinent leur questionnement ainsi : A la suite d'une requête dans un moteur de recherche, la place d'une médication dans une liste détermine-telle l'idée que se fait une population d'étudiants de sa pertinence ? En comparant trois groupes différents, les auteurs démontrent un effet de primauté et l'absence d'effet de récence. De plus, ils mettent en relief cinq variables modératrices : le sexe de l'individu, le fait qu'il soit fumeur ou non, le fait qu'il ait eu primitivement un avis ou non à propos des méthodes de sevrage tabagique, le fait qu'il soit ou pas concerné par des problèmes de santé liés au tabagisme, sa vitesse de lecture sur l'interface Web. Les auteurs concluent en plaidant pour une éducation à la culture informationnelle."
"L’analyse en terme de réseau a fait l’objet d’un certain nombre de recherches, dans le domaine des sciences humaines et sociales. Les principaux résultats sont présentés dans l’ouvrage de Wasserman (1994). L'objet de ce travail est de porter à la connaissance de la communauté académique une pratique originale de traitement de l'information qui utilise un graphe appelé réseau pour représenter une information complexe de façon synthétique. Cette technique est d'abord positionnée par rapport aux analyses de données classiques et fait ensuite l'objet d'une application dans le cas particulier du traitement des données bibliométriques."
"Depuis quelques années, le nombre de publications consacrées aux sujets sociaux dans la communication médiatique augmente en France. Nous montrons, qu'à condition de l'associer à un contexte sémiotique et pragmatique, la psychologie sociale contribue, au sein des Sciences de l'information et de la communication (SIC), à accroître les ressources théoriques et l'éventail des méthodologies, tout en garantissant, sur le plan épistémologique, une démarche de construction des connaissances rigoureuse. En répondant à certaines critiques adressées à cette discipline et en montrant sa complémen-tarité avec la sociologie et l'ethnographie des médias, nous indiquons quelques perspectives de recherche ouvertes par l'approche psychosociale de la communication médiatique en SIC, notamment dans les études de réception. ."
"Comment les publicitaires inventent-ils les bannières sur internet : le rôle des dialogues internes, de l'auto-évaluation et de leurs théories implicites de la communication. Une étude empirique qualitative sur la créativité des publicitaires."
"L'article étudie le processus de conception individuelle des messages publicitaires dans une perspective de cognition sociale située et contextualisée. Une enquête qualitative, couplée à la méthode des protocoles concurrents et rétrospectifs, montrent que, pour créer un message, les concepteurs font dialoguer en mémoire de travail des “ voix intrapsychiques ” incarnant six acteurs de la production publicitaire. En analysant les représentations et connaissances mobilisées, nous montrons que le processus de création, itératif, articule des procédures de raisonnement inductives, déductives, analogiques ainsi que des traitements heuristiques et automatiques. La création fait également appel à l'imagerie mentale activée en même temps que se réalisent des actes sensori-moteurs de production graphique manuelle. Le processus se termine par une phase où le publicitaire rationalise a posteriori ses choix esthético-sémiotiques, notamment au regard des critères d'acceptation de l'annonceur."
"98% des étudiants ont accès à un bouquet de services numériques au travers des environnement numériques de travail. La mesure de l’utilisation des services est pendante de leur coût de déploiement. L’obtention de données d’utilisation locales pour un bouquet de services est relativement directe mais elle est dépendante de la technologie. De surcroît, ces données ne permettent que des comparaisons autarciques et temporelles à usage performatif. Pourtant ce type de données, exemptes de toute considération technologique (et évidemment anonymes), couvrant le territoire ouvrirait à l’analyse comportementale statistique. D’un point de vue du fournisseur de services, au niveau de la gouvernance, il peut être intéressant de construire en sus d’autres indicateurs tels le nombre de services offerts et leur taux d’utilisation respectifs. Nous montrons dans ce qui suit comment déterminer des niveaux d’utilisation de référence au plan national qui permettraient la construction d’une politique d’établissement de développement des services numériques. Le système Agimus, libre et ouvert, est utilisé pour générer des indicateurs. En s’appuyant sur les référentiels nationaux (SISE et Supann), la syndication de données d’utilisations regroupées et normalisées par établissement, puis par Université Numérique en Région (UNR) permet d’établir des mesures indépendantes des technologies et des établissements afin d’élaborer des connaissances sur le comportement des utilisateurs (par filière, catégorie d’usager, par niveau d’étude), de déterminer le degré et la profondeur d’utilisation des services. L’agrégation au ministère constitue un ensemble de référentiels utiles guider la politique de développement du numérique. Les métriques sont issus d’indicateurs fins comparables entre établissements universitaires."
"La visualisation est un double processus à la fois une mise en visibilité (l’aboutissement d’une instrumentation d’exploration de masses de données, devenant apte à générer indicateurs, cartographies, représentations) et une redéfinition du voir pour ouvrir vers de nouvelles connexions, de nouvelles analogies, de nouvelles conditions déductives... La visualisation des données comme problème revient donc à poser les fondements d’une nouvelle herméneutique singulière et/ou collective. Les contributions donnent un panorama d’applications des techniques de visualisations de données, applications réflexives sur leur mode d’utilisation, et interrogent leur portée. La sémantique quelquefois très spécialisée de ces techniques nécessite un apprentissage. Elles montrent l’intérêt des dispositifs de « visualisation » non seulement comme support de rendu de données, mais aussi comme mise en valeur de nombreuses fonctions complémentaires : présentation synthétique, exploration et fouille dans des données relationnelles complexes de dimensions multiples y compris les données temporelles."
"L’ouverture de l’économie marocaine suite à des accords de libre échange a, dans un premier temps, fragilisé le pays en créant un déficit commercial important les exportations ayant une croissance très faible et reposant principalement sur une compétitivité prix. Progressivement, les instances dirigeantes du pays ont mis en oeuvre une politique visant à moderniser le tissu industriel national de façon à attirer des investisseurs étrangers mais aussi à développer des activités tournées vers l’international et la production de produits de qualité intégrant une forte dimension R&D. Sur les dix dernières années la compétitivité hors prix des exportations marocaines a augmenté de façon significative suite aux efforts consentis en matière d’innovation. Des secteurs innovants sont apparus : énergies renouvelables, logistique, industrie automobile, aéronautique. Les industries extractives sont montées en gamme et ont permis de positionner le Maroc sur l’exportation de produits chimiques (engrais, sels halogènes…). Pour accompagner cette politique industrielle offensive, ce pays se dote aussi progressivement de dispositifs d’intelligence économique comme outils d’aide à la décision pour renforcer la compétitivité de ses PME qui constituent plus de 90% de son tissu productif. Ces dernières années, des initiatives multiples ont été prises pour mettre en oeuvre une telle politique mais, encore à ce jour, des défis restent à relever pour favoriser la dynamique d’innovation des PME. C’est une pratique en gestation mais encore largement cloisonnée (Achchab and Ahdil, 2015). Parmi les différents volets d’action d’une politique d’intelligence économique, le brevet occupe une place de premier rang. Il est toutefois souvent valorisé dans une optique défensive avec pour objectif de sensibiliser les PME, en particulier, à l’importance de protéger leur patrimoine informationnel, clé de leur compétitivité. L’exemple des babouches marocaines (Bredeloup and Bertoncello, 2006) et l’attaque chinoise de ce produit dit du « terroir » montre l’importance d’intégrer une analyse globale des brevets sur le territoire marocain comme une source stratégique d’information pour les entreprises. Mais, il apparaît de plus en plus que le brevet peut aussi être utilisé dans une stratégie informationnelle offensive devenant ainsi un élément indispensable guidant la dynamique d’innovation des PME (Shih, Liu, and Hsu, 2010).Dans le cas spécifique de pays en développement, une telle stratégie offensive informationnelle du brevet peut contribuer à « améliorer les produits existants, valoriser les ressources naturelles et les machines et procédés de première transformations qui sont concernés » (Dou and Leveillé, 2015). Cette nouvelle perspective offerte est rendue possible grâce à l’élaboration de logiciels permettant une analyse automatique de brevets reposant sur une logique de big data. Dans cette perspective, nous souhaitons présenter ici l’apport d’un outil, Patent2Net(Reymond and Quoniam, 2014), qui permet de crawler l’univers des brevets dans le cadre d’une analyse brevet au Maroc. C’est un logiciel gratuit et sous licence libre (CECILL-B), réalisé par I3M et l’IRSIC laboratoires en sciences de l’information et de la communication, et une équipe internationale composée de professeurs et chercheurs universitaires (ibid.). Il s’agira de montrer comment une analyse des métadonnées des brevets (déposants, inventeurs, dates de dépôts, pays de protection, offices de dépôts etc...), des réseaux entre déposants, inventeurs, entre brevets citants et cités, permet d’offrir des informations stratégiques sur les technologies et connaissances utilisées par les inventeurs, et constituent, à ce titre, un levier stratégique 1 tant au niveau des institutions gouvernementales que des entreprises."
no abstract
"Nous étudions la production technologique au Maghreb à l’étude de la totalité des dépôts de brevet mondiaux de trois pays : l’Algérie, le Maroc et la Tunisie. Les brevets constituent une source pour la construction de réels indicateurs technométriques pour évaluer et comparer, entre autres, l’activité inventive des pays. En vecteur d’information, le traitement des brevets met en lumière non seulement des éléments quantitatifs mais aussi qualitatifs pour refléter potentiellement la réalité d’une société donnée en termes de production et de circulation des savoirs. Nous avons émis l’hypothèse que les demandes de dépôt de brevet analysées à l’aide de notre outil Patent2Net sur ces trois pays étaient susceptibles de témoigner de l’avancée technologique et scientifique d’un pays en matière de propriété industrielle. L’analyse montre une activité de recherche relative et permet de distinguer des singularités de chaque pays. Nos résultats peuvent être mis en regard avec les données sur la situation politique, économique, sociale ou culturelle de l’Algérie, du Maroc et de la Tunisie."
"La visualisation est un double processus à la fois comme mise en visibilité (l’aboutissement d’une instrumentation d’exploration de masses de données, devenant apte à générer indicateurs, cartographies, représentations ) et une redéfinition du voir pour ouvrir vers de nouvelles connexions, de nouvelles analogies, de nouvelles conditions déductives... La visualisation des données comme problème revient donc à poser les fondements d’une nouvelle herméneutique singulière et/ou collective Les auteurs donnent un panorama d’applications des techniques de visualisations de données, applications réflexives sur leur mode d’utilisation, et interrogent leur portée. La sémantique quelquefois très spécialisée de ces techniques nécessite un apprentissage. Ils montrent l’intérêt des dispositifs de « visualisation » non seulement comme support de rendu de synthétisation des données, mais aussi comme mettant en valeur de nombreuses fonctions complémentaires : présentation synthétiques, exploration et fouille dans des données relationnelles complexes de dimensions multiples, ou dans les données temporelles."
"Lancée par le Ministère de l’Enseignement Supérieur et de la Recherche, l’opération université numérique en région (UNR) a permis le déploiement de services numériques au sein des établissements. Actuellement, 98% des étudiants disposent d’environnements numériques de travail (ENT) accessibles via les réseaux informatiques des établissements ou depuis l’extérieur. Les principaux services numériques déployés au sein des UNR s’articulent essentiellement autour de la scolarité, la pédagogie, la communication, la documentation, la bureautique, la vie universitaire et les relations avec les entreprises. D’autres services liés, par exemple à la mobilité et au podcast des cours ont enrichi le bouquet de services numériques. Aujourd’hui un accent particulier est mis sur le développement des usages du numérique qui pose la question de la mesure de son utilisation pour offrir à la gouvernance de l’établissement des données factuelles quantitatives. Si l’intérêt de la construction des indicateurs de mesure pose la question du comportement statistique des générations internet il est d’abord utile à différents degrés : tant au niveau du suivi que du pilotage du numérique au sein d’un établissement. De même, les indicateurs sont utiles pour analyser et agir en développant des modes d’accompagnement afin de renforcer les usages. Cependant, la construction effective de ces indicateurs se heurte à des difficultés d’ordre technique, technologique et parfois une certaine frilosité politique. Nous proposons un dispositif ouvert et libre, de génération d’indicateurs enrichis, fort d’une flexibilité de mise en œuvre qui s’appuie sur des référentiels normatifs de description de profils utilisateurs et sur une catégorisation des services en gamme applicative. Ce dispositif, AGIMUS (application de gestion des indicateurs de mesure des usages des services numériques), permet la construction automatique de tableaux de bord aidant la gouvernance à piloter le numérique au niveau : •d’un établissement, •d’un ensemble d’établissements par exemple une UNR ou un PRES, •d’un ensemble d’UNR, •et enfin du ministère. Dans un premier temps les indicateurs remontés couvrent les usages des services et des ressources numériques. Ils sont indépendants des technologies des services applicatifs. Nous montrerons comment le dispositif ouvre la voie à des études comportementales ciblées dans une démarche qualité d’amélioration de la performance des services numériques mis à disposition par une institution. Les exemples d’indicateurs mis en œuvre seront pris sur le terrain : les établissements de l’enseignement supérieur et de la recherche."
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
no abstract
"Nous proposons ici une approche expérimentale appuyée par une méthodologie hybride (quantitative et qualitative) pour caractériser les contenus des interfaces. Nous générons une taxonomie en utilisant la dimension sémantique portée par les hyperliens de pages d’accueil de sites web dont les unités lexicales associées ouvrent l’accès aux contenus sous-jacents et constituent de fait des métadonnées informatives pour les usagers des sites. Après avoir présenté notre démarche méthodologique appliquée à un corpus de sites web d’universités, nous proposerons un dispositif semi-collaboratif en ligne d’enrichissement de notre taxonomie et de génération dynamique de profils de sites web."
"L’objectif de notre étude ici est de caractériser le contenu d’un site web organisationnel par les marqueurs lexicaux associés aux liens hypertextes de la page d’accueil (unités lexicales hypertextuelles – ULH). Nous nous sommes intéressés à un ensemble de sites web issus d’un domaine organisationnel homogène (i.e le domaine universitaire français), à partir duquel nous avons généré deux corpus d’ULH relevé à deux moments différents (2009 et 2011). Ces deux corpus d’ULH ont été intégrés au sein d’une taxonomie produite à partir des niveaux hiérarchiques et catégoriels des différents modes de navigation. S’appuyant sur un premier niveau d’analyse quantitative, l’intérêt est de mettre à jour, à travers cette dynamique temporelle, les évolutions au niveau des pratiques du web tout en faisant la preuve d’une stabilité certaine des corpus d’ULH."
no abstract
no abstract
"Nous décrivons la dynamique temporelle et la représentativité d’un ensemble terminologique. Le vocabulaire est constitué à partir des unités lexicales utilisées pour la construction des menus de navigation des pages d’accueil de sites web d’une zone DNS d’établissements universitaires. Ainsi positionnées, les unités lexicales sont des descripteurs des contenus sous-jacents des sites. Nous montrons que malgré l’évolution très dynamique et tangible des contenus du web, certaines propriétés remarquables émergent quant à la stabilité voire la con-vergence des choix de ces unités lexicales par les webmasters du domaine."
"En supposant que l’impact anthropique des nuisances sonores occasionnées par les visiteurs d’un parc naturel pouvait être réduit par des actions de communication, nous avons élaboré en collaboration interdisciplinaire associant bioacoustique et communication, un protocole de mise en évidence de l’impact anthropique associé à un protocole de mesure d’efficacité des actions de communication. Une communication d'éco-sensibilisation simple sera comparée à une communication de sensibilisation engageante. Nous traitons les questions suivantes : est-ce que la sensibilisation des visiteurs les conduit à modifier leurs attitudes et comportements en vue de limiter leurs propres nuisances sonores ? Est-ce que l’engagement améliore la sensibilisation ? Nous détaillons ici les principaux concepts convoqués et la méthodologie suivie afin de présenter les premiers résultats."
"Avec une décennie d'existence timide en Algérie, l'intelligence économique est une discipline peu connue des académiciens et trop négligées des praticiens. D'abord, il y a un véritable problème de formation en IE. En effet, le peu de formations existantes en Algérie sont inspirées des modèles anglo-saxons ou européens (en particulier le modèle français), et sont donc en inadéquation totale avec les besoins réels de l'entreprise algérienne. Les personnes acquièrent des formations théoriques, loin de l'action concrète sur le terrain, d'où le risque de véhiculer une mauvaise image de l'Intelligence Économique. Ensuite, le peu d'entreprises Algériennes qui ont adopté l'IE comme outil de développement et de compétitivité, souvent influencées par les écoles occidentales de l'IE, se concentrent davantage sur des aspects de sécurité de l'information ou du lobbying que sur l'innovation. Pourtant, c'est l'innovation qui devrait être le levier par excellence du développement de la compétitivité et de la croissance économique en Algérie. Pour cela, il est important de "" troquer "" le paradigme actuel contre un nouveau dont les clés sont : - Utilisation "" légale et gratuite "" de l'information stratégique contenue dans les bases de données Brevet. En effet, le brevet permet le développement des entreprises mais aussi des Etats en voie de développement. - L'adoption des "" soft technologies "" et du "" jugaad innovation "" comme modèles de développement économique à l'instar des pays du BRICS et particulièrement, le Brésil et l'Inde. Cette communication tentera de développer ce paradigme alternatif comme plaidoyer d'une IE "" utile "" au développement des entreprises algériennes et de croissance économique de l'Algérie."
no abstract
"Le capitalisme cognitif a deux caractéristiques principales, l'intelligence collective et l'utilisation intensive des technologies de l'information, fondées sur la numérisation du contenu, des procédures et des écritures. Dans cet article, nous essayons d'examiner le mouvement Open Data face à la transformation des intelligences collectives et analysons ce processus dans le cadre de l'action publique, de la science, de l'intelligence logicielle, de la stratégie. Nous montrons comment ce mouvement s'articule avec la question du web sémantique, des ontologies, avec la montée de l'algorithmique. Dans ce cadre, l'émergence du "" data mining"" se présente comme ""récit impérial"", comme le récit des sociétés performatives. S'ouvre également la possibilité de nouveaux modes de gouvernance, l'émergence de nouvelles façons de penser le politique et l'espace public."
"A l’heure d’une société marquée par les technologies de l’information, les territoires locaux sont soumis à des formes de développement de plus en plus dictées et encadrées par le haut, basées sur les paradigmes de l’économie mondialisée et du développement durable. La question est alors de savoir comment ces territoires peuvent promouvoir un développement endogène qui fasse sens pour les acteurs locaux. Pour ce faire, nous inscrivons notre recherche dans les sciences de l’information et de la communication en mobilisant le concept d’intelligence territoriale. Nous apportons notre contribution à cette approche en approfondissant le rôle que peut jouer la dimension géographique dans les dispositifs socio-techniques d’information et de communication (Distic) à l’échelon des territoires intercommunaux. Nous adoptons pour cela une posture de recherche-intervention qui alterne des réflexions théoriques avec des observations et interventions empiriques sur une étude de cas, celle du territoire de Thau. Les représentations spatiales, comme produits médiatiques constitutifs des Distic, offrent une diversité de propriétés sémiotiques et de fonctionnalités que nous mettons en évidence. Nous utilisons aussi le métamodèle de Schwarz à un niveau macro pour rendre compte de la complexification et de l’autonomisation progressive du territoire de Thau sur les vingt dernières années."
"Les frontières du profane et du sacré dans les stratégies publicitaires (discours et images) sont revisitées à partir d’exemples concrets empruntés à la communication publicitaire, qu’elle soit laïque ou religieuse. L'article met en lumière la distinction entre « communication par la foi », de « la communication de la foi » et s'interroge sur le passage entre communication sacrée et profane. De nouvelles formes de médiations être l'individuel et le social s'installent qui émergent comme figure du sensible dans la communication."
"Notre recherche se propose de décrire comment se construit un processus de pré-intégration de nouvelles villes dans une structure mutualisée de moyens, et de conseil, numériques. Nous ferons la description de ce processus depuis la phase de réflexion par les directions du SITIV et des villes nouvelles jusqu'à la phase de préparation à l’intégration administrative. Ainsi nous présenterons comment s’est construit une réflexion, un processus administratif, une analyse technique ainsi qu’une estimation budgétaire pour conduire cette pré-intégration. Dès lors, l’objectif de notre recherche, réalisée dans le cadre d’une recherche intervention, et son questionnement reposent sur : - L’observation et l’analyse d’un avant-projet, qui se déroule sous la forme d’un continuum d’étape, permettant une trouver une démarche d’intégration qui semble, a priori, raisonnable pour les directions et pour les collectivités locales concernées. Nous ferons la description du programme observé pour tenter d’en tirer une méthode de gestion. C’est-à-dire une méthode décrivant le processus de construction d’une démarche préalable à l’intégration pour mutualiser les infrastructures numériques. - La description de nos observations va nous permettre d’élaborer un outil de gestion, c'est-à-dire une méthode de construction d’un avant-projet d’intégration de ville dans un syndicat spécialisé dans l’informatique. Afin de résoudre cette problématique de pré-intégration, nous avons conduit une analyse longitudinale (B. Forgues et I. Vandangeon-Derumez ,2003) d’un processus. Notre approche emprunte une exploration hybride (S. Charrereire et F. Durieux, 2003), avec une problématique de traduction (J. Angot et P. Milano, 2003)) entre des concepts théoriques et des données empiriques. Ce travail correspond donc à une recherche à visée descriptive. Dès lors, nous allons montrer, comment se construit un système de gestion pour démarrer une intégration de collectivités pour mutualiser les besoins numériques de villes, et proposer un modèle de gestion d’un avant-projet. C’est-à-dire « en concevant avec les acteurs d’une organisation des outils de gestion nouveaux » (David A., 2000). Le recueil de données empiriques ont été réalisés lors de trois étapes distinctes : « mutualiser pour réduire les coûts du numérique dans l’administration d’une ville et profiter de ce travail en commun pour étudier comment faire évoluer les ressources numériques des villes potentiellement entrantes » ; « mobiliser les décideurs, les élus des villes et les décisionnaires du syndicat » ; « lancer une démarche d’études de faisabilités de cette demande d’intégration» ; L’analyse longitudinale nous permet d’étudier comment se forme l’avant-projet, mais elle nous fait aussi apparaître les différentes séquences qui prépare la transformation organisationnelle du syndicat et des villes potentiellement entrantes dans le dispositif. Pour faire apparaître ce processus de gestion, nous allons décomposer notre propos de recherche en trois sections distinctes, représentant les trois phases principales du processus. • Dans une première section, nous exposerons les contraintes qui semblent s’exercer sur les villes mais aussi la nécessité pour elles d’évoluer. Il s’agit probablement d’une nouvelle ère qui se dessine pour elles. Dans cette assertion, les collectivités locales vont vraisemblablement devoir développer plus de services pour les citoyens en dépensant moins (à budget constant ou voir réduit), d’où une recherche d’efficience dans les processus internes. • Dans une section deux, nous ferons apparaître le processus de communication utilisé pour que les décideurs comprennent les enjeux et autorisent le lancement de la démarche. • Enfin, dans la section trois, nous étudierons comment se sont effectués les analyses techniques, administratives et financières pour les collectivités."
"Cet article se propose de mettre en écho la pratique vécue au cours des six dernières années écoulées nourries, outre la poursuite de la direction de thèses de doctorat sur le thème de l’Intelligence Territoriale, par l’exercice de deux mandats locaux, l’un d’Adjoint au maire, le second, de Conseiller communautaire avec le capital théorisé sur l’Intelligence Territoriale. La réflexion scientifique initiale s’en est trouvée, enrichie, discutée, éclairée, contrariée même si, l’expérience étant trop récente, la distance sera, sans nul doute nécessaire, pour extraire des éléments complémentaires de réflexion dans le futur. Nous illustrerons cette confrontation par des situations de recherche doctorales qui présenteront des situations territoriales hybrides."
"Constitutifs de la vie même des organisations, les cycles et les chocs qui les accompagnent, peuvent dans certaines circonstances leur être fatal si elles ne font pas preuve de résilience pour les absorber. Les organisations se trouvent actuellement confronté à deux défis majeurs, une contraction économique depuis 2008, une transition technologique qui bouscule les modèles de gestion existants et chahute leur éco-système. Dès lors il s’agit bien d’un changement de paradigme majeur dans la gestion des organisations qui doivent tout en explorant de nouveaux terrains d’activité, préserver leurs ressources. L’organisation doit veiller à laisser s’exprimer l’agilité des éléments du système qui la composent et celle de ses collaborateurs pour leur permettre de répondre au défi de l’ambidextrie organisationnel face à l’incertitude et l’imprévisibilité qui caractérisent les environnements économiques contemporains. Pour lui permettre d’accéder à cette agilité, à développer la propriété de résilience et préserver ses capacités, l’information critique doit pouvoir être sécurisée et délivrée à temps via des systèmes d’information évolutifs sauf à être condamnée à ne pas pouvoir maintenir l’équilibre de son écosystème."
"Notre recherche se propose de décrire comment se construit un processus de pré-intégration de nouvelles villes dans une structure mutualisée de moyens, et de conseil, numériques. Nous ferons la description de ce processus depuis la phase de réflexion par les directions du SITIV et des villes nouvelles jusqu'à la phase de préparation à l'intégration administrative. Ainsi nous présenterons comment s'est construit une réflexion, un processus administratif, une analyse technique ainsi qu'une estimation budgétaire pour conduire cette pré-intégration. Les recueils de données empiriques ont été réalisés lors de trois étapes distinctes : "" mutualiser pour réduire les coûts du numérique dans l'administration d'une ville et profiter de ce travail en commun pour étudier comment faire évoluer les ressources numériques des villes potentiellement entrantes "" ; "" mobiliser les décideurs, les élus des villes et les décisionnaires du syndicat "" ; "" lancer une démarche d'études de faisabilités de cette demande d'intégration"" ; L'analyse longitudinale nous permet d'étudier comment se forme l'avant-projet, mais elle nous fait aussi apparaître les différentes séquences qui prépare la transformation organisationnelle du syndicat et des villes potentiellement entrantes dans le dispositif."
"Nos observations sur les territoires de la région urbaine de Lyon semblent montrer que les dangers peuvent prendre différentes formes, passant des aléas naturels tels que des inondations aux catastrophes nucléaires sans oublier les émeutes urbaines. Cette multitude de dangers, dans une société civile matérialiste qui réclame une chasse aux coupables à travers les médiats, sollicite une approche scientifique des périls. L'étude des sciences des dangers fait apparaître que les aléas doivent être abordés suivants plusieurs angles car l'analyse des techniques n'explique pas tout, il semble nécessaire d'incorporer les sciences humaines pour évaluer les phénomènes comportementaux. Les sciences de l'information et de la communication vont ainsi fournir une approche complémentaire à l'étude des dangers. L'Intelligence Territoriale, inscrite dans le domaine des sciences de l'information et de la communication, porte en elle la capacité à mobiliser les ressources d'un territoire pour transmettre de l'information. Elle devient ainsi une ressource disponible pour l'analyse des dangers."
"Depuis quelques années déjà des questionnaires sont mis à disposition « en ligne » pour guider une forme d’auto-évaluation. Quels sont les fondements de ces outils ? Pourquoi les porteurs de projets choisissent-ils/elles de diffuser leurs travaux de cette manière ? Comment les utilisateurs réagissent-ils/elles à ces propositions ? Comment aller au-delà de questionnaires pour susciter le partage de pratiques, la mise en projet, la conception de son propre environnement ou dispositif d’apprentissage ?"
"Le Musée Océanographique de Monaco, construit à partir de 1899, nous fournit un bel exemple du rôle et de la fonction des musées d'histoire naturelle. Au XIXe les musées étaient le lieu de la science reine : l'histoire naturelle, qui plaçait l'homme au centre de l'univers dans la perspective de son exploration et de sa domination. L'une de leurs principales fonctions était de servir à la communication des scientifiques entre eux. Aujourd'hui, les musées d'histoire naturelle pourraient avoir pour fonction de servir à la communication de la science avec le public. Car s'il est indispensable que les sociétés modernes laissent à la science la liberté et l'initiative nécessaires à son développement ; il est aussi indispensable qu'elles inventent des lieux de médiation où se développe un sens critique à l'égard de ses applications concrètes et de ses conséquences sur la vie des hommes."
"The diversion of language and the industrialization of the sacred: the example of ""Kabbalah"" movement « Publier les oeuvres maîtresses de l'ancienne littérature cabalistique est la meilleure garantie de son secret » (Gershom Scholem, Dix propositions anhistoriques sur la cabale in Biale, 2001, p.253) « On peut garder le nom quand la chose a été secrètement changée » (Guy-Ernest Debord, 1992, p.42) Résumé : Dans le dévoiement de la tradition mystique de la Kabbale hébraïque traditionnelle par le mouvement new age appelé « La Kabbale », le détournement du langage joue un rôle central. L'utilisation d'un même terme pour désigner des réalités différentes, voire opposées, en est la caractéristique principale. Cette manipulation de concepts-fondée sur quelques procédés récurrents comme la simplification extrême, la mise hors contexte (culturel, religieux, historique…), la réification de processus dynamiques-nous semble être la marque d'une industrialisation du champ de la communication religieuse."
"Partant des usages et pratiques des outils informatiques et médias numériques dans la prise en charge institutionnelle du handicap mental en France, cet article interroge l’émergence d’une éducommunication, en situant ce courant et en développant ses applications dans et pour des formes de stimulation cognitive. Il interroge aussi la place de ces technologies dans les développements organisationnels des établissements accueillant ces publics, notamment en termes de participation et d’évaluation des pratiques professionnelles. In fine, l’articulation entre l’éducommunication (approche théorico-pratique), la stimulation cognitive comme méthode et l’évaluation à visées réflexive et créative ouvre des perspectives pour l’amélioration de la prise en charge des personnes handicapées mentales."
"Nous traitons, dans cet article, de différents aspects des recherches et des développements dans la matière du croisement entre le biologique et l'artificiel. La recréation de l'espace du vivant passe aussi bien par les tentatives de simulation de l'intelligence supérieure que par les essais de la sensorialité artificielle. L'art expérimente ces domaines pour en étoffer la philosophie pratique et morale. Depuis la pensée du calcul, au dix-neuvième, jusqu'à la cybernétique se tissent les idées qui aboutissent aujourd'hui à des réalisations technologiques, mais l'art, au lieu de faire apparaître des continuités, y découvre et exhibe des redoutables choix politiques."
"Avec Internet, la rumeur semble avoir trouvé de nouvelles voies, de nouvelles formes, et de nouvelles cibles, ainsi que des conséquences liées à la mondialisation des réseaux d'information. Le présent article tentera de faire le point sur ce phénomène dans le domaine particulier de l'attaque d'image et la déstabilisation des entreprises. Summary : Is the rumor a reliable weapon which can be launched via Internet to strike a competitor? The aim of the article is to answer this question by defining the rumor and analyzing it in a context of hard competitive relationship between companies. « Les hoaxbusters ont constaté que régulièrement des personnes ou des sociétés étaient mises en cause nominativement dans les hoax. Compte-tenu de la rapidité de la diffusion de l'information via Internet, un effet d'amplification est souvent constaté. Basé sur de fausses allégations, l'image des personnes ou des sociétés se détériore très rapidement. Les effets de cette désinformation peuvent être catastrophiques et avoir des répercussions sur la vie privée des personnes citées et sur l'image de marque des entreprises mises en cause.... » 1 Telle serait aujourd'hui la problématique d'un phénomène qui n'est pourtant pas nouveau, la rumeur, mais qui via Internet semble avoir trouvé de nouvelles voies, de nouvelles formes, et de nouvelles cibles, ainsi que des conséquences liées à la mondialisation des réseaux d'information. Le présent article tentera de faire le point sur ce phénomène dans le domaine particulier de l'attaque d'image et la déstabilisation des entreprises."
"La sécurité en entreprise est généralement abordée selon des approches spécialisées, où la sécurité des systèmes d'information apparaît comme une composante majeure dédiée aux informaticiens. Toutefois, la problématique de sécurité est complexe et mérite selon nous d'être envisagée de manière systémique et globale, approche que nous qualifions d'holistique. Il s'agit en effet de traiter un ensemble de problèmes plus ou moins interactifs dont la résolution passe par une mise en synergie des compétences. L'enjeu est avant tout culturel, il procède d'un changement de paradigme : la sécurité n'est plus un objet finalisé réservé au spécialiste de l'entreprise, elle doit être envisagée dans une optique globale, permanente et universelle par l'ensemble du personnel. De ce point de vue, l'intelligence économique, par son approche transversale et collective, se présente comme une opportunité pour la mise en oeuvre d'un concept de sécurité globale au sein de l'entreprise."
"La sécurité en entreprise est généralement abordée selon des approches spécialisées, où la sécurité des systèmes d’information apparaît comme une composante majeure dédiée aux informaticiens. Toutefois, la problématique de sécurité est complexe et mérite selon nous d’être envisagée de manière systémique et globale, approche que nous qualifions d’holistique. Il s’agit en effet de traiter un ensemble de problèmes plus ou moins interactifs dont la résolution passe par une mise en synergie des compétences. L’enjeu est avant tout culturel, il procède d’un changement de paradigme : la sécurité n’est plus un objet finalisé réservé au spécialiste de l’entreprise, elle doit être envisagée dans une optique globale, permanente et universelle par l’ensemble du personnel. De ce point de vue, l’intelligence économique, par son approche transversale et collective, se présente comme une opportunité pour la mise en oeuvre d’un concept de sécurité globale au sein de l’entreprise."
"L'étude cherche à vérifier l'existence du pluralisme des informations sur l'internet en analysant les différentes manières de traiter un sujet d'actualité : ici, un sondage donnant M. Le Pen en tête au premier tour des élections présidentielles de 2012. L'article montre comment les sites d'information produisent des énonciations différenciées en fonction leurs cadres de production (professionnels ou amateurs) mais aussi d'héritages culturels établissant de véritables normes d'écriture (factuelle ou de commentaire). Au-delà de ce clivage, apparaissent des zones de porosités, au sein desquelles les sites investissent le registre de l'indignation pour occuper le terrain médiatique sur l'internet."
"La prise en considération des utilisateurs dans les dispositifs d’innovation technologique s’inscrit dans un mouvement de réappropriation de la technique par les usagers, animé par les pratiques de collaboration en réseau. L’industrie s’efforce de contrôler et d’exploiter le processus, tandis que les collectivités publiques, en quête d’un idéal “du citoyen actif ” fédèrent différentes expériences énoncées comme innovantes. La commande publique de recherche suit cette évolution sociétale. S’instaure ainsi au niveau européen, une pratique de financement rejaillissant sur les territoires décentralisés, qui encourage la coopération de la recherche et de l’industrie. Notre terrain d’étude exploite deux projets financés (Région/Europe) dans le cadre des Pacalab’s : Ecofamilies et OpeNRJ ; deux dispositifs d’innovation technologique et sociétale : la co-conception d’une interface web visant le suivi des consommations énergétiques domestiques et la génération de services et d’usages liés à l’ouverture de données de consommation énergétique de bâtiments publics et privés. Notre méthodologie déploie une ethnographie des usagers, qui interroge et vise une modélisation des compétences (mobilisables et à acquérir) des différents types d’usagers, au sein de dispositifs socio-techniques : citoyens, designers et ingénieurs ou groupes d’usagers experts, aux compétences numériques, culturelles et cognitives variées, en situation de collaboration. Si nous inscrivons notre étude dans une histoire pluridisciplinaire des usages (de la sociologie des usages à une approche anthropologique de l’usager), nous souhaitons réinvestir plus précisément la question de l’usage en son contexte développée par J. Davallon et J. Le Marec (2000), qui offre une place à l’usage dans les pratiques culturelles et envisage « une mise en culture de l’utilisation de l’objet » (Ibid., 2000 ; 176). En quel sens les injonctions participatives au design d’interface et à la co-conception d’une solution technologique ou à la génération de nouveaux services basés sur l’ouverture de données gagnent-elles à être inscrites dans nos pratiques culturelles ? Quelle place est accordée à l’usager dans ces dispositifs d’innovation technique et sociale ? En quel sens la mise en avant des pratiques culturelles des usagers permet-elle une approche critique des stratégies industrielles et territoriales d’engagement, voire d’aliénation des usagers ? Nous visons enfin l’analyse des reconfigurations des pratiques des chercheurs que ces projets collaboratifs génèrent : les attentes non plus seulement en matière d’expertise, mais de médiation, voire d’animation d’une communauté cognitive en devenir et dans une perspective plus générale, la question de l’engagement social du chercheur."
"Issu d’un projet de Recherche et Développement financé par la Région PACA, le service ecoBalade s’inscrit dans une logique de médiation institutionnelle. L’organisme territorial - ici la Communauté d’Agglomération Toulon Provence Méditerranée partenaire du projet - s’engage dans une action de valorisation de son territoire en offrant à ses usagers le service ecoBalade et une plateforme web collaborative dont l’objectif premier est de favoriser une dynamique participative et éco citoyenne créant de la conversation sur la biodiversité locale. Pack technologique conçu autour d’une plateforme web, d’un guide nature numérique et de la présence d’un accompagnateur naturaliste, l’ecoBalade entend être un service vecteur d’apprentissage, de transmissions et de découverte de la faune et la flore communes et remarquables d’un territoire donné. A l’aide d’un outil d’observations naturalistes ou de son smartphone, l’usager utilisateur a la possibilité d’identifier une espèce, de marquer une observation et de partager ses découvertes au sein d’une communauté d’utilisateurs. Toute nouvelle application entraîne des effets pour la plupart mesurables d’une façon assez habituelle et attendue. Pour autant, les retours des usagers sur ce service nous ont amené à identifier d’autres fonctions que la « seule » gestion des connaissances de la biodiversité locale. En effet, les transmissions intergénérationnelles des connaissances technologiques, culinaires, patrimoniales et naturelles ont été soulevées par les ecobaladeurs. Les intermédiations et la co-construction collective de l’appareillage ecoBalade suppose que le projet est d’abord une expérimentation de création et ensuite une innovation d’usage. Mais en quoi l’écobaladeur est-il un outil de transmissions intergénérationnelle et patrimoniale ? L’observation portera d’abord sur l’évaluation des contraintes, leur gestion et assimilation par les écobaladeurs. Nous aurons recours à des entretiens et des questionnaires fermés et ouverts sur le nouvel usage de cette nouvelle technologie. Nous interrogerons les sujets du groupe témoins sur la fonctionnalité et l’ouverture du service lui-même."
"L’accroissement important du nombre et de la taille des organisations englobent des cultures de plus en plus éloignées les unes des autres. Cependant, la gestion de ces organisations pose des problèmes tout à fait spécifiques. En effet, nombre d’entreprises constituent des cadres culturels où se confrontent des altérités identitaires. Dans cette dynamique, les tensions et conflits interculturels ne sont pas absents et cela instaure une autre mission pour le management : créer une confiance mutuelle face aux différences culturelles. On pourrait définir le management interculturel comme la recherche de cohérence entre le style de management d’une entreprise, sa culture et son environnement. L’intérêt d’un tel management est de gérer la diversité au profit de l’organisation. Tout manager qui ne tiendrait pas compte des cultures de son personnel et du pays où il opère, risque l’échec. La recherche que nous avons entreprise, conduit à penser qu’il faut saisir l’organisation comme un système culturel, où le symbolique et l’imaginaire occupent une place essentielle. Dès lors, les cultures propres à chaque acteur, interviennent dans le construit social de l’entreprise. Autrement dit, que le monde managérial a tout intérêt à admettre qu’une organisation est liée à un système de pratiques individuelles ritualisées. Ces pratiques seront d’autant mieux intériorisées qu’elles seront acceptées par les acteurs eux-mêmes. Notre interrogation porte sur la problématique de l’interculture d’entreprise dans les organisations en situation de fusion et, plus spécifiquement, des outils de la communication interne comme lien de confiance des pratiques managériales et communicationnelles aux différences culturelles des organisations. Cette étude s’appuie sur une analyse lexico-grammaticale d’un journal interne dont le but est de fédérer les salariés autour d’une réussite économique et sociale faisant état des différences comme une force et non comme une faiblesse organisationnelle. Parmi les nombreux outils attestant de ces rencontres souvent difficiles, le journal interne est le media privilégié de la mémoire collective, du savoir-être et de l’intégration mutuelle."
"Bien que médiatique, l’intelligence économique reste une dynamique difficile à saisir. Son impossible définition est consubstantielle à son caractère syncrétique. S’appuyant, en France, sur une culture du renseignement mal comprise, elle inquiète autant qu’elle rassure. Pourtant jugée indispensable, elle a été élevée au rang de politique publique et se développe, lentement mais sûrement, au sein des entreprises. Dynamique à la recherche de ses concepts opératoires, l’intelligence économique s’appuie sur des méthodologies issues du terrain qui demandent une assise théorique forte. À travers le couple agilité/paralysie, l’intelligence doit être analysée comme un avantage relatif. Pour y être opératoire, le concept d’information doit être compris et utilisé dans un cadre plus systémique qui le relie à l’action via la connaissance : c’est le passage du « savoir pour agir » au « connaître est agir ». En éveil permanent et tourné vers l’analyse, un dispositif d’intelligence économique mettra alors idéalement en œuvre une communauté stratégique de connaissance dans laquelle la communication apparaît comme centrale."
"Discussion autour de la définition du ""document numérique"
"La réflexion que nous menons part du constat suivant : les organisations sportives peinent à afficher des références aux valeurs olympiques et des communautés virtuelles en débattent et s’y substituent. En effet, les spectateurs du monde sportif par le biais des réseaux sociaux véhiculent ces valeurs que l’on devrait (re)trouver au cœur des interactions numériques entre pratiquants et institutions sportives. Nos propos sont structurés comme suit : après avoir rappelé les fondements de l’olympisme, nous tenterons de montrer que les valeurs associées à l’olympisme – censées représenter « la colonne vertébrale », non seulement de l’organisation olympique, mais de toute institution sportive – peuvent être considérées, en première analyse, comme les facettes de l’archétype d’une organisation utopienne. À la suite, après avoir exposé quelques faits qui témoignent de la controverse évoquée, nous présenterons en seconde analyse, celle de certains sites institutionnels en lien avec le comité international olympique couplée aux résultats d’une enquête par questionnaire menée auprès d’une communauté développée sur Facebook autour d’une page intitulée « Promotion des valeurs olympiques » composée de personnes ayant une expérience professionnelle dans le domaine sportif. En conclusion, nous reviendrons sur le besoin de « bonne utopie » (Morin, 2015) que, sans doute, les organisations comme les institutions (i.e. celles et ceux qui les représentent) peinent de plus en plus à traduire et qui ne se retrouvent plus, tout à fait, dans les grands discours ou dans une ambition à penser l’universel ouvrant in fine comme le suggère Michel Maffesoli (2015) sur une « normopathie »."
"Nous discuterons dans cet article de la définition de l'humain dans ses rapports actuels entretenus avec les environnements numériques, la nature et le traitement des données. Nos propos seront illustrés par les oeuvres des artistes numériques tels Ryoji Ikeda et son travail sur la visualisation des données ou celui du compositeur Zbigniew Karkowski sur la sonorisation des données. Une définition de l'homme-interfacé sera proposée. Elle montrera sa relation à un invisible devenu visible, à un inaudible devenu audible. Nous poursuivrons ensuite avec des exemples d'humanisation du non-humain. Une discussion théorique autour de L'homme-interfacé conclura notre propos."
"L'œil n'est pas simplement un instrument optique. Il participe comme source de la libido et tente de satisfaire la pulsion scopique. Le spectateur d'un film traditionnel est dans un désir de voir toujours insatisfait. L'apparition d'un curseur à l'écran invite le spectateur à participer à l'action de même que le regard d'un acteur peut le provoquer. Cette interpellation provoque un regard vers le spectateur qui de regardant devient regardé. La pulsion scopique se boucle. Le spectateur se fait regarder et devient spect-acteur quand il passe à l'action. Ce désir d'agir pour le spect-acteur est lié à l'incomplétude du symbolique des propositions faites au spectateur. Le film interactif n'est plus un objet fermé mais il est ouvert, troué. Ce vide, représenté par des mots, associé à un regard, provoque une réaction quand le spectateur accepte de jouer le jeu. L'apparition du curseur dévoile l'instance d'énonciation dans le film, dénonce le voyeurisme du spectateur et met en communication l'espace de production et l'espace de réception du film. Le sujet, n'étant plus là en tant que spectateur, ne s'identifie plus ni à l'événement ni à son récit. Il n'habite plus un espace symbolique libéré de toute contrainte. Il n'occupe plus une instance abstraite placée en un point aveugle de l'image. Il rejoint ainsi le hic et le nunc de la réalité. Il n'est plus, dès lors, suspendu au récit mais il est présent en face d'une image contenant un élément qui révèle et simule la présence d'un autre."
"L'apparition récente des mondes ou univers virtuels dans l'enseignement à distance soulève de nombreuses questions culturelles. En effet, ces environnements virtuels d'apprentissage particuliers permettent l'apparition du corps et de la gestualité par l'intermédiaire de la caractérisation d'un personnage ou avatar 3D comme projection possible de l'utilisateur. Nous proposons d'étudier ici cette phase de caractérisation et ses conséquences sur la construction d'identité de groupe par le jeu des recompositions identitaires individuelles et collectives. Nous faisons l'hypothèse que ces nouvelles modalités participent de l'apparition de nouvelles situations communicationnelles dans lesquelles chaque apprenant peut affirmer sa culture tout en s'intégrant dans son groupe. Nous proposons pour cela de nous appuyer sur une expérimentation dans le dispositif Second Life."
"Au moment où vivant et artificiel convergent, les relations humaines prétendent se simplifier. L’ Autre ne serait qu’une donnée. Pourtant, le problème reste entier : la communication bute toujours sur les difficultés de la rencontre car l’ altérité est constitutive de la communication et de son horizon. Au début est l’ incommunication. Altérités et singularités humaines ouvrent des espaces de négociation, de cohabitation et d’ invention pour chacun : Cogitat ergo est. C’ est dans la rupture et le discontinu que s’ inscrivent l’ émergence du nouveau et, souvent, la communication. Aujourd’hui, la vision occidentale qui connecte corps et dispositifs en réseaux vise donc à établir une continuité entre cerveau et esprit, tandis que la science biologique produit de nouveaux artefacts. L’ invisible « épaisseur organique » se transforme alors en « surfaces numériques » objectivables. Un être informationnel apparaît, prétendant tout savoir, tout dire et prédire. De pulsionnel et érotique, le corps devient objet et information, résolvant d’ un coup l’ épreuve et l’ expérience de la communication. La question traitée ici est celle de l’altérité à l’épreuve d’un être informationnel. Ce numéro d’Hermès ouvre de nouvelles perspectives pour les sciences de la communication. L’ originalité est de mobiliser et de confronter des disciplines aux fondements parfois incommensurables : sciences cognitives, neurosciences, psychanalyse, sciences humaines et sociales. Loin d’ une communication transparente, sans butée et sans Autre, la rencontre de ces disciplines montre une nouvelle fois que la communication humaine est sans mode d’ emploi. Le corps, la relation à l’ Autre manifestent obstinément des points de résistance. La communication s’invente toujours, au-delà des systèmes, des réseaux, des interactions."
"Lire un hypertexte signifie saisir un objet textuel, visuel et interactif très complexe. Cette lecture non linéaire sera abordée par une double approche : expérimentale et herméneutique. Il s’agit de chercher les traces du processus de signification pour le lecteur à travers des données quantitatives issues du mouvement de ses yeux pour reconstituer et comprendre le schéma de la lecture hypertextuelle. Cette expérience menée à l’aide d’un dispositif oculométrique nous permet aussi d’indiquer où l’attention d’un lecteur se pose. Pourra-t-elle toutefois retracer la conformité ou bien la typologie de nos réactions et d’interactions à l’écran ? Nous soulignons, contrairement aux chercheurs en ergonomie du web, que le parcours de l’œil à l’écran ne peut pas correspondre au parcours interprétatif. Quelles sont les limites d’un tel dispositif quand il se trouve confronté à la question de la construction du sens ? L’hypertexte au prisme de l’objectivation prend alors la figure du labyrinthe dont l’ouverture ne peut être donnée que par une approche herméneutique, interprétative."
"Dès le XIXe siècle, l’individu est l’objet de nombreuses études, dans les champs des sciences sociales, humaines et biologiques. Si l’individu est souvent mis en relation avec le social, le politique ou le vivant, il intéresse peu les sciences de l’information et de la communication. En ce XXIe siècle, au moment où l’humain établit ses relations au monde à partir d’informations personnelles ou publiques, nous soutenons que l’individu communiquant peut se définir à partir de références psychologiques et psychiques, sociologiques, biologiques ancrées dans le XXe siècle, afin d’éviter sa réduction en un fournisseur d’informations pour ferme de données."
"En juin 1999, la déclaration de Bologne engageait un processus global d’harmonisation de l’enseignement supérieur dans l’Union européenne. Après avoir touché le monde universitaire, cette normalisation à grande échelle s’applique à présent aux formations supérieures du domaine de la création et des activités artistiques. À l’aide d’exemples précis, les textes ici réunis montrent les effets de ce processus sur ces cursus, particulièrement à travers la question centrale d’une définition « académique » de la recherche, inconnue jusqu’alors dans les domaines des formations artistiques (arts plastiques, design, danse, musique, théâtre, cinéma). Les témoignages et analyses font ressortir les difficultés d’une stricte indexation de la recherche en matière de création sur les modèles qui dominent l’espace épistémologique de l’enseignement supérieur. Des dispositifs de recherche inédits se révèlent alors et affirment leurs différences. De ces réflexions émergent les éléments critiques d’une résistance active à un impératif d’uniformisation considéré comme négatif pour l’identité même de ces lieux de transmission. Cela ouvre sur les questions, fondamentales dans ces formations, de pluridisciplinarité et d’interdisciplinarité. Critiquer les procédures de standardisation, proposer de nouvelles formes collectives de recherche en art(s), avec ou sans l’Université, faire l’effort de la clarification des concepts, c’est aussi préserver des espaces de négociation et d’altérité au coeur d’un modèle de communication."
"Cette contribution s’attache à relever les enjeux inhérents à l’enseignement du design d’interaction d’un produit multimédia interactif. Le dispositif conceptuel du designer est abordé pour évoquer la dualité du processus de communication spécifique à ce produit : d’une part, la stratégie du concepteur s’appuie sur le champ culturel et empirique de l’utilisateur de manière à ce que le message soit compris, et, d’autre part, elle stimule des réactions émotionnelles spontanées. Nous montrons ce double jeu articulant la conformité à l’innovation au moyen d’un exemple, et son enseignement auprès d’étudiants concepteurs en multimédia."
"Depuis trois ans, l'UFR Ingémédia (Université du Sud Toulon Var) a développé un partenariat pédagogique avec l'ensemble de musique contemporaine Polychronies 1. L'objectif de ce partenariat est la conception et la réalisation d'une pièce musicale jouée en concert public. Dans ce cadre, le compositeur Jean Michel Bossini a créé, au fil de ces trois ans, un triptyque : la série Dodécalite (« Dodécalite intérieur», « Dodécalite extérieur», « Dodécalite carré »). L'objet de cet article est de retracer cette expérience pédagogique originale en soulignant les questions qu'elle pose : rapport compositeur/étudiants dans un contexte de réalisation collective, rapport à la musique contemporaine pour un public de non spécialistes, collaboration entre étudiants de spécialités différentes (technologie du son, musique, théâtre), place de la technologie dans ce dialogue entre spécialités, l'intermédialité, le renouvellement de l'oeuvre ouverte…"
"Dynamiques de citoyenneté, d’éco-citoyenneté, de réseaux, de médias, de territoires, de culture et d’interculturalité. Pratiques journalistiques, innovations numériques, nouvelles formes de savoir, mouvements sociaux et divulgation des sciences... Autant de problématiques et de pratiques que l’approche communicationnelle relient. En croisant les regards théoriques et empiriques, cet ouvrage offre une contribution à la compréhension et à l’analyse des logiques d’actions et de construction des savoirs dans un espace méditerranéen en pleine mutation. Car, depuis près de trois décennies, par vagues successives, les relations scientifiques entre chercheurs en communication du nord et du sud de la Méditerranée ne cessent de s’intensifier. Il semble émerger sinon une identité méditerranéenne contemporaine, du moins des dynamiques et mouvements qui renvoient à un patrimoine commun issu d’une l’histoire millénaire et qui se traduit aujourd’hui dans les paradoxes d’une unité plurielle. Cet ouvrage apporte aussi la preuve que le champ de la communication favorise le dépassement des frontières institutionnelles et académiques, et nourrit une réflexion commune entre chercheurs de l’Université et du CNRS."
"Depuis sa démocratisation au cours de la seconde moitié du XXème, le tourisme n’a cessé de se développer en recouvrant la quasi-majorité des pays du monde. Il est ainsi devenu une industrie économique majeure et une activité sociale prospère à la portée du plus grand nombre. Tantôt perçu comme une activité bénéfique pour les pays récepteurs, tantôt considéré comme un danger potentiel pour la population hôte, le tourisme ne se présente jamais comme un phénomène anodin. Il transforme en effet, les sociétés d’accueil de manière tant positive que négative, en occasionnant de multiples échanges interculturels entre les touristes et les habitants locaux. Cela provoque parfois un choc culturel, pour la population hôte, engendrant des phénomènes d’acculturation, ou inversement, une prise de conscience de ses caractéristiques identitaires.En quoi le tourisme et les relations interculturelles qu’il induit, peuvent-ils atténuer et/ou renforcer l’identité culturelle ? Cette problématique structure notre thèse de doctorat, et l’étude que nous avons menée au sein de la médina de Fès. Cette dernière, cité millénaire, classée au patrimoine mondial de l‘UNESCO depuis 1981, représente un espace traditionnel et authentique confronté à des impératifs de sauvegarde et de valorisation du bâti autant que des modes de vie et de travail séculaires. Nous avons effectué une enquête qualitative combinant l’usage de deux outils méthodologiques de recueil de données : observations directes et entretiens semi-directifs. Cette recherche nous a permis d’étudier l’impact du tourisme sur l’identité culturelle fassie en tenant compte des paramètres liés au patrimoine."
"L'accélération des échanges entre les peuples et les cultures, s'inscrit comme l'une des évolutions les plus caractéristiques de ces dernières décennies (fin du XXème et début XXième siècle). les contacts multiples, tant au niveau international qu'à l'intérieur des pays, se déploient dans l'incessante circulation des produits et des hommes, et de façon plus spectaculaire, celle de l'information, de la connaissance et de la communication par l'avènement d'internet. le tourisme voit se transformer son secteur et des etouristes choisissent, visitent, achètent leur voyage sur les sites officiels (cf. étude ARTESI, 2005) car ils leur font confiance.les normes du tourismeS'il est admis qu'internet a installé des processus de normalisation dans tous les domaines de la société, économique, technologique, socio-culturel ou bien même politique , le touristique connaît une "" normalisation "" d'un genre particulier. les échanges touristiques auraient pu être considérés comme une évolution interculturelle (cf. Demorgon, 2005), car ils tendent logiquement à l'homogénéisation du monde. la normalisation des techniques de l'information et de la communication a fait évoluer nos sociétés vers l'accès potentiel illimité aux mêmes informations, si ce n'est à la même connaissance, sauf à considérer justement, les différences d'accès entre les pays du nord et ceux du Sud. or les visites touristiques, se construisent dans ce sens la plus part du temps. Dès lors, internet n'assurerait pas dans le domaine du tourisme une médiation interculturelle (Wolton, 2007). ainsi, la normalisation (iSo), dans le domaine de l'informatique et de l'électronique n'a régulé et homogénéisé que les démarches touristiques qui s'en trouvent règlementées et s'ajustent à la normalisation généralisées des échanges commerciaux internationaux. l'activité touristique, régie par les normes imposées dans tous les pays du monde comme les formalités de sécurité douanière, en passant par les normes de loisirs aux normes d'infrastructures d'hébergement et de restauration, se représente de nos jours comme une activité particulièrement organisée, balisée et sécurisée, sachant que de nouvelles normes se mettent en place tous les ans et que de multiples projets de normalisations sont soumis régulièrement. Pourtant si ces normes administratives, conjuguées à celles des images "" marketées "" propulsées sur les sites touristiques des pays à visiter dynamisent la diversité culturelle, elles ne favorisent-pas nécessairement une rencontre interculturelle.Une problématique d'un tourisme interculturel.la standardisation des normes touristiques internationales se retrouve sur internet, premier portail de visite virtuelle des pays récepteurs de tourisme international, et s'infiltre dans le processus d'exposition des sites à visiter par les touristes. or la fracture entre les pays du nord, occidentalisés et ceux du Sud, en développement, se fait déjà sentir dans ce secteur touristique. les déplacements, facilités par l'écart entre le niveau de vie occidentale et celui des pays du Sud et notamment celui du Maghreb, déclenche une forte attraction touristique des premiers pour les seconds. Si le tourisme représente une activité génératrice de rencontres pluriculturelles, la standardisation des expositions touristiques sur internet, mettrait en péril le fonctionnement d'une communication interculturelle évolutive et efficace en termes de rencontres. Ainsi, le "" choc culturel "" (Camilleri, Cohen-emerique, 2006) qui génère la confrontation inhérente à l'activité touristique, peut être d'autant plus brutal et accru, en raison de la tension due à la fabrication et la diffusion d'images construites orientée vers l'attente des touristes occidentaux et questionne sur une rencontre "" faussée "", potentiellement décevante pour les touristes et inquiétante pour les locaux.Une méthodologie fonctionnaliste.Afin de poursuivre une étude fonctionnaliste des sites touristiques de la ville de Fès (alemanno, Charai, 2011) où nous montrions qu'ils allaient dans le sens d'une économique touristique aux dépens d'une préservation des particularismes culturels, nous avons réalisé des entretiens avec les guides touristiques de la ville de Fès pour mesurer l'écart existant entre les représentations que ceux font les touristes de la population et des lieux à visiter et la réalité de ces derniers. Quand les sites proposent des images maîtrisées par des responsables marocains, des lieux à visiter, ils construisent des représentations mirifiques et rassurantes, atten dues mais idéalisées susceptibles de gommer la diversité culturelle et l'unicité de chaque culture. or la démarche fonctionnaliste ne permet d'appréhender les représentations et leur mobilisation. C'est pourquoi nous avons opté pour des entretiens qui révèlent combien la norme socio-touristique prévaut. les communications ainsi écrites indiqueraient "" en creux "" une ratée du processus de normalisation touristique. la rencontre interculturelle engendre de plus, une dimension "" info-éthique "", une "" ratée "" dans la préservation relative à une éducation interculturelle et se redéfinie à l'aune des "" déformations "" mutuelles et des reconstructions-assimilations dans son accomplissement.Cette étude mettra en évidence deux voies de réflexions : sur la façon dont les normes administratives peuvent régir les comportements et sur leur potentiel d'émergence d'autres processus de normalisation anthropologiques. les normes de déplacements touristiques relatives à la sécurité et au confort des touristes pourraient s'arrêter au seuil de la rencontre "" de différences "", laissée à la curiosité aléatoire des deux parties porteuses de l'échange interculturel.Conclusion sur une dialogique de l'interculturel.internet et son potentiel d'informations soumises à des normes notamment touristiques, nous oblige à conduire une réflexion sur l'info-éthique, fondé sur le respect de la diversité (Kiyindou, 2009), autre processus de normalisation supérieur à la norme économique. Cette recherche s'inscrit bien dans le courant de la prise en compte de la préservation des différences culturelles qui place le respect et la culture, de la diversité et de la complexité au cœur de l'interculturel. Celui-ci se fonderait à partir de la dimension praxéologique de la notion de rencontre et celle de l'altération (ardoino et alii, 2001). ainsi, nous allons dans le sens du point de vue de l'iSeSCo et du discours de a. othoman alywaijri (2008) qui promeut le dialogue entre les pays du Sud et ceux du nord et plus généralement des civilisations musulmanes et arabes, d'un après colonialisme douloureux et un occident imprégné de néocapitalisme. notre recherche tend à contribuer à faire face au défi, désormais en cours de laisser émerger les spécificités culturelles et civilisation nelles, et non pas un modèle unique dont l'occidentalisme ne renverrait in fine qu'à la reproduction d'un même, mortifère pour les identités de tous."
"Tour à tour cinéphile, critique de cinéma, cinéaste, vidéaste, auteur d’une œuvre, elle-même objet d’une cinéphilie contemporaine, Jean-Luc Godard entretient avec la culture cinématographique, et sa propre pratique du cinéma, une relation constamment conflictuelle et transgressive. Le cinéaste non seulement trouve son inspiration dans une résurgence de la cinéphilie de la fin des années 1940, mais épuise, travaille et digère également cette matière originelle, pour nourrir un cinéma dont il annonce aujourd’hui la fin, tout en interrogeant la possibilité de sa renaissance. Le métadiscours funèbre que Godard porte sur un certain cinéma, dans une œuvre en quête de rédemption, découle d’une cinéphilie vécue sur le mode de la dévoration et de la réception solitaire, plus particulièrement depuis ces vingt dernières années. Godard semble se détourner volontairement de la philia cinématographique que supposait le rassemblement autour d’un jugement commun, pour lui préférer une énonciation décalée, nostalgique et sépulcrale, renonçant à l’idée même de transmission et incarnant une forme singulière d'outre-cinéphilie. Le cinéaste-cinéphile de la Nouvelle Vague se confondrait ainsi désormais avec la figure paradoxale d’une certaine post-cinéphilie."
"Depuis que les pays d'Europe Centrale sont entrés dans la mouvance libérale, les consommations de biens et de services de loisir s'y développent en entraînés par un mouvement d'émancipation tutélaire et idéologique des organisations sportives. Relayé par force de volontés politiques qui cherchent à l'élargir de sa seule vocation élitiste et des logiques compétitives omniprésentes lors de la mouvance communiste, on s'attend à ce que le sport investisse le temps de loisir des Roumains d'autant que les instances gouvernantes cherchent à en développer les pratiques de masse. Cependant, l'activité sportive à vocation récréative et de loisir stagne en terme de pratique, à contrario du secteur de la compétition où les athlètes Roumains occupent toujours de hauts niveaux de performance sur la scène internationale. Bien que l'on observe l'émergence de structures professionnelles qui offrent une réelle qualité de services, la pratique des activités physiques de loisir ne semble pas évoluer au même rythme que le reste de la consommation du pays. Cette situation qui se pose en paradoxe pourrait être liée à plusieurs décennies de mouvance communiste."
"Lorsque qu'une organisation décide de communiquer sur sa culture d'entreprise, elle l'appréhende généralement à partir des principales références méthodologiques en provenance d'outre-atlantique qui en donnent une définition à dominante fonctionnelle. L'avantage principal est d'en permettre une formalisation qui intègre les outils et la stratégie de communication de l'entreprise. Mais c'est dans la mise à l'épreuve sur le terrain qu'une divergence apparaît entre cette approche fonctionnelle de la culture d'entreprise et la réalité sociale observée au sein des organisations. L'exploration des processus identitaires, au sein des organisations françaises, nous apporte des élèments divergents concernant la culture d'entreprise. Elle semble relever des identités des acteurs et s'éloigner de la logique fonctionnelle issue du concept anglo-saxon. Ce constat nous engage à vérifier, par étude de cas, que le concept est en échec. Les invariants, qui émergent de l'étude de six grandes firmes françaises, valident l'inéfficience du concept de culture d'entreprise. Le terrain diverge du modèle anglo-saxon fondé sur la fonctionnalité des activités professionnelles. Les enseignements que l'on en retire pourraient intéresser les organisations françaises qui cherchent à se communiquer en valorisant leurs particularités. Concevoir une communication, essentiellement à partir d'une rationalité professionnelle, qui ne reflète que partiellement le réel sociologique de l'organisation, risque de banaliser l'image des métiers avec des conséquences négatives à moyen terme sur le climat social et le marché. Ce mode de communication gagnerait en efficacité si, tout en cherchant à valoriser une identité collective, basée sur le socle commun du métier de l'entreprise, elle y intégrait l'expression des différences qu'elle regroupe."
"Alors qu'une croissance de la consommation sportive s'observe en France, le développement de son offre dans le tourisme tarde à se structurer. Ce paradoxe entraîne une interrogation de nature communicationnelle sur les causes de cette problématique. L'identité des professionnels du sport y occupe une place centrale. Leur construction identitaire repose sur les valeurs intrinsèques que véhicule la représentation sociale du sport. Face à la matérialité marchande du tourisme, les professionnels du sport entrent dans un processus de résistance culturelle. Ce phénomène incite à explorer la littérature managériale pour rechercher comment les organisations font face à ces résistances. Toute rationalisation de l'activité professionnelle engendre en retour un sentiment de déqualification et de manque de reconnaissance sociale de la part des employés. Ce sentiment est d'autant plus accentué que les organisations utilisent un modèle instrumental pour gérer leur culture d'entreprise. Pour certains métiers comme ceux du sport, les acteurs s'identifient d'avantage dans la représentation sociale de leur métier que dans sa matérialité. Six étude de cas d'entreprise valide cette approche. Lorsque les organisations transforment leur matérialité professionnelle, elles l'accompagnent d'une communication valorisant le métier des acteurs. Cette récurrence témoigne de l'émergence d'une quête identitaire. Vingt entretiens affine la spécificité de cette construction dans le tourisme sportif et en permet une modélisation.: Les acteurs, confrontés à une matérialité qu'il réfutent, entrent dans un processus de résistance. Ce processus s'affirme dans l'authenticité d'une communication qu'ils construisent avec leurs clients. Cette construction identitaire maille la matérialité professionnelle de l'organisation de tourisme sportif et les valeurs intrinsèques que véhiculent les acteurs du sport. Ces particularités se remarquent et s'opposent à la banalisation de l'offre en maintenant une représentation sociale du sport que les consommateurs attendent"
"L’objet du présent article consiste en une réflexion théorique sur l’appropriation technique et technologique d’un certain nombre de fonctionnalités représentatives des différentes sphères du web. Au travers du concept de Distic2, il s’agit de procéder à une analyse de la situation d’usage de ce contenu fonctionnel. Ce jeu des usages sera ainsi analysé, d’une part,au moyen d’une typologie du comportement des internautes dans leur stratégie de recherche d’information et création de contenu, d’autre part, grâce aux nouvelles compétences à disposition des collectivités territoriales pour s’approprier tant techniquement que technologiquement une panoplie d’outils caractéristiques des évolutions du web 2.0."
"La problématique environnementale est caractérisée par la complexité : complexité des conceptions sur l'environnement et la nature, complexité des champs disciplinaires concernés, complexité des enjeux... qui impliquent une multitude d'approches de cette problématique. Ces approches de l'environnement vont alors orienter différentes formes de médiation muséale des problèmes environnementaux. Par exemple, certains musées s'inscrivent dans une approche naturaliste de l'environnement, alors que d'autres l'abordent par une approche plus scientifique qui considère l'environnement comme une affaire de spécialistes. Enfin, une nouvelle tendance des musées qui est celle de mettre la science en débat apparaît aujourd'hui et permet d'aborder l'environnement comme un domaine ouvert à tous, où chacun est amené à participer au débat : les musées tendent ainsi à devenir des espaces publics. Nous nous proposons alors de réfléchir sur ce que peut être concrètement cette nouvelle forme de médiation au sein d'une exposition, au travers des collections."
"Notre réflexion part du constat d'une forte attente chez le consommateur d'une dimension éthique au sein de l'activité de l'entreprise. En effet, selon l'observatoire sur les valeurs éthiques des entreprises (OVE) le développement durable va devenir incontournable dans la stratégie des entreprises et leur communication sur les consommateurs. L'enquête prouve qu'une entreprise doit prendre position vis à vis du grand public sur l'éthique mais qu'il est difficile d'avoir à la fois une responsabilité « sociale » et « commerciale » notamment pour les entreprises privées dans l'esprit du consommateur français. En effet il faut préciser qu'en général la communication « responsable » de l'entreprise est dissociée de la communication de marque dans l'esprit du consommateur. La communication responsable doit toucher la conscience civile de la personne et porte sur des éléments d'information (performances, conduite, stratégie, culture, gouvernance, éthique, valeur). La communication de marque vise à séduire, convaincre et susciter le désir d'achat. Elle porte sur des éléments de valorisation, de différenciation concernant l'imaginaire et doit développer des notions de valeur ajoutée émotionnelle, de statut, d'innovation... Il n'est pas toujours facile de concilier les deux éléments mais cela est nécessaire car dans de nombreux cas la marque et l'entreprise se confondent (Axa, La Mondiale, Air France, ...). L'entreprise se retrouve dans une situation étrange ou la communication doit rester responsable même si elle a affaire à des citoyens qui ne seront pas toujours des consommateurs responsables tout en intégrant ses impératifs en terme de compétitivité par rapport à une pression concurrentielle sans cesse accrue. Il paraît donc délicat pour celle-ci de cerner le juste territoire d'une communication responsable."
"Les recherches sur les stratégies de communication commerciale développées par les grandes enseignes montrent combien les points de vente peuvent être les supports d'une rhétorique finement ordonnancée, fondée sur le plaisir d'achat, la représentation plus ou moins idéalisée de la vie quotidienne ainsi que sur une théâtralisation qui transforme ces lieux d'achat en une aire de spectacle permanent ou les consommateurs sont en fait des « consommacteurs ». Les magasins vont être structurés de façon à influencer les clients par l'utilisation d'orientations narratives, de métaphores et de récits assimilés à mythes populaires. Dans le même temps, afin, à la fois de mieux développer des relations de partenariat avec les enseignes mais aussi de se différencier des marques de distributeurs ; les grandes marques accentuent leur valeur ajoutée émotionnelle en jouant sur leur histoire, leurs mythes et la part de rêve qu'elles peuvent apporter au consommateur et ce, afin d'être pour ce dernier un référant voire même un catalyseur de ses valeurs dans leur secteur d'activité. Pour finir, il est également important d'aborder la question éthique de ces pratiques de ré enchantement du consommateur par une communication expérientielle et sensorielle sur le point de vente ainsi que par la « mythification » des grandes marques. Ce dernier peut en effet être plus facilement manipulé, influencé dans ces choix et induit en erreur par ces pratiques communicationnelles intégrées dans la stratégie marketing des fabricants et des distributeurs"
"We can observe that the Open Data phenomenon is becoming increasingly important within public and private organizations. In this context a release strategy data will cause an expansion and a greater permeability digital enterprise boundary. These open data will indeed create more interactions with stakeholders who could capture and evolve the structure and function of these companies. In the first part, we present the outline of the Open Data and its impact on the business world. We then illustrate our presentation and the first results of the research project OPENRJ (PACA Labs device) which aims to build a federation of organizations who provide free and freely energy consumption in real-time to their buildings."
"Les réseaux numériques, entraînent une évolution radicale des relations entre les individus et les organisations par la modification du processus d'échange et de leurs comportements . Dans ce contexte, les communautés virtuelles sont devenues un phénomène incontournable et les organisations doivent tenir compte de la volonté de participation de l'individu au sein de leurs stratégies, volonté souvent relayée par les blogs et réseaux sociaux numériques . Notre recherche sera centrée sur les interactions entre les réseaux sociaux et les organisations sportives au niveau de la promotion des valeurs de l'olympisme à savoir l'amitié, le respect et l'excellence (Premat ,2009 ). L'intérêt et la spécificité de cette recherche consiste dans le manque d'études qui portent sur les relations entre les individus et les organisations sportives via les réseaux sociaux et plus particulièrement sur la promotion des valeurs de l'olympisme. Par ce travail de recherche, nous espérons mieux cerner le positionnement et la perception du mouvement olympique par le biais de ses valeurs. Nous allons également pouvoir mettre en exergue l'impact de celles-ci sur les internautes par l'intermédiaire des réseaux sociaux. Par ailleurs, la question de la présence des valeurs olympiques dans le domaine sportif semble être des plus pertinentes aujourd'hui. En effet, les organisations sportives doivent nécessairement prendre en considération les réseaux numériques pour renforcer leur notoriété, leur réputation et surtout pour rappeler et sensibiliser les individus au niveau de ces valeurs universelles. Au plan méthodologique, après l'analyse du contenu et de l'architecture de sites institutionnels comme celui du Comité International Olympique et de certains comités olympiques nationaux, nous avons opté de combiner deux méthodes : la netnographie (Bernard, 2004 ) ainsi que entretiens semi-directifs en ligne. Notre étude portant sur les réseaux sociaux, nous avons créé sur Facebook une page intitulée "" Promotion des valeurs olympiques "" qui nous permettra d'appliquer les techniques qualitatives citées précédemment. La population visée par cette recherche est composée de personnes ayant une expérience dans le domaine sportif (champions du monde, olympique et nationaux, membres des comités nationaux, des professeurs de sport et d'éducation physique, etc.). Il est bien évident que l'importance et la récence de ce type d'étude au niveau des organisations sportives devra nécessiter par la suite une démarche plus approfondie et étendue, relayée par des études quantitatives sur des publics aux profils plus diversifiés."
"A l'heure actuelle nous assistons à la mise en relation et en réseaux d'outils et de technologies qui jusqu'alors développaient de façon isolées. Dans ce contexte, il semble pertinent d'étudier les perspectives d'évolution de la télévision mobile interactive La problématique centrale qui sous tend ce projet d'article se traduit par les interrogations suivantes: Peut-on identifier des communautés d'utilisateurs de ce nouveau dispositif télévisuel? Quels usages vont-ils développer? S'agit-il d'une simple corrélation de services complémentaires ou l'émergence d'une nouvelle forme de communication via le mobile? Pour tenter de répondre à ces questions; nous présenterons tout d'abord, les caractéristiques principales du marché de la télévision mobile notamment interactive à partir d'une veille informationnelle afin de pouvoir esquisser les grandes lignes d'un cadre conceptuel de sa consommation. Nous utiliserons ensuite une méthodologie qualitative exploratoire par le biais d'une trentaine d'entretiens individuels semi-directifs assortis d'une présentation de cette forme de télévision mobile. Il s'agira donc, à partir de ces entretiens d'établir une typologie des différentes pratiques informationnelles ainsi qu'une classification des usages pour ce nouveau mode d'utilisation du portable."
"Avec 5% du PIB et 8,1% de l'emploi mondial (235.758.000 emplois en 2010), 9,2% de l'investissement international total (1 200 milliards de dollars) et 3,8% des dépenses publiques mondiales (436 milliards de dollars), le tourisme est le premier secteur d'activité au monde. En 2010, le nombre d'arrivées de touristes internationaux a été de 1 milliard, et devrait approcher les 1,6 milliards en 2020 (Conseil national du Tourisme 2010 ). Selon l'OMT, les trois premières destinations touristiques sont restées la France (avec près de 80 millions de visiteurs étrangers, soit une progression de plus de 3% par rapport à 2010), les Etats-Unis et l'Espagne tant en termes d'arrivées de touristes internationaux que de recettes du tourisme international. Toutefois, ces données statistiques cachent une grande diversité dans les expressions que peut revêtir le tourisme. A ce niveau certaines activités de ce secteur à l'instar du tourisme responsable ou de patrimoine sont en forte croissance. Nous nous intéresserons ici au tourisme de patrimoine industriel qui peut se définir par la mise en tourisme des sites de production qui ne sont plus en activité (C. PIERRE 2005 ). Le patrimoine industriel intègre les usines mais aussi les machines de production, les objets ou produits fabriqués ou manufacturés qui ont été produits par ces sites. Au plan immatériel il faut également prendre en compte les savoir faire qui ont été créés et transmis en ces lieux, et qui ont évolué au fil du temps. L'histoire sociale et donc le bâti social (les cités ouvrières et les habitats patronaux) font partie de ce patrimoine et sont une composante du tourisme industriel. En termes de vecteurs de cette offre touristique nous pouvons distinguer : - Les musées qui recèlent des collections relatives à une (des) activité(s) industrielle(s) ou encore musées de collection. - Les sites industriels (et notamment les usines) considérés comme des musées de site - Les sites d'interprétation du patrimoine industriel dans lesquels l'ensemble des lieux d'un territoire défini constitue les éléments de ce site. Après avoir présenté un état de la situation du tourisme industriel en France, nous tenterons de montrer, par le biais d'exemples de lieux de tourisme industriel significatifs sur le plan national ainsi que d'une étude récente effectuée sur plusieurs régions françaises que le tourisme industriel peut contribuer à la préservation et à la valorisation d'un patrimoine. Pour finir, à la vue de ces observations et résultats, nous essayerons d'identifier les facteurs explicatifs de la valorisation d'un patrimoine industriel par le biais du tourisme en soulignant les avantages et limites de ces derniers."
"La contribution des SIC aux débats publics peut s’exprimer par sa capacité à identifier et aider au décryptage de tendances émergentes au plan sociétal mais également organisationnel et technologique. Il paraît de ce fait important de s’interroger sur la capacité des SIC à observer, expliquer, amplifier et diffuser de nouveaux dispositifs d’échanges sociotechniques associés à des enjeux sociétaux forts. Dans ce contexte, il semble opportun d’évaluer dans quelle mesure les SIC permettent de susciter, de mieux comprendre et s’approprier les problématiques communicationnelles et éducatives associées au développement durable."
"Nous pouvons observer que le phénomène d'Open Data prend de plus en plus d'ampleur au sein des organisations publiques et privées. Dans ce contexte une stratégie de libération des données va provoquer une extension et une plus grande perméabilité des frontières numériques des entreprises. Ces données ouvertes vont en effet créer plus d'interactions avec les parties prenantes qui vont s'approprier et faire évoluer la structure et les fonctions de ces entreprises. Dans une première partie, nous présenterons les grandes lignes de l'Open Data et ses conséquences sur le monde entrepreneurial. Nous illustrerons ensuite nos propos par la présentation et les premiers résultats du projet de recherche OPENRJ (Dispositif PACA Labs) qui vise à construire une fédération d'organisations qui mettent à disposition gratuitement et librement les consommations énergétiques en temps-réel de leurs bâtiments."
"ABSTRACT PROBLEMATIQUE A) CONTEXTE HISTORIQUE La publication de Stewart MACCAULEY en 1963 traitant des relations non contractuelles des entreprises a permit le développement de l'analyse de la confiance dans les relations commerciales interentreprises et à l'étude formelle des formes de gouvernances pour la théorie des contrats comme pour l'économie (GALANTER 1981, MAC NEIL 1980, WILLIAMSON 1975). Vers la fin des années 1970 (ARNDT 1979) ; certains chercheurs en marketing se sont orientés vers l'étude de la gouvernance hors marché aboutissant à la prise en compte des échanges relationnels interentreprises. Ces derniers impliquent un accord mutuel entre les parties dans l'attente d'une collaboration future de laquelle « ces parties peuvent retirer une satisfaction complexe, personnelle et non économique » (dic-cit IYER, SHARMA et BEJOU 2005). Ces recherches permettent de développer le concept de marketing relationnel (MORGAN et HUNT 1994) par la théorie de l'engagement- confiance. Par la prise en compte de cette optique relationnelle, le marketing évolue d'une vision centrée sur les biens (prédominance de la matérialité et des transactions continues) à une vision tournée vers les services qui est caractérisée par l'immatérialité, l'organisation des échanges et des relations. B) SITUATION PRESENTE A l'heure actuelle, l'évolution de l'environnement économique et concurrentiel des entreprises renforce l'importance du marketing relationnel pour optimiser leurs relations avec leurs partenaires professionnels ainsi que leurs clients. Nous pouvons citer comme exemple. Le fort développement d'Internet qui a resserré les liens entre les consommateurs et les marques par le biais notamment du marketing interactif. L'importance grandissante de la gestion de la relation commerciale (GRC) dans un environnement turbulent et dans lequel la pression concurrentielle induit de multiples rapprochements interentreprises. Témoin, de nombreuses sociétés de conseils ont développé des processus et des logiciels en GRC soulignant son importance stratégique et ce, quelque soit la taille de l'entreprise. La diffusion du « Supply Chain Management » ou gestion de la chaîne logistique, basé sur les principes du marketing relationnel et constituant un nouveau champ de bataille concurrentiel. L' apparition et la croissance de clans d'audiences sur le Web (DEBOS 2004) par les consommateurs toujours plus exigeants, demandeurs de biens et services « sur mesure » et d'un véritable dialogue interactif avec l'entreprise ou avec d'autres clients et ce, indépendamment du champ de contrôle de cette dernière."
"En terme de communication persuasive, les responsables Marketing ont jusqu'à présent eu tendance à opposer les techniques promotionnelles aux outils de fidélisation de la clientèle. Or, la mise en place des NIP (Carte cagnotte, Lot virtuel, Offre fédérative...) développés par les enseignes de distribution en collaboration (plus ou moins forcée il est vrai) avec les fabricants ainsi que d'autres techniques promotionnelles spécifiques aux marques comme la promotion « Quasi Publicitaire », sont à la fois des outils d'accroissement des ventes à court terme mais aussi de fidélisation et de valorisation efficace du capital de marque des fabricants associés aux distributeurs. Les NIP doivent alors être réellement intégré dans une véritable réflexion et stratégie Marketing des fabricants et être associées à leurs propres actions promotionnelles"
"Le Développement durable (DD) peut se définir comme un processus qui concilie l'écologique, l'économique et le social et qui établit un cercle vertueux entre ces trois pôles : c'est un développement, économiquement efficace, socialement équitable et écologiquement soutenable. Il doit être respectueux des ressources naturelles et des écosystèmes tout en garantissant l'efficacité économique, sans perdre de vue les finalités sociales que sont la lutte contre la pauvreté, les inégalités, l'exclusion et la recherche de l'équité."
"Les recherches sur les stratégies de communication commerciale développées par les grandes enseignes montrent combien les points de vente peuvent être les supports d'une rhétorique finement ordonnancée, fondée sur le plaisir d'achat, la représentation plus ou moins idéalisée de la vie quotidienne ainsi que sur une théâtralisation qui transforme ces lieux d'achat en une aire de spectacle permanent ou les consommateurs sont en fait des « consommacteurs ». Les magasins vont être structurés de façon à influencer les clients par l'utilisation d'orientations narratives, de métaphores et de récits assimilés à mythes populaires. Dans le même temps, afin, à la fois de mieux développer des relations de partenariat avec les enseignes mais aussi de se différencier des marques de distributeurs ; les grandes marques accentuent leur valeur ajoutée émotionnelle en jouant sur leur histoire, leurs mythes et la part de rêve qu'elles peuvent apporter au consommateur et ce, afin d'être pour ce dernier un référant voire même un catalyseur de ses valeurs dans leur secteur d'activité. Pour finir, il est également important d'aborder la question éthique de ces pratiques de ré enchantement du consommateur par une communication expérientielle et sensorielle sur le point de vente ainsi que par la « mythification » des grandes marques. Ce dernier peut en effet être plus facilement manipulé, influencé dans ces choix et induit en erreur par ces pratiques communicationnelles intégrées dans la stratégie marketing des fabricants et des distributeurs."
"Les marques doivent intégrer la volonté de participation des internautes au sein de leurs stratégies, volonté souvent relayée par de multiples blogs et communautés virtuelles. De ce fait, que le site Internet doit être la pierre angulaire de la stratégie de développement du territoire virtuel de la marque. Il peut être assimilé à un véritable « Hub Marketing » capable de piloter au mieux la destinée des marques."
"Le Projet OpeNRJ a développé une plateforme de données ouvertes autour de l’énergétique des bâtiments. Elle engage une communauté d'organisations tertiaires à mettre à disposition de façon libre et ouverte les consommations énergétiques de leurs bâtiments. Ces données sont facilement téléchargeables et sont gratuites, accessibles sur www.openrj.eu. Ce document constitue le rapport final du projet : il précise le contexte (Chapitre 2) qui a conduit à la genèse du projet, rappelle ses objectifs (Chapitre 3), puis présente de manière synthétique les principaux résultats (Chapitre 4). Ce rapport propose également un résumé des autres livrables (Chapitre 5) produits dans le cadre du projet pour les lecteurs intéressés par des conclusions plus détaillées, un rapport de dissémination (Chapitre 6), ainsi qu’un bilan global (Chapitre 7), qui présente également les perspectives après la clôture du proj"
"Résumé: Internet crée une transformation en profondeur des relations entre les individus et les entreprises qui doivent tenir compte de la volonté de participation de l'individu au sein de leurs stratégies, volonté souvent relayée par de multiples blogs et communautés virtuelles. Il s'agit ici de montrer que ces communautés virtuelles sont à l'origine d'une nouvelle stratégie d'accroche communicationnelle, de conquête et d'optimisation de la relation client."
"L'objectif de ce papier est de tenter de démontrer que les différents acteurs de l'innovation dans les produits et services pourraient communiquer et interagir de façon structurée autour de pôles d'excellence dans le but de développer de véritable innovations de rupture en tenant compte à la fois des imaginaires des concepteurs mais également des consommateurs. A ce niveau, la technopole de Sophia-Antipolis peut elle devenir un modèle de référence au plan de la mise en place de ces nouveaux processus et dispositifs d'innovation?"
"L'objectif de ce papier est de tenter de démontrer que les différents acteurs de l'innovation dans les produits et services pourraient communiquer et interagir de façon structurée autour de pôles d'excellence dans le but de développer de véritable innovations de rupture en tenant compte à la fois des imaginaires des concepteurs mais également des consommateurs. A ce niveau, la technopole de Sophia-Antipolis peut elle devenir un modèle de référence au plan de la mise en place de ces nouveaux processus et dispositifs d'innovation?"
Le vin représente bien plus qu'une boisson. Indépendamment de son poids économique ; il est le symbole majeur de nombreuses civilisations et fait partie intégrante de notre histoire et notre culture. Le vin peut constituer également un thème porteur pour le tourisme dont l'originalité se décline à travers une certaine variété de produits et de concepts. L'objectif de cet article consiste à identifier des pistes de réflexion quand à l'élaboration de produits de tourisme viticoles véritablement attractifs pour le consommateur. A partir de l'étude de plusieurs initiatives locales réparties sur le territoire national ; il s'agira de démontrer l'importance d'un regroupement de moyens entre viticulteurs et institutionnels du tourisme afin de concevoir et développer une offre touristique vitivinicole.
"L'impact du numérique dans la stratégie marketing de l'entreprise doit lui permettre de mieux cerner les attentes et comportement des individus. Ce constat n'est toutefois valable que si la créativité et une véritable logique client en collaboration avec ses partenaires économiques sont au centre de sa réflexion stratégique. Par la mise en lien de la gestion de l'information, des TIC et du marketing, l'organisation de l'entreprise devra associer flexibilité, désintermédiation et réponse immédiate au consommateur."
"RESUME. A l'heure actuelle un besoin d'interactivité renaissant se manifeste chez les consommateurs. Il traduit des attentes fortes en terme de proximité relationnelle, de convivialité et d'identification mutuelle. Cette interactivité numérique modifie le modèle de représentation du monde et pose le problème du cadre d'interprétation de cette information. Les entreprises, notamment les marques ayant une forte notoriété et valeur ajoutée émotionnelle veulent devenir auprès des individus de véritables référents et guides à partir de leur propre univers sémantique. Cependant, maîtriser l'interactivité numérique et « accrocher » le consommateur implique l'intégration d'une certaine humanité, une présence émotionnelle et affective ainsi qu'une réelle assistance dans la vie quotidienne de l'individu. La communication commerciale de l'entreprise doit de ce fait évoluer vers un véritable dialogue direct avec le consommateur qui n'acceptera comme seuls messages que ceux qui concernent ses attentes, ses désirs, ses référents socioculturels."
"La communication est au cœur des stratégies de développement durable dans les entreprises. En terme de communication interne, il paraît important d'identifier des outils pouvant évaluer l'appropriation par les salariés cette notion mais également leur processus d'engagement à ce niveau. Le théâtre forum est une forme d'éducation active, participative,esthétique et réflexive particulièrement adaptée à la mise en œuvre de dynamique de changement ou de résolution individuelle et collective de problèmes. Non seulement il traite de sujets concernant directement les participants mais il les fait intervenir dans la création elle même et son déroulement. Ce projet propose d'évaluer la portée, les atouts et les limites du théâtre forum spécifiquement écocitoyen en tant qu'instrument d'appropriation de la notion de développement durable des salariés d'une organisation. La thématique portera sur le Plan de Déplacement de l'Entreprise (PDE)."
"L'instauration de politiques publiques favorisant l'élaboration d'une véritable société du savoir nécessite de s'interroger sur la capacité à innover, à créer des concepts et produire des idées. Lorsque l'on analyse les conditions du développement d'une société du savoir, nous identifions la croissance et le partage des innovations, mais aussi la nécessité d'une polarisation territoriale à l'instar d'une technopole organisée efficacement pour créer une synergie entre ces facteurs afin de renforcer la communication inter-organisationnelle. Le processus d'innovation doit être considéré comme une démarche complexe, collective et participative, intégrant l'ensemble des acteurs et parties prenantes internes et externes de l'entreprise sur du long terme. Au sein des technopoles comme celle de Sophia-Antipolis, les lieux d'innovation devraient correspondre à des réseaux d'acteurs en interaction et organisés autour de grands centres ou pôles d'excellence (ou encore de compétitivité) qui regroupent les entreprises, les structures publiques (université, organismes et collectivités publics, sociétés d'économie mixte, les laboratoires (publics et privés), le milieu associatif, les start up, ainsi que les écoles d'ingénieurs, de designers, de commerce et d'art. Ces lieux d'innovation doivent ainsi créer des synergies entre des compétences et des formations complémentaires, notamment par l'intégration des TIC et une qualité organisationnelle croissante de ces différents acteurs. Il convient de préciser que par dispositif d'innovation nous entendons un concept organisationnel et culturel permettant de structurer la démarche d'innovation et créer des synergies entre ses multiples acteurs. Une question demeure : « dans le cadre de la création d'une société du savoir, comment y parvenir, comment communiquer et interagir de façon structurée autour de pôles d'excellence, de compétitivité rassemblant universités, entreprises, laboratoires, start up, artistes, écoles de commerce et d'ingénieurs ? » l'objectif de cette proposition consiste à esquisser dans ce contexte des voies d'amélioration au fonctionnement d'une technopole en prenant le cas de Sophia-Antipolis. A partir du modèle de la roue de la technopole développé par Smilor, Gibson et Kozmetsky en 1988, notre recherche envisage de proposer un dispositif intégrant de nouveaux acteurs qui permettent de mieux appréhender le nouveau profil d'une technopole dans une société du savoir, c'est à dire centrée sur une véritable chaîne de l'innovation et qui doit sur un territoire rassembler tous les créateurs d'imaginaire. Dans ce contexte, le seul capital de connaissance d'une organisation (publique ou privée, entrepreneuriale ou associative) ne suffit pas, il lui faut coopérer avec de multiples acteurs. Cette dernière a également besoin d'outils et d'un territoire bien structuré pour mener à bien ces projets novateurs, pour identifier et compléter ses « missing links », comme la mise en place d'incubateurs et de véritables réseaux de « capital risqueurs », beaucoup plus développés dans les pays anglo-saxons (jusqu'à cent fois plus aux USA et déjà dix fois plus au Royaume Uni). C'est apparemment cette mise en lien qui à la lumière de premiers entretiens semi directifs reste problématique et qui doit être renforcée sur la technopole sophipolitaine ou certains acteurs clés considèrent que peu de choses ont changé depuis sa création. Dans une première partie, nous présenterons les formes d'innovation et de R&D mis en place par les organisations et qui semblent les plus adaptées à la création d'une société du savoir. La partie suivante s'attachera à définir un dispositif de diffusion et d'organisation de l'innovation à partir du modèle de la roue de la technopole et de sa nécessaire évolution. Enfin, nous tenterons de démontrer que le positionnement, la notoriété et surtout la philosophie fondatrice de la technopole de Sophia-Antipolis (la fertilisation croisée, l'agora des connaissances) peuvent la prédisposer à devenir un modèle de référence au plan de la mise en place de ces nouveaux processus et dispositifs d'innovation au sein d'une société des savoirs. Méthodologie : • Veille informationnelle au niveau de la presse économique locale, d'Internet et des supports de communication des principales associations de décideurs de la technopole ( Fondation Sophia-Antipolis, Telecom Valley, SAEM,etc.). • Entretiens semi directifs et Focus Group auprès d'acteurs de la technopole représentatifs du microcosme sophipolitain."
"Le nom « Sophia-Antipolis » est déposé à l'Institut National de la Propriété Industrielle. La marque « Sophia-Antipolis » est protégée (ainsi que « Sophia » et « Sophipolitain ») et défendue au même titre qu'une marque commerciale. Dans le même temps, nous pouvons constater que les grandes marques commerciales bénéficient d'une identité forte (à l'instar de Sophia-Antipolis) synonyme de constance et de stabilité. Elles accentuent leur valeur ajoutée émotionnelle en jouant sur leur histoire, leur mythe et la part de rêve qu'elles peuvent apporter aux consommateurs. L'objectif de cette recherche est de déterminer quelle valeur ajoutée le concept d'identité de marque peut apporter à la communication d'une technopole en prenant pour exemple le cas de Sophia-antipolis. Dans une première partie nous traiterons du contexte identitaire de la marque, notamment à travers les notions de prisme d'identité de la marque et de « Marque Mythique » qui correspond à l'ultime étape du cycle de vie d'une marque. La deuxième partie présentera l'application de ces concepts à la technopole sophipolitaine. Nous partirons des résultats d'entretiens semi directifs qui seront menés auprès d'une dizaine d'acteurs représentatif du microcosme sophipolitain L'analyse documentaire de la presse économique locale au niveau d'articles traitant de Sophia-Antipolis dans son ensemble ou de certains de ces acteurs viendra compléter cette approche."
"Ce travail aborde le luxe dans les sciences de l’information et de la communication au prisme des médias dans une approche interdisciplinaire en s’appuyant sur l’esthétique, la sémiotique, l’étude des identités visuelles, la muséographie et l’analyse de discours. En utilisant le concept d’artification mis en avant par Nathalie Heinich et Roberta Shapiro en 2012 pour décrire le passage à l’art de pratiques non artistiques, nous analysons les dispositifs sociotechniques d’information et de communication mis en œuvre pour tenter de transformer l’objet de luxe en objet d’art, et sa consommation en expérience esthétique. Nous interrogeons les discours médiatiques opérant sur cette mise en exposition pour vérifier l’opérativité du dispositif et analyser le mode de fonctionnement de la convergence des signes et du sens qui produit l’artification. Ce travail est une des réponses possibles à la question « Un dispositif peut-il transformer un sac à main en œuvre d’art ? »"
"Les stratégies de communication du luxe et d'expansion de son domaine ont conduit à un rapprochement croissante entre le luxe et l'art contemporain dans le discours médiatique créant un halo arty autour du luxe : fondations, design de l'environnement du produit, points de vente, musées, galeries toutes les représentations du luxe et de l'art sont réécrites et rechargées de signification."
"Notre recherche sera centrée sur les interactions entre les réseaux sociaux et les organisations sportives au niveau de la promotion des valeurs de l’olympisme à savoir l’amitié, le respect et l’excellence. L’intérêt et la spécificité de cette recherche consiste dans le manque d’études qui portent sur les relations entre les individus et les organisations sportives via les réseaux sociaux et plus particulièrement sur la promotion des valeurs de l’olympisme. Par ce travail de recherche, nous espérons mieux cerner le positionnement et la perception du mouvement olympique par le biais de ses valeurs. Au plan méthodologique, après l’analyse du contenu et de l’architecture de sites institutionnels comme celui du Comité International Olympique et de certains comités olympiques nationaux, nous avons opté de combiner deux méthodes : la Netnographie ainsi que des entretiens semi-directifs en ligne et un questionnaire à choix multiple qui ont été analysé par les logiciels Sphinx et Tropes. Notre étude portant sur les réseaux sociaux, nous avons créé sur Facebook une page intitulée « Promotion des valeurs olympiques » qui nous permettra d’appliquer les techniques qualitatives citées précédemment. La population visée par cette recherche est composée de personnes ayant une expérience dans le domaine sportif (champions du monde, olympique et nationaux, membres des comités nationaux, des professeurs de sport et d’éducation physique, etc.). Il est bien évident que l’importance et la récence de ce type d’étude au niveau des organisations sportives devra nécessiter par la suite une démarche plus approfondie et étendue, relayée par des études quantitatives sur des publics aux profils divers."
"A l'heure de la mondialisation, il est primordial d'acquérir des marchés étrangers et, par conséquent, d'entrer en contact avec d'autres cultures basées sur des valeurs fondamentalement différentes. Les entreprises se veulent multinationales ou transnationales, elles démultiplient les pratiques de fusions-acquisitions, de rachats, d'alliance ; mais pour être à la hauteur des attentes de performance, elles doivent prendre en compte dans leurs modes de gestion et dans les processus d'intégration, les différences culturelles : culture nationale ou régionale, de groupe, d'établissement, de métier, différences collectives auxquelles s'ajoutent les différences individuelles. Si l'on définit l'interculturalité comme la mise en contexte d'interactions, de réciprocités, de rapprochements, de communication et de compréhension mutuelle, il n'en demeure pas moins qu'à l'heure où les entreprises s'internationalisent, la dimension interactionnelle interculturelle est rarement prise en compte. Elle fait place à la dimension économico-financière et au schéma traditionnel de culture dominante/dominée. La communication interculturelle, pour sa part, vise à la création de nouvelles conditions de coopération fondées sur une connaissance et/ou une reconnaissance mutuelle et cette reconnaissance nécessite une implication particulière des deux parties. Le concept d'interculturalité serait-il l'élément prépondérant au succès des fusions-acquisitions d'entreprises ? C'est au moyen d'une analyse de contenu qualitative par catégorisation du journal interne d'une entreprise multinationale que nous tenterons de montrer l'évidence de la réussite d'une fusion-acquisition d'entreprise par le concept d'interculturalité."
"Beaucoup d'entreprises multinationales ou transnationales multiplient les pratiques de fusions-acquisitions, de rachats, d'alliance. Challenge économique ou impératif de survie, le processus s'étend désormais à des entreprises dont la culture familiale et l'enracinement local laissent craindre le pire pour l'identité, notamment de l'entreprise rachetée. Le changement obligé, celui inscrit dans le processus d'évolution « naturelle » s'il en est économiquement parlant, se trouve bousculé par ces phénomènes de fusion-acquisition d'entreprises, et provoque une radicalité dans le changement que les communications interne et externe relaient dans les média avec des formules rhétoriques plus ou moins crédibles. De fait on peut se demander comment la « culture d'entreprise » réagit ou résiste à ce changement profond ? Il semblerait que les discours accessibles au public sur ces nouvelles alliances aillent dans le sens d'une communication interculturelle qui vise à la création de nouvelles conditions de coopération fondées sur une connaissance et/ou une reconnaissance mutuelle, et cette reconnaissance nécessitant une implication particulière des deux parties. Les communications organisationnelles utilisent des outils et des dispositifs pour faire passer les messages managériaux, ceux-là même qui étayent la culture d'entreprise. Ces messages présentent de façon sous-jacente l'intentionnalité managériale sur le type de changement voulu à l'occasion de cette fusion-acquisition. Le schéma traditionnel socio-anthropologique de culture dominante/dominée nous présente des changements organisationnels sous forme d'absorption ou de métissage, or on peut se demander si au cœur des discours, des mots clés que nous offre les annonces des dirigeants, la dimension interculturelle de la communication plus assertive, ne serait pas le point de liaison, réel lien social « communicationnel », entre ces deux cultures ? Qu'il soit du quotidien ou ponctuel pour des « grandes manœuvres » décisionnelles, le changement organisationnel est sans cesse opérant dans les entreprises en mouvement. Ainsi l'exemple des entreprises souhaitant s'étendre, se voit évoluer selon différents styles. L'une de ses dimensions qui retiendra notre attention est l'alliance commerciale et/ou la fusion-acquisition. Cette dynamique de changement organisationnel met à contribution les services communications de toute entreprise souhaitant œuvrer en ce sens. Le but est certes, financier et économique à la condition que la dimension culturelle soit présente. Suivant une analyse de type sémiotique des journaux internes organisationnelles, la communication interculturelle, non pas au sens des différences de chacune des entités formant la culture de chaque entreprise, mais plutôt au sens de cohabitation d'au moins deux cultures d'entreprises, induit une façon de travailler ensemble avec des normes, des valeurs, des rites des mythes différents. L'uniformisation en une culture d'entreprise unique ne paraît pas être la solution. En effet, actuellement, on estime à plus de 65% le taux d'échec des fusions des entreprises. Ce taux anormalement élevé ne tient pas compte de la seule dimension économico-financière En effet, les différences culturelles de chacune des parties ne sont pas ou peu prises en compte, car difficilement identifiables, ce sont elles qui portent souvent préjudice aux fusions. Les Sciences de Gestion nous ont appris durant les années 70-80 qu'il « suffisait » pour rationaliser la culture d'entreprise d'en insuffler une toute nouvelle ou d'en contrôler tous ses aspects. A contrario, les Sciences Humaines et Sociales nous ont démontré qu'il n'était pas possible de contrôler ou rationaliser les éléments de la culture d'entreprise, mais elles nous ont donné les éléments prépondérants afin de définir cette dernière. Pourtant, nous sommes dans le cadre d'une instrumentalisation de la culture d'entreprise qui s'inscrit dans une approche interactionniste. Dans le prolongement de ce courant théorique nous posons cette notion de culture d'entreprise au cœur du lien entre interaction et émergence de significations partagées où nécessairement s'inscrivent les éventuelles crises de l'entreprise. Si les entreprises doivent faire face au changement constant, l'idée de la fusion ou d'une alliance stratégique reste une menace à part entière autant en interne qu'en externe. Nous posons l'hypothèse d'une nouvelle forme de communication organisationnelle hybride (encore minoritaire) ; phénomène émergent des processus de changement dans les multinationales soucieuses de leur succès. Nous montrerons qu'elle s'inscrit dans les discours des journaux internes des plus confidentiels au plus publics suivant une terminologie témoin d'une re-structuration originale dans l'action et le projet. Par l'approche méthodologique qualitative (analyse de contenu qualitative par catégorisation) des discours managériaux organisés par les services de communications interne et externe au moment d'une fusion de deux grandes entreprises du secteur des transports, nous tenterons de démontrer l'enjeu fondamental d'une forme de communication interculturelle, redéfinie dans le champs des Sciences de l'Information et de la Communication, dans un processus de changement organisationnel construit."
"L'article présente les ruptures et les convergences dans le processus de traitement de l'Information Scientifique et Technique (IST), consécutives à l'émergence de normes et de standards des documents numriques. Le champ d'application concerne l'activité des chercheurs en biologie moléculaire, plus particulièrement la génomique. Pour appréhender les changements dans le processus de traitement de l'IST, trois types d'objets sont à considérer : les objets biologiques (au sens d'entités physiques, de concepts et de leurs relations : une fonction physiologique, une régulation, un gène, une molécule, etc .), les langages et les outils informatiques, avec la bioinformatique, et pour finir, les documents numériques. Pour signifier les ruptures et les convergences dans le processus de traitement de l'IST, les objets indiqués sont analysés selon trois phases : (i) Une phase de standardisation en amont, relative à l'émergence de standards ou de normes créant une rupture du processus de traitement de l'IST préexistant. (ii) Une phase de standardisation liée au processus de traitement de l'IST, qui montre un traitement possible de documents numériques au regard d'une certaine convergence des standards de la précédente phase. (iii) Une phase de standardisation en aval, qui met en évidence de nouveaux usages et de nouveaux besoins de normalisation, orientés vers l'intégration des connaissances, les environnements coopératifs, avec la création de projets et de groupes ad hoc. . Nous concluons cet article en soulignant certaines des caractéristiques des changements opérés."
"Depuis quelques années, les grandes entreprises françaises prônent l'introduction dans leurs murs, de règles de normalisation de leurs pratiques. Elles visent ainsi l'amélioration de leurs performances commerciales passant par une meilleure communication. Aussi, voit-on fleurir des normes qualité mettant en œuvre des techniques d'organisation concourant à rendre conforme à un standard, la production de biens ou de services. Des protocoles écrits sont élaborés afin d'homogénéiser les pratiques des salariés au travers de normes. Ces processus, tout en étant de puissants outils de régulation et de normalisation internes, contribuent à la construction d'une image de sérieux de l'entreprise. Dans la même optique, les entreprises rédigent des chartes éthiques, documents déclarés comme étant la référence à des principes fondamentaux qu'elles s'engagent spontanément à respecter. Ces codes de bonnes conduites n'ont aucune valeur juridique mais sont présentés comme des guides fondés sur des principes généraux normalisés et médiatisés dans le but de construire une image respectable de l'entreprise. Ainsi, la Déclaration universelle des Droits de l'Homme et la Charte de l'Organisation internationale du Travail font partie des principales références du Code de Conduite de Total, et sur son site, ce Groupe affirme qu'il ne saurait accepter de travailler là où il ne se sentirait pas en mesure de respecter ses principes et de les faire respecter par ses sous-traitants. Si les chartes éthiques comme la démarche qualité agissent directement sur l'image de l'entreprise qui les met en œuvre, leur rapport à la norme en ce qu'elle se définit comme une règle, n'est pas identique. La démarche qualité formalise les pratiques dans un souci d'amélioration constante en mesurant les écarts entre la pratique "" normalisée "" et la pratique réelle. Les chartes éthiques définissent des règles morales à appliquer sans toujours se donner les moyens de vérifier leur application concrète. Dès lors, la fonction attribuée à la norme diffère. Dans le cours "" Sécurité, territoire, population"" donné au Collège de France , Foucault distingue entre discipline et sécurité à propos de la fonction de la norme. Il appelle la normalisation disciplinaire, normation, parce que ce qui est prioritaire dans ces procédures de normalisation n'est pas la définition du normal et de l'anormal, mais celle de la norme qui a un caractère "" primitivement prescriptif "". Elle désignerait l'imposition coercitive de dispositions durables à agir de façon déterminée. Ces concepts appliqués aux entreprises font émerger deux processus d'utilisation de la norme: la normalisation qui désignerait un processus permettant d'élaborer une norme à partir des usages et des meilleures pratiques, et la "" normation "" qui correspondrait selon nous, au processus permettant d'évaluer l'écart entre une situation et la norme qui lui est liée. Certaines entreprises allient normalisation et "" normation "". Ainsi, l'Oréal définit sa Charte éthique comme le document de référence qui aide les collaborateurs à intégrer l' "" Esprit l'Oréal "" dans leur travail au quotidien. Cette Charte éthique s'adresse à tous les salariés du Groupe l'Oréal et à ses filiales dans le monde. Chaque collaborateur en reçoit personnellement un exemplaire. Dans ce texte, l'Oréal s'engage au respect des droits fondamentaux et des Conventions Fondamentales de l'Organisation Internationale du Travail par ses fournisseurs. L'ensemble des fournisseurs de l'Oréal doit s'engager à respecter ces normes éthiques qui pourront faire l'objet de contrôles. En effet, des audits sociaux, effectués par des auditeurs externes, portant sur le travail des enfants, le travail forcé, la santé et la sécurité au travail, la liberté d'association, la non-discrimination, les pratiques disciplinaires, le harcèlement sexuel et moral, le temps de travail, la rémunération sont réalisés à raison de 500 par an dans le monde entier. Ainsi trouve-t-on sur le site de l'Oréal l'affirmation suivante "" Les audits ont confirmé des points importants tels que l'absence totale d'employés âgés de moins de 16 ans. Cependant, des domaines d'amélioration ont pu être identifiés comme par exemple certains problèmes mineurs d'hygiène et sécurité et la nécessité de mettre en place un meilleur contrôle des horaires de travail Ces points ont été intégrés à des plans d'actions correctives qui font l'objet d'un suivi en interne et d'audits complémentaires de nos auditeurs extérieurs "". Ce positionnement des entreprises entre normalisation et/ou "" normation "" se confronte naturellement à la norme juridique, qui, elle, est obligatoire. Pourquoi créer ces normes par définition, différentes de la loi mais qui ne peuvent être hors la loi ? Quel objectif est-il poursuivi ? Le commercial passerait-il au second plan face à des principes humanistes prônés à l'intérieur comme à l'extérieur par les entreprises ? L'articulation de toutes ces normes au sein de l'entreprise se fait-elle harmonieusement ? Que se passe-t-il quand il y a inadéquation entre le discours normalisé et les pratiques ? Quelle norme va primer sur les autres ou au contraire, se crée-t-il un ensemble de normes se complétant les unes avec les autres ? Foucault distingue et oppose la loi et la norme , la loi étant ce qui s'applique aux individus de l'extérieur, essentiellement à l'occasion d'une infraction, la norme étant ce qui s'applique aux individus à l'intérieur, puisqu'il s'agit pour elle d'atteindre leur intériorité même en imposant à leur conduite une courbe déterminée . Par l'utilisation des Chartes éthiques et des normes qualité, cette distinction semble dépassée puisqu'elles s'adressent autant à la clientèle qu'aux collaborateurs de l'entreprise. Cette communication institutionnelle des entreprises est créatrice de normes aux effets divers, volontaires ou involontaires, maitrisés ou non. Le choc des normes juridiques générales avec les normes conventionnelles crées par les entreprises pour leur propre usage aura obligatoirement des effets collatéraux. Nous nous attacherons au travers d'exemples concrets, à rechercher les décalages entre le discours normé de l'institution et la norme véritablement créée au sens juridique du terme en mettant en évidence les différents effets de cette communication institutionnelle."
"Les auteurs, interpellés par la faible communication des dirigeants d’entreprises via les blogs, tentent d’en déceler les motivations plus ou moins conscientes. Ils posent comme hypothèse la crainte, plus ou moins consciente de ceux-ci, d’un affaiblisse-ment de la dimension du sacré liée à la légitimité de leur pouvoir. Pour étayer leurs propos, les auteurs construisent leur propre grille de lecture à partir d’outils reconnus dans l’étude de sites web, pour analyser les blogs de quelques dirigeants qui font figure de proue dans le domaine. Et si la part de sacré de l’autorité traditionnelle d’un « patron » reposait sur la conception d’un temps long (celui des mythes, des rites…) ou sur une certaine distanciation hiérarchique (en grande part symbolique), qu’en advient-il d’une communication par et dans les dispositifs numériques, symboles d’immédiateté, d’interactions et d’égalité ?"
"Le financement territorial de la recherche scientifique offre l’opportunité de développer de la recherche appliquée au sein de consortiums pluridisciplinaires. Sous les injonctions de transparence et de participation citoyenne, et dans le cadre de la promotion de technologies innovantes redynamisant un territoire, un laboratoire en SIC, et plus largement les laboratoires de recherche en sciences sociales interrogeant la question des usages, ont intérêt à intégrer de tels consortiums. Il s’agit alors de rester vigilant quant à la réception et la compréhension de notre méthodologie d’enquête et d’analyse sur les usages et les usagers, qui légitime notre positionnement scientifique. Il s’agit encore d’adopter une distance critique face aux attentes différenciées des nombreux acteurs, face au terrain et aux retours d’usages auxquels nous sommes confrontés et qui remodèlent nos « manières de faire » de la recherche au cœur d’un dispositif où interagissent les objets, les techniques, les usagers et les chercheurs. Dans le cadre du projet Pacalabs OpeNRJ, financé par la Région PACA, nous avons engagé une recherche sur les usages axée sur l’ouverture de données et le partage d’expériences liées à la consommation énergétique de bâtiments à usage collectif. Notre consortium réunit des acteurs pluridisciplinaires : le CSTB, Centre Scientifique et Technique du Bâtiment à Sophia Antipolis, le laboratoire I3M de recherche en Sciences de l’Information et de la Communication de l’Université de Nice Sophia Antipolis, en partenariat avec des acteurs professionnels et territoriaux : QUALISTEO, une entreprise qui développe des solutions de mesure et d’analyse des consommations énergétiques des bâtiments et une collectivité territoriale : la CASA, Communauté d’Agglomération de Sophia Antipolis. Nous visons le recrutement et l’équipement d’organisations publiques et privées régionales, prêtes à s’engager dans la collecte et la diffusion libre de données énergétiques de leurs bâtiments, sur un serveur dédié. L’« Open Data », l’ouverture ou la libération de données, peut être rapprochée du mouvement plus général de l’Open Knowledge, de la diffusion de connaissances libres. Elle s’est aussi rapidement instaurée sur le socle de l’ouverture de données publiques. Renouvelant la question de la transparence démocratique, le mouvement n’est encore adopté que par un nombre restreint de collectivités territoriales, en France, mais suscite un réel intérêt du côté des différents acteurs publics et privés du territoire. La perspective principale de la connaissance, dimension qui s’attachera plus précisément à définir ce que sont les données, quelles sont les pratiques professionnelles, de recherche et citoyennes qu’elles génèrent et qui peuvent se constituer en connaissance, construit notre approche amont de l’accès public à un dispositif sociotechnique de diffusion de données ouvertes, données qu’il s’agira encore de réutiliser, en visant leur appropriation par les usagers. Cette approche par les usages vise la construction d’un écosystème de la libération de données. D’un point de vue méthodologique, notre communauté d’usagers, typologisée en offreurs et utilisateurs de données, est soumise à différents questionnaires, entretiens semi-directifs, recueil de témoignages par le biais de récits d’expérience et pratiques de focus group, instaurant une approche ethnographique qualitative de ces usagers et de leurs usages. Nos outils d’enquête et d’analyse visent une démarche de co-conception, en appelant aux retours d’usages des différents utilisateurs, et visant l’évaluation et la co-construction de services, générés par l’ouverture des données énergétiques et territoriales, services qui produiront de nouveaux usages. Or, les premières avancées de la recherche sur les innovations et les usages liés à l’ouverture de données, montrent la place prépondérante qu’occupent les Leader Users ou usagers-concepteurs aux fortes compétences techniques (Von Hippel, 2005 ; FING, 2011), dans la catégorie des usagers innovants. Notre programme de recherche comprend d’ailleurs l’organisation d’un Hackathon, s’adressant nécessairement à ce type d’usagers, afin de co-construire ces nouveaux services générés par la libération des données. Il est intéressant de croiser la « domination » des compétences techniques de cet environnement à nos premiers éléments d’enquête. Ceux-ci suggèrent, parmi la communauté d’experts interrogés, un manque d’aisance vis-à-vis des données ouvertes et de leur utilisation. Notre interrogation devient plus aiguisée : en quel sens l’usage d’une technique n’est-il pas limité à un groupe primaire, mais peut s’inscrire dans un collectif plus large (Cf. Flichy, 2008) ? En quel sens l’ouverture de données peut-elle donner accès à une connaissance pratique, c’est-à-dire articulant savoirs, savoir-faire et compétences, en vue d’usages sociaux partagés ? De manière plus générale, nous aspirons à ce que la question de l’ouverture des données sous l’angle des usages nous permette de réinterroger la dimension pratique et sociale de différents processus de reconfiguration des savoirs au sein d’un écosystème ouvert."
"La rencontre de l'informatique et de la connectique a ouvert d'immenses perspectives. L'ouvrage met en évidence les capacités des sciences de l'information et de la communication à penser le rapport des technologies digitales à la société. Les technologies s'inscrivent dans des rapports de pouvoir déjà existants; au final, elles servent à accroître la puissance des puissants, mais néanmoins dans leurs marges permettent aussi à l'expression de nouvelles formes de créativité, de sociabilité et de citoyenneté."
"Nous exposons dans cet article les caractéristiques de l’intelligence territoriale en tant que théorie, posture, et démarche ascendante d’intelligence collective fondée sur une approche citoyenne de la valorisation territoriale. Et sur la capacité (ou l’incapacité) des acteurs à co-écrire le scénario de leur futur dans un cadre élargi, l’U.E, et cela en connexion avec la dimension locale. La vision statique du territoire a vécu et le brouillage des territoires et des références spatiales qui en résulte pose problème à tout acteur local dans la mesure où il fonctionnait sur la délimitation territoriale de ses compétences. L’espace invisible prend un poids croissant et la dynamique territoriale contemporaine suppose une communication double : bottom up et signal down. La complexité qui résulte de l’intrication de l’espace physique et virtuel nécessite la captation puis l’utilisation d’une quantité plus grande d’information de qualité sur le territoire.."
"Nous exposons dans cet article les caractéristiques de l'intelligence territoriale en tant que théorie, posture, et démarche ascendante d'intelligence collective fondée sur une approche citoyenne de la valorisation territoriale. Et sur la capacité (ou l'incapacité) des acteurs à co-écrire le scénario de leur futur dans un cadre élargi, l'U.E, et cela en connexion avec la dimension locale. La vision statique du territoire a vécu et le brouillage des territoires et des références spatiales qui en résulte pose problème à tout acteur local dans la mesure où il fonctionnait sur la délimitation territoriale de ses compétences. L'espace invisible prend un poids croissant et la dynamique territoriale contemporaine suppose une communication double : bottom up et signal down. La complexité qui résulte de l'intrication de l'espace physique et virtuel nécessite la captation puis l'utilisation d'une quantité plus grande d'information de qualité sur le territoire.."
Cette rubrique de Quadrature propose d’offrir aux lecteurs une approche des problèmes de mathématiques par les textes originaux. Chaque numéro comporte une « Question mathématique » (l’énoncé commenté d’un problème tel qu’il fut posé par le passé) et une « Question historique » (détail d’un des aspects ou champs disciplinaires impliqués dans le problème posé). Les lecteurs sont invités à répondre à ces questions et les meilleures suggestions seront incorporées à celles des auteurs dans le numéro suivant.
Cet article traite de l'intégration industrielle de PME se situant en marge des projets de R&D d'un pôle alors même qu'elles représentent un enjeu de développement économique pour le territoire. Nous mobilisons la notion de non-complémentarité des connaissances pour éclairer cette situation et établir un diagnostic sur un plan technologique. Nous montrons qu'une issue favorable pour sortir de cette difficulté ne peut être envisagée qu'à l'échelle d'une gouvernance territoriale unissant l'ensemble des acteurs territoriaux de l'innovation.
"Un village de moins de 500 habitants des Alpes-Maritimes s’est doté d’un système de vidéoprotection. Dans ce village tranquille, isolé, territoire de chasse, de cyclotourisme et de randonnées, la délinquance reste très limitée. Dans cet article, à la suite d’une enquête que nous sommes en train de mener, nous voudrions mettre à jour l’existence d’un récit sécuritaire et approcher sa source, lequel récit contribue à l’acceptation par une population d’un dispositif sociotechnique relativement onéreux pour un tel type de commune rurale. Cet ancrage sécuritaire semble, telles sont nos hypothèses, doublement inspiré par un discours et des actions en matière d’innovations technologiques organisées par les pouvoirs publics locaux et nationaux ainsi que par un sentiment d’insécurité amplifié par l’isolement du village en regard d’une baisse de la présence policière et de son éloignement géographique progressif. Dans un tel contexte, la multiplication des dispositifs sociotechniques contribue-t-elle à rompre l’isolement ou au contraire accentue-t- elle le sentiment de solitude en limitant les échanges intersubjectifs ?"
"Dans cette communication nous analysons un certain nombre de dispositifs sociotechniques liés aux Big data, à la cybersurveillance et à la videosurveillance en examinant la manière dont ils abordent les singularités quelconques et les singularités individuelles."
La fabrique de la ville semble déterminée par des impératifs fonctionnels. Une batterie d'indicateurs est donc utilisée pour définir la forme de la ville en lien avec des impératifs de déplacement par exemple. Ces indicateurs standards semblent pouvoir être appliqués au développement d'une smart-city comme Nice Méridia. Ainsi le développement de la smart-city semble indifférent à l'utilisation de données. Le présent article montre que la configuration urbanistique de Nice Méridia correspond au contraire à une vision de la ville prenant en charge l'utilisation des datas et leur effet sur les usagers.
"Au printemps 2008, Antoine Schmitt présenté à Paris sa première exposition personnelle intitulée; » Objet petit a ». Sous ce titre énigmatique faisant directement appel a la théorie psychanalytique Lacanienne, l’artiste proposa une série d’oeuvres comportementales invitant le public a une expérience communicationnelle avec un programme informatique."
"La réflexion que nous menons part du constat suivant : les organisations sportives peinent à afficher des références aux valeurs olympiques et des communautés virtuelles en débattent et s’y substituent. En effet, les spectateurs du monde sportif par le biais des réseaux sociaux véhiculent ces valeurs que l’on devrait (re)trouver au cœur des interactions numériques entre pratiquants et institutions sportives. Nos propos sont structurés comme suit : après avoir rappelé les fondements de l’olympisme, nous tenterons de montrer que les valeurs associées à l’olympisme – censées représenter « la colonne vertébrale », non seulement de l’organisation olympique, mais de toute institution sportive – peuvent être considérées, en première analyse, comme les facettes de l’archétype d’une organisation utopienne. À la suite, après avoir exposé quelques faits qui témoignent de la controverse évoquée, nous présenterons en seconde analyse, celle de certains sites institutionnels en lien avec le comité international olympique couplée aux résultats d’une enquête par questionnaire menée auprès d’une communauté développée sur Facebook autour d’une page intitulée « Promotion des valeurs olympiques » composée de personnes ayant une expérience professionnelle dans le domaine sportif. En conclusion, nous reviendrons sur le besoin de « bonne utopie » (Morin, 2015) que, sans doute, les organisations comme les institutions (i.e. celles et ceux qui les représentent) peinent de plus en plus à traduire et qui ne se retrouvent plus, tout à fait, dans les grands discours ou dans une ambition à penser l’universel ouvrant in fine comme le suggère Michel Maffesoli (2015) sur une « normopathie »."
"Le propos de cet article est d'étudier l'appropriation de Facebook au sein des bibliothèques dans une double perspective. D'une part, il s'agit de voir comment les professionnels des bibliothèques municipales s'approprient le réseau social numérique Facebook et comment ces bibliothèques sont présentes sur ce média. D'autre part, l'ambition de cette étude est d'analyser la façon dont les usagers et les professionnels des bibliothèques reçoivent cette nouvelle forme de communication à travers cette plateforme collaborative et évolutive"
"Devant le champ de recherche, insuffisamment exploré, de la dimension communicationnelle de l'intelligence économique, face à un fort besoin d'écoute interne exprimé par les salariés de l'organisation, nous posons la question de l'association, méthodologique et pratique, de la démarche de veille et d'intelligence économique à celle de la communication interne. Nous montrons, dans ce sens, que la mise en œuvre innovante d'un modèle expérimental d' "" Intelligence Communicationnelle Interne "" constitue, pour le décideur, une opportunité stratégique susceptible de favoriser une fertilisation croisée de la relation entre information et communication au sein de l'organisation."
"Devant le champ, insuffisamment exploré, de la recherche de synergies entre management et communication au sein de l'organisation, dans un contexte de crise généralisée de la santé mentale au travail, nous posons la question des méthodes de Direction de la communication interne, et envisageons en quoi la communication numérique constitue une opportunité de changement stratégique susceptible de réhabiliter la fonction communication au cœur du management."
"Devant le champ, insuffisamment exploré, de la recherche de synergies entre management et communication au sein de l'organisation, dans un contexte de crise généralisée de la santé mentale au travail, nous posons la question des méthodes de Direction de la communication interne, et envisageons en quoi la communication numérique constitue une opportunité de changement stratégique susceptible de réhabiliter la fonction communication au cœur du management."
"Les rapports pathogènes que nous observons, de longue date, entre management, culture et organisation, au sein de la grande entreprise, apparaissent peser lourdement sur le domaine de sa communication interne. L'acuité de cette problématique complexe et insidieuse nous laisse percevoir les symptômes d'une forme de syndrome communicationnel que nous caractérisons par les termes de « maladie de la communication interne » dans l'organisation. Après avoir approché la description préalable de ce contexte, nous montrons que la mise en œuvre d'un dispositif de communication interne innovant est possible, en articulant opportunément les technologies organisationnelles de l'information, de la communication et de la collaboration, les apports des théories orchestrale et systémique de la communication, les concepts de veille, d'intelligence collective et de Knowledge Management. En effet, par la pratique complémentaire d'un forum intranet de discussion et d'un groupware que nous mettons en place et expérimentons au sein d'une grande entreprise française de services, nous proposons une alternative innovante, au travers d'un modèle original de management de la communication interne."
"Le Dircom du Service Communication de l'entreprise observée dispose des Technologies de l'Information et de la Communication depuis l'année 2000. Au regard du concept de Communication Médiatisée par Ordinateur et de l'analyse de la transformation, par les TIC, de l'activité de ce service, nous tentons de montrer en quoi il peut y avoir révolution dans la fonction de Directeur Communication tant en termes d'organisation que de méthodologie d'approche... jusqu'à proposer, via l'anagramme ""Dircmo "", la nouvelle appellation de ""e-Dircom"", accompagnée de ses déclinaisons opérationnelles en termes de stratégie de CMO interne et externe (ou e-communication interne & externe )."
"En posant la question principale de la fonction et des méthodes d'approche communicationnelle interne du Directeur de la Communication (Dircom) dans les organisations en général, et du Dircom régional de l'entreprise étudiée, en particulier, notre travail de recherche nous amène à nous demander si l'association de ce même Dircom aux Technologies de l'Information, de la Communication et de la Collaboration (Ticc), ainsi qu'à des processus de veille et d'intelligence spécifiques, peut amener, au sein de l'organisation, une nouvelle forme de communication interne. En se positionnant dans le champ des Sic et de la communication organisationnelle, par une référence dominante aux théories orchestrale et systémique de la communication, notre recherche, fruit d'une dialectique « terrain - abstraction », s'appuie tant sur la mise en perspective du Dircom avec les principaux champs théoriques de la communication interne, de la veille, de l' « entreprise Intelligente » (dont les Ticc, l'Intelligence collective et le Knowledge management) que sur une expérimentation de plusieurs mois d'un groupware et d'un forum Intranet de discussion. Il ressort principalement de ces croisements pluriels, à l'appui des concepts de e-Dircom et de veille « Infocom » que nous amenons, la proposition d'un modèle original d'intelligence, spécifiquement adapté au domaine de la communication interne dont ce même e-Dircom a la charge dans l'organisation : l'« Intelligence Communicationnelle Interne ». Comme réponse à notre question principale de recherche, traduction d'une nouvelle forme de communication interne dans l'organisation, mais aussi d'un nouveau processus de gestion de cette même communication interne, l'« Intelligence Communicationnelle Interne » est susceptible d'offrir de nouvelles perspectives de recherche en Sic, ainsi qu'une dimension stratégique complémentaire de la profession de Directeur de la Communication de l'organisation. Nous montrons in fine que ce modèle, dont nous observons par ailleurs une certaine articulation, de nature synergique, entre les champs de l'information et de la communication s'y trouvant en œuvre, présente plusieurs convergences intéressantes avec certaines réalités exogènes, tant issues des sphères de la recherche en Sic que professionnelles."
"Plusieurs contraintes conjoncturelles apparaissent peser sur le domaine de la communication interne dans l'organisation, au point de nous laisser percevoir les symptômes d'une forme de « maladie de la communication interne ». Devant cette problématique à laquelle nous associons l'influence de la Rtt, du management et de la culture de l'organisation, nous nous appuyons sur certaines opportunités offertes par les Technologies de l'Information, de la Communication et de la Collaboration pour proposer une alternative, au travers d'un modèle original d'« Intelligence communicationnelle interne ». Celui-ci nous permet de nous interroger sur la question du lien entre l'information et la communication en œuvre en son sein, ce qui nous amène à y observer une articulation de nature synergique, une forme de « synergie info-communicationnelle interne »."
"Les enjeux des rapports entre management et communication ne sont plus à démontrer tant l'articulation entre les deux domaines, sous ses multiples aspects, constitue un vecteur considérable de fonctionnement de l'organisation. Or, dans bien des cas, il ne nous semble pas que les conditions managériales soient suffisamment réunies pour favoriser, au-delà des récurrentes bonnes intentions, tant la légitimité des professionnels de la communication en entreprise, que ce nécessaire humanisme communicationnel représentant, à nos yeux, toute la noblesse et la valeur ajoutée des métiers de la communication organisationnelle. Qui plus est, à la lumière des travaux convergents de plusieurs chercheurs, nous nous demandons même si nous n'assistons pas, en quelque sorte, à une forme de retour en arrière paradoxal, à une certaine régression contradictoire de la valeur de l'Humain dans le système communicationnel interne de l'entreprise. C'est au regard de cet inquiétant constat que nous proposons de concevoir un schisme entre management et communication, et de conduire notre réflexion, selon un positionnement épistémologique constructiviste, dans le sens d'une recherche de régénération du lien social, propice à la construction d'une nouvelle forme de communication interne organisationnelle."
"Le déploiement des DISTIC (Dispositifs Socio-Techniques d'Information et de Communication), conjugué aux Technologies de l'Information et de la Communication ainsi qu'à certaines attitudes managériales, n'est sans doute pas sans incidences sur la culture de l'organisation...à moins que ce ne soit l'inverse ou bien réciproque ? En nous appuyant sur la pratique de l'animation d'un forum intranet dont nous avons la responsabilité au sein de l'entreprise observée, nous proposons de nous interroger sur les interactions et les corrélations pouvant apparaître entre culture et DISTIC de l'organisation, et ce, sous l'angle de l'appréciation d'une performance infocommunicationnelle interne de cette dernière."
"Le forum intranet de discussion tend progressivement à prendre sa place dans la recherche en Sciences de l'Information et de la Communication ainsi qu'en pratique, au cœur même des organisations. Le présent travail de recherche pose comme questionnement les attributs d'un tel dispositif au regard du champ de la communication interne des organisations. Il tente ensuite d'en caractériser, par une approche exploratoire aux plans théorique et pratique, les principales finalités stratégiques pour le Directeur Communication."
"Les contributions réunies ici s'intéressent à ce que pourraient être les conditions d'un multiculturalisme réussi, dont deux dimensions sont ici privilégiées : celle de conflits religieux qui n'ont cessé d'être instrumentalisés au service de causes moins saintes que profanes ; celle des modalités d'intégration dans un espace que les institutions internationales ne sont manifestement pas parvenues à unifier, laissant aux organisations privées la charge de reprendre à leur compte cette responsabilité sociale dont les sociétés civiles portent la demande pressante et qui paraît bien détenir la clé de la soutenabilité d'un avenir commun."
"En premier lieu, se posent les questions de médiatisation. Ici, l'événement est constitué par le Festival d'Avignon, “In” et “Off” confondus. Il y a bien une exception avignonaise, liée à l'histoire singulière du Festival, à la dynamique sociale qu'il a suscitée, mais aussi à la ville d'Avignon elle-même. Et le public ne s'y trompe pas, lorsqu'il se plonge avec délices dans une communauté éphémère et bruyante, mais riche en sensations et significations. Événement culturel, le Festival d'Avignon est également un événement médiatique. Tant et si bien que l'on observe un fort contraste entre le traitement régulier et dense par tous ces médias du spectacle vivant au moment du Festival, et celui, beaucoup plus clairsemé et limité, qui se produit tout le reste de l'année. Dès lors, il est également possible d'appréhender Avignon comme un rituel médiatique... Dans une perspective constructiviste, on peut s'interroger sur les raisons (voire les dénoncer...) qui conduisent effectivement la plupart des médias généralistes nationaux et locaux à surmédiatiser le Festival “In”..."
"Quelle a été la place de l'utopie dans la construction de Sophia Antipolis ? L'utopie sophipolitaine induit-elle aujourd'hui des comportements, des modes d'organisation et de management de la technopole ? Lesquels ? Comment la technopole a-t-elle représentée sur les sites d'information en ligne ou au bien encore dans les livres scolaires ? Enfin quelles sont les caractéristiques « sociologiques » des usagers de Sophia Antipolis ? Telles sont les questions auxquelles l'Héritage d'une utopie essaie de répondre."
"L'objectif de cette contribution théorique est d'inviter au dialogue les chercheurs issus de deux courants de la recherche francophone qui se sont longtemps ignorés mutuellement : d’une part, les recherches sociologiques sur les usages ; d’autre part, les approches plus généalogiques en termes de dispositifs. Cette invitation épistémologique représente l’aboutissement d’un travail collectif de plusieurs années mené au sein du séminaire Usages des Dispositifs Sociotechniques Numériques (UDSN) dans le cadre de l’Institut des Sciences de la Communication du CNRS. Historiquement, la tradition française de la sociologie des usages a connu différents âges. D'une approche centrée sur la question de l'appropriation afin d'analyser les écarts entre usages prescrits et usages réels, l'étude des usages a vu progressivement ses interrogations renouvelées par des approches plurielles et interdisciplinaires, inspirées notamment par les l’ergonomie des interfaces homme-machine, l’anthropologie des sciences et techniques, la sociologie pragmatique et cognitive, les recherches en ethnographie du travail humain, etc. De son côté le concept de dispositif a connu de multiples acceptions. Son interrogation par les sciences humaines et sociales a révélé sa richesse, que la trajectoire même des travaux de Foucault repris par Deleuze illustre bien. En posant simultanément les questions de diffusion ou de centralisation du pouvoir, de finalités des acteurs impliqués, d'intégration ou de degré d'autonomie des actions par rapport aux agencements qui le composent, ce concept et ses prolongements ultérieurs ont ouvert une voie fructueuse d'appréhension du complexe. En fait, que le modèle choisi soit celui des usages ou des dispositifs, ce sont deux manières de questionner les relations entre social et technique qui ne semblent pas avoir suffisamment discuté ensemble. Ces deux courants de recherche sont nés pourtant au même moment, à la fin des années 1970, dans un contexte d’interrogation portant sur l'autonomie des individus. Depuis, ils semblent avoir évolué en sens inverse : les recherches sur les usages ont cherché à monter progressivement en théorie et dépasser les approches simplement empiriques afin de gagner en épaisseur critique ; d’autre part, les travaux sur le dispositif, ont tenté peu à peu de transformer un modèle théorique en concept tel que celui de dispositif sociotechnique d’information et de communication (DISTIC) susceptible d'organiser les approches empiriques. Mais cette évolution croisée esquisse aussi les conditions épistémologiques d’un rapprochement, comme l’ont montré nombre d’interventions au séminaire UDSN de chercheurs issus des deux traditions et tentant d’établir des passerelles. A leur lumière, le paradigme de l’approche sociotechnique, lui-même nourri par plusieurs courants (théories de l’acteur réseau, ethnologie cognitive, double médiation sociotechnique, etc.), est de nature à faciliter ce dialogue. Il permet d’éviter tout technicisme ou sociologisme, pour aborder d’une manière relationnelle les déterminations réciproques des différents acteurs humains et non humains composant un dispositif. Il envisage aussi d’une manière originale les enjeux soulevés par les technologies numériques les plus récentes, en termes de participation, de performativité dans les prescriptions, ou de répartition inégale des capacités d’influencer la fabrique des objets et de leurs usages. Enfin, la recherche française gagnerait aussi à s’inspirer davantage, à l’image des chercheurs du Québec, des Science and Technology Studies, qui insistent tout particulièrement sur le « making of » des dispositifs socionumériques, et des Media and Cultural Studies, qui renouvellent en profondeur et diversifient les problématiques de leur pères fondateurs tels que Hall ou De Certeau."
"Les incidences des NTIC sur la profession journalistique sont repérables à un double niveau : celui des procédures de travail (nouveaux outils de recherche et de partage des informations dans les organisations) et celui des supports de diffusion (productions en ligne sous la forme de cédéroms et surtout de sites web). En ce qui concerne ce dernier aspect, la contribution du journalisme à la production de médias en ligne apparaît déjà très diverse, voire hétéroclite : transposition en ligne de supports existants, création d'espaces d'information spécifiques ou encore combinaisons plus ou moins réussies de formes traditionnelles avec des innovations que seuls les nouveaux réseaux numériques rendent possibles. En arrière plan de ce développement de médias en-ligne se profilent d'ailleurs d'importants enjeux de société : les effets structurants et déstructurants que peuvent avoir les nouveaux "" portails "" de communication électronique sur les territoires ; l'accès socialement partagé aux ressources numérisées ; la place des médias et des médiateurs dans un paysage élargi des acteurs de l'information. C'est désormais un fait indiscutable : le journalisme n'est plus, à lui seul, l'information. Les organisations médiatiques sont aujourd'hui concurrencées sur ce terrain par les acteurs les plus divers : administrations centrales, collectivités locales, grands groupes des industries culturelles et énergétiques, opérateurs de télécommunications...sans oublier les créateurs de start-up à vocation informationnelle (guides de ville, par exemple) mais non "" journalistiques "" au sens classique du terme. Ces mutations amènent ainsi le journaliste à devoir se positionner par rapport à des professions aussi diverses que celles de publicitaire, technologue, manager, et surtout de spécialiste en systèmes d'information (bases de données, veille, intelligence économique, sécurisation, marketing, datamining...). Cette nécessité n'a pas échappé à certaines formations en journalisme, qui tentent dorénavant de faire évoluer le journalisme vers le management de systèmes d'information, dans une perspective épistémologique sous-tendue par la prévision d'une convergence de l'économie de l'information autour du tryptique "" télécommunications-audiovisuel-informatique "". A l'heure où les sciences de l'information et de la communication se donnent de plus en plus à penser au travers de modèles inspirés par les épistémologies constructivistes, il est d'ailleurs étonnant que constater le fossé existant entre la prolifération des discours pédagogiques euphorisants sur les réseaux et la réalité des contenus pédagogiques dispensés par les acteurs, largement marqués par un néo-positivisme et un néo-utilitarisme à peine réaménagés afin de satisfaire à la fois les besoins du marché et un souci de légitimation intellectuelle et académique. Le journaliste, pris dans une "" Toile "" qu'il aura contribué lui-même à tisser, ne deviendra-t-il plus qu'un rouage parmi d'autres d'une "" société de l'information "" dont il devrait être pourtant l'un des acteurs essentiels ? Au contraire, sa résistance et sa recomposition permettront-elles d'éviter que l'information ne devienne la première victime de cette "" société "" qui en porte ironiquement le nom ? Le "" cyberjournalisme "" correspond-il vraiment à un changement de paradigme dans la profession, comme nous l'annoncent nos collègues anglo-saxons ?"
"Le présent ouvrage regroupe les textes des communications acceptées par le conseil scientifique du colloque «Vers de nouveaux modèles d’apprentissage, de pratiques pédagogiques innovantes et TIC pour l’éducation au développement durable » qui s’est tenu à la Faculté Polydisciplinaire de Ouarzazate, relevant de l’Université Ibn Zohr d’Agadir, Maroc, les 25-27 octobre 2016, afin de les faire connaître au plan international par l’intermédiation de l’internet. L’ouvrage se divise en quatre thématiques qui sont celles que le colloque a fait émerger à partir d’un appel à communication à horizon très large. Sont traités les usages des TICE/TIC en développement durable ; TIC et pratiques pédagogiques innovantes ; les usages des TIC dans l’éducation ; et des contributions diverses. Leurs auteurs entendent faire le point sur les avancées scientifiques permettant de mettre en perspective le sens donné à l’apprentissage et aux pratiques pédagogiques innovantes dans différents contextes de formation, notamment dans les pays du Sud de la Méditerranée. Ces paradigmes fondamentaux dans les systèmes éducatifs, soulèvent des questions sur les aspects scientifiques, éthiques et civiques liés à ce sujet de l’usage des TICE."
"L'émergence des nouvelles technologies de l'information et de la communication, des systèmes d'intelligence et des usages qui se profilent, a engendré de profondes mutations organisationnelles. Elles touchent toutes, de près ou de loin, la relation de l'ensemble des acteurs et impactent les territoires sur lesquels ils reposent. Afin de déterminer les phénomènes, de les appréhender et de les représenter, il convient d'établir des indicateurs d'immersion de l'information au sein des territoires qui s'en trouvent ainsi bouleversés. Il apparaît opportun d'établir comment l'espace se réorganise et se redéfini, quelles sont les influences de l'espace physique sur le domaine informationnel, et comment l'espace virtuel peut à son tour, impacter les territoires physiques. Nous nous attachons à déterminer les nouveaux assemblages de territoires, les combinaisons et les interactions qui sont opérés entre les territoires physiques et les nouveaux territoires virtuels constitués. Des initiatives surgissent, qui visent à régir les espaces et envisager les usages, mais le manque de réelle coordination, la dépendance aux avancées technologiques, l'absence de clairvoyance ou bien encore de vision globale, endiguent les aboutissements constructifs qui pourraient sourdre d'une réelle cohésion. L'apport de l'intelligence territoriale, couplée à l'adoption et à la maîtrise d'analyses cybermétriques, nous permet d'aborder le sujet dans ses caractéristiques relationnelles, à la fois entre les différents espaces, mais également par l'étude plus précise du lien au sein même de ces territoires. Tout ceci afin de contribuer à approfondir et à projeter : quels territoires pour aujourd'hui et pour demain ?"
"Région, régionalisation, régionalisme sont des formes lexicales dont la floraison témoigne de l'importance que prend actuellement le territoire dans l'espace public européen. Le développement durable est un concept émergent depuis que la mondialisation a fait percevoir aux responsables politiques comme aux citoyens le danger et l'inanité d'une croissance débridée des activités humaines sur la planète Terre. L'intelligence territoriale est une notion introduite par les penseurs de toutes les disciplines des sciences humaines pour donner une cohérence conceptuelle aux tentatives de comprendre la complexité et encourager le développement territorial. L'objet de cette communication est de fournir aux participants à la conférence européenne de Caenti à Alba Iulia (Roumanie, 20-23 sept 2006) des fondements théoriques et des axes de recherche mettant en relation les concepts de région et de développement durable avec celui de l'intelligence territoriale. Dans un monde que les technologies de l'information communication font rétrécir tout en faisant voir à la planète entière le désastre de l'impérialisme d'état et de la concurrence effrénée, le développement des régions en Europe est une chance pour voir émerger une autre gouvernance publique. Les régions considérées comme des foyers culturels et économiques rayonnants peuvent établir leur identité et régler leurs relations sur des logiques autres que les antagonismes à somme nulle, constitutifs de la pensée politique héritée des siècles précédents. Mais les régions sont-elles le meilleur échelon de cette mise en œuvre du développement durable à l'Européenne? L'histoire et la raison nous conduise à répondre: Oui, mais en coopération avec les autres échelons comme les départements, les pays, les villes."
"Les auteurs questionnent l’appropriation des tweets par les dirigeants d’entreprises à travers les effets que ces derniers escomptent en termes de visibilité, d’identité narrative, de notoriété… La spécificité du réseau social numérique Twitter, par son format limité et ses enjeux partagés entre « communication médiatisée » et « communication d’influence », méritait une analyse aussi bien de ses usages que de la dimension symbolique dans laquelle ces derniers s’inscrivent. Posé d’emblée comme un Dispositif Sociotechnique d’Information et de Communication (DISTIC, en abrégé) dans le droit fil des travaux de Michel Foucault qui envisageait dans le dispositif autant la structure réticulaire que l’intention qui la sous-tendait, Twitter semble révéler les injonctions paradoxales qui frappent les dirigeants. Ceux-ci apparaissent à la fois désireux d’utiliser ce réseau pour accroître leur visibilité et effrayés par les conséquences préjudiciables d’une communication fortuite."
"La présente contribution propose une analyse des pratiques émergentes de coaching au sein des organisations. Elle s’inscrit dans la continuité d’une réflexion de l’auteur sur les dynamiques de changement liées à une co-évolution des formes organisationnelles et des logiques de communication. Le coaching sera tout d’abord abordé avec un regard critique, comme prolongement d’un autre phénomène de mode, celui du courant de la programmation neurolinguistique. Dans un deuxième temps, les recherches actuelles en communication des organisations (nouvelles formes d’organisation, communication de changement, médiation) permettront d’éclairer le succès de ces pratiques professionnelles comme étant une réponse aux enjeux économiques, techniques, organisationnels et communicationnels vécus par les organisations."
"À partir de l’étude du cas Danette, nous voulons montrer la transition allagmatique d’une génération à l’autre, comment cette communication médiatique a engendré un basculement d’une « verticalité des pères » (On se lève tous pour Danette) à une « horizontalité des pairs » (On vote tous pour Danette). À travers cette question générationnelle, se pose une évolution d’une économie de la transaction à une économie de la relation, où pour demeurer compétitives, les entreprises doivent considérer le « cycle de vie client » plutôt que le « cycle de vie produit ». L’avantage concurrentiel de Danone se jouera sur un « ré-enchantement » consumériste, par la mise en œuvre d’une communication transgénérationnelle où, dans une démarche de co-création, le consommateur est étroitement associé à la commercialisation des produits et à la médiatisation de la marque. Danette construit alors rétrospectivement un véritable imaginaire sociotechnique à partir de son histoire. L’internet soutient ce dispositif d’un marketing de la nostalgie amorcé en 2006 et qui atteint son point culminant en 2010 pour l’événement « On se lève tous pour les 40 ans de Danette »."
"Les auteurs observent les traces que laissent les usagers sur le réseau socionumérique Facebook. Derrière la figure de l’internaute, Facebook s’intéresse au consommateur potentiel ainsi qu’aux entreprises partenaires que celui-ci consomme en ligne (cyberconsommateur) ou dans les circuits de distribution physiques. À partir d’une étude de huit pages d’adeptes (fan pages), les auteurs montrent que le public est invité, de trace en trace, à se muer en un public prescripteur. L’objectif est de mesurer le décalage entre une illusion du nombre — publics imaginés — et la réalité des contributions, beaucoup plus mesurée — publics réels."
"Internet a créé une transformation en profondeur des relations entre les individus et les organisations, alors que nous pouvons observer, depuis la fin des années 1990, l'émergence successive du consom'expert, du consom'acteur et du consom'auteur (MAILLET 2008). Un effet d'époque est alors franchi permettant une véritable restructuration du rapport de forces entre les organisations et le citoyen. D'abord centrées sur les caractéristiques intrinsèques de leurs offres de produits et services, les organisations ont dû évoluer sur leur registre discursif à destination des consommateurs. Ces dernières doivent tenir compte de la volonté de participation de l'individu au sein de leurs stratégies, notamment leurs stratégies de communication, volonté souvent relayée sur Internet par de multiples blogs et plus particulièrement les réseaux sociaux. Les années 2000 sont d'ailleurs marquées par l'explosion des médias numériques qui se conjugue à l'évolution de l'internet vers les réseaux numériques considérés comme sociaux. Nommé "" Convergence "" aux Etats-Unis (Op. Cit), ce mouvement met au centre de toute réflexion stratégique l'idée de participation entre les organisations et leurs parties prenantes. Dans ce contexte, et ce, par rapport à notre champ de recherche, nous avons pu noter que la plupart des études traitant de l'impact d'Internet sur la communication des organisations, tant au plan externe qu'interne, sont plutôt centrées sur les blogs, les forums de discussion ou Facebook dans les réseaux sociaux. Ce constat est valable aussi bien au plan international qu'au plan national . A la vue de ce premier état de l'art, l'analyse des usages stratégiques de Twitter, par les professionnels de la communication aux plans externe et interne, apparait comme insuffisamment traitée. L'étude que nous proposons peut donc être considérée comme une première approche pour combler ce manque, qu'il conviendra nécessairement d'approfondir. Notre recherche s'interroge sur les rapports entre le DISTIC (Dispositifs Sociotechnique d'Information et de Communication , ARASZKIEWIEZ 2003) Twitter, les pratiques et les stratégies de communication des organisations, avec comme problématique centrale : "" Pour les professionnels de la communication des organisations, quelles potentialités du DISTIC Twitter dans ses dimensions stratégiques et opérationnelles de communication interne et externe ? "". Au plan méthodologique, nous avons mis en place une étude de cas de type instrumentale (MUCCHIELLI, 1996) où, à partir d'une sélection de certains matériaux du cas, nous allons chercher à montrer la dimension explicative, confirmative, de modèles théoriques déjà construits par d'autres chercheurs et procéder à une approche heuristique sur un corpus de 53 questionnaires exploitables , complétée par une analyse des sites internet correspondants. Notre terrain d'expérimentation, l'AFREP (Association Française des Relations Publiques), est la plus ancienne et la plus importante association des professionnels de la communication de la côte d'azur. Ses membres sont des professionnels du secteur de la communication occupant des fonctions différentes et complémentaires (Directeurs d'agences, Conseil en communication, Directeurs de la communication, Responsables des relations publiques et relations presse) au sein de grandes entreprises, d' institutions et agences parmi les plus importantes du département des Alpes Maritimes. La première partie de ce chapitre propose une contextualisation théorique de notre étude sous l'angle de l'imaginaire sociotechnique défini par FLICHY (2001a & b), dans la lignée de travaux que nous avons conduits sur Google (DUMAS & DUVERNAY, 2009), à partir des historiques des deux grands réseaux sociaux que sont Facebook et Twitter. De façon concomitante, le deuxième volet du chapitre s'interroge sur le mode d'intégration de Twitter par les professionnels de la communication au plan de la stratégie de communication externe d'une organisation. Une réflexion sur la position de ce réseau social au plan de la communication interne et ses premières incidences managériales est présentée dans la troisième partie de chapitre. La dernière partie permet de discuter l'ensemble des résultats obtenus dans les deux parties précédentes, sous l'angle théorique de l'imaginaire sociotechnique présenté dans la première partie."
"Les écrits dans les organisations assurent la mise en forme des traces d'évènements, de pratiques professionnelles, d'évaluation de celles-ci, de normes et structurent l'organisation (Borzeix, Fraenkel, 2003) en assurant une fonction pragmatique socio-professionnelle. Or parmi ces écrits, se tient en marge du fonctionnel, le journal d'entreprise. Ce dernier vient informer au mieux le personnel sur l'évolution de la vie de l'entreprise pour permettre de fédérer les employés de l'organisation autour d'un projet commun. Outil de motivation du personnel l'incitant à participer à la vie de l'entreprise, il encourage les contacts, les liens entre les sites délocalisés. Porte-parole de la culture propre à l'entreprise, le journal interne se montre divertissant tout en sensibilisant le personnel aux grands problèmes de l'entreprise par l'explication des orientations à venir. En somme il doit "" informer, motiver, fédérer, distraire et "" décloisonner "" (Lardellier, 1998). L'histoire de l'entreprise se raconte et se fixe dans un rituel mensualisé le plus souvent comme la somme des informations susceptibles de susciter l'intérêt des acteurs de l'entreprise. Cette histoire les forme à un esprit d'entreprise (Etchégoyen, 1990) et génère par un apprentissage des changements de l'organisation ou plus précisément comme le dit N. Alter (2004), "" de l'activité organisatrice "" un construit organisationnel. Et si l'on considère que l'intériorisation des règles (Le Goff, 1993) répond à la "" recherche de l'adhésion idéologique des travailleurs aux objectifs définis par les directions "" (Lagacé, 1998), alors clairement les informations dans le journal institutionnel ou d'entreprise participent à l'édification du récit (d'Almeida, 2001) d'entreprise vecteur de l'idéologie managériale. Notre objet de recherche sera plus le discours écrit qui passe par le journal interne lequel dispositif socio-technique (DISTIC) il permet la diffusion d'une intentionnalité managériale à la totalité des salariés d'une entreprise sur le sens à donner et à prendre aux mutations en cours comme celle des fusions-acquisitions. Il s'agit donc de repérer dans la structure narrative du journal d'entreprise de la production d'un sens collectif qui vise la configuration de l'action notamment à l'occasion d'un évènement restructurant la forme organisationnelle. Le récit véhicule alors l'histoire de l'entreprise, selon une chronique de réussite annoncée. Il est storytelling porteur d'une domination symbolique managériale (B. Floris, 1996) qui met en tension les aspirations économiques, sociales et anthropologique et anticipe un scénario où le collectif participe à la performance de l'entreprise. Raconter, en appui sur le passé glorieux de l'entreprise permet d'anticiper l'inscription de l'évènement dans le cours de l'histoire de l'entreprise (Dangel, Blancherie, 2009). Nous parlons du storytelling qui ouvre la question de la performation entrepreneuriale par la culture de la connivence dans une mise en scène dont les lecteurs doivent penser qu'ils sont aussi les acteurs. Nous voulons montrer comment dans le cas de la fusion de deux entreprises internationales, la forme du récit mise en évidence par une étude au moyen du logiciel Alceste révèle une idéologie particulière de construction organisationnelle. En effet, Alceste est un logiciel d'analyse textuelle numérisée pour en extraire les structures signifiantes les plus fortes. Cet outil nous aide à mieux comprendre notre matériel discursif. L'objectif de ce logiciel est de déterminer comment s'organise les éléments qui constituent le corpus à analyser, de réduire l'arbitraire sans la description du corpus et de mettre en évidence l'information essentielle contenue dans le corpus. En l'occurrence, le corpus analysé porte sur les journaux de la première année du rapprochement des deux entreprises (soit 25 numéros en un an) et a été soumis à l'analyse en utilisant un paramétrage standard, où les valeurs sont prédéfinies par Alceste en fonction de la taille du corpus. Le journal d'entreprise comme l'outil majeur des moyens de communication interne lorsqu'il répond aux critères de qualités (forme et fond), apprécié et reconnu de tous, est aussi le faire-valoir et le faire-penser de l'entreprise à l'égard des salariés. De fait, il est un outil de communication prépondérant où tous les éléments qui définissent la culture d'entreprise transitent. Ici, il atteste d'une complexité double par un récit enchanté et enchanteur, porté sur l'imaginaire d'une part, et d'un outil informationnel intégré au dispositif communicationnel global d'autre part. C'est toute la question des narrations performatives organisationnelles qui se pose et son corollaire celle d'un formatage de l'entreprise."
"Les auteurs, interpellés par la faible communication des dirigeants d’entreprises via les blogs, tentent d’en déceler les motivations plus ou moins conscientes. Ils posent comme hypothèse la crainte, plus ou moins consciente de ceux-ci, d’un affaiblissement de la dimension du sacré liée à la légitimité de leur pouvoir. Pour étayer leurs propos, les auteurs construisent leur propre grille de lecture à partir d’outils reconnus dans l’étude de sites web, pour analyser les blogs de quelques dirigeants qui font figure de proue dans leur domaine. Et si la part de sacré de l’autorité traditionnelle d’un « patron » reposait sur la conception d’un temps long (celui des mythes, des rites…) ou sur une certaine distanciation hiérarchique (en grande part symbolique), qu’en advient-il d’une communication par et dans les dispositifs numériques, symboles d’immédiateté, d’interactions et d’égalité ?"
"Processualité documentaire et agencement des textures numériques : bases de données, archives ouvertes, épi-revues, « deconstructed / distributed journals »"
"Dans un contexte de sociétés marquées par un procès d'informationnalisation et de circulation croissante et accélérée des flux d'information édités ou non, autant dans la sphère privative, dans celle du travail que dans l'espace public, le web est devenu une source pour les chercheurs en sciences humaines et sociales. La pluralité des formes du web, sa profondeur et sa dimension dynamique exigent une réflexion sur la notion de "" documents "" notamment lorsqu'il s'agit de constituer des corpus pour les chercheurs. Les auteurs interrogent ces conditions de constitution et de recueil des mémoires observables et de leurs dispositifs d'engrammation, le passage du document aux traces numériques, dans leurs dimensions épistémologiques, pragmatiques, éthiques et sociétales."
"La plupart des activités humaines reposent sur des interactions communicatives qui sollicitent la parole et le corps et prennent de multiples formes : apprendre, travailler, accomplir une action impliquent en effet d'interagir avec autrui, avec des intentions plus ou moins explicites et des effets plus ou moins prévisibles. C'est pourquoi l'étude des ressorts qui sous-tendent ces interactions ne peut s'envisager sous un seul angle disciplinaire. L'ouvrage se propose de rendre compte de ce que la psychologie, au contact d'autres disciplines, peut apporter à l'étude des interactions et des stratégies discursives à l'oeuvre dans les situations de communication courante, de communication pédagogique, professionnelle ou médiatique."
"On présente dans cette brève note, quelques réflexions rapides sur la question démocratique dans le contexte général du plissement numérique du monde."
"RÉSUMÉ Dans cet article, la prolifération de data est examinée dans le cadre du processus d'artificialisation du monde et comme effet d'une sémiotique générale pour assurer la permanence et la transformation de la fabrique de nous-mêmes et de notre milieu associé. Cette prolifération de data est réglée sur les mouvements, intensités, des régimes de désirs et sur les processus de subjectivation qui lui sont immanents. Mais elle active en même temps de nouveaux états du Virtuel qui les enveloppent. On prend comme exemples l'urbanisation et le marketing en insistant sur certains aspects de la transformation des intelligences collectives. Ce faisant est esquissée une perspective sur le « trans et post humaniste », expression et exprimé de ces désirs."
"Dans un contexte de sociétés marquées par un procès d'informationnalisation et de circulation croissante et accélérée des flux d'information édités ou non, autant dans la sphère privative, dans celle du travail que dans l'espace public, le web est devenu une source pour les chercheurs en sciences humaines et sociales. La pluralité des formes du web, sa profondeur et sa dimension dynamique exigent une réflexion sur la notion de "" documents "" notamment lorsqu'il s'agit de constituer des corpus pour les chercheurs. Les auteurs interrogent ces conditions de constitution et de recueil des mémoires observables et de leurs dispositifs d'engrammation, le passage du document aux traces numériques, dans leurs dimensions épistémologiques, pragmatiques, éthiques et sociétales."
"Cet article pose des élements de réflexion sur les effets de la production croissante et rapide de corpus de traces, d'empiries numériques dans les SHS, en se situant dans le cadre plus général de la montée de l'algorithmique et du Data Mining au sein des sociétés dites ""performatives"", au sens de François Laruelle. En prenant appui sur le débat épistémologique et politique provoqué par Chris Anderson, on s'attache à mettre en perspective certaines transformations au coeur des pratiques théoriques, et dans le domaine des SHS , on met en évidence, à travers les concepts d'Agencement (Deleuze -Guattari) et d' ANT (Actor Network Theory), le travail complexe de la pensée confrontée à la fabrication de ce que Bruno Latour propose d'appeller ""les obtenues"" en lieu et place des ""données""."
"Quelle est la véritable nature des interactions sonore/visuel dans le contexte des nouvelles technologies ? Ces interrogations, éminemment pluridisciplinaires, invitent à une approche historique, comparative et analytique soucieuse de présenter la création artistique sous des angles variés. Au travers des textes de spécialistes appartenant à plusieurs disciplines, cet ouvrage donne à voir les enjeux considérables de telles réflexions, en mettant l'accent, également, sur l'impact de ces nouvelles pratiques sur l'enseignement artistique."
"L'improvisation collective musicale a connu un engouement remarqué dans les décennies 1960 -1970 et l'éclosion de cette pratique artistique s'est réalisée sous l'égide de deux grands mythes que sont la liberté et la spontanéité. Elle a été intégrée dans des processus compositionnels, au sein de cursus d'enseignement et surtout accompagne l'essor des nouvelles technologies dans des réalisations en temps réel. Cet ouvrage constitue un croisement d'observations, et permet donc de faire le point au sujet de l'improvisation collective, son évolution, sa réception et ses applications actuelles."
"Le cinéma de Jean-Luc Godard, plus particulièrement les films qu’il réalise depuis ces vingt dernières années, questionne sans relâche l’articulation de l’art à la communication, au politique et à l’économique. Depuis les Histoire(s) du cinéma jusqu’à Film Socialisme , sont interrogés le régime esthétique de l’art, la nécessité pour l’art de résister à la captation de l’œuvre devenue marchandise et les paradoxes d’une Europe dont la liberté reconquise peut aussi sonner le glas de la pensée, de l’autonomie des peuples et de l’intégrité de l’art. Cependant, cette exploration de la puissance ou de l’impuissance politique de l’art émane d’un cinéaste dont la posture et l’énonciation solitaires, inscrites dans les marges des institutions, induisent le risque de l’incommunication d’une pensée critique dont le medium cinématographique est pourtant l’un des grands témoins de notre temps."
fr_abstract_s
"Dans cette thèse, nous nous intéressons à l’analyse mathématique théorique et numérique des équations deNavier-Stokes compressibles en régime barotrope. La plupart des travaux présentés ici combinent desméthodes d’analyse des équations aux dérivées partielles et des méthodes d’analyse numérique afin de clarifierla notion de solution faible ainsi que les mécanismes de convergence de méthodes numériques approximant cessolutions faibles. En effet les équations de Navier-Stokes compressibles sont fortement non linéaires et leuranalyse mathématique repose nécessairement sur la structure de ces équations. Plus précisément, nousprouvons dans la partie théorique l’existence de solutions faibles pour un modèle d’écoulement compressibled’entropie variable où l’entropie du système est transportée. Nous utilisons les méthodes classiques permettantde prouver l’existence de solutions faibles aux équations de Navier-Stokes compressibles en regime barotrope.Nous étudions aussi dans cette partie la réduction de dimension 3D/2D dans les équations de Navier-Stokescompressibles en utilisant la méthode d’énergie relative. Dans la partie numérique nous nous intéressons auxestimations d’erreur inconditionnelles pour des schémas numériques approximant les solutions faibles deséquations de Navier-Stokes compressibles. Ces estimations d’erreur sont obtenues à l’aide d’une versiondiscrète de l’énergie relative satisfaite par les solutions discrètes de ces schémas. Ces estimations d’erreur sontobtenues pour un schéma numérique académique de type volumes finis/éléments finis ainsi que pour le schémanumérique Marker-and-Cell. Nous prouvons aussi que le schéma Marker-and-Cell est inconditionnellement etuniformément asymptotiquement stable en régime bas Mach. Ces résultats constituent les premiers résultatsd’estimations d’erreur inconditionnelles pour des schémas numériques pour les équations de Navier-Stokescompressibles en régime barorope."
"Les phénomènes d'érosion sont une cause majeure dans la rupture des ouvrages hydrauliques tels que les digues ou les barrages. Le territoire compte 10000 km de digues, d'où un enjeu important en terme de risque d'inondation. Une approche plus générale est essentielle à une compréhension fine des phénomènes. La modélisation des mécanismes de l'érosion doit permettre l'élaboration de solutions pour sécuriser les ouvrages et les rendre moins sensibles à ces phénomènes. Ce travail est consacré à la modélisation d'une interface entre un milieu de sol et un milieu d'écoulement. Le mouvement de cette interface est régi par l'érosion du sol sous l'effet de l'écoulement. L'écoulement est calculé par les équations de Stokes munies d'un terme de pénalisation permettant la représentation unifiée des comportements des deux milieux. Dans une première partie, nous aborderons la problématique de la représentation de l'érosion. La seconde partie traite plus précisément de la représentation unifiée des comportements des milieux, notamment grâce à l'utilisation des méthodes de pénalisation. La troisième partie constitue une revue des principales méthodes permettant le suivit d'une interface. Une description précise de la méthode Level Set est donnée. Dans la quatrième partie, nous abordons la résolution numérique des équations d'écoulement. La cinquième partie rassemble l'ensemble des tests de la méthode. Enfin, la dernière partie est consacrée à l'application du modèle sur trois cas : le "" Hole Erosion Test "", le phénomène de suffusion et l'érosion d'un lit de rivière autour d'une pile de pont."
"Le présent travail est entièrement dédié à la construction de modèles simplifiant l’opérateur de collision de Boltzmann. On pourrait trouver cette thématique très réductrice et pourtant... Les travaux de Ludwig Boltzmann portent sur l’introduction d’une description cinétique des gaz, même si cette idée peut être largement co-attribuée à James Clerk Maxwell et aux thermodynamiciens de cette époque. Mais ils s’étendent aussi et surtout à leur évolution en dehors de l’équilibre. Si le terme de transport de son équation est relativement naturel eût égard aux équations du mouvement avec forces extérieures, le comptage des collisions contribuant qui à diminuer la densité du nombre de molécules ayant la vitesse v pour t et x fixés, qui à l’augmenter, est d’une grand beauté géométrique. Et le passage de la micro-réversibilité à la ”flèche” du temps (théorème H) est révolutionnaire. La relative simplicité de la démonstration fascine. Pourtant, Boltzmann lui-même déclara tandis qu’il cherchait des développements pour obtenir des solutions à son équation : ”la simplicité sied au tailleur, pas au mathématicien”. Il est vrai que bon nombre de travaux qui suivirent, dont la hiérarchie BBGKY et le théorème d’existence global en temps de solutions, peuvent lui donner raison. Mais reste son oeuvre immense. Bon nombre d’équations cinétiques suivront telles celles de Fokker-Planck, Landau, Vlasov ou même Uehling-Uhlenbeck pour ne citer qu’elles. Ces équations n’auraient peut-être pas vu le jour sans son travail. Le point de vue cinétique permet d’appréhender bon nombre de problèmes issus de la physique. ”Sasha” Bobylev m’a dit un jour : ”Jâcques, à partir du moment où tu as commencé avec l’équation de Boltzmann, tu ne pourra jamais la quitter”. En ce qui me concerne, il a eut raison !"
"Le fameux Théorème d’Extension de MacWilliams affirme que, pour les codes classiques, toute isométrie deHamming linéaire d'un code linéaire se prolonge en une application monomiale. Cependant, pour les codeslinéaires sur les alphabets de module, l'existence d'un analogue du théorème d'extension n'est pas garantie.Autrement dit, il existe des codes linéaires sur certains alphabets de module dont les isométries de Hammingne sont pas toujours extensibles. Il en est de même pour un contexte plus général d'un alphabet de module munid'une fonction de poids arbitraire. Dans la présente thèse, nous prouvons des analogues du théorèmed'extension pour des codes construits sur des alphabets et fonctions de poids arbitraires. La propriétéd'extension est analysée notamment pour les codes de petite longueur sur un alphabet de module de matrices,les codes MDS généraux, ou encore les codes sur un alphabet de module muni de la composition de poidssymétrisée. Indépendamment de ce sujet, une classification des deux groupes des isométries des codescombinatoires est donnée. Les techniques développées dans la thèse sont prolongées aux cas des codesstabilisateurs quantiques et aux codes de Gabidulin dans le cadre de la métrique rang."
"<p><span style=""line-height: 115%; font-family: 'Calibri','sans-serif'; font-size: 7pt; font-variant: small-caps; mso-bidi-font-size: 11.0pt; mso-ascii-theme-font: minor-latin; mso-fareast-font-family: Calibri; mso-fareast-theme-font: minor-latin; mso-hansi-theme-font: minor-latin; mso-bidi-font-family: 'Times New Roman'; mso-bidi-theme-font: minor-bidi; mso-ansi-language: FR; mso-fareast-language: EN-US; mso-bidi-language: AR-SA;"">RÉSUMÉ</span><span style=""line-height: 115%; font-family: 'Calibri','sans-serif'; font-size: 11pt; mso-ascii-theme-font: minor-latin; mso-fareast-font-family: Calibri; mso-fareast-theme-font: minor-latin; mso-hansi-theme-font: minor-latin; mso-bidi-font-family: 'Times New Roman'; mso-bidi-theme-font: minor-bidi; mso-ansi-language: FR; mso-fareast-language: EN-US; mso-bidi-language: AR-SA;"">. </span><span style=""line-height: 115%; font-family: 'Times','serif'; font-size: 11pt; mso-fareast-font-family: Calibri; mso-fareast-theme-font: minor-latin; mso-bidi-font-family: 'Times New Roman'; mso-bidi-theme-font: minor-bidi; mso-ansi-language: FR; mso-fareast-language: EN-US; mso-bidi-language: AR-SA;"">Dans le cadre du projet TANDEM -Tsunamis en Atlantique et MaNche : Définition des Effets par Modélisation- une première phase de l'étude a pour but (pour Principia) de qualifier différents modèles CFD (Computational Fluid Dynamics) 3D pour la simulation de tsunamis. Un premier modèle (code EOLE) résout un écoulement bi-fluide 3D couplé avec une méthode de reconstruction de type VOF, sur des maillages structurés multi-blocs. Un autre modèle (code EOLENS) basé sur une approche de capture d'interface, utilise des maillages non-structurés. Cet article présente, pour différents cas tests académiques de propagation de vagues et de run-up en zone côtière, des comparaisons entre simulations et données expérimentales. Les résultats obtenus sur ces cas-tests sont dans l'ensemble satisfaisants et démontrent tout le potentiel de ces deux codes pour simuler en situation réelle l'impact côtier 3D d'un tsunami.</span></p>"
"Cette thèse traite de la modélisation des tsunamis, des grandes échelles de propagation aux impacts sur des structures côtières. Un inventaire des phénomènes physiques associés est établi et des modèles adéquats sont présentés. Une étude numérique avec le modèle de Saint-Venant est effectuée avec le développement d’une méthode de raffinement de maillage à seuil automatique. La simplicité et les performances de l’approche sont démontrées. Pour améliorer la précision des prévisions, un système original approchant le modèle Serre-Green-Nahgdi est investigué. Une méthode pour prendre en compte la dissipation d’énergie au déferlement est proposée. Ce modèle permet d’envisager la modélisation fine de la propagation et de l’arrivée à la côte des tsunamis dispersifs et non linéaires en des temps de calcul acceptables. Les différents types d’impact sur des structures sont modélisés grâce à un modèle diphasique compressible permettant de considérer les écoulements à phases séparées et les milieux aérés. Pour envisager une résolution à tous les régimes, des schémas Tous Mach sont investigués. Un schéma Tous Mach à variation totale limitée est proposé. Grâce à cette approche, des impacts incompressibles et compressibles sont investigués avec le même modèle. Les impacts d’écoulements aérés induisent des pressions moins élevées mais sur des temps plus longs que leurs homologues en phases pures. Bien que le schéma Tous Mach proposé soit moins sujet aux oscillations numériques que les préconditionnements classiques de la littérature, des oscillations non physiques à bas nombre de Mach sont mises en évidence sur certains cas tests. Pour finir, une méthode de couplage entre modèles de propagation et d’impact est proposée, afin de pouvoir simuler un tsunami finement avec des modèles appropriés à chacune de ces phases."
"Cette thèse concerne l'étude des graphes quantiques, c'est à dire, des systèmes quantiques dans lesquels une particule non relativiste est confinée sur un graphe. Nous proposons une nouvelle voie pour représenter des conditions aux limites, et à l'aide de ce résultat nous résolvons le problème, resté longtemps ouvert, d'approximation par des graphes réguliers de tous les couplages singuliers aux sommets dans un graphe quantique. Nous présentons une construction dans laquelle les arêtes sont disjointes et les paires d'extrémités ainsi obtenues sont raccordés par des arêtes additionnelles de longueur 2d. Chacune de ces arêtes porte un potentiel delta et un potentiel vectoriel . Nous montrons que lorsque d tend vers zéro et les potentiels dépendent convenablement de d, la limite peut produire tout couplage singulier de sommets requis. Ce type de conditions aux limites est utilisé pour examiner les propriétés de diffusion par des sommets singuliers de degré 3. Nous montrons que les couplages entre chaque paire de lignes issues du sommet sont réglables individuellement ce qui pourrait permettre la conception de filtre quantique de type ""aiguillage spectral"". Nous étudions aussi les opérateurs de Schrödinger sur un graphe infini en forme de chaîne composée de cercles identiques couplés aux points de contact par les interactions. delta Si le graphe est périodique, l'hamiltonien a un spectre de bande. Nous considérons une déformation ""courbée"" de la chaîne qui consiste en un changement de la position du point de contact entre deux cercles. On montre que cette déformation a pour conséquence la naissance de valeurs propres et analyse leur dépendance par rapport à l""angle de courbature""."
"Cette thèse aborde deux sujets de recherches, le premier est sur l’existence et l’unicité des solutions des Équations Différentielles Doublement Stochastiques Rétrogrades (EDDSRs) et les Équations aux Dérivées partielles Stochastiques (EDPSs) multidimensionnelles à croissance surlinéaire. Le deuxième établit l’existence d’un contrôle optimal strict pour un système controlé dirigé par des équations différentielles stochastiques progressives rétrogrades (EDSPRs) couplées dans deux cas de diffusions dégénérée et non dégénérée.• Existence et unicité des solutions des EDDSRs multidimensionnels :Nous considérons EDDSR avec un générateur de croissance surlinéaire et une donnée terminale de carré intégrable. Nous introduisons une nouvelle condition locale sur le générateur et nous montrons qu’elle assure l’existence, l’unicité et la stabilité des solutions. Même si notre intérêt porte sur le cas multidimensionnel, notre résultat est également nouveau en dimension un. Comme application, nous établissons l’existence et l’unicité des solutions des EDPS semi-linéaires.• Contrôle des EDSPR couplées :Nous étudions un problème de contrôle avec une fonctionnelle coût non linéaire dont le système contrôlé est dirigé par une EDSPR couplée. L’objective de ce travail est d’établir l’existence d’un contrôle optimal dans la classe des contrôle stricts, donc on montre que ce contrôle vérifie notre équation et qu’il minimise la fonctionnelle coût. La méthode consiste à approcher notre système par une suite de systèmes réguliers et on montre la convergence. En passant à la limite, sous des hypothèses de convexité, on obtient l’existence d’un contrôle optimal strict. on suit cette méthode théorique pour deux cas différents de diffusions dégénérée et non dégénérée."
"Le sujet principal de la Thèse est l’optimisation de la compliance des structures élastiques minces. Le problème consiste en déterminer la configuration la plus résistante, lorsqu'une quantité infinitésimale de matériau élastique est soumis à une force fixée et est confinée dans une région de volume infinitésimal.La résistance au chargement peut être mesurée en calculant une fonctionnelle de forme, la compliance, dans laquelle la forme représente le volume occupé par le matériau élastique. Donc nous sommes conduits à étudier un problème de minimisation d'une fonctionnelle de forme, sous une contrainte appropriée.Nous nous intéresserons plus particulièrement au cas où la région de design est un fil fin, représenté par un cylindre de section transverse infinitésimale. L'étude est motivée par des problèmes d'ingénierie: les structures minces sont très intéressantes d'un point de vue pratique.La stratégie utilisée tire son inspiration des travaux récents par I. Fragalà, G. Bouchitté et P. Seppecher, dans lesquels les auteurs considèrent des plaques élastiques [G. Bouchitté, I. Fragalà, P. Seppecher: Structural optimization of thin plates: the three dimensional approach., Arch. Rat. Mech. Anal. (2011)]. Cependant il faut souligner que le cas du cylindre est loin de se résumer à une variante technique du cas des plaques. Comme nous le verrons en effet, le modèle limite obtenu dans l'analyse asymptotique 3d-1d est plus riche et subtile que celui correspondant à une analyse asymptotique 3d-2d.L'étude des configurations optimales pour le modèle limite obtenu nous a conduit à une problématique nouvelle: l'existence de vraies forme optimales (donc sans apparition de structures composites) pour une barre en régime de pure torsion est liée à l'existence de solutions pour un problème non standard de frontière libre dans le plan. Ce problème représente un challenge et nous nous contenterons de donner quelques premiers résultats et perspectives.Par ailleurs, en liaison avec ce problème, nous développerons une stratégie nouvelle pour caractériser la dérivée de forme pour le minimum d'une fonction intégrale. La théorie de dérivées de forme est un sujet très largement étudié (voir e.g. la monographie de A. Henrot et M.Pierre Variations et Optimisation de Formes. Une Analyse Géométrique, Springer Berlin (2005), et les références qui y sont contenues), mais les techniques classiques qui y sont utilisées s'appuient sur des hypothèses de régularité non vérifiées dans notre cas."
"Ce travail concerne la reconstruction 3D de vaisseaux sanguins à partir de coupes transversales en nombre éventuellement réduit. Si des données sont manquantes, une reconstruction cohérente avec un réseau de vaisseaux est obtenue. Cette approche permet en outre de limiter les interventions humaines lors du traitement des images des coupes transversales 2D. Sachant que les images utilisées sont obtenues par scanner,la difficulté est de connecter les vaisseaux sanguins entre deux coupes espacées pour obtenir un graphe qui correspond au cœur des vaisseaux. En associant les vaisseaux sanguins sur les coupes à des masses à transporter, on construit un graphe solution d’un problème de transport ramifié. La reconstruction 3D de la géométrie résulte des données 2D d’imagerie issues des différentes coupes transversales et du graphe. La géométrie 3D des vaisseaux sanguins est représentée par la donnée d’une fonction Level Set définie en tout point de l’espace dont l’iso-valeur zéro correspond aux parois des vaisseaux. On s’intéresse ensuite à résoudre numériquement le modèle de Navier-Stokes en écoulement incompressible sur un maillage cartésien inclus dans la géométrie reconstruite. Ce choix est motivé par la rapidité d’assemblage du maillage et des opérateurs discrets de dérivation, en vue d’éventuelles déformation des vaisseaux. L’inadaptation du maillage avec l’interface de la géométrie amène à considérer une condition limite modifiée permettant un calcul consistant des contraintes aux parois."
"L’objectif de cette étude est de modéliser et de simuler numériquement des phénomènes d’interaction fluide-structure dans un cadre compressible pour des écoulements non-visqueux. La modélisation proposée repose sur une formulation monolithique du couplage fluide-structure en considérant une unique équation permettant de résoudre simultanément le mouvement du fluide et du solide. Un terme supplémentaire dans l’équation de quantité de mouvement traduit la présence de l’obstacle dans l’écoulement. La contribution de ce terme de pénalisation est étudiée à travers l’analogie avec une formulation variationnelle et un intérêt est porté à la rigueur physique, mathématique et numérique de l’unification des deux milieux, en particulier à l’interface. L’approche numérique correspond à une méthode à pas fractionnaire, en tout point identique aux méthodes de prédiction correction utilisées en incompressible. Quelques résultats numériques clôturent ce travail et permettent de préciser les conditions d’application de ce modèle d’interaction fluide-structure en régime compressible."
"On étudie le problème d'une inclusion élastique de grande rigidité dans un domaine 3D. Cette inclusion est d'abord vue comme un domaine géométrique de type plaque, puis plus généralement comme un domaine géométrique de type coque. On compare les modèles obtenus formellement à ceux de Chapelle–Ferent et de Bessoud et al."
"Ces dernières décennies ont vu un renouveau d’intérêt pour les matériaux composites élastiques qui s’avèrent très utiles dans la conception de structures. Pour comprendre le comportement macroscopique de ces matériaux, on fait appel aux méthodes d’homogénéisation. Dans cette thèse, nous nous intéressons à étudier rigoureusement le comportement macroscopique des matériaux composites élastiques périodiques présentant des hétérogénéités à fort contraste dans le cadre de l'élasticité linéaire. Dans un premier temps, nous étudions l’homogénéisation de structures périodiques constituées d’un matériau élastique linéaire isotrope homogène de grande rigidité. Sous certaines hypothèses sur la géométrie des structures considérées, nous montrons que leur étude peut se réduire à l’étude de systèmes discrets correspondant à des réseaux périodiques de nœuds reliés entre eux par des interactions élastiques. Ensuite, en prenant en compte les différents ordres de grandeur des raideurs en extension, en flexion et en torsion, nous montrons que l’homogénéisation des structures considérées peut conduire à des matériaux de « second gradient », c’est-à-dire, des matériaux dont l’énergie élastique homogénéisée dépend des composantes du premier gradient et du second gradient du champ de déplacement. Dans un deuxième temps, nous réalisons des essais de traction sur des structures pantographiques pour étudier la faisabilité des matériaux de second gradient."
"Dans cette thèse, nous étudions un principe général de convexification permettant de traiter certainsproblèmes variationnels non convexes sur Rd. Grâce à ce principe nous pouvons mettre en oeuvre lespuissantes techniques de dualité et ramener de tels problèmes à des formulations de type primal–dualdans Rd+1, rendant ainsi efficace la recherche numérique de minima globaux. Une théorie de ladualité et des champs de calibration est reformulée dans le cas de fonctionnelles à croissance linéaire.Sous certaines hypothèses, cela nous permet de généraliser un principe d’exclusion découvert parVisintin dans les années 1990 et de réduire le problème initial à la minimisation d’une fonctionnelleconvexe sur Rd. Ce résultat s’applique notamment à une classe de problèmes à frontière libre oumulti-phasique donnant lieu à des tests numériques très convaincants au vu de la qualité des interfacesobtenues. Ensuite nous appliquons la théorie des calibrations à un problème classique de surfacesminimales avec frontière libre et établissons de nouveaux résultats de comparaison avec sa varianteoù la fonctionnelle des surfaces minimales est remplacée par la variation totale. Nous généralisonsla notion de calibrabilité introduite par Caselles-Chambolle et Al. et construisons explicitementune solution duale pour le problème associé à la seconde fonctionnelle en utilisant un potentiellocalement Lipschitzien lié à la distance au cut-locus. La dernière partie de la thèse est consacrée auxalgorithmes d’optimisation de type primal-dual pour la recherche de points selle, en introduisant denouvelles variantes plus efficaces en précision et temps calcul. Nous avons en particulier introduit unevariante semi-implicite de la méthode d’Arrow-Hurwicz qui permet de réduire le nombre d’itérationsnécessaires pour obtenir une qualité satisfaisante des interfaces. Enfin nous avons traité la nondifférentiabilité structurelle des Lagrangiens utilisés à l’aide d’une méthode géométrique de projectionsur l’épigraphe offrant ainsi une alternative aux méthodes classiques de régularisation."
"L’objectif de cette étude est de simuler l’érosion d’un sol cohésif sous l’effet d’un écoulement incompressible. Le modèle élaboré décrit une vitesse d’érosion interfaciale qui dépend de la contrainte de cisaillement de l’écoulement. La modélisation numérique proposée est une approche eulérienne, où une méthode de pénalisation de domaines est utilisée pour résoudre les équations de Navier-Stokes autour d’un obstacle. L’interface eau/sol est décrite par une fonction Level Set couplée à une loi d’érosion à seuil.L’approximation numérique est basée sur un schéma DDFV (Discrete Duality Finite Volume) autorisant des raffinements locaux sur maillages non-conformes et non-structurés. L’approche par pénalisation a mis en évidence une couche limite d'inconsistance à l'interface fluide/solide lors du calcul de la contrainte de cisaillement. Deux approches sont proposées pour estimer précisément la contrainte de ce problème à frontière libre. La pertinence du modèle à prédire l’érosion interfaciale du sol est confirmée par la présentation de plusieurs résultats de simulation, qui offrent une meilleure évaluation et compréhension des phénomènes d'érosion"
"We propose an adaptive numerical scheme for hyperbolic conservation laws based on the numerical density of entropy production (the amount of violation of the theoretical entropy inequality). Thus it is used as an a posteriori error which provides information on the need to refine the mesh in the regions where discontinuities occur and to coarsen the mesh in the regions where the solutions remain smooth. Nevertheless, due to the CFL stability condition the time step is restricted and leads to time consuming simulations. Therefore, we propose a local time stepping algorithm. This approach is validated in the case of classical 1D numerical tests. Then, we present a 3D application. Indeed, according to an eulerian bi-fluid formulation at low Mach, an hyperbolic system of conservation laws allows an easily parallelization and mesh refinement by "" blocks "" ."
"La modélisation de vagues et de leur impact côtier et offshore (déferlement, interactions avec les structures, tsunami) reste un problème difficile à appréhender du fait de la complexité des phénomènes physiques mis en jeu. Dans cette thématique, une étude numérique des processus physiques est effectuée dans le cadre de cette thèse. L’objectif de la thèse est ainsi d’améliorer le domaine de validité du code en y développant des méthodes numériques performantes qui permettraient une grande précision des résultats des simulations et des gains en temps de calcul. Le modèle numérique utilisé repose sur les équations d'Euler 3D multi-fluides. Une méthode de compressibilité artificielle permet une approche explicite et une parallélisation efficace. Le modèle bi-fluide à faible Mach, déjà validé avec des données expérimentales, repose sur une approximation par volumes finis avec un schéma de Godunov du second ordre en temps et en espace. Dans le cadre de nos travaux, une modification de la technique d’intégration en temps du solveur basée sur l’intégration d’Adams-Bashforth multi-pas avec une approche multi-échelle dans laquelle le pas de temps est ajusté à la taille locale du maillage et une méthode de compression d’interface pour une meilleure précision de l’interface entre les fluides sont implémentées dans le code. Ces méthodes numériques ont été validées avec des mesures expérimentales dans le cas d’un déferlement 2D et de la rupture 3D de barrage avec obstacle. Des comparaisons expérimentales et numériques ont permis de constater la pertinence des développements apportés au logiciel avec une amélioration de la précision des résultats et une diminution des temps de calcul."
"Cette thèse est consacrée à la modélisation des structures fibreuses avec des milieuxcontinus généralisés. Dans l’Introduction, l'état de l'art concernant les milieuxcontinus généralisée et applications aux structures fibreuses sont décrits et lesproblèmes ouverts pertinents sont mis en évidence. Dans le Chapitre 1 et 2, uneprocédure d'homogénéisation rigoureuse basée sur des arguments de Gammaconvergenceest appliquée à une structure en treillis et à un model de poutrediscrétisé. Dans le Chapitre 3, un traitement variationnel est utilisé pour formuler unapproche favorable du point de vue numérique. Dans le Chapitre 4 sont discutées lesrésultats expérimentaux concernant le comportement de la structure dans différentstypes de déformation. Cela à motivé les études effectuées dans le Chapitre 5, ou lesMéthodes directes de calcul des variations sont appliquées à poutres d’Euler engrandes déformations."
"Nous considérons les équations différentielles stochastiques (EDS) de Mc Kean-Vlasov, qui sont des EDS dont les coefficients de dérive et de diffusion dépendent non seulement de l'état du processus inconnu, mais également de sa loi de probabilité. Ces EDS, également appelées EDS à champ moyen, ont d'abord été étudiées en physique statistique et représentent en quelque sorte le comportement moyen d'un nombre infini de particules. Récemment, ce type d'équations a suscité un regain d'intérêt dans le contexte de la théorie des jeux à champ moyen. Cette théorie a été inventée par P.L. Lions et J.M. Lasry en 2006, pour résoudre le problème de l'existence d'un équilibre de Nash approximatif pour les jeux différentiels, avec un grand nombre de joueurs. Ces équations ont trouvé des applications dans divers domaines tels que la théorie des jeux, la finance mathématique, les réseaux de communication et la gestion des ressources pétrolières. Dans cette thèse, nous avons étudié les questions de stabilité par rapport aux données initiales, aux coefficients et aux processus directeurs des équations de McKean-Vlasov. Les propriétés génériques de ce type d'équations stochastiques, telles que l'existence et l'unicité, la stabilité par rapport aux paramètres, ont été examinées. En théorie du contrôle, notre attention s'est portée sur l'existence et l'approximation de contrôles relaxés pour les systèmes gouvernés par des EDS de Mc Kean-Vlasov."
Nous donnons une description fine des zones capillaires à l'aide de la théorie du second gradient ce qui permet de clarifier les phénomènes de tension superficielle et de tension de ligne. Nous montrons comment par un processus d'intégration à travers les zones capillaires cellesci peuvent être considérées comme des surfaces ou des lignes géométriques porteuses de propriétés matérielles. Pour obtenir des conditions de bilan physiquement acceptables nous faisons une analyse des ordres de grandeur des termes interfaciaux. Enfin nous utilisons la thermodynamique linéaire des phénomènes irréversibles pour écrire les lois de comportement des interfaces et des lignes de contact
"Cette thèse porte sur la modélisation et l'analyse mathématique de modèles asymptotiques utilisés en océanographie et décrivant la propagation des ondes longues.L'objectif de cette thèse est de construire et de justifier de nouveaux modèles asymptotiques en tenant compte de la variation de la topographie et de la section transversale.Pour ce faire, plusieurs hypothèses sont formulées sur la profondeur de l'eau et les déformations de la section transversale. La première partie de cette thèse consiste à mettre le problème en équations et à trouver des modèles asymptotiques et à les étudier mathématiquement, voir l'analyse linéaire de la dispersion et de shoaling.Dans la deuxième partie, un modèle unidimensionnel des ondes longues, moyennées par section, est développé. Des équations tridimensionnelles du mouvement des fluides non visqueux et incompressibles sont d'abord intégrées sur une section transversale du canal, ce qui donne les équations de type SGN. Le nouveau modèle est donc adéquat pour décrire des ondes fortement non linéaires et faiblement dispersives le long d'un canal de section transversale arbitraire et non uniforme. Plus précisément, le nouveau modèle étend le modèle de Saint-Venant à moyenne de section et généralise les équations de Serre-Green-Naghdi à toute section.Ce nouveau modèle a été reformulé d'une manière plus appropriée pour la résolution numérique en conservant le même ordre de précision que l'original et en améliorant ses propriétés de dispersion. Enfin, nous présentons quelques simulations numériques pour étudier l'influence du changement de section sur la propagation d'une onde solitaire.La dernière partie de cette thèse est consacrée à la simulation numérique du modèle SGN avec une nouvelle reformulation."
"Nous étudions l'homogénéisation d'un milieu élastique renforcé périodiquement par des fibres de beaucoup plus grande rigidité que la matrice. Nous supposons que les fibres sont parallèles, cylindriques, de section circulaire, et de rayon beaucoup plus petit que la distance de séparation des fibres. Ces hypothèses diffèrent de celles faites dans le cadre classique de la théorie de l'homogénéisation et conduisent à un résultat très différent: nous montrons que, lorsque le rayon des fibres tend vers zéro et que les coefficients de Lamé du matériau les constituant tendent vers l'infini avec des vitesses adaptées, le milieu homogénéisé se comporte comme un milieu de second gradient, c'est-à-dire un milieu dont l'énergie dépend du second gradient du déplacement."
"Dans cette thèse, nous étudions les écoulements de fluides compressibles décrits par les équations de Navier-Stokes-Fourier dans les cas stationnaire et instationnaire et avec des conditions de bord assurant l’isolation thermique et mécanique du fluide. On commence par le cas stationnaire barotrope et des conditions de Navier à la frontière du domaine. La pression est donc de la forme p(%) = % où est appelé coefficient adiabatique et nous arrivons à montrer l’existence de solutions faibles pour > 1.On généralise ensuite ce résultat aux équations de Navier-Stokes-Fourier avec conduction de la chaleur et glissement (partiel ou total) au bord, toujours dans le cas stationnaire. On montre cette fois-ci l’existence de solutions faibles particulières appelées solutions entropiques variationnelles respectant l’inégalité d’entropie pour > 1 et l’existence de solutions faibles respectant le bilan de l’énergie totale au sens faible pour > 5/4. On travaille ensuite sur les écoulements instationnaires décrits par les équations de Navier-Stokes-Fourier sur une large variété de domaines non bornés, tout d’abord pour des conditions de bord d’adhérence puis pour des conditions de Navier à la frontière (ce qui restreintquelque peu la diversité des domaines non bornés admissibles). On arrive à montrer l’existence de solutions faibles particulières respectant l’inégalité d’entropie et une inégalité de dissipation remplaçant l’égalité de conservation d’énergie totale dans le volume qui n’a plus de sens dans les domaines non bornés. Par après, on met en place une inégalité dite d’entropie relative dont on montre qu’elle est respectée par certaines des solutions faibles exhibées auparavant. Ces solutions sont appelées solutions dissipatives. On parvient à prouver que pour chaque donnée initiale, il existe au moins une solution dissipative. Cette inégalité d’entropie relative nous permet de démontrer le principe d’unicité forte-faiblepour nos solutions dissipatives. Précisément, cela signifie qu’une solution dissipative et une solution forte issues des mêmes données initiales coïncident sur le temps maximal d’existence de la solution forte. La propriété d’unicité forte-faible donne un fondement à la notion de solution dissipative pour les domaines non bornés."
"Cette thèse porte sur la modélisation et l'analyse mathématique de modèles asymptotiques utilisés en océanographie et décrivant la propagation des ondes longues. L'objectif de cette thèse est de construire et de justifier de nouveaux modèles asymptotiques en tenant compte de la variation de la topographie et de la section transversale. Pour ce faire, plusieurs hypothèses sont formulées sur la profondeur de l'eau et les déformations de la section transversale. La première partie de cette thèse consiste à mettre le problème en équations et à trouver des modèles asymptotiques et à les étudier mathématiquement, voir l'analyse linéaire de la dispersion et de shoaling. Dans la deuxième partie, un modèle unidimensionnel des ondes longues, moyennées par section, est développé. Des équations tridimensionnelles du mouvement des fluides non visqueux et incompressibles sont d'abord intégrées sur une section transversale du canal, ce qui donne les équations de type SGN. Le nouveau modèle est donc adéquat pour décrire des ondes fortement non linéaires et faiblement dispersives le long d'un canal de section transversale arbitraire et non uniforme. Plus précisément, le nouveau modèle étend le modèle de Saint-Venant à moyenne de section et généralise les équations de Serre-Green-Naghdi à toute section. Ce nouveau modèle a été reformulé d'une manière plus appropriée pour la résolution numérique en conservant le même ordre de précision que l'original et en améliorant ses propriétés de dispersion. Enfin, nous présentons quelques simulations numériques pour étudier l'influence du changement de section sur la propagation d'une onde solitaire. La dernière partie de cette thèse est consacrée à la simulation numérique du modèle SGN avec une nouvelle reformulation."
Nous établissons de nouvelles majorations et minorations pour le nombre de points rationnels des variétés abéliennes et des Jacobiennes sur un corps fini. Nous déterminons de plus les nombres maximum et minimum de points rationnels des surfaces Jacobiennes sur un corps fini donné. We give upper and lower bounds for the number of points on abelian and Jacobian varieties over finite fields. We also determine the values for the maximum and minimum number of points on Jacobian surfaces on a given finite field.
"Cette thèse porte sur deux éléments actuellement incontournables de la cryptographie à clé publique, qui sont l’arithmétique modulaire avec de grands entiers et la multiplication scalaire sur les courbes elliptiques (ECSM). Pour le premier, nous nous intéressons au système de représentation modulaire adapté (AMNS), qui fut introduit par Bajard et al. en 2004. C’est un système de représentation de restes modulaires dans lequel les éléments sont des polynômes. Nous montrons d’une part que ce système permet d’effectuer l’arithmétique modulaire de façon efficace et d’autre part comment l’utiliser pour la randomisation de cette arithmétique afin de protéger l’implémentation des protocoles cryptographiques contre certaines attaques par canaux auxiliaires. Pour l’ECSM, nous abordons l’utilisation des chaînes d’additions euclidiennes (EAC) pour tirer parti de la formule d’addition de points efficace proposée par Méloni en 2007. L’objectif est d’une part de généraliser au cas d’un point de base quelconque l’utilisation des EAC pour effectuer la multiplication scalaire ; cela, grâce aux courbes munies d’un endomorphisme efficace. D’autre part, nous proposons un algorithme pour effectuer la multiplication scalaire avec les EAC, qui permet la détection de fautes qui seraient commises par un attaquant que nous détaillons."
"Cette thèse s'est intéressée aux processus hydrodynamiques dans une baie semi-fermée telle que la Rade d Toulon et leurs importances pour la dispersion de contaminants dissous. Pour cette étude, une configuration à très haute résolution (100 m de résolution spatiale) nommée TBAY100, basée sur le modèle de circulation océanique MITgcm a été mise en place. Un emboîtement multi-modèle a été effectué pour arriver à une telle résolution, en partant d'une configuration NEMO-GLAZUR64 à 1.3 km de la Méditerranée Nord-Occidentale puis NEMO-NIDOR à 400 m du littoral Varois pour forcer correctement les frontières de TBAY100. Dans un premier temps, une analyse mathématique a permis de quantifier les échanges d'énergie pour un système simplifié pour ensuite étendre cette réflexion à la Rade de Toulon et mieux comprendre les échanges aux frontières ouvertes du domaine. Cette configuration a été ensuite validée avec diverses observations dont des données d'ADCP et des trajectoires de flotteurs qéolocalisables dérivants. Des schémas de circulation typiques dépendant des conditions hydrodynamiques et météorologiques ont été dégagés. Dans un deuxième temps, nos recherches se sont portées sur les processus de distribution de polluants en s'appuyant sur des prélèvements chimiques, principalement le cuivre relargué par les peintures-antifouling. Les hypothèses de contamination (sources et taux) ont fait l'objet d'un travail collaboratif avec une équipe de chimie. Le transport des contaminants a été analysé à l'aide de traceurs passifs implémentés dans TBAY100 et a abouti à 4 schémas de dispersion de polluants indiquant un export soit vers le Courant Nord, soit vers le Parc National Marin de Port-Cros, ainsi qu'une rétention des polluants dans la Grande Rade de Toulon. Cette maquette pourra avoir d'autres applications sociétales importantes puisqu'elle peut servir d'outil de prédiction de courants et de dispersion de contaminant dans la Rade de Toulon."
"Nous étudions, dans le cadre de la Γ-convergence et de l'homogénéisation périodique de matériaux fortement contrastés, des structures cylindriques constituées d'un unique matériau élastique linéaire et de vide. L'intérêt actuel pour l'homogénéisation à fort contraste est important mais en général des hypothèses ad hoc sont faites de manière à obtenir un modèle limite qui reste dans le cadre de l'élasticité classique. Nous cherchons, au contraire, à obtenir des énergies homogénéisées prenant en compte des effets de second gradient du déplacement (ou, de manière équivalente, de gradient de la déformation). Nous montrons d'abord que l'étude des structures considérées peut se réduire à l'étude de systèmes discrets correspondant à des réseaux périodiques de noeuds liés par des interactions élastiques. Notre étude de tels réseaux diffère de celles que l'on peut trouver dans la littérature par le fait que nous prenons en compte la différence d'ordre de grandeur des raideurs à l'extension et à la flexion des éléments élancés qui relient les noeuds du réseau. Cela nous permet de traiter des structures qui auraient été mobiles si l'on avait négligé les raideurs en flexion et complètement rigides si l'on avait considéré qu'elles étaient du même ordre de grandeur que les raideurs en extension. À notre connaissance, cette étude est le premier résultat rigoureux d'homogénéisation dans lequel l'énergie limite peut dépendre de toutes les composantes du second gradient du déplacement."
"Les écoulements en milieux poreux non-saturés sont modélisés par l'équation de Richards qui est une équation non-linéaire parabolique dégénérée. Ses limites et les défis que soulèvent sa résolution numérique sont présentés. L'obtention de résultats robustes, précis et efficaces est difficile en particulier à cause des fronts de saturation raides et dynamiques induits par les propriétés hydrauliques non-linéaires. L'équation de Richards est discrétisée par une méthode de Galerkine discontinue en espace et des formules de différentiation rétrograde en temps. Le schéma numérique résultant est conservatif, d'ordre élevé et très flexible. Ainsi, des conditions aux limites complexes sont facilement intégrées comme la condition de suintement ou un forçage dynamique. De plus, une stratégie adaptative est proposée. Un pas de temps adaptatif rend la convergence non-linéaire robuste et un raffinement de maillage adaptatif basée sur des blocs est utilisée pour atteindre la précision requise efficacement. Un indicateur d'erreur a posteriori approprié aide le maillage à capturer les fronts de saturation raides qui sont également mieux approximés par une discontinuité introduite dans la solution grâce à une méthode de Galerkine discontinue pondérée. L'approche est validée par divers cas-tests et un benchmark 2D. Les simulations numériques sont comparées à des expériences de laboratoire de recharge/drainage de nappe et une expérience à grande échelle d'humidification, suite à la mise en eau du barrage multi-matériaux de La Verne. Ce cas exigeant montre les potentialités de la stratégie développée dans cette thèse. Enfin, des applications sont menées pour simuler les écoulements souterrains sous la zone de jet de rive de plages sableuses en comparaison avec des observations expérimentales."
"Nous considérons une digue en terre soumise à un écoulement de surverse. La modélisation physique et numérique d’un tel phénomène est en général un compromis entre pertinence physique, précision numérique et temps de calcul. Nous nous sommes attachés à l’analyse de l’écoulement à surface libre caractéristique d’une surverse, éludant momentanément les phénomènes d’érosion et de transport sédimentaire qui seront inclus ultérieurement par couplage. L’hydrodynamique complexe associée à la surverse est traitée grâce à un modèle bi-fluide Eulérien à faible nombre de Mach. Ce modèle bi-fluide, non visqueux et faiblement compressible est physiquement plus pertinent que des modèles de type Saint-Venant généralement utilisés. En effet, on observe dans l’écoulement des jets et des recirculations plus facilement appréhendés par un modèle diphasique. Ce modèle est évidemment moins riche qu’un modèle de Navier-Stokes avec ou sans turbulence, mais sa formulation permet des simulations tridimensionnelles beaucoup plus économes en temps de calcul pour une représentation hydrodynamique réaliste. En effet, la formulation hyperbolique du problème à résoudre autorise l’utilisation d’un solveur volumes finis explicite parallèle rapide. Ce solveur bénéficie d’un outil de raffinement dynamique de maillage par blocs sur critère de production numérique d’entropie, qui permet d’optimiser le nombre d’inconnues, et donc le temps de calcul, tout en augmentant la précision de la simulation dans les zones d’intérêt. Ce modèle est appliqué à l’étude de la surverse d’une digue expérimentale. Le parement aval de la digue est considéré lisse ou bien avec un profil en marches représentatif des observations expérimentales. Grâce au raffinement dynamique de maillage, l’établissement de l’écoulement est simulé rapidement. Puis, lorsque l’écoulement stationnaire est atteint, les hauteurs et champs de vitesse sont confrontés avec succès à quelques mesures expérimentales. Ces résultats sont également comparés avec des simulations réalisées avec un code commercial utilisant un modèle Navier-Stokes diphasique avec turbulence k-ωSST."
"L’objet de l’étude est le système de Serre-Green-Naghdi afin de modéliser des écoulements incompressibles à surface libre en peu profondeur, tout en tenant compte les effets de la pression non-hydrostatique et les phénomènes dispersifs qui jouent un rôle important en particulier en océanographie côtière. Plusieurs schémas numériques sont présentés, basés sur une méthode de splitting, en séparant la partie hyperbolique (prédiction) et la partie non-hydrostatique (correction). Les algorithmes sont comparés en temps de calcul à travers des simulations avec des ondes solitaires. Le modèle est également validé avec des données expérimentales concernant les ondes de Favre [11]."
"Dans cette thèse, on se propose d'étudier rigoureusement le comportement macroscopique de matériaux composites fortement contrastés dans le cadre de l'électromagnétisme. Nous considérons des structures constituées de micro-inclusions réparties périodiquement (ou aléatoirement), au sein desquelles un matériau de très grande permittivité, ou de très grande conductivité, sera disposé. En pratique, une telle structure occupe un domaine borné 3D et est éclairée par une onde incidente monochromatique (de fréquence xée) venant de l'inni. Notre approche mathématique consiste à passer à la limite dans le système de Maxwell décrivant le problème de diffraction lorsque la distance séparant les inclusions tend vers zéro, et que l'indice électromagnétique des inclusions tend vers l'infini (`fort contraste'). Nous étudions deux types de structures diffractantes 3D qui permettent de réaliser des matériaux de permittivité ou perméabilité négatives. L'étude asymptotique et basée sur la méthode de convergence double-échelle (parfois dans une variante stochastique), et les problèmes sur la cellule de périodicité qui en résultent sont résolus par méthode spectrale. Ceci permet d'obtenir explicitement les tenseurs effectifs en fonction de la fréquence, mettant ainsi en évidence leurs grandes variations autour de fréquences de résonances."
"Nous présentons une nouvelle méthode de reconnaissance et de maillage d’un domaine d’intérêt d’une image médicale. Notre approche se base sur une méthode de segmentation à partir d’un atlas, et dépend de la boîte à outils modulaire pour la co-registration d’images développée par S. Bertoluzza et al. [25]. Le maillage du domaine spécifique au patient est généré par déformation du maillage correspondant sur une image de référence segmentée a priori (l’atlas). Notre méthode vise à automatiser le processus à l’interface entre l’imagerie médicale et la simulation numérique, avec pour but de réduire le coût de calcul dans les situations dans lesquelles des simulations doivent être faites sur de nombreuses images similaires."
no abstract
"Dans cet article, nous étudions un nouveau modèle de relaxation homogène qui décrit le comportement d'un fluide diphasique en régime à bas nombre de Mach. Ce modèle peut être obtenu comme la limite asymptotique à bas nombre de Mach du modèle HRM. Pour ce modèle, nous définissons une équation d'état décrivant la thermodynamique du fluide diphasique. Nous montrons quelques propriétés théoriques vérifiées par les solutions du modèle, et introduisons un schéma équilibre. Par la suite, nous nous intéressons au régime de relaxation instantané, et nous montrons la convergence formelle de ce modèle vers l'approximation à bas nombre de Mach du modèle HEM. Nous introduisons un schéma préservant l'asymptotique permettant des simulations numériques du couplage spatial entre deux régions présentant des temps caractéristiques de relaxation différents."
fr_abstract_s
"Les matériaux composites à matrices thermodurcissables et renforcés de fibres de verre ont des caractéristiques thermomécaniques élevées à l'état initial. Le développement de ces structures composites est étroitement liée à une meilleure connaissance de leur comportement en service face au vieillissement hygrothermique en particulier. La sensibilité des interfaces à l'humidité est retenue comme étant une cause majeure de la chute des propriétés mécaniques. La démarche de ce travail de thèse consiste à mener des investigations sur des matériaux composites unidirectionnels, permettant ainsi une sollicitation préférentielle de l'interface fibre/matrice. Une étude fine de l'interphase est menée afin de réunir des informations précises sur l'étendue de cette zone, sa structure et sa sensibilité vis-à-vis du vieillissement. L'étude des comportements de ces composites repose sur l'utilisation de techniques expérimentales de caractérisation à différentes échelles. Trois systèmes différents par leur mobilité moléculaire et leur comportement face au vieillissement sont étudiés. A l'état initial, l'étendue de la zone interfaciale a été estimée par la micro-analyse thermique localisée qui montre une augmentation de la mobilité moléculaire depuis la matrice jusqu'aux monofilaments. Quelle que soit la nature de la matrice, l'interphase a été trouvé sous-réticulée et /ou plastifiée. L'étude du vieillissement hygrothermique passe par la détermination des cinétiques d'absorption d'eau. Le modèle de Langmuir à deux phases est en accord avec les évolutions physiques observées. Dans les premiers temps de vieillissement, les molécules d'eau se lient par simple liaison hydrogène aux sites hydrophiles entraînant la plastification. Puis elles s'organisent en créant un réseau secondaire par des ponts intermoléculaires. La microanalyse thermique révèle en effet un réseau plus dense dans l'interphase en lien avec la chute des propriétés mécaniques du composite fragilisé à l'interface. La mobilité moléculaire et la nature de la matrice influencent donc le comportement hygrothermique."
"En tant qu’espèces ingénieures de leurs écosystèmes et que producteurs primaires, les macroalgues marines jouent un rôle crucial au sein de leur écosystème. Les interactions chimiques avec leurs microorganismes épiphytes semblent particulièrement essentielles pour leur physiologie. Cependant les relations macroalgues-microbiote et le rôle des paramètres environnementaux dans ces interactions restent encore peu explorées. L’objectif général de la thèse est de comprendre comment varie la structure de la communauté procaryotique épiphyte de l’algue brune Taonia atomaria en lien avec les variations de la production métabolique de surface de l’hôte et quelle est l’influence de l’environnement sur ces variations qui affectent et façonnent ce modèle d’holobionte. Une approche multi-omiques, couplant l’étude des communautés procaryotes épiphytes par metabarcoding et l’étude des métabolites de surface par des analyses optimisées en métabolomique, a ainsi été employée conjointement avec d’autres analyses telle que la cytométrie en flux. Les résultats obtenus ont révélé que la communauté microbienne épiphyte de T. atomaria, lui était spécifique en comparaison avec les communautés de biofilms de substrats rocheux et celles planctoniques, suggérant un rôle possible du métabolome de surface. Entre outre, d’importantes co-variations entre le métabolome et le microbiote à la surface de l’algue ont été observées à différents niveaux, que ce soit à l’échelle du thalle, de la dynamique temporelle ou encore d’un point de vue biogéographique. Certains paramètres environnementaux semblent être particulièrement impliqués dans les interactions au sein de l’holobionte, tels que la température, la contamination en cuivre, mais aussi l’intensité lumineuse. Dans un contexte de Changement Global, ces travaux apportent de nouvelles perspectives permettant de mieux appréhender la dynamique des macroalgues-holobiontes."
"Alors que les premières formes de vie sont apparues dans les océans il y a près de 4 milliards d'années, les premières espèces terrestres remontent seulement à 400 millions d'années. Malgré cela, les substances naturelles d'origine marine ne représentent que 10% de l'ensemble des molécules isolées à ce jour à partir d'organismes vivants. Ces composés chimiques sont pourtant très spécifiques du fait des particularités du Monde Marin. Au sein des organismes qui les biosynthétisent, ils peuvent notamment intervenir dans la défense chimique contre les parasites et les autres compétiteurs. Ces molécules apparaissent donc comme des alternatives potentielles aux oxydes du tributylétain (TBTO) présents dans les revêtements antifouling et interdits depuis 2008 en raison de leur toxicité. Dans ce contexte et dans le cadre d'un partenariat avec le Parc national de Port-Cros, l'étude de la composition chimique de plusieurs organismes marins méditerranéens, des algues vertes (Caulerpa taxifolia) et brunes (Dictyota sp., Dictyota dichotoma, Cystoseira foeniculacea) ainsi que des bryozoaires, a été entreprise. Ces travaux ont permis l'isolement et la caractérisation structurale d'une vingtaine de métabolites secondaires (dérivés terpéniques et lipidiques, stérols), parmi lesquels huit sont originaux. L'identification de l'ensemble de ces molécules a été réalisée par le biais de l'utilisation de différentes méthodes spectroscopiques (RMN 1D et 2D, SM-HR) et la stéréochimie de certaines d'entre-elles a été établie par RMN (expérience NOESY 1H-1H) et modélisation moléculaire. Des corrections de données spectrales ont été également proposées pour cinq composés connus. Par ailleurs, l'activité anti-adhésion de plusieurs métabolites issus d'algues a été évaluée vis-à-vis d'un biofilm bactérien marin (Pseudoalteromonas sp.) afin de déterminer leur potentiel en tant qu'agents antifouling : plusieurs de ces composés présente une activité intéressante (CE50 = 30 μM), certes inférieure à celle du TBTO (CE50 = 10 μM), mais largement supérieure à celles de co-biocides utilisés actuellement. Parallèlement à ces travaux, l'analyse des variations temporelles et spatiales de l'expression métabolique de certaines des espèces étudiées a été réalisée à des fins écologiques. D'une part, les extraits mensuels de C. taxifolia présentent généralement une activité anti-adhésion importante quand le taux de caulerpényne dans l'extrait est élevé. D'autre part, d'importantes fluctuations ont notamment été observées dans la composition chimique des bryozoaires Myriapora truncata et Pentapora fascialis en fonction de leur lieu de récolte."
"La durabilité des revêtements organiques constitue une problématique industrielle et scientifique importante. La durée de vie d'un revêtement dépend en grande partie de son adhérence sur son substrat. L'objectif de ce travail consiste à comprendre les phénomènes d'interphase et d'adhérence entre un revêtement anticorrosion et un substrat métallique. Nous chercherons également à valider des traceurs de la dégradation du revêtement et du développement d'une délamination et/ou d'une corrosion à l'interface métallique. Ce travail expérimental est mené, à la fois, sur un revêtement non-chargé, dont on fait varier la stœchiométrie, et sur un primaire anticorrosion commercial. Tout deux sont formés par le même liant polymère DGEBA- Polyamidoamine. Le revêtement anticorrosion incorpore dans sa formulation un terpolymère vinylique ainsi que deux pigments anticorrosion : l'oxyde de fer et le phosphate de zinc. Le talc constitue sa charge principale. Différentes méthodes sont employées pour caractériser l'interphase des systèmes DGEBA/ Polyamidoamine appliqués sur acier: l'infrarouge à transformée de Fourrier (IRTF), la micro analyse thermique (µTA) et une méthode basée sur des mesures de déflexion. Nous observons des différences de comportement entre les propriétés d'interphase et celles du revêtement massique. Les limites inhérentes à la méthode de caractérisation par les mesures de déflexion conduisent à une surestimation de l'épaisseur de l'interphase. Les caractérisations par 1R et par µT A ont perm is d'observer l'interphase à une échelle plus faible. Nous observons ainsi par IR une forte présence d'amine sur une épaisseur 40 µm correspondant à la zone d'interphase. Cet excès d'amine est également confirmé par les résultats de µTA qui montrent une diminution de la Tg du revêtement massique à proximité de l'interface métallique. Suite à un vieillissement hygrothermique cyclique, on observe la disparition de la zone d'interphase. Par ailleurs, une étude en spectroscopie d'impédance électrochimique (SIE) permet de suivre les évolutions des propriétés de protection du revêtement. Pour cela, des mesures de résistances des pores et de transfert de charge ainsi que de la capacité du revêtement sont conduites en fonction de la durée d'immersion et du vieillissement hygrothermique préalable. Ces évolutions mettent en évidence 3 étapes pour la perte des propriétés de protection ainsi que le contrôle des phénomènes à l'interface métallique par les propriétés barrières. Enfin, des mesures d'adhérence (POT et test au solvant NMP) complètent et confirment les résultats précédents en précisant le rôle bénéfique des amines sur l'adhérence et sur sa réversibilité après séchage."
"La pollution des eaux par les métaux est devenue un problème majeur. Il est donc nécessaire de contrôler régulièrement la qualité des eaux de consommation et de rejet, selon les fréquences et les paramètres d'analyses définis par le décret européen -2001-1220 de Décembre 2001. Face à de nouvelles normes de plus en plus exigeantes, la sensibilité et la limite de détection des appareillages (ICP-AES par exemple) deviennent problématiques pour la quantification des métaux. Une étape de préconcentration préliminaire à l'analyse serait une alternative. Parmi les différentes méthodes de préconcentration, l'extraction en phase solide chélatante est particulièrement adaptée à l'extraction d'espèces métalliques en milieu aqueux. Elle fait intervenir des interactions entre une phase liquide et un support solide poreux modifié par un ligand, générant un mécanisme de rétention spécifique vis-à-vis de l'analyte (Chélation). Différents types de supports solides peuvent être employés en SPE. Les polymères organiques de synthèse et plus particulièrement les copolymères à base de styrène et de divinylbenzène (EVBDVB) offrent les performances attendues pour une telle application : stabilité sur une large gamme de pH, bonnes propriétés chimiques, physiques et thermiques. Afin d'améliorer les propriétés complexantes des copolymères EVB-DVB vis-à-vis des métaux, l'introduction d'un ligand est nécessaire pour obtenir un support poreux chélatant. Dans le cadre de ces travaux, le 1,2- benzènediol (catéchol) a été sélectionné pour sa capacité à retenir les espèces métalliques et parce qu'il permet d'envisager deux voies d'incorporation : 1. Greffage du catéchol sur un copolymère EVB-DVB commercial (Amberlite® XAD4 de Rohm et Haas) via des ponts imines réduits et diazonium ; 2. Copolymérisation en suspension d'un monomère styrénique comportant le catéchol avec du divinylbenzène. Les supports solides fonctionnalisés issus des deux modes de préparation sont caractérisés par ATG-DSC, Py-GC/MS et spectroscopie IRTF afin de valider l'incorporation du catéchol. Les propriétés complexantes pour le Cd(II), Cu(II), Ni(II) et Pb(II) ont été déterminées par ICP-AES et sont reliées aux taux de fonctionnalisation (évalués par dosage acido-basique en retour des fonctions hydroxyles du catéchol) et aux propriétés texturales (surface spécifique, volume poreux, diamètres de pores déterminés par adsorption/désorption de gaz). Par rapport au greffage, la copolymérisation en suspension permet de faire varier la quantité de ligand introduit, de conserver la sphéricité des supports et de contrôler la taille des particules, en réduisant à la fois le coût et les contraintes de mise en oeuvre. Ce procédé de préparation a permis d'aboutir à des matériaux originaux (peu colorés) ouvrant des perspectives notamment en terme d'application : détection directe des métaux retenus par spectroscopie de réflexion diffuse."
"Ce travail de thèse porte sur la synthèse et la caractérisation d'une nouvelle classe de tensioactif hybride organiques/inorganiques POSS (Polyhedral Oligomeric SilSesquioxane) greffé poly(oxyde d'éthylène), désigné POSS-POE. Ces composés ont été utilisés comme agent tensioactif dans l'élaboration de formulations époxy-amine à base aqueuse. Parmi les différentes voies de synthèse des POSS-POE étudiées, la voie qui consiste à greffer la chaîne POE et les groupements hydrophobe par hydrosilylation de l'octa(diméthylsiloxy)-octasilsesquioxane (Q8M8H) a été privilégiée. En effet cette méthode nous a permis d'élaborer des POSS amphiphiles à balance hydrophile/hydrophobe facilement modulable. Les composés R7Q8M8-POE avec R (alkyle) de C5 à C8 et POE de 350 à 5000 g/mol ont été synthétisés et caractérisés par RMN 1H, 13C et 29Si. Des chaînes alkyle ramifiées ont été également testées comme hydrophobe afin d'évaluer l'influence de la ramification des groupements alkyle sur les propriétés des POSS POE. L'analyse des POSS POE par diffraction de rayons X aux grands angles (WAXD) a permis de mettre en évidence que les chaînes POE cristallisent selon la même structure cristalline que dans l'homopolymère POE. Même si les cages POSS sont exclues des lamelles cristallines, un ordre à courte distance entre les cages POSS avec un certain degré d'interdigitation entre les chaînes alkyle a pu être mis en évidence. Le degré d'organisation des cages POSS dépend de la longueur de la chaîne POE. La stabilité thermique des POSS POE sous air et sous azote a été étudiée par analyse thermogravimétrique (ATG). Sous azote, les cubes R7Q8M8 ont tendance à augmenter la stabilité thermique des chaînes POE de faible masse molaire (350 g/mol) mais ont un effet déstabilisant dans le cas des chaînes POE plus longues (> 350 g/mol). Sous air, les POSS POE les plus stables présentent une chaîne alkyle hydrophobe en C6. La ramification des chaînes alkyle a un effet négatif sur la stabilité thermique. Le comportement associatif des POSS POE dans l'eau a été étudié par viscosimétrie et par mesure de solubilisation de molécule sonde hydrophobe. La formation d'agrégats micellaires à partir d'une concentration comprise entre 10-4 et 4x10-4 mol/L, selon la longueur des chaînes POE et des groupements hydrophobes, a été mise en évidence. Les propriétés émulsifiantes de ces POSS POE vis-à-vis d'un prépolymère DGEBA sont comparables à un tensioactif non ionique conventionnel nonylphénoxylpolyéthoxyéthanol. Un système époxy amine à base aqueuse incorporant ces unités POSS POE a été développé. Les films réticulés obtenus présentent de bonnes propriétés thermiques et une hydrophilie de la surface nettement inférieure en comparaison des films préparés à partir de l'émulsifiant conventionnel."
"Le contrôle du biofouling sur des surfaces inertes immergées ou en atmosphère humide est une nécessité dans le secteur marin, tant pour des raisons économiques qu’environnementales. La formation de biofilm microbien, étape préalable à la formation du biofouling, est souvent intrinsèquement liée chez les bactéries au système de communication “Quorum Sensing” (QS). Chez certaines bactéries Gram négatif, le QS est basé sur la perception de petites molécules diffusibles appelées N- Acyl Homosérine Lactones (AHLs). L’une des stratégies antifouling en voie de développement de nos jours repose sur l’inhibition du QS bactérien. L’objectif de cette thèse est d’utiliser certaines bactéries marines afin d’identifier des molécules anti-QS capables de perturber la formation de biofilm. Ce travail a donc porté sur la mise en évidence de molécules AHLs impliquées dans le QS chez certaines bactéries marines isolées de la rade de Toulon, l’étude de la modulation de certains phénotypes dont la formation du biofilm, par ces molécules et, la mise en place d’un test préliminaire d’inhibition du QS. Parmi les trois bactéries isolées de la rade de Toulon (TC8, TC14 et TC15) du genre Pseudoalteromonas, connues pour produire de nombreuses molécules actives, et testées pour leur capacité à sécréter des AHLs, seule Pseudoalteromonas sp. TC15 a produit la C12-HSL. P. ulvae TC14, capable de produire un biofilm conséquent et de la violacéine, ne produit aucune AHL. Afin d’évaluer la possibilité d’utiliser une bactérie marine comme outil de criblage anti-QS, interférant avec les AHLs et les conséquences sur son biofilm, des AHLs exogènes ont été testées sur la production de violacéine, la formation de biofilm et la mobilité de TC14. Certaines AHLs ont montré qu’elles pouvaient réguler la production de violacéine et la formation de biofilm chez TC14, suggérant l’existence d’un récepteur AHLs fonctionnel. Des tests préliminaires d’inhibition du QS ont été effectués avec des molécules commerciales et des analogues synthétiques. La 3-oxo-C6-HSL commerciale, ainsi que l’esculétine et la p- benzoquinone, connues pour interférer avec le QS bactérien, ont été capables d’inhiber la production de violacéine ainsi que la formation de biofilm de TC14 à des concentrations n’affectant pas sa croissance. Cette étude suggère donc que P. ulvae TC14 pourrait être utilisée comme un outil de recherche de molécules anti-QS en conditions proches de celles trouvées dans l’environnement marin, et ce dans le but d’être ultérieurement testées sur la formation de biofilm. L’objectif à plus long terme reste de trouver un moyen de limiter la formation du biofilm en utilisant des molécules non toxiques pour l’environnement."
"La mise en place de nouvelles normes, de plus en plus contraignantes, est un défi pour l’élaboration de nouveaux matériaux résistants à haute température. Le premier objectif de l’étude est de mettre au point un panneau – coupe-feu 2h à base de plâtre. Il devra présenter à la fois de bonnes qualités d’isolant thermique et des propriétés mécaniques suffisantes pour maintenir l’intégrité d’un ouvrage d’art. Le second objectif est de comprendre, d’une part, l’influence de divers paramètres sur le phénomène de prise du plâtre et d’autre part, de déterminer les propriétés thermomécaniques du composite. Ce type de matériau est obtenu par l’association d’eau, d’une matrice céramique composée essentiellement de sulfate de calcium dihydraté et de charges utilisées en tant que renforts thermiques et/ou mécaniques.Dans une première partie, l’étude porte essentiellement sur la matrice pour laquelle une granulométrie permettant d’optimiser les propriétés mécaniques est déterminée. La matrice est ensuite caractérisée chimiquement. Une étude par calorimétrie isotherme de la réaction d’hydratation du sulfate de calcium semihydraté (plâtre) est réalisée afin de comprendre le mécanisme de prise du plâtre et de maîtriser les temps de prise. Pour cela, on étudie l’influence de la taille des grains, de la quantité d’eau, de la composition chimique du plâtre et de la présence ou non d’adjuvants sur la cinétique d’hydratation du plâtre.Dans une seconde partie, les renforts nécessaires à l’élaboration du composite sont sélectionnés. Les relations entre les quantités de charges et les propriétés thermomécaniques (conductivité thermique, module d’Young, dureté Shore C) du système sont étudiées. Ainsi, une modélisation du comportement du composite sous sollicitations thermique et mécanique est proposée. Cette étude a permis de définir une formulation de panneau présentant de très bonnes propriétés thermiques et des propriétés mécaniques suffisantes pour assurer l’intégrité d’un ouvrage d’art en cas d’incendie. La formulation mise au point a fait l’objet d’un dépôt de brevet (n° BIP207506FR00 en décembre 2010). Cette formulation est actuellement commercialisée par la société EXTHA sous forme de plaques."
"Ce travail porte sur l’étude de revêtements polysilazane (PSZ) élaborés dans le but d’inhiber l’adhésion des bactéries marines. Deux stratégies ont été étudiées : le greffage de chaînes poly(oxyde d’éthylène) (POE) (350, 750 ou 2000 g/mol) sur les chaînes PSZ par réactions d’hydrosilylation d’une part, et l’incorporation de composés du cuivre dans un film céramique issu de la pyrolyse d’un précurseur PSZ, d’autre part. Les conditions optimales de synthèse des PSZ greffés POE ont été définies afin de limiter notamment la réaction d’isomérisation de l’allyl-POE. La réticulation des PSZ greffés POE est alors effectuée par voie humide à température ambiante et procède par hydrolyse-condensation des fonctions alcoxysilane, SiH et SiN du précurseur PSZ. La prédominance des Si de type T3 (RSi(OSi)3) a été mise évidence par spectrométrie RMN du 29Si à l’état solide. La cristallisation des chaînes courtes de POE (350 g/mol) est totalement inhibée alors que l’aptitude des chaînes longues POE (750 et 2000 g/mol) à cristalliser est préservée. Le caractère hydrophile-hydrophobe et la capacité des films à inhiber l’adhésion bactérienne a été étudiée sur des revêtements à densités de greffons et à longueur de chaînes POE différentes. Les PSZs greffés POE 350g/mol avec un taux de greffage maximal conduisent à l’activité bactérienne la plus élevée de l’ensemble des revêtements étudiés.Les films céramiques incorporant du cuivre ont été préparés par pyrolyse d’un oligovinylsilazane en présence d’acétylacétonate de cuivre II sous atmosphère oxydante ou non oxydante. L’identification des phases des composés du cuivre a été effectuée par analyse DRX. L’environnement réducteur généré lors de la pyrolyse du PSZ favorise la formation d’espèces de cuivre (0) et cuivre (I) au détriment de cuivre (II). Les études bactériennes suggèrent que les cristaux CuO et Cu2O sont plus efficaces que le cuivre métal pour lutter contre l’adhésion bactérienne."
"Les biofilms microbiens peuvent avoir des conséquences néfastes dans des domaines aussi divers que les activités marines ou le domaine biomédical. Ce travail a été entrepris afin d'approfondir les résultats obtenus avec des analogues d'hémibastadines à noyau 1,2,3-triazole. Il a porté sur des composés de cette famille résultant de variations de structures approfondies. Les essais menés ont permis de confirmer que ces composés sont bien actifs comme inhibiteurs de formation de biofilms sans activité antibactérienne, même lorsqu'il s'agit de biofilms multi-souches. Cependant, les tests ont montré leur toxicité vis-à-vis du phyto et du zooplancton. Les études de relation structures­ activité ont permis de déterminer les éléments favorables l'activité anti-biofilm : nécessité de la présence d'au moins un hydroxyle phénolique sur l'un des noyaux aromatiques, intérêt des atomes de brome, perte d'activité lorsqu'il y a présence de chaînes alkylamines. En plus des bactéries marines, des essais ont également été menés sur des biofilms formés par une souche clinique de Candida albicans. Ils ont montré que ces composés sont également actifs comme inhibiteurs de formation de biofilms de cette levure, toujours sans activité antifongique."
"La technique de l’impression moléculaire permettant d'obtenir des polymères hautement sélectifs est utiliséedans de nombreuses applications. Dans le cadre de cette thèse, cette technique est employée pour préparer despolymères spécifiques du benzo(a)pyrène (BaP) en vue d’une application capteur. L’introduction d’une sonderédox au sein même du réseau de polymère offre la possibilité d’une détection directe de la cible, par unetechnique électrochimique. Dans le domaine des MIPs, ce travail est le premier à décrire l’utilisation d’unmonomère participant à la formation du réseau tridimensionnel tout en apportant une propriété rédox. Lespolymères synthétisés par polymérisation par précipitation ont été caractérisés en termes de composition, destabilité thermique et de propriétés texturales. Les propriétés de rétention ont été évaluées en mode batch parHPLC-UV en présence de BaP et d'interférents de type hydrocarbures aromatiques polycycliques. Ces MIPsprésentent de bonnes propriétés d’adsorption vis-à-vis du BaP avec des facteurs d’empreinte supérieurs à 1montrant l’efficacité de l’impression. Ces propriétés sont conservées en présence d’une matière organique (seld’acide humique), mais également après plusieurs cycles d’utilisation. Les analyses électrochimiques ontdémontré que les propriétés électrochimiques de ces MIPs diffèrent selon la présence ou pas de la cible avecune limite de détection de 0,09 μM en BaP, ouvrant la porte à la réalisation de capteurs."
"Dans cette étude, nous avons préparé deux matériaux composites conducteurs par le procédé d’électropolymérisation. Lebut de la première partie de la thèse était l’élaboration de films composites polyaniline /LiMn2O4 pour leur applicationcomme matériau de cathode dans les batteries lithium-ion. Les analyses par diffraction des rayons X, analyse EDX etspectrométrie IRTF ont confirmé l'incorporation de LiMn2O4 dans les films composites. Les analyses électrochimiquesdes films obtenus ont mis en évidence une conductivité plus élevée des films composites comparée à la conductivité desfilms de PANI. La deuxième partie a été consacrée à la préparation de l’oxyde de graphène (OG) à partir de graphiteselon 2 méthodes de synthèse dérivées de la méthode de Hummers. Les résultats obtenus ont montré que le de gréd’oxydation des feuillets n’était pas le seul critère à prendre en compte pour évaluer la stabilité du OG dans l’eau. Lesfeuillets d’OG obtenus ont été incorporés à la polyaniline par électropolymérisation d’une suspension d’OG et d’anilineen milieu neutre. Le milieu électrolytique neutre conduit à des chaînes oligomères de PANI de faible masse molaire avecune proportion significative d’unités aromatiques mono et 1,2-disubstituées dans la structure finale. Différents substratsmétalliques ont été testés et les films ont été en particulier déposés sur fer afin d’évaluer leur protection anticorrosion."
"La corrosion des canalisations métalliques pour le transport de gaz ou d’hydrocarbures est un problème critique qui peut avoir des répercussions financières et environnementales très importantes. Les revêtements polyoléfines tricouches sont largement utilisés pour préserver l’intégrité des structures. Ils sont constitués d’une sous couche mince époxy, d’une couche mince d’adhésif et d’une couche épaisse le plus souvent en polyéthylène. Ce système de revêtement bénéficie de très bonnes qualités d'adhérence. Néanmoins, des cas de décollements de ces revêtements à l’interface époxy/acier ont été constatés sur des pipelines en service depuis quelques années seulement, alors que la durée minimale de vie escomptée des tubes dans le sol est de quelques dizaines d’années. Ces décollements pourraient être dus à une dégradation progressive des liaisons interfaciales entre le primaire époxy et la surface métallique, associée à la présence de contraintes interfaciales importantes entre les différentes couches de l'assemblage. Cette étude vise alors d'une part à proposer des solutions permettant d’aboutir à la meilleure performance en vieillissement de la liaison adhésive, et d'autre part à quantifier les niveaux de contraintes au sein de la canalisation en acier revêtue depuis sa mise en œuvre, jusqu'à sa mise en service.Les liaisons interfaciales dépendant nécessairement de la préparation de surface de l’acier, des procédés de nettoyage ont été testés afin d’évaluer leur influence sur l’adhérence initiale et la durabilité des assemblages. Les préparations de surface permettent d'obtenir un degré de propreté et une rugosité. Ces deux éléments maximisent les forces de liaison et donc l'adhérence du revêtement. Le revêtement résiste alors mieux dans des environnements agressifs. Tous les procédés de nettoyage testés ont conduits à des niveaux de propreté équivalents du substrat en acier. Les essais effectués sur substrats polis miroir ont mis en évidence qu’une rugosité est nécessaire pour améliorer la durabilité des assemblages. La rugosité permet d'obtenir des adhérences supérieures à celles sur substrats polis miroir, de ralentir les effets du vieillissement humide et donc de prolonger la durée de vie du système. Il a été mis en évidence que les fortes rugosités étaient particulièrement bénéfiques pour les adhérences sèches. Par contre, au-delà d’une certaine rugosité, l’augmentation de la rugosité ne s’accompagne pas d’une amélioration significative de l’adhérence humide. Une étude sur l’apport des traitements de surface a aussi été menée. L’addition d’un traitement de surface a peu d’impact sur les adhérences initiales des assemblages, en comparaison avec une préparation de surface classique. Par contre les traitements de surfaces améliorent considérablement les adhérences humides, et donc la durabilité des assemblages. Notre travail prouve que le traitement aminosilane est un candidat potentiel en vue du remplacement du traitement toxique de chromatation, référence en matière de traitements de surface dans l'industrie des pipelines et dont l'utilisation sera interdite dans un futur proche compte tenu de l'évolution de la réglementation. Dans de bonnes conditions d'application et associé avec des primaires époxy appropriés, les adhérences sèches et humides obtenues avec les silanes sont comparables (voire supérieures) à celles de la chromatation.Par ailleurs, la modélisation par éléments finis du système tricouches a permis de préciser les niveaux de contraintes aux interfaces résultant de la mise en œuvre et de prévoir leur évolution au cours du temps et du vieillissement humide."
"Un composite époxy/fibre de verre élaboré à partir de matériaux simplifiés a été soumis en parallèle à du vieillissement artificiel (UV et thermohydrique) et à du vieillissement naturel (climat tropical humide). Une étude des matériaux (résine seule et composite) à travers un large panel de techniques de caractérisation physico-chimiques, mécaniques et de moyens d’observation (MEB, AFM) a permis d’identifier clairement la structure, la morphologie et les principales propriétés du réseau époxy-amine de l’état initial. Une caractérisation systématique des échantillons par couches de 20 microns d’épaisseur a permis, en particulier, d’identifier un gradient de structure et de propriétés dans les 200 premiers microns à la surface des plaques de résine et de composite. Ce gradient est attribué à l’évaporation du durcisseur amine lors de l’élaboration des matériaux. Dans les plaques de composites, le DMA ainsi que l’AFM ont permis de mettre en évidence une zone d’interphase autour des fibres pour laquelle le réseau époxy-amine présente des caractéristiques différentes de celles de la résine en masse.La même méthodologie a été adoptée pour suivre l’évolution de ces matériaux lors des vieillissements artificiels et naturels.Les études séparées des vieillissements UV et thermohydrique ont permis de mettre en évidence les altérations chimiques et physico-chimiques de la matrice seule d’une part, et des interphases fibres/matrice d’autre part. Le vieillissement photochimique se montre le plus dégradant pour la surface des plaques, alors que les effets du vieillissement thermohydrique sont principalement observés au niveau des interfaces fibres/matrice dans les composites. Dans les deux cas également, nous pouvons proposer des mécanismes simplifiés de dégradation de la résine époxy-amine.Enfin, les résultats de caractérisation après le vieillissement naturel nous permettent de faire des corrélations avec les vieillissements artificiels et de pointer les effets prépondérants des deux paramètres de vieillissement, ainsi que d’avancer un facteur d’accélération."
"Dans le milieu marin, toute surface immergée est rapidement colonisée par des bactéries, puis par d’autres micro-organismes, conduisant à la formation de structures tridimensionnelles complexes appelées biofilms. Cette étape est généralement suivie par l’installation de macro-colonisateurs. Néanmoins, un certain nombre d’organismes marins, tels que les macro-algues, présentent des surfaces peu épiphytées à l’échelle macroscopique. Des algues méditerranéennes (Taonia atomaria et Dictyota spp.) ont été sélectionnées dans le cadre de ces travaux de thèse pour leur capacité à conserver leur surface peu colonisée. Cependant, des observations de leurs surfaces par microscopie ont montré l’existence de biofilms diversifiés à la surface de leurs thalles. Le but de cette thèse est de mieux comprendre les mécanismes de médiation chimique entre ces algues et les bactéries associées à leur surface. La première partie de ce travail a été consacrée à l’étude du rôle de molécules d’origine algale vis-à-vis de l’adhésion de bactéries marines. Pour cela, la composition chimique totale des algues sélectionnées a été analysée conduisant à l’isolement et à la caractérisation structurale de 12 molécules, dont trois se sont révélées être originales. L’activité anti-adhésion de la majorité de ces composés a ensuite été évaluée : le 1-O-octadecenoylglycérol s’est avéré être le produit le plus actif (20 µM < CE50 <55 µM). La deuxième partie a été dédiée plus particulièrement à l’étude du métabolome de surface de T. atomaria dans le but d’évaluer son implication dans les interactions écologiques entre l’algue et les bactéries associées à sa surface. Un protocole d’obtention et d’analyse spécifique des extraits surfaciques a tout d’abord été développé. Ce protocole est basé sur le trempage des thalles dans des solvants organiques et un contrôle de l’intégrité des cellules membranaires des algues y est associé. L’échantillonnage a été effectué mensuellement à Carqueiranne (Nord-ouest de la Méditerranée, France) durant la période allant de février à juillet 2013. Les résultats obtenus montrent qu’un sesquiterpène est exprimé majoritairement à la surface de l’algue. Il a été démontré que ce composé inhibe l’adhésion de souches bactériennes de référence tout en restant inactif vis-à-vis de celles isolées à la surface de l’algue. Une telle spécificité n’a pas été observée ni dans le cas de biocides commerciaux, ni pour les autres métabolites produits par T. atomaria. Dans un second temps, un suivi saisonnier des extraits de surface ainsi que des communautés bactériennes associées a été effectué par métabolomique (LC-MS) et DGGE, respectivement. Des fluctuations saisonnières de ces deux paramètres ont été reportées sans mettre en évidence de corrélation évidente entre eux. La présence de la molécule majeure de surface durant tout le suivi saisonnier a été notée ainsi que sa capacité à diffuser dans l’eau de mer. Enfin, l’étude de l’implication potentielle des bactéries associées à T. atomaria dans le contrôle du biofilm a été entreprise en évaluant l’activité de leurs extraits vis-à-vis de l’adhésion de souches de référence. En conclusion, nous émettons l'hypothèse que T. atomaria pourraient contrôler partiellement le biofilm associé à sa surface en faisant intervenir des métabolites spécifiques."
"L’utilisation de séquences RMN de diffusion permet d’étudier les échanges dynamiques du PEO en tant que petite molécule qui diffuse à travers les parois de nanocapsules creuses de PMMA afin de maîtriser son relargage en fonction du milieu. Ces objets nanostructurés étant élaborés via la formation de stéréocomplexe entre les PMMA isotactique et syndiotactique dans l’acétonitrile, une étude préliminaire a été consacrée à l’influence d’interactions PMMA/PEO sur la morphologie des nanocapsules creuses.Dans un premier temps, nous avons étudié les interactions stéréospécifiques au sein des systèmes binaires PMMA stéréorégulier/PEO dans le chloroforme, solvant non complexant, puis dans l’acétonitrile. L’utilisation de la méthode RMN DOSY nous a donc permis de mettre en évidence la formation d’une double distribution de nanoagrégats de tailles et de composition contrôlés. Le PEO étant présent dans une des populations de nanoagrégats, sa présence induit et stabilise les agrégats de PMMA. Les interactions PMMA/PEO dépendantes de la tacticité du PMMA sont responsables de ces nanostructures. Elles sont non seulement contrôlées par la tacticité du PMMA mais également par la concentration et la masse molaire du PEO. Dans le chloroforme, un modèle d’interaction en trois étapes en fonction de la concentration en PEO, en a été déduit soulignant l’effet prépondérant de la conformation des polymères sur ces interactions. De plus, une généralisation dans plusieurs solvants de ce modèle d’interactions stéréospécifiques a permis de mettre en évidence la corrélation entre la longueur des séquences isotactiques ou syndiotactiques, leurs conformations et la masse molaire du PEO. Dans un second temps, nous avons pu en déduire une corrélation avec la diminution du phénomène de stéréocomplexation dans les systèmes stéréocomplexe du PMMA/PEO/CD3CN sur la base des interactions observées entre les deux polymères en solution dans l’acétonitrile. Le PEO perturbe en partie la formation du stéréocomplexe, permettant ainsi de mettre en évidence l’influence de la présence de PEO sur la structure des nanocapsules creuses de PMMA et son effet désintégrant.Ces études ont montré que les mécanismes de diffusion à travers les parois utilisés dans les systèmes à relargage contrôlé ne peuvent faire l’impasse sur l’analyse préalable des interactions potentielles entre les molécules sondes et la structure des parois polymériques. En effet, ces interactions peuvent fortement modifiés la structure de ces containers et les mécanismes de diffusion doivent prendre quantitativement en compte ces effets pour être efficients.Ces travaux laissent entrevoir la démarche scientifique à suivre pour le développement de modèle de libération contrôlée de molécules actives à travers les membranes des nanocapsules, elles-mêmes incluses dans des systèmes polymériques."
"L’objectif de ces travaux est de synthétiser des copolymères diblocs et triblocs à base d'unités monomères méthacrylate de tert-butyldiméthylsilyle et diméthylsiloxane. Le choix de ces unités monomères repose sur l'élaboration de films polymères hydrolysables dans le milieu marin et de faible énergie libre de surface, respectivement. Ces polymères ont été caractérisés puis utilisés comme liants dans la formulation de peintures anti-salissures marines SPC/FRC hybrides. Les performances des revêtements obtenus ont alors été comparées aux deux types de revêtements anti-salissures marines disponibles sur le marché : - les revêtements auto-polissants (Self-polishing copolymer, SPC), à base de liants polymères hydrolysables, efficaces par relargage de biocides dans le milieu marin et par érosion, mais toxiques pour l’environnement ; - les revêtements Fouling Release"" (FRC), hydrophobes à base de silicone, et non toxiques, qui limitent la force d’adhésion des salissures mais sans efficacité en mode statique.Le procédé de polymérisation RAFT a été employé afin de synthétiser des polymères avec des architectures, des compositions et des masses molaires contrôlées. Des macro-agents de transfert de chaîne à base de poly(diméthylsiloxane)s ont été préalablement synthétisés à partir de poly(diméthylsiloxane)s mono et di-hydroxylés, de masses molaires 1000 , 5000 et 10000 g.mol-1. Trois séries de copolymères ont été préparées avec des masses molaires allant de 12000 à 60000 g.mol-1 et des teneurs en unités diméthylsiloxanes allant de 3% à 57%.Les propriétés de prise en eau, d'érosion (type SPC) et de mouillabilité (type FRC) ont été étudiées pour les liants seuls et les revêtements formulés avec et sans biocides. L'évolution de l'hydrophobie de surface des revêtements pendant leur immersion en eau de mer artificielle a été suivie. L'efficacité anti-adhésion bactérienne d'une série de copolymères, sous forme de vernis et de revêtements formulés, a été étudiée vis-à-vis de deux souches de bactéries marines. Enfin, l’efficacité anti-salissure marine des vernis et des revêtements formulés avec et sans biocides a été évaluée lors d'une immersion in-situ en Mer Méditerranée pendant 16 mois au maximum."
"Le biofouling est un phénomène invasif qui engendre des problèmes économiques et écologiques importants. Dans ce mode de vie privilégié, les micro-organismes communiquent grâce à un système de communication intra- et inter-espèces, voire inter-genre, appelé quorum sensing (QS). Le QS est basé sur les petites molécules diffusibles telles que les N-acyl homosérine lactones (AHL), et est donc impliqué dans de nombreux processus physiologiques, notamment la production de facteurs de virulence, l'émission de bioluminescence et la production de pigments. Dans une étude antérieure, le QS a été mis en évidence chez une bactérie marine dénommée Pseudoalteromonas ulvae TC14 isolée de la rade de Toulon. P. ulvae TC14 étant non seulement capable de produire un biofilm important et de la violacéine mais régulerait ces deux paramètres (biofilm et violacéine) par la détection d' AHLs exogènes car elle n'en produirait pas. Cette étude a consisté dans un premier temps à poursuivre la caractérisation du système QS chez P. ulvae TC 14 par une étude moléculaire. Tout en vérifiant l'absence de synthase d' AHL (Luxl) et des gènes potentiels de l'auto-induction de type 2 (AI-2) chez P. ulvae TC14, la présence de huit séquences régulatrices luxR a pu être détectée et une évaluation de l'expression de ces séquences a été faite par technique RT q-PCR. Dans le but de déterminer l'impact de la modulation du QS sur l'adhésion et la formation du biofilm chez P. ulvae TC 14 des molécules régulatrices du QS ont été testées en combinaison avec trois analogues synthétiques à effet antifouling. La restauration de l'adhésion et de la formation du biofilm par les molécules combinées a montré que la modulation du QS chez P. ulvae TC14 ne semble pas être la voie privilégiée pour l'inhibition du biofilm. Cependant l'évaluation d'autres cibles telles que la membrane bactérienne et les pompes d'efflux à travers des perméabilisants membranaires et les inhibiteurs de pompes d' efflux a montré un effet synergique sur l'adhésion et la formation du biofilm. Ainsi, la perméabilisation membranaire et l'inhibition des pompes d'efflux semblent être des meilleures voies de potentialisation de l'effet antifouling des analogues synthétiques"
"Actuellement, il existe deux types de peintures anti-salissures marines sur le marché : - Les Self-Polishing Coatings (SPC), revêtements auto-polissants à base de liants polymères hydrolysables, efficaces par relargage de biocides dans le milieu marin mais toxiques pour l’environnement ; - Les Fouling Release Coatings (FRC), revêtements hydrophobes à base de silicone, sans biocides, qui limitent la force d’adhésion des salissures mais ne sont pas efficaces en mode statique.L’objectif de cette étude est d’élaborer des peintures anti-salissures marines hybrides FRC/SPC. Pour ce faire, des polymères diblocs à la fois hydrolysables et présentant une faible énergie de surface ont été synthétisés et caractérisés puis utilisés en tant que liants dans la formulation de peintures anti-salissures marines.Le procédé de polymérisation RAFT a été employé afin de synthétiser des polymères avec une architecture et une masse molaire contrôlée. Deux approches ont été abordées :- Des polymères ont été synthétisés à base de monomères de faible énergie de surface et hydrolysables (le méthacrylate de (heptaméthyl-trisiloxy)diméthylsilyle et le méthacrylate de bis(triméthylsilyloxy)méthylsilyle) avec un co-monomère méthacrylate de méthyle ou méthacrylate de butyle. Il a notamment pu être montré que les copolymères de structure dibloc présentent une énergie de surface plus faible que les copolymères statistiques.- Des polymères ont été synthétisés à partir de monomères hydrolysables tel que le méthacrylate de tert-butyldiméthylsilyle et de monomères de faible énergie de surface tel que le méthacrylate de poly(diméthyl-siloxane).Les propriétés d’érosion (type SPC) et d’énergie de surface (type FRC) ont été étudiées pour les liants seuls et les peintures formulées, avant et pendant leur immersion en eau de mer artificielle. L’efficacité anti-salissures marines des peintures formulées a été évaluée lors de leur immersion in-situ en Mer Méditerranée."
"Dans l’environnement marin, les surfaces artificielles sont rapidement colonisées par des bactéries qui s’organisent en communautés appelées biofilms, s’entourant d’une matrice de substances polymériques extracellulaires (EPS). La formation d’un biofilm est une étape critique du processus nommé biofouling, c’est-à-dire l’accumulation de micro- et de macro-organismes sur une surface immergée, pouvant conduire à des conséquences néfastes dans le secteur marin. Dans cette étude, il s’agit d’identifier des souches bactériennes isolées de supports immergés en Mer Méditerranée et de les caractériser phénotypiquement par diverses approches. Leur capacité à former un biofilm in vitro a été évaluée dans différentes conditions avec une attention particulière portée sur leurs capacités à produire une matrice polymérique abondante riche en polysaccharides; l’objectif étant d’isoler des exopolysaccharides originaux à activité antifouling. Treize souches ont ainsi fait l’objet d’analyses phylogénétiques et d’une caractérisation phénotypique. Sept genres et douze espèces différentes ont été identifiés au sein desquelles deux isolats peuvent être affiliés à une nouvelle espèce, nommée Persicivirga mediterranea. Ce genre n’a jamais été décrit en Mer Méditerranée jusqu’à présent. L’extraction des EPS de chaque souche cultivée en biofilm a permis de déterminer leur composition générale en glucides, protéines, acides nucléiques et lipides. Une souche, Pseudoalteromonas ulvae TC14, se distingue par sa capacité à produire des exopolysaccharides en quantité importante. Il s’agit essentiellement de polymères du glucose dont les analyses chromatographiques et spectroscopiques ont révélé la diversité de taille (Mw ~ 1–4000 kDa), de charge (neutre ou anionique) et de fonction associée (lactate ou acétate). Les fractions d’EPS enrichies en polysaccharides inhibent la formation de biofilm par d’autres souches marines. Ces derniers sont également synthétisés par les bactéries en culture planctonique mais en proportions très différentes."
"L’objectif de cette étude consiste à synthétiser et caractériser de nouveaux matériaux hybrides organique/inorganique obtenus par greffage de poly(méthacrylate)s d’alkyle sur des nanoparticules de dioxyde de titane (TiO2) et d'oxyde de zinc (ZnO). Afin de mieux comprendre les facteurs influents les réactions mises en jeu lors d'un greffage à partir de la surface d'un oxyde métallique, nous avons choisi de travailler avec des nanoparticules disponibles commercialement et/ou élaborées. Des nanoparticules d’oxyde de zinc, de dimensions allant de 5 à 100 nm, ont été synthétisées par la méthode de précipitation, à température ambiante. La diffractométrie de rayons X (DRX) et la microscopique électronique à transmission (MET) ont permis de déterminer la structure cristalline, les dimensions et la morphologie des particules ainsi préparées. Les conditions de synthèse ont été optimisées afin d'augmenter la surface spécifique des particules tout en favorisant la présence de groupes hydroxyles en surface. La méthode de greffage de polymères méthacryliques a consisté à modifier préalablement la surface des nanoparticules par un agent de couplage réactif de type 3-méthacryloxypropyltriméthoxysilane (MPS). Les nanoparticules ainsi modifiées ont été caractérisées par spectroscopie (IRTF et RMN CP-MAS 13C et 29Si) et par analyse thermogravimétrique afin de confirmer la présence et la quantité de MPS greffé. Cet alcoxysilane, porteur d'une fonction méthacrylate, a permis de greffer des chaînes de poly(méthacrylate de méthyle), de poly(méthacrylate de tert-butyldiméthylsilyle) à partir de la surface des nanoparticules. La polymérisation radicalaire contrôlée par addition-fragmentation réversible (procédé RAFT) a été sélectionnée pour obtenir un contrôle des masses molaires, de faibles indices de polymolécularité et le greffage de copolymères diblocs. L'observation de ces nanoparticules hydrides en microscopie électronique à transmission montre clairement la présence d'une couronne de polymères à la surface des particules. L'étude de la stabilité thermique des nouvelles nanoparticules hybrides à base de ZnO a été réalisée par analyse thermogravimétrique sous atmosphère inerte. L'impact du procédé de polymérisation sur les mécanismes de dégradation thermique des polymères méthacryliques étudiés a été mis en évidence. Pour la première fois, des valeurs d'énergie d'activation ont été calculées sous atmosphère inerte et oxydante."
"En raison des propriétés redox réversibles du ferrocène et de son activité antibactérienne, les polymères à base de ferrocène sont intéressants pour synthétiser de nouveaux liants pour des revêtements anti-adhésifs bactériens. Cette étude rend compte de l’homopolymérisation et de la copolymérisation de monomères méthacryliques porteurs de groupes ferrocényles avec le méthacrylate de lauryle (LM). Le méthacrylate de méthylferrocène (FMMA) mais aussi quatre nouveaux monomères nommés méthacrylate de 2- ferrocénylméthoxyéthyle (FMOEMA), méthacrylate de 3-ferrocénylméthoxypropyle (FMOPMA), 4- ferrocénylméthoxybutyle (FMOBMA) et de 2-ferrocénylméthoxyméthyléthyle (FMOMEMA) ont d’ bord été synthétisés et ensuite polymérisés via le procédé RAFT. Les cinétiques ’homopolymérisation ont été étudiées par RMN 1H in situ. La polymérisation a été contrôlée en utilisant le 2-cyanoprop-2-yl dithiobenzoate (CPDB) comme agent de transfert de chaîne, à 70°C, dans le toluène deutéré. Ces monomères contenant le groupement ferrocényle se sont révélés très réactifs via le procédé RAFT, conduisant à des conversions de 96% et à des polymères de faibles indices de polymolécularité (Ð<1,6). La conversion des monomères suit une cinétique de premier ordre (jusqu’à 80%) avec une augmentation linéaire de la masse molaire en fonction de la conversion en monomère. En utilisant le monomère FMMA comme référence, l’ spacement entre la partie polymérisable et le groupement ferrocényle a été augmenté pour le FMOEMA, FMOPMA, FMOMEMA et FMOBMA afin d’ méliorer la mobilité des groupements latéraux. Cette mobilité se traduit par une diminution notable des températures de transition vitreuse des homopolymères entre le pFMMA et le pFMOBMA. De plus, les copolymères diblocs préparés par voie séquencée présentent deux températures de transition vitreuse spécifiques à chaque bloc, démontrant une incompatibilité de ces derniers. Les propriétés électrochimiques des monomères et celles des polymères ont été caractérisées par voltampérométrie cyclique. Enfin, les propriétés anti-adhésives de ces homopolymères et copolymères diblocs vis-à-vis d’une bactérie marine ont été évaluées."
"Dans cette étude nous avons étudié la polymérisation de l’aniline en milieu aqueux micellaire eau-acide décylphosphonique (DcPA) ainsi qu’en milieu micellaire inverse acide décylphosphonique/eau/n-heptane : chloroforme (2 :1 v/v). Contrairement au système aqueux micellaire, la vitesse de polymérisation dans le système micellaire inverse augmente lorsque la concentration en DcPA dans le milieu réactionnel diminue. Ce résultat a été attribué à des différences de compacité de la couronne hydrophobe des micelles inverse, qui impactent la diffusion de l’aniline au cœur des gouttelettes d’eau, le lieu préférentiel de la polymérisation.La conductivité électrique maximale obtenue est de 3.6 S.cm-1. Cette valeur de conductivité est plus élevée de 4 ordres de grandeur comparée à la conductivité électrique de PANI dopée DcPA préparée par post-traitement de la PANI EB avec une solution de DcPA (2.3x10-4 S cm-1). Les analyses de diffraction des rayons X (DRX) ont révélé une structure lamellaire de la polyaniline préparée en milieu micellaire eau-DcPA dans laquelle les chaînes de polyaniline sont séparées par les longues chaînes alkyle du DcPA. Cette organisation diminue les interactions interchaînes de la polyaniline et contribue à l’augmentation de la solubilité en milieu organique de la polyaniline.La polyaniline dopée DcPA a été incorporée comme pigment dans un liant polymère polyvinyl butyral (PVB), puis appliqué sur un acier à faible teneur en carbone. Dans le cas de la polyaniline préparée dans le milieu aqueux micellaire de DcPA, la dispersion de polyanilinea été également appliquée après dialyse sur acier en couche mince, puis revêtue d’une couche de finition PVB. Les films PVB-PANI ont été exposés à des environnements neutres chlorurés (brouillard salin et milieu NaCl 3.5%). A partir de mesures de spectrométrie d’impédance électrochimique, nous avons montré que l’efficacité de la protection anticorrosion de la polyaniline dopée DcPA augmentait avec le taux de dopant DcPA du polymère. Ces résultats suggèrent un rôle actif du DcPA dans le mécanisme de protection. Ce rôle actif du DcPA a été confirmé par analyse de la surface métallique après exposition qui a révélé la présence de sels de phosphonates de fer insoluble. La protection anticorrosion apportée par polyaniline dopée DcPA apparaît supérieure à la polyaniline non dopée ainsi qu’à la polyaniline dopée HCl."
"En milieu marin, toute surface immergée est soumise à une colonisation par de nombreux organismes (biofouling). Le développement de biofilms est une étape clé du phénomène. Les systèmes de communication y sont contrôlés par le biais de signaux chimiques. Dans ce travail, l’étude de la signature métabolique de biofilms naturels formé in situ a été réalisée selon un gradient de pollution en contaminants métalliques dans la rade de Toulon et selon la nature du revêtement de la surface immergée. De nettes variations chimiques des biofilms prélevés sont observées et sont corrélées avec des variations en termes de communauté microbiennes. L’étude in vitro de 4 souches bactériennes issues de biofilms naturels a permis, après optimisation des méthodologies d’analyse, une discrimination selon leur profil métabolique. Des biomarqueurs ont été mis en évidence, avec notamment la production de lipides ornithine par la souche Pseudoalteromonas lipolytica. La réponse biologique de cette souche en fonction de son phénotype et face à un stress cuprique a été étudiée par métabolomique et protéomique révélant d’importantes modulations de certaines voies biosynthétiques."
"Dans un contexte où les revêtements antifouling à base de biocides toxiques sont de plus en plus réglementés, il est indispensable de développer des solutions plus écologiques telles que les Fouling Release Coatings (FRCs) principalement à base d'élastomères silicones. Les FRCs peuvent relarguer facilement la salissure marine grâce à leurs propriétés physico-chimiques telles que leur faible énergie libre de surface et leur faible module élastique qui minimisent les interactions entre la salissure et la surface et diminuent les forces d'adhérence. Une autre catégorie de revêtements antifouling nommée Self-Polishing Coating, allie une érosion contrôlée de la surface à un relargage de biocides, permettant une protection efficace des coques de bateau contre la salissure marine. Cette thèse a pour objectif de créer de nouveaux FRCs dit « hybrides » car ils combinent différents composants et mécanismes d'action. Pour cela, deux stratégies ont été développées :1)des élastomères silicones renfermant des additifs hydrolysables avec des quantités allant de 5 à 20 %mass.2)des réseaux réticulés PDMS/polyester contenant de 12 à 27 % massique en polyester hydrolysable.La finalité de ces nouveaux revêtements était d'obtenir des surfaces chimiquement et/ou physiquement ambiguës vis-à-vis des organismes marins : (1) soit par la migration d'additifs hydrolysables en surface, par exemple, grâce à l'ajout de poly(méthacrylate de bis(triméthylsilyloxy)méthylsilyl), (2) soit par l'érosion du réseau hybride PDMS/polyester , par exemple, grâce à la réticulation de la poly(E-caprolactone) ou du poly(D,L-lactide-co-glycolide) avec les chaînes PDMS. Les propriétés physico-chimiques telles que la mouillabilité, l'énergie de surface, le module élastique et les propriétés d'hydrolyse/érosion des revêtements ont été étudiées avant et pendant leur immersion en milieu aqueux. L'efficacité antisalissure marine des revêtements a été évaluée lors de leur immersion in situ en mer Méditerranée et lors de tests biologiques ciblant des organismes marins spécifiques."
"Tout support immergé dans l’eau est rapidement colonisé par de nombreux organismes micro- et macroscopiques. Ce phénomène séquentiel et complexe appelé biofouling est à l’origine de nombreux préjudices économiques et écologiques, notamment dans le milieu marin. L’interdiction récente de certaines substances toxiques, utilisées comme biocides dans les revêtements antifouling des coques de bateaux notamment, a relancé l’intérêt de rechercher de nouvelles molécules antifouling respectueuses de l’environnement. L’objectif de cette thèse a été de développer et d’amorcer l’étude de la représentativité d’un bio-essai permettant d’évaluer le potentiel antifouling de molécules et de revêtements sur des « biofilms » mono et plurispécifiques in vitro en microplaques grâce à l’utilisation de fluorochromes. Le choix a été fait de se focaliser sur le biofilm primaire car il est envisagé que l’élimination ou la limitation de ce dernier réduisent le biofouling. Cinq souches de bactéries marines pionnières, isolées de la Rade de Toulon et en Bretagne, ont été utilisées afin de comparer l’efficacité anti-adhésion de molécules commerciales et naturelles. Deux dérivés de synthèse de substances naturelles marines (TFA E et Z) ont présenté une activité significative associée à une absence de toxicité sur bactéries, suggérant ainsi un mode d’action anti-adhésion spécifique. En outre, les différences de sensibilité entre souches ont confirmée l’importance de réaliser le bio-essai avec un panel diversifié de bactéries.Afin de voir si les données obtenues en laboratoire reflétaient ce qui se produit dans le milieu naturel, une comparaison entre les résultats du bio-essai en microplaque appliqué à six revêtements et les biofilms qui ont colonisés ces mêmes peintures immergées un mois dans la Rade de Toulon (analysés par cytométrie de flux, microscopie et PCR-DGGE), a été effectuée. Les analyses quantitatives ont suggéré une cohérence entre les deux approches même si l’absence de revêtement d’efficacité intermédiaire et le nombre de systèmes testés limite la portée de nos conclusions"
"Les matériaux composites sont largement utilisés pour la réalisation de pièces et de structures à haute performance mécanique. Pour développer de nouveaux composites, le recourt à la simulation numérique est de plus en plus fréquent. Il convient de pouvoir décrire avec une assez bonne précision les comportements des constituants et du matériau global à l'état initial et au cours du temps. Dans cette étude, le rôle des interphases sur les propriétés mécaniques à l'échelle macroscopique (principalement dans le domaine élastique) a été mis en évidence. A partir d'un plug-in développé dans ce travail, la génération automatiquement de microstructures de composite UD intégrant ou pas l’interphase a été faite. Pour fiabiliser les modèles, la majorité des données d'entrée reposent sur des valeurs expérimentales acquises à différentes échelles. Au final, cette étude montre la nécessité de prendre en compte l'interphase dans les simulations numériques pour améliorer les prédictions des calculs."
"Les polymères à empreintes moléculaires sont des matériaux aux propriétés de reconnaissance spécifiques qui peuvent être mis à profit pour la détection d’une large gamme d’analytes. Ainsi, depuis quelques années, des travaux décrivent leur utilisation dans des capteurs en raison de leur capacité à piéger une cible définie.L’objectif de ce travail est d’ajouter des propriétés redox à des polymères à empreintes moléculaires pour détecter le Bisphénol A (BPA) par des méthodes électrochimiques simples. Ces polymères électroactifs sont synthétisés par polymérisation par précipitation d’une sonde redox, le méthacrylate deméthylferrocène (Fc), et du diméthacrylate d’éthylène glycol (EDMA) en présence du BPA pour le polymère imprimé (e-MIP-Fc) et en son absence pour le polymère non-imprimé (e-NIP-Fc).L’introduction d’un deuxième monomère fonctionnel, la 4-vinyl pyridine (4-VP), conduit à deux autres polymères imprimé (e-MIP-Fc-VP) et non-imprimé (e-NIP-Fc-VP). Les propriétés d’adsorption des polymères ainsi obtenus sont caractérisés en batch à l’aide de la LC-MS et présentent une capacité de reconnaissance du BPA avec un facteur d’empreinte de 2,5 et 1,3 respectivement pour l’e-MIP-Fc-VP et e-MIP-Fc justifiant de l’efficacité de l’empreinte. Leurs caractérisations par voltampérométrie cyclique confirment d’une part la bonne intégration du monomère ferrocényle dans les e-MIP/e-NIP et d’autre part la capacité de ces polymères à révéler la présence ou pas de la cible. Les particules e-MIP-Fc ont ensuite été intégrées dans des dispositifs type micro électrode ou transistor OECT (Organic ElectroChemical Transistor). Les premiers résultats, mêmes s’ils doivent être confirmés, s’avèrent encourageants avec,comme attendu, des modifications des propriétés électriques en présence du BPA. L’e-MIP-Fc-VP après mélange avec de la pâte de carbone, a été utilisé en sérigraphie pour obtenir une électrode de travail modifiée dans des électrodes sérigraphiées (Screen Printed Electrode). Ces électrodes permettent la reconnaissance du BPA avec des limites de détection et de quantification de 60 pM et 190 pM respectivement pour une gamme de concentrations comprise entre 0,15 et 1,84 nM, ouvrant ainsi des perspectives intéressantes pour la détection du BPA en milieu aqueux."
"Importance quantitative, structure et diversité des bactéries prédatrices de bactéries, les Bdellovibrio et organismes apparentés (BALOs), dans les grands lacs péri-alpins (Annecy, Bourget, Léman)"
"Les polymères à empreintes ioniques (IIPs) sont des matériaux poreux hautement réticulés présentant des cavités de reconnaissance spécifiques d'un ion cible, leur permettant ainsi d'avoir une sélectivité élevée. Le travail de thèse présenté se concentre sur la préparation de polymères à empreintes ioniques fluorescents du plomb (Il), c'est-à-dire capables de transformer la reconnaissance de cet ion en un signal de fluorescence, en vue de leur application pour la détection de ce contaminant en milieu marin. Ainsi, dans une première étape, la stratégie adoptée a été de sélectionner un ligand fluorescent original, sélectif du plomb, de l'étudier et de l'utiliser pour l'élaboration des polymères. A partir de ce ligand, un monomère fluorescent de type styrénique a donc été synthétisé (ANQ-ST) dont le signal de fluorescence est exalté lors de l'ajout de plomb (11). La deuxième étape a été consacrée à l'élaboration d'IIPs du plomb (11) à base d'ANQ-ST. Différents paramètres ont été testés: le solvant de polymérisation, la nature de l'agent de réticulation, le diméthacrylate d'éthylène glycol (EGDMA) ou le divinylbenzène (DVB), ainsi que le ratio de monomère fonctionnel (ANQ-ST) par rapport à l'agent de réticulation (2 % et 5 % molaires). Un panel diversifü de techniques de caractérisation a permis d'étudier les propriétés structurales des différents polymères synthétisés ainsi que de valider l'intégration du monomère fonctionnel ANQ-ST dans la matrice polymère. La dernière étape a consisté à évaluer les performances des IIPs pour la détection du plomb (11) par fluorescence Les polymères préparés avec l'EGDMA et 5 % de monomère fonctionnel présentent les meilleurs résultats. Pour ces polymères, l'intensité de fluorescence des IIPs augmente fortement en présence de plomb (11). De plus, la réponse est très peu impactée par l'ajout d'une espèce ionique interférente, contrairement à leurs analogues non-imprimés, soulignant l'efficacité de l'effet d'empreinte. Des droites de calibration ont été établies en milieu aqueux à différents pH et dans différentes matrices, paramètres qui n'ont pas eu d'influence majeure. Les limites de détection obtenues, de 2, 1 et 2,4 µg.L-1, sont inférieures à la recommandation de 1 O µg.L-1 de l'OMS. Ces résultats ont permis de valider avec succès l'utilisation des IIPs fluorescents synthétisés dans le cadre de ce travail de thèse pour la détection du plomb (Il) dans des échantillons naturels en particulier marins."
"Les unités de dinanderies de la ville de Fès produisent des effluents très fortement contaminés par des éléments métalliques tels que le nickel, l’argent, le plomb, le cuivre ou encore le zinc. Ces effluents, déversés directement dans le réseau collectif, ont un impact négatif sur la station d’épuration proche. Il s’avère ainsi nécessaire de décontaminer ces effluents en amont de leur traitement. Dans ce contexte, les objectifs de ce travail étaient la valorisation de substances minérales naturelles, bentonite et diatomite, minerais abondants dans le sol national et peu exploités au Maroc, pour leur utilisation en tant que phase d’adsorption du nickel et de l’argent. L’objectif de ce travail a été de modifier les propriétés physico-chimiques de ces matériaux naturels pour maximiser leur capacité d'adsorption. Dans une première étape, la modification de la diatomite a été réalisée par traitement thermique à différentes températures (de 550 à 950 °C). La diatomite calcinée à 550 °C présente le meilleur pouvoir adsorbant vis-à-vis du nickel et de l’argent. La deuxième étape a été consacrée à la modification de la bentonite par traitement thermique (550 °C et 750 °C), par activation chimique par le carbonate de sodium et par activations combinées chimique et thermique à 450 °C. Les bentonites modifiées par activation chimique par le carbonate de sodium et par activations combinées chimique et thermique à 450 °C présentent les meilleurs résultats. En outre, des polymères à empreintes ioniques (IIPs) du nickel ont été considérés comme des matériaux alternatifs pour remplacer les matériaux naturels afin d'améliorer la sélectivité. La dernière partie a consisté à comparer les performances de ces matériaux, afin de sélectionner le meilleur matériau pour une future application dans la dépollution des effluents contaminés avant leur traitement par la station d'épuration de la ville de Fès. Les concentrations en nickel et argent résiduelles deviennent négligeables dans les échantillons naturels après l'adsorption par les matériaux étudiés dans ce travail, ce qui répond parfaitement aux Normes de Qualité Environnementale. Ces résultats ont permis de valider avec succès l’utilisation des matériaux naturels et de synthèse pour l’extraction du nickel et de l’argent issus d’effluents contaminés."
"La rade de Toulon est un écosystème côtier fortement anthropisé qui présente un gradient multiple de contamination en éléments traces métalliques, qui en fait un site atelier remarquable. Le couplage des campagnes de terrain et des expérimentations en laboratoire a permis d'étudier l'impact de la contamination métallique sur les communautés microbiennes planctoniques et en biofilm. L'utilisation de la chimie analytique, de la cytométrie en flux et du métabarcoding a permis d'étudier plusieurs aspects des communautés, comme l'abondance et la diversité taxonomique en réponse à la contamination métallique dans la rade de Toulon. Ainsi, les communautés d'ultraphytoplancton et de bactérioplancton ont montré une structuration spatiale forte le long des gradients métalliques. Les expérimentations en laboratoire ont montré que les ETMs jouaient un rôle important sur l'abondance et la diversité des communautés ultraplanctoniques, par des effets directs (toxicité) et indirects (couplage phytoplancton-bactérioplancton). La communauté microbienne de biofilm a été beaucoup moins impactée par les gradients métalliques dans la rade de Toulon que la communauté ultraplanctonique. En revanche, la communauté de biofilm a semblé influencée par sa proximité avec le compartiment sédimentaire, qui ont pu fournir des microorganismes colonisateurs lors d'épisodes de remise en suspension. En conclusion, les ETMs ont semblé avoir un impact sur l'ensemble des communautés microbiennes de la rade de Toulon avec toutefois, des variations d'influence selon le compartiment."
"Le « quorum sensing » (QS) est un moyen de communication bactérienne impliquant des petites molécules appelées auto-inducteurs qui au-delà d’un certain seuil de concentration induisent une synchronisation de l’expression génétique au sein de la communauté bactérienne. Ce mécanisme est impliqué dans plusieurs processus bactériens tels que la luminescence, la formation du biofilm, ce qui en fait une cible privilégiée pour l’inhibition du biofilm bactérien nuisible aux activités humaines. Plusieurs systèmes QS ont été identifiés ; les plus étudiés sont le système AHL (acyl homoserine lactone) et le système AI2 (auto inducteur 2). L’objectif principal de cette thèse est de caractériser le(s) système(s) QS de Shewanella woodyi, une bactérie marine luminescente capable de coloniser rapidement une surface et de former un biofilm. L’utilisation de biosenseurs de référence et des expériences de LC-MS ont montré que S. woodyi synthétise la C8-HSL et l’AI2. La mutation des gènes impliqués dans la synthèse ou la détection des HSL abolit la luminescence mais n’affecte pas la formation du biofilm. De plus, le système AI2 ne semble pas impliqué dans la luminescence et la formation de biofilm de S. woodyi. L’absence d’un récepteur d’AI2 suggère que cette molécule n’a pas un rôle régulateur et qu’elle ne serait qu’un produit secondaire du métabolisme cellulaire. Ce travail a donc permis de caractériser les 2 principaux systèmes QS de S. woodyi et pourrait permettre d’en faire un nouveau biosenseur marin."
"Les résultats de simulation numérique directe (DNS) en turbulence homogène isotrope (THI) pour un fluide Newtonien et pour un fluide visco-élastique (basé sur le modèle FENE-P) montrent que la visco-élasticité modifie qualitativement le comportement des plus petites échelles : nous observons une loi en k −6 dans la zone dissipative du spectre d'énergie cinétique. Nous montrons que c'est une caractéristique robuste quasiment indépendante de la dynamique des grandes échelles. Nous montrons plus loin que la réduction de traînée dans un tel écoulement, mesurée par la différence de la dissipation d'énergie entre fluide Newtonien et fluide visco-élastique, dépend fortement de la condition initiale."
fr_abstract_s
"Aux termes de l’article 5 alinéa 2 de la loi du 30 septembre 1986, « les membres du CSA ne peuvent, directement ou indirectement, exercer des fonctions, recevoir d’honoraires, sauf pour des services rendus avant leur entrée en fonction, ni détenir d’intérêts dans une entreprise de l’audiovisuel, du cinéma, de l’édition, de la presse, de la publicité ou des télécommunications ». Or, le départ précipité en mars 2002 d’un conseiller, concomitamment à une enquête menée par la Cour des comptes en raison de la détention par celui-ci de stocks-options d’un grand groupe de communication audiovisuelle, a relancé la question de l’indépendance des membres de l’organe de régulation de l’audiovisuel. Cette démission a soulevé la question de la définition de la notion d’ « intérêts », le CSA n’ayant jamais fixé de règles précises à ses membres en matière de détention de patrimoine. C’est la première fois que la question se posait de savoir si les dispositions de 1986 interdisaient la détention de stock-options. Cet article était également l’occasion de comparer les règles auxquelles sont soumis les membres de différentes autorités de contrôle et de régulation en matière de conflits d’intérêts. Il permet ainsi de mieux connaître les règles de déontologie applicables en la matière aux diverses autorités administratives indépendantes et d’affirmer la nature particulièrement contraignante de celles du CSA."
"La Convention européenne de sauvegarde des droits de l’homme et des libertés fondamentales (CEDH), adoptée le 4 novembre 1950 dans le cadre du Conseil de l’Europe, consacre à son article 9 la liberté de pensée, de conscience et de religion et prévoit les restrictions légitimes. Inspiré par la Déclaration universelle des droits de l’homme de 1948 et le Pacte international relatif aux droits civils et politiques de 1966, l’article 9 de la CEDH constitue la garantie sur le plan européen des principes énoncés au niveau international et les soumet à un contrôle judiciaire. La jurisprudence de la Commission et de la Cour européenne des droits de l’homme semble octroyer aux individus des droits étendus dans la mise en œuvre de la liberté de pensée, de conscience et de religion. Il s’agissait de déterminer la portée exacte d’une telle liberté, compte tenu des impératifs de protection de la santé parfois impliqués. Deux aspects de la dignité de l’individu entrent alors en jeu. Certaines dispositions de droit interne imposent explicitement de garantir la santé et la vie ou interdisent de leur porter atteinte. Il ressort de l’analyse menée que la CEDH elle-même dicte alors expressément aux États la solution de conflits éventuels entre ces règles nationales et la liberté de manifester sa religion ou ses convictions. Les restrictions à une liberté, « prévues par la loi », doivent constituer des « mesures nécessaires dans une société démocratique » (article 9 §2). Des exemples sont étudiés afin de préciser une telle formulation ainsi que les modalités de mise en œuvre. Mais en l’absence de telles législations internes, des conflits opposant la protection de la santé et la liberté de pensée, de conscience et de religion sont également susceptibles de survenir. A titre d’exemple, il existe des rites religieux qui, sans être interdits par la loi, portent atteinte à l’intégrité physique mais revêtent une signification symbolique. Ces rites varient d’une communauté à l’autre et selon l’époque. La CEDH ne prévoit explicitement aucune solution à de telles situations et ne permet pas de délimiter les droits de chacun sur son propre corps. En France, l’étude montre que le libre arbitre de l’individu l’emporte. Deux types de cas sont identifiés dans la deuxième partie des développements : celui où une loi protège les convictions des individus (cas de l’interruption volontaire de grossesse mettant en jeu l’intégrité du fœtus) et celui où le consentement, préalable aux soins, est indispensable (cas du refus de transfusion sanguine des Témoins de Jéhovah). Cet article permet de confronter deux impératifs qui découlent du principe de la dignité humaine, reconnu par le Conseil constitutionnel depuis l’examen en 1994 des lois relatives à la bioéthique. Si ce principe ne connaît aucune dérogation, on peut admettre que des droits et libertés qui en sont dérivés soient confrontés, afin que soit réalisé un équilibre assurant le respect le plus adéquat de la dignité humaine."
"Les conséquences sur les rapports juridiques entre personnes privées d’une directive communautaire non transposée en droit interne ont trouvé le 14 juillet 1994 une solution : une directive ne peut pas créer, par elle-même, d’obligations à l’égard d’une personne privée. La solution alors donnée par la juridiction communautaire ne paraît pourtant pas incontestable. Une série d’arguments avancés en faveur de l’effet direct horizontal semblent pouvoir être relevés. Il s’agissait d’identifier ces arguments, de justifier leur pertinence et de souligner que l’analyse de la Cour de justice pouvait être réfutée. On montre que s’il existe d’autres procédés qui permettent d’améliorer la mise en œuvre des dispositions communautaires (obligation pour le juge national d’interpréter les règles de droit interne de sorte qu’elles soient en conformité avec les directives, mise en jeu de la responsabilité d’un État pour non transposition d’une directive), seul l’effet direct horizontal semble susceptible de donner une pleine efficacité à l’harmonisation que suppose le droit communautaire."
"Le constat de départ est celui de l’existence de rapports privilégiés entre les États européens et ceux du bassin méditerranéen. Les raisons en sont multiples et reposent sur les liens historiques et culturels, les migrations humaines, les flux financiers et économiques... L’établissement d’un partenariat de l’Union européenne avec les pays de la Méditerranée concrétise cet état de fait. L’objet de cet article est de mettre l’accent sur l’importance stratégique que revêtent les États du sud pour l’Europe. Il s’agit en effet de souligner, d’une part que le partenariat est nécessaire à l’Union qui ne saurait maintenir sa stabilité et sa prospérité sans celle de ses voisins immédiats, d’autre part que le partenariat est indispensable aux pays méditerranéens qui attendent de la Communauté l’accès au marché, des investissements ainsi qu’une meilleure compréhension sur le plan culturel. Cette étude démontre qu’il est possible de distinguer deux périodes clés dans la politique méditerranéenne de l’Union européenne : -une étape de prise de conscience par la Communauté de la nécessité de prendre en considération les pays du Sud et de mettre en place des relations euro méditerranéennes en matière douanière. Il est remarquable que les États ayant conclu des accords d’union douanière (Grèce, Espagne, Portugal) soient devenus membres de l’Union ou aient posé leur candidature en vue d’y adhérer (Turquie). Par conséquent, de tels accords douaniers peuvent être envisagés en tant que prélude à une appartenance aux Communautés et leur contenu devait être analysé. -une période de transformation des relations après 1995 et de développement d’une coopération dans tous les domaines avec les pays du Maghreb et du Machrek. On rappelle qu’une Europe paisible, stable et prospère est difficilement concevable sans qu’il en soit de même pour le bassin méditerranéen. Il convenait ici de justifier le recours aux diverses formes d’association (accords d’union douanière, de coopération, euro méditerranéen) et de préciser leurs spécificités. Les mécanismes fondamentaux des accords les plus élaborés sont analysés de manière à cerner les éléments susceptibles de faire progresser la coopération euro méditerranéenne. Il s’agit d’inciter les États de l’UE et du pourtour méditerranéen à un dialogue entre partenaires très différents les uns des autres. Le volet financier de la politique euro-méditerranéenne ne pouvait être ignoré. Le budget communautaire engagé en faveur de la région méditerranéenne est révélateur de l’importance stratégique de cette dernière. L’utilisation des fonds souligne l’accent mis sur certains programmes prioritaires et permet d’identifier des secteurs privilégiés de coopération. L’analyse menée permet de dresser un bilan des avancées du processus euro méditerranéen, d’envisager de stimuler certaines actions et de répondre aux critiques parfois lancées contre l’inefficacité du processus enclenché. On n’ignore cependant pas que le processus de coopération euro méditerranéen est relativement récent et que son originalité n’a pour égal que l’ambition qu’il poursuit."
"Ce commentaire d’arrêt cherche à déterminer, en matière de santé, les droits des étrangers en situation irrégulière en Italie. Il s’agissait de savoir si ces derniers possèdent les mêmes privilèges que les nationaux. Selon la Cour constitutionnelle italienne, les étrangers disposent du droit de recevoir des traitements médicaux, la Constitution assurant « à tous » la garantie d’un noyau irréductible de droits dont fait partie la protection de la santé en tant que composante de la dignité humaine. Le droit aux soins est-il susceptible d’être revendiqué de la même manière par les individus dont la présence sur le territoire national est illégitime ? Il convenait ainsi de déterminer les obligations à la charge des professionnels de la santé. La solution italienne est rapprochée de celle qui est en vigueur en France où le contexte est analogue : le préambule de la Constitution de 1946 « garantit à tous... la protection de la santé » et le Conseil constitutionnel rappelle que si le législateur peut prendre à l’égard des étrangers des mesures spécifiques, « il lui appartient de respecter les libertés et droits de valeur constitutionnelle reconnus à tous ceux qui résident sur le territoire de la République » (décision Maîtrise de l’immigration, 13 août 1993, §2)."
"Actualisation de la rubrique ""Enrichissement sans cause"
"La présentation à une chambre de compensation équivaut à une présentation au paiement. Par ailleurs, les chèques ou les effets de commerce qui ne sont pas rejetés dans le délai déterminé de compensation sont considérés comme payés. Selon la jurisprudence cette règle n'a de ""valeur contractuelle qu'entre les banquiers qui y adhèrent et ne saurait s'imposer à leurs clients, ni être invoquée par eux, à leur profit"". La présomption de paiement qui en découle, et qui ne peut donc jouer qu'entre banquiers - banquier tiré ou domiciliataire et banquier présentateur - ne devrait pas s'opposer à ce que dans l'hypothèse de l'émission d'un chèque sans provision ou de l'insolvabilité du tiré, débiteur d'une lettre de change, le profit, procuré par ce paiement, puisse être contesté par le banquier tiré ou domiciliataire qui a laissé passé le délai de compensation et qui souhaite agir contre l'émetteur d'une traite ou d'un chèque ou contre le bénéficiaire d'un chèque. L'enrichissement injustifié ou la répétition de l'indu vont lui permettre d'agir. L'analyse prend appui sur la jurisprudence pour déterminer l'efficacité de ces mesures en étudiant les recours du banquier tiré ou domiciliataire contre le tireur du titre, ou contre le bénéficiaire de ce titre."
"Le législateur en 1975 (L. n° 75-1334, 31 décembre 1975 relative à la sous-traitance) a voulu assainir les pratiques professionnelles, dans le domaine de la construction, en s'efforçant de donner aux sous-traitants de marchés, privés ou publics, une protection efficace contre les conséquences, souvent désastreuses pour eux, de la défaillance de l'entrepreneur principal. Au vu de la jurisprudence, il convient de s'interroger sur l'efficacité de l'action directe, en tant que garantie de paiement, accordée aux sous-traitants de marchés privés. Il faut en effet compter avec d'autres situations avantageuses conférées par le législateur à certains créanciers et pouvant résulter du droit des sûretés, du droit cambiaire ou plus largement du droit du crédit. Des conflits peuvent naître, de la confrontation de ces différents droits, qui sont d'autant plus variés que les modalités de transmission de créances le sont elles-mêmes. Les conflits envisagés opposent le sous-traitant de marchés privés au banquier dispensateur de crédit. L'action directe est comparée au privilège du créancier nanti sur marchés privés ainsi qu'à diverses formes de transmission de créances."
"Les moyens de prévention des difficultés des entreprises ne cessent de se renforcer et de se diversifier au fil des réformes du droit des entreprises en difficulté. La rédaction de cette chronique a été terminée en octobre 1999. On raisonnait alors essentiellement en termes de mandat ad hoc et de règlement amiable. Un projet de réforme du traitement des difficultés des entreprises -inclus dans le programme plus vaste de réforme de la justice consulaire -, était à l'étude à la Chancellerie et son volet consacré à la loi du 1er mars 1984 sur la prévention des difficultés suscitait l'intérêt des commentateurs. Il s'agissait de reconsidérer les moyens de détection et de prévention afin de tenter d'éviter le traitement judiciaire des difficultés, ou, à défaut, de permettre le déroulement de la phase judiciaire dans des conditions nécessairement plus favorables à tous ceux qui, à des titres divers, sont concernés par la vie de l'entreprise. Des avancées notables seront constatées avec la loi de sauvegarde des entreprises du 26 juillet 2005 qui sera suivie d'autres réformes. Dans le contexte de l'époque, la preuve de l'utilité des modes spécifiques de règlement des litiges n'était plus à faire, mais il fallait en accroître l'efficacité en les rendant plus opérationnels et en en facilitant l'accès. Parler de prévention du risque de défaillance d'une entreprise, et non de ses difficultés, était à cet égard symptomatique de la nécessité d'une évolution législative."
"Dans une hypothèse de virement bancaire, la banque qui reçoit les fonds doit créditer le compte du bénéficiaire afin que ces fonds soient mis à la disposition du titulaire du compte. La banque dépositaire ne saurait impunément différer la date de mise à disposition des fonds pour refuser d'honorer un titre émis et assigné sur ce compte, alors qu'il présente un solde nécessairement positif au jour de la présentation du titre au paiement. La condamnation de la banque doit être appréciée indépendamment du jeu éventuel des dates de valeur."
"Le droit peut entretenir avec les notions de faute et de causalité des relations complexes. Le constat n'est pas nouveau. il s'agit de provoquer, au profit des victimes, une ""dilution de la charge des dommages"". Les différentes formes que peut revêtir cette complexité ont en commun de reposer sur le jeu subtil de variations sur les thèmes de la faute et du risque. C'est dans un espace de liberté, qui pourrait être laissé à la faute, dans un système de responsabilité objective, que réside l'originalité de la responsabilité du fait d'autrui. Son évolution reflète le ballet incessant entre ces deux fondements de la responsabilité, lesquels, au rythme des partitions interprétées, se heurtent ou se confondent - on a pu parler de risque-fautif-, ou encore retrouvent droit de cité dans une coexistence critiquée ou approuvée. Deux grandes tendances se dégagent. La faute est tout à la fois atteinte et redécouverte dans les régimes spéciaux de responsabilité du fait d'autrui (C.civ., art. 1242, al.1, 4 et 5, Comp.art. 1384, al. 1, 4 et 5 anc.)."
"Refonte de la rubrique ""Enrichissement sans cause"" (Généralités ; conditions de l'action de in rem verso ; effets de l'action de in rem verso), dans le contexte doctrinal, jurisprudentiel et législatif de l'année 1998."
"L'arrêt de la chambre commerciale du 8 février 1994 rappelle qu'en présence de sociétés fictives, soumises à un redressement ou à une liquidation judiciaire, l'unité de procédure s'impose ; il précise principalement que la confusion de patrimoine ne saurait être un critère de fictivité. En l'espèce, un gérant commun à quatre sociétés avait poursuivi, en se servant de cadres juridiques différents mais fictifs, la même activité commerciale. Alors que l'unicité patrimoniale est originelle en matière de fictivité, la confusion de patrimoine suppose, pour sa concrétisation, la présence de deux patrimoines au moins dont les éléments actifs et passifs ne peuvent plus être dissociés en raison essentiellement de l'anormalité observée des mouvements ou des flux financiers. Le temps est révolu, où, alors que la fictivité de tous les apports était établie, la confusion de patrimoine a pu être présentée par les juges comme l'indice, se suffisant à lui-même, du caractère fictif de la société, sauf à admettre que pareille confusion de patrimoine doit alors s'entendre d'une confusion réalisée entre les éléments d'un patrimoine qui n'a jamais cessé d'être unique. Aujourd'hui, ces deux types d'abus de la personnalité morale, que sont la fictivité et la confusion de patrimoine, ont des critères propres précisés par la jurisprudence. La loi de sauvegarde des entreprises du 26 juillet 2005 les reconnaît en tant que fondements de l'extension véritable de procédure collective, emportant unité de procédure et de masse. La confusion de patrimoine demeure caractérisée sur le plan procédural par des effets spécifiques prenant en compte l'existence de patrimoines distincts puis devenus inextricablement mêlés."
"S'il est indispensable de préserver la stabilité financière et donc l'intégration du système financier européen - cela est d'ailleurs en marche -, il faut aussi encourager la prise de risques économiques. La question centrale posée dans ce contexte est celle de l'effectivité des investissements. Une politique monétaire accomodante n'est pas sans dangers. La transparence est encouragée mais les acteurs financiers ne sont pas nécessairement au service de l'économie réelle. Un plan européen d'investissement existe et les PME peuvent obtenir des prêts auprès de la Banque Européenne d'Investissement. Dans l'optique d'une reprise, il faut privilégier l'adéquation du volume des crédits à la demande. Toutefois, le critère déterminant s'agissant de la relance du crédit reste la confiance restaurée des entreprises et des ménages. Les régulations bancaires et la politique monétaire de la BCE (QE) pourront alors se rejoindre. Des réformes structurelles s'imposent. La stabilité financière est une quête. La prise de risques économiques est une exigence plus immédiate. Des mesures sont adoptées en ce sens par les Etats."
La recevabilité de l'action individuelle des actionnaires- la société serait-elle soumise à un procédure collective - n'est pas subordonnée à la preuve que les fautes des dirigeants sont séparables des fonctions sociales. L'indemnisation du préjudice personnel est accordée à hauteur de la perte d'une chance.
"L'activité médicale a un contenu riche et varié. L'exigence de l'indemnisation des victimes d'accidents médicaux est de plus en plus largement satisfaite. Le recours ciblé à l'obligation de sécurité de résultat y contribue. Dans la sphère du contrat médical, et s'agissant de l'obligation de soins stricto sensu, l'obligation de moyens a seule droit de cité. La théorie du risque n'a pas trouvé grâce aux yeux de la Cour de cassation pour justifier l'indemnisation de l'aléa thérapeutique. Une loi était attendue, elle sera adoptée le 4 mars 2002( loi n°2002-303, relative aux droits des malades et à la qualité du système de santé, JO, 4 et 5 mars 2002, p.411 s. ; le recours à la solidarité nationale y est prévu pour l'indemnisation de l'aléa thérapeutique). Dans le droit antérieur, si les divergences en la matière entre les jurisprudences judiciaire et administrative doivent être relevées, une volonté de rapprochement entre les deux ordres de juridiction est cependant indéniable à propos de la question fondamentale -et dans ses différents aspects- de l'information due aux malades."
"L'activité entrepreneuriale doit être préservée et encouragée. En France, le droit des sociétés, le droit des entreprises en difficulté, notamment, répondent dans nombre de leurs dispositions à cet impératif. Ainsi l'anticipation du risque de défaillance ne peut être dissociée de la prise de risques économiques adaptée à la situation financière de l'entreprise qui peut craindre un état de cessation des paiements ou le vivre. Il en résulte un renforcement mais aussi un renouvellement des moyens de l'anticipation largement entendue selon que l'on raisonne en termes de conciliation ou de plans de sauvegarde ou de redressement. Le sauvetage de l'entreprise est à ce prix. On observe une nouvelle répartition des droits entre acteurs internes et externes à l'entreprise. Les créanciers sont parfois dans une situation d'attentisme, ainsi lorsqu'ils sont titulaires de sûretés, mais il peut arriver que leurs pouvoirs soient accrus face à un débiteur et à son actionnaire majoritaire. Si la cession des droits sociaux est libre, elle peut aussi être forcée. L'évolution législative et la jurisprudence attestent de ce nouvel ordonnancement de la place et du rôle de chacun. Le droit français des entreprises en difficulté privilégie la recherche d'un équilibre entre la sauvegarde de l'entreprise et l'offre ou le maintien du crédit qui peut être de nature bancaire. Le droit européen s'inspire du droit français et souhaite en accroître l'efficacité."
"La cessation des paiements permet de distinguer l'amiable et le judiciaire dans le traitement des difficultés des entreprises. Le droit français, en la matière, tirant les enseignements de la jurisprudence, permet, depuis l'adoption de la loi du 26 juillet 2005, que la procédure de conciliation, substituée au règlement amiable, puisse être ouverte alors que le constat est fait d'un état de cessation des paiements existant depuis 45 jours au plus. La même observation peut aujourd'hui être faite à propos des procédures de sauvegarde accélérée. Qu'en est-il en droit marocain et en droit OHADA ? La réponse doit être nuancée. Si le renforcement de l'approche économique du droit des procédures collectives est incontournable, encore faut-il, dans l'évolution constatée ou attendue, prendre en compte l'impact de l'environnement économique et social. Il en est ainsi s'agissant de la place et du rôle de la cessation des paiements, comme il en est dans d'autres domaines en lien avec le droit ici concerné. La culture des Etats est déterminante."
"Les questions de genre dans le développement pour l’avancement des droits des femmes et de leur statut ont connu un intérêt croissant au niveau universel, notamment depuis les années 1990 et la Conférence mondiale sur les femmes du Beijing de 1995.L’enthousiasme suscité autour de ces questions est ainsi illustré par les moyens mis en œuvre au Timor par la communauté internationale dans ce domaine en parallèle du processus d’établissement de l’Etat de droit et de construction de ce nouvel Etat dans le contexte post-conflit dans lequel celui-ci se trouve, l’assimilant parfois à un « laboratoire » des Nations Unies pour la mise en œuvre des principes internationaux relatifs à l’établissement de l’Etat de droit et à l’approche de genre dans la reconstruction et la gestion d’un Etat ainsi que des mécanismes de contrôle des droits de l’homme au bénéfice de la femme. Se basant sur une recherche empirique, la présente étude tente ainsi d’analyser de quelle manière le droit international influence non seulement les droits des femmes au Timor d’un point de vue formel, mais aussi et surtout leur statut au sein de la société timoraise."
"Les Cahiers du C.D.P.C. ont toujours eu "" pour ambition d'être utiles en informant et, chaque fois que cela est possible, en allant plus loin "" (JCE). Informer : ce nouveau numéro de la Revue a bien cet objectif. La réforme de la procédure d'appel est entrée en vigueur en 2011. Nul doute que les actes de la Journée d'études organisée par le CDPC JEAN-CLAUDE ESCARRAS et le Barreau de Toulon trouvaient leur place dans cette livraison. Aller plus loin : la vocation première des Cahiers, l'étude du droit comparé, est poursuivie. Ce numéro met à l'honneur les premières Journées de la Jeune Recherche qui se sont déroulées à l'Université du Sud Toulon-Var. Les jeunes chercheurs français et italiens ont contribué au sein de l'atelier du CDPC JEAN-CLAUDE ESCARRAS à rendre compte de l'état du droit positif sur le thème "" Droit et vin "" dans l'espace euro-méditerranéen."
"Selon une formule célèbre "" le journalisme, c'est la vie "" (J. Fauvet)... Au cœur de cette vie saturée par la multiplicité et marquée du sceau de la différence, la tâche du journaliste consiste à trier les faits susceptibles de nourrir l'actualité. La parole journalistique assume ainsi la responsabilité de proposer un cadre interprétatif au réel. En ce sens, le journaliste qualifie l'événement, c'est ce qui lui confère une place particulière dans tout espace de communication (D.Wolton). Cette manière de penser l'évènement, de le nommer, doit nécessairement faire l'objet d'un questionnement épistémologique. Ce questionnement s'impose d'évidence au regard d'une pratique journalistique en mutation s'appliquant à un espace méditerranéen lui-même caractérisé par sa complexité. Le monde méditerranéen est, en effet, circonscrit par l'affirmation paradoxale d'une identité commune et par l'existence de variations dans notre conception des identités. Cette complexité, liée à l'espace géographique, doit être confrontée au développement récent des réseaux. Ces derniers engendrent une redéfinition des frontières de l'espace public, une déterritorialisation possible du contexte de réception et favorisent l'apparition de nouvelles pratiques de production de l'information. L'avènement de la "" société de l'information "" a ainsi bouleversé les habitudes de la profession. Les possibilités de diffusion, de recueil et de traitement de l'information se sont élargies entraînant une mutation des modèles de production de l'information. Désormais, chacun a la possibilité d'occuper une place dans un espace communicationnel, d'être le témoin de son époque, au jour le jour, en diffusant, sans limite ou presque, les informations de son choix. Chacun peut alors s'affranchir des contraintes liées au journalisme traditionnel. Cette mutation implique un véritable partage du pouvoir de "" nommer "", et de "" faire "" l'événement. La prise en compte de cette nouvelle réalité impose ainsi une réflexion éthique et sociétale sur l'exercice de cette responsabilité. Cette analyse doit s'articuler sur la constitution d'un espace public étendu, aux dimensions d'un espace politique méditerranéen caractérisé par une certaine complexité. * Axe 1 : Le pouvoir de nommer et la construction de l'évènement dans l'espace euro méditerranéen. * Axe 2 : Nouveaux médias et logique "" dialogique "" en méditerranée. * Axe 3 : Mutation des modèles de production de l'information : perspectives pour une éthique renouvelée."
"Les autoroutes de l’information entraînent des changements majeurs dans le monde de la communication où la liberté d’expression et la liberté du commerce sont les principes directeurs. L’analyse développée dans cet article montre que, contrairement aux idées reçues, il est possible d’envisager un cadre juridique susceptible de régir en Europe le contenu des informations véhiculées sur les réseaux multimédias ainsi que l’accès à ces derniers. En effet, les infrastructures permettant une symbiose entre les secteurs de l’audiovisuel, des télécommunications et de l’informatique sont régies dans chaque État par des textes. L’objectif poursuivi est de démontrer que le fantôme du vide juridique, agité pour faire prévaloir à tout prix les principes du libéralisme, n’existe pas. Les législations diffèrent toutefois, plus ou moins précises, parfois peu adaptées aux autoroutes de l’information. Il s’agit donc de définir des mesures communes en Europe car la diffusion par Internet ne s’arrête pas aux frontières nationales, compte tenu de la nature même du réseau. Les autoroutes de l’information et les services multimédias soulèvent des questions qui ont le plus souvent été traitées au plan interne. Il est apparu intéressant d’étudier les règles en vigueur en France où le cadre de réglementation de ces nouveaux services de communication, loin d’être balbutiant, est élaboré et précis. On souligne d’abord que la loi s’applique pour régir de nombreuses questions : établissement des réseaux et leur sécurité, secret des correspondances, cryptage des messages, traitement des données nominatives... Le Conseil supérieur de l’audiovisuel, la Commission nationale de l’informatique et des libertés, le Conseil de la concurrence, le Conseil supérieur de la télématique sont autant d’autorités prévues par la législation, compétentes pour contrôler les informations diffusées. D’autre part, le droit des contrats vient combler les lacunes de la loi. On précise comment les contrats lient soit les opérateurs entre eux (Microsoft et Time Warner, British Telecom et Média 7 notamment), soit les fournisseurs d’accès aux utilisateurs de réseaux (contrats d’hébergement de Web, abonnements à Internet). Les questions de responsabilité des parties sont généralement prévues contractuellement. Le droit français est une référence utile pour favoriser à l’échelle communautaire le développement du multimédia. L’Europe doit élaborer un cadre juridique qui favorise la libre circulation des informations et des idées par les technologies modernes de transmission et qui garantit le respect des bonnes mœurs, de la jeunesse, des droits d’auteur, etc. L’article montre que les embryons de solutions existent. A plus lointaine échéance, l’harmonisation devrait s’étendre aux États-Unis et au Japon, lieux d’établissement des principaux fournisseurs d’accès à Internet."
"A l’heure de l’administration électronique, alors que des relations d’un type nouveau sont créées entre les citoyens et leur administration, cette dernière modifie ses modalités d’intervention. La perception qu’en ont les usagers est bouleversée. La contractualisation de l’action publique permet de fixer des objectifs à plus ou moins long terme, tout en garantissant la stabilité et la sécurité juridique, et donc d’apparaître comme un procédé efficace d’assurer l’exécution de décisions. Les moyens de l’administration, en même temps que ses finalités s’élargissent. Cet article rend compte de l’originalité de ce phénomène au plan communautaire et de la variété des conventions conclues. L’action normative communautaire ne tient pas toujours suffisamment tenu compte du rôle des régions et des collectivités locales dans la mise en œuvre des politiques définies par Bruxelles. L’article montre comment les collectivités peuvent être davantage prises en compte par l’UE. Il s’agit, dans le respect des dispositions en vigueur des traités, d’améliorer la mise en œuvre de certaines politiques communautaires grâce à des contrats et conventions d’objectifs tripartites qui lient collectivités régionales et locales, en plus des parties classiquement impliquées que sont les États et la Communauté européenne. Ces instruments constituent un moyen souple et novateur d’assurer la mise en application des règles adoptées par Bruxelles. L’objectif est de démontrer que l’on voit ainsi apparaître une nouvelle catégorie d’acteurs pilotant l’exécution du droit communautaire."
"Cet article veut montrer que la construction d’un espace audiovisuel européen puissant est nécessaire et possible. Vingt propositions sont à ce titre formulées pour servir de base à une politique capable de faire face à l’hégémonie américaine. Dans une telle optique, ce travail ne devait pas constituer une simple synthèse des lacunes juridiques mais proposer une série de mesures à partir desquelles les institutions européennes pourraient bâtir un plan cohérent : -il faut redresser l’industrie européenne des œuvres télévisuelles. Des mesures financières et fiscales d’incitation à la production et à la diffusion des œuvres européennes sont présentées, -il s’agit de remédier aux imperfections dans l’organisation et le fonctionnement de l’espace télévisuel (en matière de droits d’auteur et de droits voisins, dans le domaine du droit des concentrations de médias, en ce qui concerne l’adoption d’une norme numérique haute définition...). La Commission doit compléter les instruments juridiques en vigueur et proposer des réglementations spécifiques. Les mesures énoncées poursuivent un double objectif : parfaire l’espace audiovisuel européen et rendre le secteur de la télévision en Europe plus autonome. L’espace audiovisuel ne se réalisera pleinement qu’à la double condition de se parfaire à l’intérieur et de s’imposer à l’extérieur."
"Selon la doctrine classique, la notion de partie est incompatible avec la nature objective d'un contentieux. Cette approche, qui a longtemps été retenue dans le cadre du contentieux administratif de la légalité, a été ensuite transposée au contentieux constitutionnel. Ce dernier a, dès lors, été souvent décrit comme un "" procès sans parties "". Cependant, à la lumière du droit comparé, il est possible de dépasser cette appréciation par trop tranchée. En effet, dans de nombreux pays, le constituant et le législateur ont organisé le contrôle de constitutionnalité sous la forme d'un procès entre des plaideurs défendant des intérêts opposés ou des interprétations divergentes de la Constitution. Néanmoins, les caractéristiques du contentieux constitutionnel ont nécessité l'adaptation des principes procéduraux appliqués par les juridictions ordinaires. Cette indispensable adaptation se traduit, le plus souvent, par la limitation de la marge de manoeuvre et du rôle des plaideurs. Toutefois, ces restrictions ne sont nullement le signe d'une quelconque incompatibilité entre la notion de partie et le contrôle de constitutionnalité, elles répondent simplement aux spécificités du contentieux constitutionnel."
"La chronique consacrée par la revue Constitutions à la fonction juridictionnelle aura pour vocation première de rendre compte de l'actualité relative au rôle et à la place du juge dans le système institutionnel et normatif, qu'il soit français, étranger, européen ou international. Elle n'exclura pas pour autant les analyses théoriques, toujours indispensables à l'appréhension des réalités politiques et juridiques. Une large place y sera délibérément accordée au comparatisme et c'est d'ailleurs par celui-ci que s'ouvre cette première chronique consacrée au commentaire de la décision de la Cour constitutionnelle italienne du 7 octobre 2009 censurant la loi dite « Alfano » instituant une immunité judiciaire temporaire au bénéfice des titulaires des quatre plus hautes fonctions politiques de l'État italien. Le lecteur saura certainement en tirer une comparaison fort utile avec les solutions retenues en droit positif français."
"La proposition de directive communautaire susmentionnée, du 27 novembre 1992, a soulevé des problèmes constitutionnels en France notamment car la loi du 6 janvier 1978 relative à l’informatique, aux fichiers et aux libertés garantit une protection des personnes physiques parmi les plus élevées en Europe. De façon générale, se posait la question de déterminer le bien fondé d’un instrument communautaire dans le domaine du droit des personnes, qui ne relève pas a priori du champ de compétences des Communautés européennes. En outre, il s’agissait de savoir si un texte supranational pouvait abaisser le niveau de protection des droits des individus garanti constitutionnellement, la loi de 1978 contribuant au respect de la vie privée protégée par la Déclaration des droits de l’homme et du citoyen. L’objet de l’analyse était donc de répondre non seulement aux contestations relatives à la légitimité de la proposition communautaire mais également aux critiques sur le fond. Une comparaison de la loi de 1978 et de la proposition communautaire, article par article, est à la base de ce travail qui souligne un aspect des difficultés d’articulation entre ordres juridiques, national et supranational. Cet article apporte des précisions sur l’appréciation de l’effet de cliquet et sur le contrôle de constitutionnalité en matière de protection des droits des personnes, plus particulièrement du respect de la vie privée. Le texte de la Commission, en cours d’élaboration à l’époque de la rédaction de cet article, a été adopté depuis et la loi de 1978 a été remaniée sur certains points en vue de transposer la directive. Mais le principe de l’effet de cliquet anti-retour, qui impose au législateur dans l’exercice de ses pouvoirs de ne pas priver de garanties légales des exigences de caractère constitutionnel, continuait à soulever des questions de mise en œuvre compte tenu de l’adoption du texte communautaire. Le Conseil constitutionnel, qui avait été saisi de la loi de 1978 modifiée, a estimé en 2004 que si l’on appliquait strictement le principe de l’effet de cliquet anti-retour, « la législation devrait être toujours plus protectrice... La protection de tel ou tel droit deviendrait une exigence sans cesse croissante ce qui jouerait au détriment d’autres droits ou d’autres exigences de valeur constitutionnelle tout aussi éminents mais historiquement en retard dans leur protection législative » (décision n° 2004-499 DC du 29 juillet 2004). Dans sa version définitive, la directive prévoit que ses dispositions constituent un standard minimum susceptible d’être complété par les Etats membres. Avec l’approbation de Bruxelles, la France a ainsi pu maintenir sa législation plus protectrice, prévoyant notamment des formalités préalables à la création d’un traitement de données, des conditions rigoureuses relatives au contenu du fichier, un contrôle a posteriori renforcé. Soulignons enfin que cet article permet d’illustrer les retards régulièrement imputables à la France en matière de transposition des actes communautaires : on peut en effet déplorer qu’il ait fallu sept ans pour transposer la directive sur la protection des personnes physiques à l’égard du traitement des données à caractère personnel."
"La notion de dangerosité a intégré le champ pénal et est aujourd’hui plus que jamais au cœur des politiques criminelles dont l’objectif principal est d’assurer la protection de la société. Toutefois, le caractère incertain de cette notion suscite des interrogations tant chez les professionnels du droit que chez les experts cliniciens. L’évaluation de la dangerosité étant imparfaite en raison entre autres de l’absence de définition légale de la notion ainsi que de l’interaction des conceptions criminologique et psychiatrique de la dangerosité, le traitement de celle-ci s’en trouve nécessairement affecté. Face à ces difficultés, le législateur a toutefois exprimé sa volonté de lutter contre la dangerosité en faisant le choix d’une politique pénale sécuritaire dont la conformité aux libertés individuelles peut poser problème."
"La comparaison des statuts suisse et français de la copropriété des immeubles bâtis permet d'apprécier l'originalité de deux solutions très opposées. Le statut français s'attache principalement à protéger le droit individuel de propriété, ce qui explique son succès quantitatif, alors que le texte suisse de la propriété par étages reflète une vision essentiellement communautaire. De plus, la liberté contractuelle est une donnée fondamentale en Suisse, ce qui n'est pas le cas en France avec un statut impératif et en constante évolution. La multiplication des copropriétés en difficultés est ainsi devenue en France un enjeu de politique publique, alors que le législateur suisse considère qu'il n'appartient pas aux pouvoirs publics d'apporter toutes les réponses aux problèmes d'ordre privé. Mais le droit est nécessairement marqué par l'histoire et la société dans laquelle il se manifeste."
"Cette note de jurisprudence a pour ambition d’examiner la constitutionnalité de l’article 15 de la loi italienne n° 47 de 1948 sur la presse. Une telle disposition soulève en effet des difficultés dans la mesure où elle prévoit des sanctions pénales en cas de publications « susceptibles de pouvoir perturber le sentiment commun de la morale ». L’imprécision de la formulation pourrait engendrer des dérogations à la liberté de publication, fréquentes et non clairement circonscrites. Il s’agissait de savoir si le concept philosophique de « sentiment commun de la morale » pouvait être traduit en termes juridiques. A cet effet, la jurisprudence de la Cour constitutionnelle italienne est analysée. Il en ressort que la Haute juridiction semble assimiler la morale au respect de la dignité de la personne humaine, protégé par l’article 2 de la Constitution. La référence à un concept élastique trouve donc une limite juridique et le danger d’un développement arbitraire des incriminations semble exclu. Les différents organes judiciaires procèdent à une évaluation attentive des faits et des mentalités. J’ai alors tenté de comparer les solutions retenues en Italie et en France. Quelle hiérarchie la loi Guigou, relative au renforcement de la présomption d’innocence et des droits des victimes, adoptée le 15 juin 2000 (c’est-à-dire à la même période que la sentence de la Cour italienne), consacrait-elle entre la dignité humaine et la liberté d’expression ? Le champ d’application de l’article 38 du Code pénal français, qui punit les reproductions de crimes et délits, était-il réduit et subordonné à l’existence d’une menace pour la dignité de la personne ? Il convenait d’analyser les arrêts de la Cour de cassation."
"Les organismes génétiquement modifiés (OGM) voient leur patrimoine génétique transformé par l’insertion de gènes issus d’un autre organisme vivant. Des avantages considérables justifient d’y recourir : ils rendent les produits agricoles plus résistants aux maladies et aux insectes, réduisent la quantité de pesticides utilisés, et peuvent augmenter significativement la productivité. Les cultures transgéniques représentent un marché colossal. Toutefois, par le biais de la pollinisation, la diffusion des gènes de résistance à d’autres plantes pourrait représenter un danger sanitaire, entraînant notamment des réactions d’intolérance allergique ou rendant certains antibiotiques inefficaces chez les consommateurs. Cette étude souligne que les OGM, au carrefour du droit communautaire et du droit international compte tenu de l’internationalisation des échanges, devaient faire l’objet de règles supranationales. Il s’agissait de savoir si ces dernières avaient été adoptées et si elles étaient adaptées aux enjeux commerciaux, sanitaires, environnementaux et éthiques. Sur le plan régional, il fallait déterminer la position de la Communauté européenne et exposer comment cette dernière a concilié progrès technologique, nécessités économiques et préoccupations des consommateurs. L’étude propose de faire le point sur les obligations communautaires qui incombent aux producteurs et importateurs d’OGM et sur les lacunes à combler. A ce titre, est analysée la procédure d’autorisation de mise sur le marché envisagée par la Commission ainsi que l’évolution juridique qui résulterait de l’approbation de cet instrument par le Conseil. Différentes phases sont identifiées selon qu’il existe ou non une objection de la part de l’autorité compétente d’un État membre, vis-à-vis du dossier de demande d’autorisation transmis par la Commission. Par ailleurs, il a paru important de prendre en compte les critiques formulées à l’encontre de la proposition communautaire. L’objectif était d’identifier les dispositions qui méritaient d’être modifiées, afin de faire évoluer le cadre juridique vers davantage de clarté. Les conditions de mise en œuvre de la responsabilité d’une entreprise agroalimentaire en cas de dommages sanitaires causés par des OGM doivent donc être définies : les dispositions communautaires ne prévoient aucun régime spécifique et la seule solution est de recourir aux règles générales du droit civil interne. Mais le lien de causalité entre le préjudice et l’activité d’un producteur de semences, nécessaire pour engager dans l’ordre juridique national la responsabilité pour faute, est difficile à prouver. De plus, les règles d’étiquetage méritent d’être revues. Le droit communautaire prévoit certes des obligations en la matière mais des questions demeurent, qui nuisent à la protection des consommateurs. Que préciser sur l’étiquette ? Quelles sont les sanctions encourues en cas d’irrégularités dans l’étiquetage ? Quelle doit être méthode utilisée pour quantifier la proportion d’OGM dans un produit ? L’intérêt de cette étude consiste également à analyser les avancées de l’accord international adopté sur le commerce des OGM. L’établissement de règles contraignantes en Europe risque en effet d’accroître la menace d’un affrontement avec les États-Unis et les dispositions du traité ne peuvent demeurer balbutiantes. Certes, l’accord a le mérite d’exister alors que son adoption a longtemps été compromise par les oppositions entre les pays qui produisent et commercialisent des OGM (tels les États-Unis, le Canada, le Brésil, le Mexique, l’Argentine, l’Australie) et l’Europe où seules quelques variétés sont autorisées à la culture et à l’importation. Mais la Communauté souhaitait que des règles de sécurité sanitaire se rapprochant des critères communautaires soient entérinées à l’échelle mondiale. Cette contribution tente donc d’intégrer les interférences entre les ordres juridiques en matière de sécurité alimentaire et d’analyser le contenu des règles internationales adoptées, au regard des dispositions communautaires."
"La notion de ""discriminations positives"", même si elle n'a pas encore été définie de façon stable, désigne une différenciation juridique de traitement, établie par l'autorité normative, dans l'objectif de favoriser des catégories de personnes au détriment d'autres, pour remédier aux inégalités subies par ces catégories dans le passé et dont les effets persistent encore aujourd'hui. Il ressort donc de cette définition sommaire que celles-ci créent des inégalités, pour parvenir à l'égalité réelle entre tous ces individus."
"Avec le développement de la compétition entre Empires coloniaux, divers modes d’appropriation des espaces se sont succédés. L’attribution des compétences relatives à un territoire sans maître s’est ainsi faite suivant des critères qui ont varié avec les siècles. Dans un premier temps, de tels territoires ont été concédés par décision pontificale. On sait que les terres d’Amérique qui venaient d’être abordées ont été attribuées par l’Église, dans la bulle Inter Cœtera du 4 mai 1493, aux souverains d’Espagne et du Portugal « au nom d’un pouvoir de répartition originaire sans équivalent ultérieur ». Mais l’époque de la Réforme a initié un déclin de l’autorité pontificale face aux souverainetés étatiques. Désormais la découverte, c’est-à-dire le fait d’être le premier à aborder un territoire jusqu’alors inconnu, s’impose comme l’acte nécessaire qui confère un titre de souveraineté. Ainsi, les puissances coloniales ont fréquemment déclaré, au moment des grandes conquêtes, que la découverte constituait une base suffisante pour une revendication de souveraineté sur les terres nouvelles. Ainsi, le représentant des États-Unis a fait valoir dans l’affaire de l’Ile de Palmas jugée par la Cour permanente d’arbitrage en 1928 que « la découverte en tant que telle c’est-à-dire le seul fait d’avoir vu le pays, sans qu’il y ait d’acte même symbolique de prise de possession, entraînait ipso jure la souveraineté territoriale et non pas simplement un titre non parfait (...) que devrait éventuellement compléter une prise de possession véritable et durable dans un délai raisonnable ». Toutefois, avec le temps, la théorie de l’occupation effective s’est imposée. Cette théorie repose sur une justification fonctionnelle : l’exercice de fonctions étatiques est l’élément essentiel par lequel la souveraineté s’acquiert et se maintient. L’article a pour but de montrer comment le juge international prend en compte les manifestations de l’autorité étatique. Certes, doivent être prises en considération les preuves matérielles qui se rapportent directement à la possession effective du territoire, c’est-à-dire les actes d’administration locale. L’occupation effective doit en effet se traduire par l’exercice des compétences étatiques traditionnelles comme l’édiction de règlements, de lois, dans les domaines politique, économique, militaire... Toutefois, on souligne que l’exigence d’effectivité s’adapte en fonction des particularités du territoire revendiqué, de son organisation sociale et politique ainsi que de l’attitude des États tiers. L’analyse s’appuie sur de nombreux différends ayant donné lieu à des sentences arbitrales. Il en ressort que la jurisprudence internationale considère que la notion d’effectivité n’est pas indépendante des critères géographiques, politiques, sociaux et culturels. Le degré d’effectivité exigé est donc relatif et doit être évalué au cas par cas. Dans le cas d’une île difficilement accessible, une simple déclaration faite à bord d’un navire à proximité de l’île, accompagnée de relevés géographiques et d’actes de surveillance, peut valoir occupation effective compte tenu de la difficulté pour accoster. On constate que l’effectivité ne vient souvent que confirmer la situation juridique. L’article mène à la conclusion selon laquelle le juge international, lorsqu’il recourt au principe d’effectivité, ne fait souvent que conforter des titres préexistants. Il adopte ainsi une position conservatrice favorisant la stabilité au détriment du changement et d’une attitude progressiste susceptible de remettre en cause la situation juridique."
"Il est fait état, dans cette étude en langue italienne à l’usage d’un lectorat spécialisé, des apports de la loi de révision du 28 mars 2003 relatifs à l’approfondissement et à la constitutionnalisation de la décentralisation administrative en France."
"Cette article, consacré au problème itératif des sources du droit en Italie, se veut une critique résolue de la logique compromissoire qui a présidé aux travaux de la Commission bicamérale pour les réformes institutionnelles installée en 1997. Les intentions déclarées de rationalisation et de modernisation ont en effet buté contre la réalité politique d’objectifs partisans inavoués qui n’ont fait qu’accroître l’illisibilité du projet. Les apports du comparatisme (principalement du droit constitutionnel de la Vème République française) ont été détournés par les « néo-constituants » qui, loin de rationaliser le système des sources, l’ont rendu plus obscur encore. Ce projet de réorganisation des sources, quoique laissé, pour l’heure, lettre morte, présente un réel danger non seulement au plan juridique mais aussi au point de vue politique."
"Ces « propositions » visent à réhabiliter l’instrument référendaire français, tombé en quasi désuétude depuis la déconnexion entre son emploi et la responsabilité du Président de la République, soit depuis la fin de la dramatisation gaullienne. A rebours de la révision de 2008 (cette étude est parue antérieurement mais critique par anticipation toute initiative « mixte » associant les parlementaires, telle que celle proposée par le rapport Vedel), on suggérait une initiative minoritaire « pure » émanant d’une fraction du corps électoral, en ce qu’elle permettrait que le processus de décision législative échappe ponctuellement au circuit institutionnel classique. Cette initiative doit par ailleurs faire l’objet d’un contrôle juridictionnel préventif, pour en éviter tout dévoiement, non exclusif d’un contrôle répressif, dont la juxtaposition permettrait paradoxalement de libérer l’initiative référendaire."
"Les droits d’auteur et les droits voisins défendent la création intellectuelle et garantissent une large propagation de la culture. De plus, ils jouent un rôle économique non négligeable. La question qui s’est posée était de déterminer comment éliminer les zones non protégées en Europe et éviter qu’il puisse s’en créer d’autres. L’Europe de l’audiovisuel ne doit pas en effet s’organiser au seul gré des opportunités juridiques. Le Conseil de l’Europe et la Communauté, prenant pour base le droit international existant, ont adopté des instruments juridiques spécifiques à la télévision, sans chercher une harmonisation globale et utopique des droits d’auteur et des droits voisins. Cet article cherche à identifier et analyser les avancées par rapport aux textes internationaux. La convention européenne et la directive communautaire ont pour objet de remédier aux imperfections de ces derniers et comblent des lacunes importantes. La création d’un espace audiovisuel commun en est facilitée : redéfinition de l’acte de radiodiffusion, détermination de la loi applicable et de l’origine de la transmission, harmonisation des conditions d’obtention des droits."
"On examine dans cette étude l’influence du droit communautaire sur le droit interne en matière audiovisuelle. Il s’agit donc d’analyser l’évolution du régime juridique français, compte tenu des dispositions communautaires adoptées depuis les années 90, période clé pour ce secteur. Deux thèmes ont retenu notre attention : les messages commerciaux et les quotas de diffusion. En effet, la question des conditions de coexistence entre l’information, le divertissement et le monde de l’argent est essentielle. Celle des quotas, qui imposent un certain protectionnisme et peuvent aider à créer une situation favorable au développement d’un secteur économiquement fragile, l’est également. Au fil de l’étude, il apparaît que le droit français est fréquemment précurseur et les actes adoptés par la Communauté n’ont fait que conforter et généraliser en Europe des règles ancrées en France : durée, forme, présentation, insertion de la publicité dans les programmes, parrainages interdits... Les dispositions communautaires visant à garantir au public un minimum d’œuvres d’origine européenne sont plus souples que celles, en vigueur en France, instaurant des quotas et destinées à protéger la diversité des programmes. Ainsi, la loi du 30 septembre 1986 prévoit la diffusion de 50% au moins d’œuvres communautaires pendant les « heures de grandes écoute ». Les chaînes nationales sont unanimes à dénoncer ces contraintes horaires d’autant plus pénalisantes qu’elles ne trouvent aucune correspondance dans les instruments communautaires. Par rapport à leurs concurrents européens, les diffuseurs français ont une marge de manœuvre réduite pour atteindre les quotas avec les tranches horaires de leur choix. De plus, la loi du 18 janvier 1992 a porté le quota à 60%, alors que la directive communautaire « Télévision sans frontières » ne mentionne qu’une « proportion majoritaire » d’œuvres européennes. L’analyse permet de comprendre pourquoi la France est considérée comme une exception en matière culturelle et plus spécifiquement dans le domaine de la politique audiovisuelle. Le droit communautaire n’a souvent fait que s’inspirer de principes déjà existant en droit français. Les dispositions communautaires ne sont d’ailleurs pas toujours aussi ambitieuses que celles en vigueur en France. Ainsi, contrairement au postulat de départ, force est de conclure que ce sont les règles françaises qui ont pu influencer le contenu des instruments communautaires."
"Cette publication propose une lecture des écrits kantiens relatifs au droit international public. L’actualité du projet de l’auteur de Königsberg, inséré dans l’exigence morale d’un « devoir-espérance » ou « impératif historique », met cependant en lumière les apories du « devoir-faire » propres, précisément, au droit international public. Ces impasses résultent des contradictions entre l’« impératif historique » et l’impératif catégorique pratique du « souverain bien ». Le Droit international ne saurait, en d’autres termes, qu’être projet."
"Cette étude, issue d’une communication présentée à la Cour des comptes italienne, visait à comparer le fonctionnement de la décentralisation de l’Etat unitaire français pour en marquer les spécificités en regard du système propre de l’Etat régional, à rebours de la vision franco-centrée et de l’influence, pourtant non négligeable, que l’hexagone put avoir sur la péninsule italienne."
janvier 2006 - février 2007
novembre 2007 - octobre 2008
"La résolution 757 du conseil de sécurité de l'O.N.U. du 30 mai 1992 plaçant la Yougoslavie (Serbie et Monténégro) sous ""Embargo sportif"" - Analyse des problèmes juridiques"
"L'arrivée au pouvoir de Vladimir Poutine marque un tournant politique pour la Russie. Ce pays, qui a connu une perte de puissance sur la scène internationale pendant la décennie 1990, affiche désormais une volonté de retrouver sa place historique de grande nation. Face à elle se trouve l'OTAN, créée jadis pour s'opposer à l'URSS. Après la guerre froide, cette organisation de défense s'est étendue dans l'ancien espace soviétique avec l’adhésion des États d'Europe centrale et orientale. Une telle situation, combinée à la stratégie russe de retrouver sa puissance, notamment dans son étranger proche, fait que les tensions reviennent. Toutefois, il semble erroné de considérer l'OTAN comme un bloc unique. En effet, en partant du fait que les décisions de l'Alliance sont prises à l'unanimité, il apparaît pertinent d'analyser la politique entre l'OTAN et la Russie à travers les relations bilatérales des membres de l’organisation. La combinaison de celles-ci sur la scène internationale amène soit un rapprochement basé sur des intérêts communs soit une confrontation reposant sur des divergences. Cette analyse permet de mettre en avant les dynamiques politiques, économiques et sécuritaires aboutissant à un équilibre des puissances à la fois européen et global."
Le texte constitutionnel dans sa dimension diachronique
"A travers l’étude de deux décisions de la Cour constitutionnelle italienne qui bouleversèrent, à l’orée des années 1990, le « proportionnalisme » italien, vécu à tort comme la Constitution matérielle de notre voisin transalpin, ce travail propose de questionner le rôle politique d’une juridiction constitutionnelle : peut-elle, en période de crise, assurer la survivance des institutions, dans le contexte dramatique d’une remise en cause fondamentale de tout l’appareil partidaire de la péninsule ? Ce fut en tous les cas le résultat des décisions que la Consulta rendit dans deux décisions de 1991 et 1993, qui rebattirent durablement les cartes du jeu politique italien, apaisant ainsi une crise qui faillit renverser avec elle Institutions et Constitution italiennes."
"Corte costituzionale, sent. n° 23/2011 du 25 janvier 2011 sur l'inconstitutionnalité partielle de la loi n° 51/2010"
"Soc. 20 janvier 2010, n° 08-42.207, à paraître au Bulletin"
"CEDH, 5e sect., 23 septembre 2010, Obst c. Allemagne (1re esp.), req. n° 425/03 et Schüth c. Allemagne (2e esp.), req. n° 1620/03"
"Conseil européen, Résolution du 28 novembre 2008 sur l'institution d'un réseau de coopération législative des ministères de la Justice de l'Union européenne"
"Compte tenu des innovations technologiques, certains États au sein de l'Union européenne ont élaboré des législations nationales relatives à la protection des personnes âgées face à l'utilisation de robots domestiques. D’autres États n'ont que des projets en la matière. Quoi qu'il en soit, les dispositions diffèrent selon les pays, ce qui créé des disparités relatives aux droits des personnes. Soulignons que les robots compagnons ou domestiques d’assistance aux séniors interviennent en faveur d’utilisateurs en état de faiblesse. Ils sont destinés à des individus n’étant pas en possession de toutes leurs facultés physiques ou intellectuelles. Il est important de veiller à ce qu’un cadre clair soit mis en place en Europe relatif à l’utilisation au profit des personnes âgées des techniques de l’information et de la communication. Le recours à des robots doit être envisagé dans le respect de la dignité des séniors, ceux-ci jouissant de prérogatives inaliénables. Par ailleurs, des questions se posent en ce qui concerne la responsabilité pour les dommages causés par les robots aux personnes âgées. Le Parlement européen, conscient de la portée de la révolution technologique en cours, s’est saisi de ces problématiques et a publié le 31 mai 2016 un « projet de rapport contenant des recommandations à la Commission européenne concernant des règles de droit civil sur le robotique » . Le 16 février 2017, ce document était adopté sous la forme d’une résolution . La Commission n’est pas contrainte de suivre les recommandations du Parlement mais elle doit expliquer ses raisons en cas de refus. Cette initiative ambitieuse traduit la volonté des institutions de l’Union européenne (UE) de voir émerger un droit des robots, harmonisé et basé sur des principes éthiques. Les dispositions constituent la première trame d’une réglementation en la matière."
"Les enfants constituent un groupe vulnérable de la société et à ce titre ils nécessitent une protection spécifique, en particulier en temps de conflits armés où leurs droits peuvent être violés, qu'ils appartiennent à la population civile ou qu’ils soient impliqués militairement dans des conflits. Cette protection relève d’une problématique récente et qui demeure plus que jamais actuelle. Elle pose la question de savoir quel est le but de la spécificité annoncée dans la mesure où existe déjà une protection générale des civils. Faut-il entendre alors que cette dernière est insuffisante à protéger les enfants dans les situations de guerre ? Également, la protection tant générale que spécifique appliquée aux enfants, varie-t-elle selon que le conflit armé est de type international ou non-international ? L’ensemble de ces questions fera l’objet de la première partie de la thèse, intitulée : « La protection des enfants civils en temps de conflits armés ». La seconde partie de la thèse portant sur « La protection des enfants soldats en temps de conflits armés » se penche sur les conséquences juridiques de la participation des enfants à des hostilités. Et, dans ce cadre, savoir si ces enfants capturés par l’ennemi obtiendront le statut de prisonnier de guerre et s’ils seront poursuivis pénalement en cas de commission de crimes de guerre. L’autre question soulevée dans cette partie est celle de la responsabilité de l’État, du groupe, de l’individu, qui recrute des enfants aux fins de les utiliser dans des conflits armés, en dépit de leur engagement à ne pas le faire. Le cas de la Libye apparaît ici le plus indiqué ; en effet, le pays a traité de ces questions dans sa législation qui présente cependant des failles que nous mettons en exergue, d’autant que dans cet État a éclaté en février 2011 une guerre où sont recrutés et utilisés des enfants."
"La législation irakienne définit le contrat comme étant l’union d'une offre faite par la partie contractante avec l'acceptation d'une autre partie et ce de manière à établir les effets dans l'objet du contrat. Ainsi, la place occupée par le contrat de vente en droit irakien est importante. Lorsque les parties relèvent d’ordres juridiques différents, leurs rapports sont régis par le droit international privé, qui détermine le tribunal apte à trancher le litige. Cette thèse de doctorat vise alors à vérifier la capacité à appliquer les règles de conflit de compétence internationale en droit irakien sur des contrats « virtuels » ou dématérialisés. Comme nous le verrons, dans ce domaine, « virtuel » ne veut pas pour autant dire que ce contrat n’est pas réel, comme le spécifie très clairement la loi irakienne. Il reste rattaché au territoire. Le problème est que le droit irakien, en ignorant les notions de frontière et de territorialité, ne reconnaît pas sa propre « immatérialité ». Cette réalité dans les textes et la pratique implique que les opérations qui se produisent sur Internet ne sont pas prises en compte par les règles de conflits de compétence internationale. C’est la raison pour laquelle nous avons souhaité vérifier et comprendre la capacité et l’effectivité des règles de conflit de compétence internationale dans le cadre de litiges sur Internet. Ce faisant, nous espérons mettre en lumière les règles les plus appropriées, qui correspondent le mieux à la nature du contrat virtuel, à savoir son immatérialité. Pour ce faire, nous entreverrons quelques développements sur les litiges de l’Internet. Ainsi nous disposons de deux domaines de recherche : un premier au niveau de la législation nationale, comme le droit français et le droit américain ; un deuxième au niveau des conventions internationales, comme les conventions des Nations-Unies en 2005, la convention de la Haye en 2005, la convention de Bruxelles en 1968 ou encore les règlements de Bruxelles 2000 et 2012."
"De la volonté des parties de s’accorder sur les éléments essentiels d’un contrat, naît tout un processus contractuel qui se traduit par la création d’obligations, éléments susceptibles d’être à l’origine d’un désaccord. Dès lors qu’un juge est saisi d’un litige, les opérations d’interprétation et de qualification du contrat litigieux auxquelles il se consacre se définissent par une double fonction. Dans un premier temps, l’interprétation du contenu du contrat permet au juge de repérer les éléments de fait qui ont été déterminants de la volonté des parties de contracter. Dans un second temps, une fois déterminés, ces éléments qui sont porteurs du sens du contrat, vont permettre au juge d’apporter, une solution au désaccord qui oppose les parties. Or, la solution ne trouvera son efficacité que si le juge applique aux éléments de fait qu’il a identifiés le droit approprié ; il faut pour cela qualifier le fait au sens où la qualification, consiste à déterminer la catégorie dans laquelle s’inscrit le contrat, afin de lui appliquer le régime juridique qui lui correspond. Elle est en cela le préalable à l’application d’une règle juridique. Opération intellectuelle, la qualification fait ainsi office de charnière entre les deux fonctions attachée à l’opération d’interprétation que sont l’interprétation des données de fait et la solution apportée par le juge sur le contenu contractuel litigieux."
"Depuis l'entrée en vigueur de loin° 94-475 du 10 janvier 1994, le législateur français s'est inscrit dans un processus de protection, au demeurant intéressé, du dirigeant caution dans l'optique de favoriser le redressement du débiteur en difficulté. Ce processus qui a atteint son point culminant lors de la réforme de 2005 a eu une influence sur le droit des procédures collectives applicables dans l'espace OHADA non sans heurter l'équilibre de l'institution du cautionnement dans sa globalité. Depuis la réforme de l'AUC du 10 septembre 2015, le droit OHADA adopte le même régime de traitement de la caution du débiteur en difficulté que le législateur français. Celui-ci consiste à favoriser le sort de la caution en instrumentalisant sa situation tant que l'espoir de sauver le débiteur en difficulté subsiste réellement. Cela se traduit notamment par une application ciblée de la règle de l'accessoire dans différentes étapes de la procédure selon un fil conducteur presque identiquement défini pat' chaque législateur, pourtant dans un environnement juridique et social différent. L'impact de ce paradoxe sur la protection efficiente de la caution se fait ressentir dans l'application des mesures de discipline collective à la caution d'une part, et l'exercice des recours de celle-ci d'autre part."
15 juillet 2009-15 mai 2010
Obligations réciproques ou obligations mutuelles ?
Aspects juridiques français et communautaire
15 mai 2010  15 novembre 2010
(15 novembre 2010  15 mai 2011)
"(Civ. 1re, 1er déc. 2010, n° 09-79.132, Rev. crit. DIP 2011. 102, note H. Gaudemet-Tallon ; Civ. 1re, 23 févr. 2011, n° 10-14.106)"
"(CJUE, 22 déc. 2010, n° C-497/10, D. 2011. 248 ; ibid. 1374, obs. F. Jault-Seseke)"
"(CJUE, 22 déc. 2010, n° C-491/10, D. 2011. 248 ; ibid. 1374, obs. F. Jault-Seseke)"
"L'évolution des textes a engendré une fragilisation de la propriété privée immobilière. Il convient de scinder cette recherche en deux parties. La première partie aborde l'intérêt privé et la seconde partie concerne l'intérêt général. Ainsi, il convient tout d'abord d'évoquer l'histoire de la propriété privée afin de comprendre la place réservée à ce droit après sa reconnaissance. Cette étude s'avère indispensable afin de saisir très précisément les raisons qui ont engendré une fragilisation. La multiplication des textes favorables aux personnes en difficultés a vu apparaître un nouvel intérêt qualifié d'intérêt social. Ce nouvel intérêt a entraîné un devoir de justice sociale du propriétaire. Parallèlement, la multiplication de l'utilisation du terme intérêt général a lui aussi engendré une fragilisation. Ainsi, l'étude du droit de l'urbanisme et du droit de l'environnement apparaît essentielle. Ces différentes notions tendent à réduire les pouvoirs du propriétaire. Cette maîtrise des sols est particulièrement étendue puisque le propriétaire participe à la préservation des paysages. Cette protection environnementale favorise le tourisme. Ce dernier domaine engendre aussi une fragilisation de la propriété privée immobilière."
"Les États ont depuis de nombreuses années incriminé la corruption des agents publics nationaux. La crise de l'énergie de 1973 et la fin de la guerre froide de 1989 ont néanmoins stimulé l'apparition d’une forme spécifique de corruption jusqu’alors passé sous silence par les textes juridiques : la corruption d’un agent public étranger. La lutte contre la corruption des agents publics étrangers a récemment connu un essor international normatif considérable notamment avec l’entrée en vigueur de plusieurs conventions internationales à visée régionale et universelle. La genèse de cette réaction internationale est avant tout d’inspiration étatsunienne. Promulgué aux États-Unis en 1977 dans le sillage de l’affaire du Watergate, le Foreign Corrupt Practices Act fut la première loi nationale incriminant la corruption d’un agent public étranger. Cette loi est progressivement devenue la matrice des textes internationaux ultérieurs. La lutte contre la corruption internationale doit faire face aux difficultés de mise en œuvre qui sont inhérentes à l’application des conventions internationales. Il est parfois estimé que ce dispositif juridique international n’est pas toujours d’une efficacité parfaite. Un premier niveau de recherche qui consiste à opérer une analyse des textes juridiques, conduira à étudier la dimension opératoire des dispositifs en vigueur. A la lumière des avis des praticiens contemporains de lutte contre la corruption, un second niveau d’analyse complétant le premier aura pour objectif final de formuler des propositions d’amélioration des mécanismes juridiques existants en droit international positif."
"Les droits de jouissance à temps partagé ont connu un essor exponentiel au sein d'un mécanisme juridique original. Leurs caractéristiques propres se rattachent à de multiples qualifications. La mouvance de leur nature juridique conduit inévitablement à une difficulté de conception et d'appréhension du phénomène. La situation du cocontractant démontre, dans cette hyptohèse, une précarité juridique de ses garanties. La compréhension des droits de jouissance à temps partagé, par une réflexion plus avancée de ceux-ci, permettrait une cohérence entre la nature juridique intrinsèque de ces droits et la qualification effectivement retenue. La difficulté de la démarche procède de l'extranéité du système de Common Law. Après la directive 94/47/CE "" concernant la protection des acquéreurs pour certains aspects des contrats portant sur l'acquisition d'un droit d'utilisation de biens immobiliers "", ayant évité de légiférer sur la nature juridique de ces droits, les Etats membres de l'Union européenne ont transposé cette norme et ont opéré des rattachements divers sans donner lieu à des prémices de consensus sur ce point. En ce sens, l'approche internationale est indispensable. L'expérience de certains Etats membres de l'Union européenne tend à légitimer le droit de propriété au sein d'un immeuble en jouissance à temps partagé. Il est également permis de s'interroger sur l'existence d'une acception différente du droit de propriété. La détermination d'une qualification juridique, en adéquation avec le mécanisme des droits de jouissance à temps partagé, permettrait, ainsi, une plus grande lisibilité du régime juridique applicable et plus largement, des règles applicables, lorsqu'un tel litige est porté devant les tribunaux. L'objet de cette étude est plus que jamais d'actualité. En effet, suite à la procédure de révision de la directive engagée par la Commission européenne depuis le mois de juillet 2006, une proposition de directive a été rendue publique au mois de juin 2007."
